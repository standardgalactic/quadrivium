J.H. Hubbard
B.H. West
Texts in 
Applied 
Mathematics 
5
Differential 
Equations: 
A Dynamical 
Systems 
Approach
Ordinary Differential 
Equations У
(Ци Springer

Texts in Applied Mathematics
Springer Science+Business Media, LLC
Editors
F. John {deceased) 
J.E. Marsden 
L. Sirovich 
M. Golubitsky 
W. Jager
Advisor 
G. looss

Texts in Applied Mathematics
1. 
Sirovich: Introduction to Applied Mathematics.
2. 
Wiggins: Introduction to Applied Nonlinear Dynamical Systems and Chaos.
3. 
HaleIКодак: Dynamics and Bifurcations.
4. 
Chorin!Marsden: A Mathematical Introduction to Fluid Mechanics, 3rd ed.
5. 
Hubbard/West: Differential Equations: A Dynamical Systems Approach: 
Ordinary Differential Equations.
6. 
Sontag: Mathematical Control Theory: Deterministic Finite Dimensional 
Systems.
7. 
Perko: Differential Equations and Dynamical Systems, 2nd ed.
8. 
Seaborn: Hypergeometric Functions and Their Applications.
9. 
Pipkin: A Course on Integral JEquations.
10. 
Hoppensteadt/P eskin: Mathematics in Medicine and the Life Sciences.
11. 
Braun: Differential Equations and Their Applications, 4th ed.
12. 
Stoer/Bulirsch: Introduction to Numerical Analysis, 2nd ed.
13. 
Renardy/Rogers: A First Graduate Course in Partial Differential Equations.
14. 
Banks: Growth and Diffusion Phenomena: Mathematical Frameworks and 
Applications.
15. 
Brenner!Scott: The Mathematical Theory of Finite Element Methods.
16. 
Van de Velde: Concurrent Scientific Computing.
17. 
Marsden!Ratiu: Introduction to Mechanics and Symmetry.
18. 
HubbardlWest: Differential Equations: A Dynamical Systems Approach: 
Higher-Dimensional Systems.
19. 
Kaplan!Glass'. Understanding Nonlinear Dynamics.
20. 
Holmes: Introduction to Perturbation Methods.
21. 
CurtainIZwart: An Introduction to Infinite-Dimensional Linear Systems 
Theory.
22. 
Thomas: Numerical Partial Differential Equations: Finite Difference 
Methods.
23. 
Taylor: Partial Differential Equations: Basic Theory.
24. 
Merkin: Introduction to the Theory of Stability.
25. 
Naber: Topology, Geometry, and Gauge Fields: Foundations.
26. 
PoldermanIWillems: Introduction to Mathematical Systems Theory:
A Behavioral Approach.
27. 
Reddy: Introductory Functional Analysis: with Applications to Boundary­
Value Problems and Finite Elements.

John H. Hubbard Beverly H. West
Differential Equations: A 
Dynamical Systems Approach
Ordinary Differential Equations
With 144 Illustrations
Springer

John H. Hubbard 
Beverly H. West 
Department of Mathematics 
Cornell University 
Ithaca, NY 14853 
USA
Series Editors
J.E. Marsden 
Department of
Mathematics
University of California 
Berkeley, CA 94720 
USA
M. Golubitsky 
Department of Mathematics 
University of Houston 
Houston, TX 77204-3476 
USA
L. Sirovich
Division of Applied
Mathematics
Brown University
Providence, RI 02912
USA
W. Jager
Department of Applied Mathematics
Universitat Heidelberg
Im Neuenheimer Feld 294 
69120 Heidelberg, Germany
Library of Congress Cataloging-in-Publication Data 
Hubbard, John.
Differential equations: a dynamical systems approach / John
Hubbard, Beverly West.
p. cm. — (Texts in applied mathematics : 5, 18)
Contents: pt. 1. Ordinary differential equations —pt. 2. Higher­
dimensional systems.
1. Differential equations. 2. Differential equations, Partial.
I. West, Beverly Henderson, 1939- . II. Title. III. Series.
QA371.H77 1990
515'.35- dc20 
90-9649
Printed on acid-free paper.
© 1991 Springer Science+Business Media New York
Originally published by Springer-Verlag New York, Inc. in 1991
Softcover reprint of the hardcover 1st edition 1991
All rights reserved. This work may not be translated or copied in whole or in part without the 
written permission of the publisher (Springer Science+Business Media, LLC), except for brief 
excerpts in connection with reviews or scholarly analysis.Use in connection with any form of 
information storage and retrieval, electronic adaptation,computer software, or by similar or 
dissimilar methodology now known or hereafter developed is forbidden, 
forbidden.
The use of general descriptive names, trade names, trademarks, etc., in this publication, even if 
the former are not especially identified, is not to be taken as a sign that such names, as understood 
by the Trade Marks and Merchandise Marks Act, may accordingly be used freely by anyone.
9 8 7 6 5 4 3 (Corrected third printing, 1997)
SPIN 10631374
ISBN 978-1-4612-6952-6 ISBN 978-1-4612-0937-9 (eBook)
DOI 10.1007/978-1-4612-0937-9

Series Preface
Mathematics is playing an ever more important role in the physical and 
biological sciences, provoking a blurring of boundaries between scientific 
disciplines and a resurgence of interest in the modern as well as the clas­
sical techniques of applied mathematics. This renewal of interest, both in 
research and teaching, has led to the establishment of the series: Texts in 
Applied Mathematics (TAM) .
The development of new courses is a natural consequence of a high 
level of excitement on the research frontier as newer techniques, such as 
numerical and symbolic computer systems, dynamical systems, and chaos, 
mix with and reinforce the traditional methods of applied mathematics. 
Thus, the purpose of this textbook series is to meet the current and future 
needs of these advances and encourage the teaching of new courses.
TAM will publish textbooks suitable for use in advanced undergraduate 
and beginning graduate courses, and will complement the Applied Mathe­
matical Sciences (AMS) series, which will focus on advanced textbooks and 
research level monographs.

Preface
Consider a first order differential equation of form x' = f(t, x). In elemen­
tary courses one frequently gets the impression that such equations can 
usually be “solved,” i.e., that explicit formulas for the solutions (in terms 
of powers, exponentials, trigonometric functions, and the like) can usually 
be found. Nothing could be further from the truth. In fact, only very ex­
ceptional equations can be explicitly integrated—those that appear in the 
exercise sections of classical textbooks. For instance, none of the follow­
ing rather innocent differential equations can be solved by the standard 
methods:
X — X t, 
x' = sin(ta), 
x' = etx.
This inability to explicitly solve a differential equation arises even earlier 
—in ordinary integration. Many functions do not have an antiderivative 
that can be written in elementary terms, for example:
/(t) = e-t (for normal probability distribution), 
f(t) = (t3 + I)1/2 (elliptic function), 
/(t) = (sint)/£ (Fresnel integral).
Of course, ordinary integration is the special case of the differential equa­
tion xf = f(t). The fact that we cannot easily integrate these functions, 
however, does not mean that the functions above do not have any an­
tiderivatives at all, or that these differential equations do not have solu­
tions.
A proper attitude is the following:
Differential equations define functions, and the object of the 
theory is to develop methods for understanding (describing and 
computing) these functions.
For instance, long before the exponential function was defined, the dif­
ferential equation xf = rx was “studied”: it is the equation for the interest 
x' on a sum of money x, continuously compounded at a rate r. We have 

viii
Preface
records of lending at interest going back to the Babylonians, and the for­
mula that they found for dealing with the problem is the numerical Euler 
approximation (that we shall introduce in Chapter 3) to the solution of the 
equation (with the step h becoming the compounding period).
Methods for studying a differential equation fall broadly into two classes: 
qualitative methods and numerical methods. In a typical problem, both 
would be used. Qualitative methods yield a general idea of how all the 
solutions behave, enabling one to single out interesting solutions for further 
study.
Before computer graphics became available in the 1980’s, we taught qual­
itative methods by handsketching direction fields from isoclines. The huge 
advantage now in exploiting the capabilities of personal computers is that 
we no longer need to consume huge amounts of time making graphs or 
tables by hand. The students can be exposed to ever so many more ex­
amples, easily, and interactive programs such as MacMath provide ample 
opportunity and inducement for experimentation any time by student (and 
instructor).
The origin of this book, and of the programs (which preceded it) was 
the comment made by a student in 1980: “This equation has no solutions.” 
The equation in question did indeed have solutions, as an immediate conse­
quence of the existence and uniqueness theorem, which the class had been 
studying the previous month. What the student meant was that there were 
no solutions that could be written in terms of elementary functions, which 
were the only ones he believed in. We decided at that time that it should be 
possible to use computers to show students the solutions to a differential 
equation and how they behave, by using computer graphics and numeri­
cal methods to produce pictures for qualitative study. This book and the 
accompanying programs are the result.
Generally speaking, numerical methods approximate as closely as one 
wishes a single solution for a particular initial condition. These methods in­
clude step-by-step methods (Euler and Runge-Kutta, for instance), power 
series methods, and perturbation methods (where the given equation is 
thought of as a small perturbation of some other equation that is better 
understood, and one then tries to understand how the solution of the known 
equation is affected by the perturbation).
Qualitative methods, on the other hand, involve graphing the field of 
slopes, which enables one to draw approximate solutions following the 
slopes, and to study these solutions all at once. These methods may much 
more quickly give a rough graph of the behavior of solutions, particularly 
the long term behavior as t approaches infinity (which in real-world math­
ematical modeling is usually the most important aspect of a solution). In 
addition, qualitative techniques have a surprising capacity for yielding spe­
cific numerical information, such as location of asymptotes and zeroes. Yet 
traditional texts have devoted little time to teaching and capitalizing on 
these techniques. We shall begin by showing how rough graphs of fields of 

Preface
ix
can be used to zero right in on solutions.
In order to accomplish this goal, we must introduce some new terminol­
ogy right at the beginning, in Chapter 1. The descriptive terms “fence,” 
“funnel,” and “antifunnel” serve to label simple phenomena that have ex­
ceedingly useful properties not exploited in traditional treatments of dif­
ferential equations. These simple new ideas provide a means of formalizing 
the observations made by any person familiar with differential equations, 
and they provide enormous payoff throughout this text. They give simple, 
direct, noniterative proofs of the important theorems: an example is the 
Sturm comparison and oscillation theorem, for which fences and funnels 
quickly lead to broad understanding of all of Sturm-Liouville theory. Ac­
tually, although the words like fences and funnels are new, the notions have 
long been found under the umbrella of differential inequalities. However, 
these notions traditionally appeared without any drawings, and were not 
mentioned in elementary texts.
Fences and funnels also yield hard quantitative results. For example, 
with the fences of Chapter 1 we can often prove that certain solutions to 
a given differential equation have vertical asymptotes, and then calculate, 
to as many decimal places as desired, the location of the asymptote for the 
solution with a particular initial condition. Later in Part III, we use fences 
forming an antifunnel to easily calculate, with considerable accuracy, the 
roots of Bessel functions. All of these fruits are readily obtained from the 
introduction of just these three well-chosen words.
We solve traditionally and explicitly few types of first order equations— 
linear, separable, and exact—in Chapter 2. These are by far the most useful 
classical methods, and they will provide all the explicit solutions we desire.
Chapter 4 contains another vital aspect to our approach that is not 
provided in popular differential equations texts: a Fundamental Inequality 
(expanding on the version given by Jean Dieudonne in Calcul Infinitesimal; 
see the References). This Fundamental Inequality gives, by a constructive 
proof, existence and uniqueness of solutions and provides error estimates. 
It solidly grounds the numerical methods introduced in Chapter 3, where 
a fresh and practical approach is given to error estimation.
Part I closes with Chapter 5 on iteration, usually considered as an en­
tirely different discipline from differential equations. However, as another 
type of dynamical system, the subject of iteration sheds direct light on 
how stepsize determines intervals of stability for approximate solutions to 
a differential equation, and to gain understanding (through Poincare map­
ping) of solutions to periodic differential equations, especially with respect 
to bifurcation behavior.
In subsequent volumes, Parts II, III and IV, as we add levels of com­
plexity, we provide simplicity and continuity by cycling the same concepts 
introduced in Part I. Part II begins with Chapter 6, where we extend 
x1 — f(t,x) to the multivariate vector version x' = f(t,x). This is also the 
form to which a higher order differential equation in a single variable can 

X
Preface
be reduced. Chapter 7 introduces linear differential equations of the form 
x' = Ax, where eigenvalues and eigenvectors accomplish transformation 
of the vector equation into a set of decoupled single variable first order 
equations. Chapters 8 and 9 deal with nonlinear differential equations and 
bifurcation behavior.
In Part III, Chapters 10 and 11 discuss applications to electrical circuits 
and mechanics respectively. Chapter 12 deals with linear differential equa­
tions with nonconstant coefficients, x' = A(t)x4-q(t), and includes Sturm- 
Liouville theory and the theory of ordinary singular points. Finally, Part 
III again fills out the dynamical systems picture, and closes with Chapter 
13 on iteration in two dimensions.
In Part IV, partial differential equations and Fourier series are introduced 
as an infinite-dimensional extension of the same eigenvalue and eigenvector 
concept that suffuses Part II. The remaining chapters of the text continue 
to apply the same few concepts to all the famous differential equations 
and to many applications, yielding over and over again hard quantitative 
results. For example, such a calculation instantly yields, to one part in a 
thousand, the location of the seventh zero of the Bessel function Jo; the 
argument is based simply on the original concepts of fence, funnel, and 
antifunnel.
Ithaca, New York 
February, 1997
John H. Hubbard 
Beverly H. West

Acknowledgments
We are deeply indebted to all the instructors, students, and editors who 
have taught or learned from this text and encouraged our approach.
We especially thank our colleagues who have patiently taught from the 
earlier text versions and continued to be enthusiastically involved: Bodil 
Branner, Anne Noonburg, Ben Wittner, Peter Papadopol, Graeme Bailey, 
Birgit Speh, and Robert Terrell. Additional vital and sutained support 
has been provided by Adrien Douady, John Martindale, and David Tall. 
The book Systemes Differentiels: Etude Graphique by Michele Artigue and 
Veronique Gautheron has inspired parts of this text.
The students who have been helpful with suggestions for this text are 
too numerous to mention individually, but the following contributed par­
ticularly valuable and sustained efforts beyond a single semester: Francois 
Beraud, Daniel Brown, Fred Brown, Frances Lee, Martha MacVeagh, and 
Thomas Palmer. Teaching assistants Mark Low, Jiaqi Luo, Ralph Oberste- 
Vorth, and Henry Vichier-Guerre have made even more serious contribu­
tions, as has programmer Michael Abato.
The enormous job of providing the final illustrations has been shared 
by the authors, and Jeesue Kim, Maria Korolov, Scott Mankowitz, Kat­
rina Thomas, and Thomas Yan (whose programming skill made possible 
the high quality computer output). Homer Smith of ArtMatrix made the 
intricate pictures of Mandelbrot and Julia sets for Chapter 5.
Anne Noonburg gets credit for the vast bulk of work in providing solu­
tions to selected exercises. However, the authors take complete responsibil­
ity for any imperfections that occur there or elsewhere in the text.
Others who have contributed considerably behind the scenes on the more 
mechanical aspects at key moments include Karen Denker, Mary Duclos, 
Fumi Hsu, Rosemary MacKay, Jane Staller, and Frederick Yang.
Evolving drafts have been used as class notes for seven years. Uncount­
able hours of copying and management have been cheerfully accomplished 
semester after semester by Joy Jones, Cheryl Lippincott, and Jackie White.
Finally, we are grateful to the editors and production staff at Springer- 
Verlag for their assistance, good ideas, and patience in dealing with a com­
plicated combination of text and programs.
John H. Hubbard
Beverly H. West

Ways to Use This Book
There are many different ways you might use this book. John Hubbard uses 
much of it (without too much of Chapter 5) in a junior-senior level course 
in applicable mathematics, followed by Part II: Higher Dimensional Dif­
ferential Equations, Part III: Higher Dimensional Differential Equations 
Continued, and Part IV: Partial Differential Equations. In each term dif­
ferent chapters are emphasized and others become optional.
Most instructors prefer smaller chunks. A good single-semester course 
could be made from Chapters 1 and 2, then a lighter treatment of Chapter 
3; Chapter 4 and most of Chapter 5 could be optional. Chapters 6, 7, and 8 
from Part II comprise the core of higher dimensional treatments. Chapter 
7 requires background in linear algebra (provided in the Appendix to Part 
II), but this is not difficult in the two- and three-dimensional cases. Chapter 
8 is important for showing that a very great deal can be done today with 
nonUnear differential equations.
This series of books has been written to take advantage of computer 
graphics. We’ve developed a software package for the Macintosh computer 
called MacMath (which includes Analyzer, DiffEq, Num Meths, Cascade, 
ID Periodic Equations') and refer to it throughout the text.
Although they are not absolutely essential, we urge the use of computers 
in working through this text. It need not be precisely with the MacMath 
programs. With IBM/DOS computers, readers can, for example, use Phaser 
by Huseyn Ko$ak or MultiMath by Jens Ole Bach. There are many other 
options. The chapter on numerical methods has been handled very success­
fully with a spreadsheet program like Excel.
Because so much of this material is a new approach for instructors as 
well as students, we include a set of solutions to selected exercises, as a 
guide to making the most of the text.

Contents of Part I
Ordinary Differential Equations 
The One-Dimensional Theory x' = f(t,x)
Series Preface for Texts in Applied Mathematics 
v
Preface 
vii
Acknowledgments 
xi
Ways to Use This Book 
xiii
Introduction 
1
Chapter 1 Qualitative Methods 
11
1.1 
Field of Slopes and Sketching of Solutions 
11
1.2 
Qualitative Description of Solutions 
19
1.3 
Fences 
23
1.4 
Funnels and Antifunnels 
30
1.5 
The Use of Fences, Funnels, and Antifunnels 
34
1.6 
Vertical Asymptotes 
42
Exe rcises 
49
Chapter 2 Analytic Methods 
67
2.1 
Separation of Variables 
68
2.2 
Linear Differential Equations of First Order 
71
2.3 Variation of Parameters 
76
2.4 Bank Accounts and Linear Differential Equations 
79
2.5 Population Models 
81
2.6 Exact Differential Equations 
85
2.7 Series Solutions of Differential Equations 
92
Exe rcises 
98

xvi 
Contents of Part I
Chapter 3 Numerical Methods 
111
3.1 Euler’s Method 
111
3.2 Better Numerical Methods 
118
3.3 Analysis of Error, According to Approximation Method 
125
3.4 Finite Accuracy 
133
3.5 What to Do in Practice 
143
Exercises 
149
Chapter 4 Fundamental Inequality, Existence, 
and Uniqueness 
157
4.1 Existence: Approximation and Error Estimate 
157
4.2 Uniqueness: The Leaking Bucket Versus Radioactive Decay 158
4.3 The Lipschitz Condition 
165
4.4 The Fundamental Inequality 
169
4.5 Existence and Uniqueness 
174
4.6 Bounds on Slope Error for Other Numerical Methods 
180
4.7 General Fence, Funnel, and Antifunnel Theorems 
183
Exercises 
189
Chapter 5 Iteration 
197
5.1 Iteration: Representation and Analysis 
199
5.2 The Logistic Model and Quadratic Polynomials 
214
5.3 Newton’s Method 
223
5.4 Numerical Methods as Iterative Systems 
235
5.5 Periodic Differential Equations 
245
5.6 Iterating in One Complex Dimension 
255
Exercises 
273
Appendix. Asymptotic Development 
297
Al. Equivalence and Order 
297
A2. Technicalities of Defining Asymptotic Expansions 
298
A3. First Examples; Taylor’s Theorem 
301
A4. Operations on Asymptotic Expansions 
304
A5. Rules of Asymptotic Development 
306
References 
307
Answers to Selected Problems 
309
Index
347

Contents of Part II
Systems of Ordinary Differential Equations: 
The Higher-Dimensional Theory xz = f(t, x)
Chapter 6. Systems of Differential Equations
Graphical representation; theorems; higher order equations; essential 
size; conservation laws; pendulum; two-body problem.
Chapter 7. Systems of Linear Equations, with Constant Coeffi­
cients 
x' — Ax
Linear differential equations in general; linearity and superposition prin­
ciples; linear differential equations with constant coefficients; eigenvec­
tors and decoupling, exponentiation of matrices; bifurcation diagram 
for 2 x 2 matrices, eigenvalues and global behavior; nonhomogeneous 
linear equations.
Chapter 8. Nonlinear Autonomous Systems in the Plane
Local and global behavior of a vector field in the plane; saddles, sources, 
and sinks; limit cycles.
Chapter 8*. Structural Stability
Structural stability of sinks and sources, saddles, and limit cycles; the 
Poincare-Bendixson Theorem; structural stability of a planar vector 
field.
Appendix. Linear Algebra
Ll. Theory of Linear Equations: In Practice
Vectors and matrices; row reduction.
L2. Theory of Linear Equations: Vocabulary
Vector space; linear combinations, linear independence and span; lin­
ear transformations and matrices, with respect to a basis; kernels and 
images.
L3. Vector Spaces with Inner Products
Real and complex inner products; basic theorems and definitions; or­
thogonal sets and bases; Gram-Schmidt algorithm; orthogonal projec­
tions and complements.

xviii
Contents of Part II
L4. Linear Transformations and Inner Products
Orthogonal, antisymmetric, and symmetric linear transformations; in­
ner products on Rn; quadratic forms.
L5. Determinants and Volumes
Definition, existence, and uniqueness of determinant function; theorems 
relating matrices and determinants; characteristic polynomial; relation 
between determinants and volumes.
L6. Eigenvalues and Eigenvectors
Eigenvalues, eigenvectors, and characteristic polynomial; change of bases; 
triangularization; eigenvalues and inner products; factoring the charac­
teristic polynomial.
L7. Finding Eigenvalues: The QR Method
The “power” method; QR method; flags; Hessenburg matrices.
L8. Finding Eigenvalues: Jacobi’s Method
Jacobi’s method: the 2x2 case, the n x n case; geometric significance 
in R3; relationship between eigenvalues and signatures.

Contents of Part III
Higher-Dimensional Equations continued, xz = f(t,x)
Chapter 10. Electrical Circuits
Circuits and graphs; circuit elements and equations; analysis of some 
circuits; analysis in frequency space; circuit synthesis.
Chapter 11. Conservative Mechanical Systems and Small Oscil­
lations 
x" = Ax
Small oscillations; kinetic energy; Hamiltonian mechanics; stable equi­
libria of mechanical systems; motion of a system in phase space; oscil­
lation systems with driving force.
Chapter 12. Linear Equations with Nonconstant Coefficients
Prufer transforms for second order equations; Euler’s differential equa­
tion; regular singular points; linearity in general (exclusion of feedback); 
fundamental solutions
Chapter 13. Iteration in Higher Dimensions
Iterating matrices; fixed and periodic points; Henon mappings; New­
ton’s method in several variables; numerical methods as iterative sys­
tems.

Contents of Part IV
Partial Differential Equations
As Linear Differential Equations in Infinitely Many 
Dimensions: Extension of Eigenvector Treatment 
e.g., x" = c2(d2x/ds2) = Xx
Chapter 14. Wave Equation; Fourier Series x” = c2(d2x/ds2) = Xx
Wave equation as extension of system of masses and spring; solutions 
of wave equation; Fourier series.
Chapter 15. Other Partial Differential Equations
Heat equation; Schroedinger’s equation.
Chapter 16. The Laplacian
Chapter 17. Vibrating Membranes; Bessel Functions
Vibrating membranes; the circular membrane; Bessel’s equation and its 
solutions; behavior of Bessel functions near zero and for large a.

Introduction
Differential equations are the main tool with which scientists make math­
ematical models of real systems. As such they have a central role in con­
necting the power of mathematics with a description of the world.
In this introduction we will give examples of such models and some of 
their consequences, highlighting the unfortunate fact that even if you can 
reduce the description of a real system to the mathematical study of a dif­
ferential equation, you may still encounter major roadblocks. Sometimes 
the mathematical difficulties involved in the study of the differential equa­
tion are immense.
Traditionally, the field of Differential Equations has been divided into 
the linear and the nonlinear theory. This is a bit like classifying people into 
friends and strangers. The friends (linear equations), although occasionally 
quirky, are essentially understandable. If you can reduce the description of 
a real system to a linear equation, you can expect success in analyzing it, 
even though it may be quite a lot of work.
The strangers (nonlinear equations) are quite a different problem. They 
are strange and mysterious, and there is no reliable technique for dealing 
with them. In some sense, each one is a world in itself, and the work of 
generations of mathematicians may give only very partial insight into its 
behavior. In a sense which is only beginning to be really understood, it is 
unreasonable to expect to understand most nonlinear differential equations 
completely.
One way to see this is to consider a computer; it is nothing but a sys­
tem of electrical circuits, and the time evolution of an electrical circuit is 
described by a differential equation. Every time a program is entered, it 
is like giving the system a set of initial conditions. The time evolution of 
a computer while it is running a program is a particular solution to that 
differential equation with those initial conditions. Of course, it would be 
an enormously complicated differential equation, with perhaps millions of 
voltages and currents as unknown functions of time. Still, understanding 
its evolution as a function of the initial conditions is like understanding 
all possible computer programs; surely an unreasonable task. In fact, it is 
known (a deep theorem in logic) that there is no algorithm for determin­
ing whether a given computer program terminates. As such the evolution 
of this differential equation is “unknowable,” and probably most differ­
ential equations are essentially just as complicated. Of course, this doesn’t 

2
Introduction
mean that there isn’t anything to say; after all, there is a discipline called 
computer science.
Consequently, most texts on differential equations concentrate on linear 
equations, even though the most interesting ones are nonlinear. There are, 
after all, far more strangers than friends. But it is mostly one’s friends that 
end up in a photograph album.
There is an old joke, which may not be a great joke but is a deep metaphor 
for mathematics:
A man walking at night finds another on his hands and knees, 
searching for something under a streetlight. “What are you 
looking for?”, the first man asks; “I lost a quarter,” the other 
replies. The first man gets down on his hands and knees to help, 
and after a long while asks “Are you sure you lost it here?”. 
“No,” replies the second man, “I lost it down the street. But 
this is where the light is.”
In keeping with this philosophy, this text, after Chapter 1, will also deal 
in large part with linear equations. The reader, however, should realize that 
the really puzzling equations have largely been omitted, because we do not 
know what to say about them. But, you will see in Chapter 1 that with 
the computer programs provided it is now possible to see solutions to any 
differential equation that you can enter in the form x' — 
ж), and so you
can begin to work with them.
For now we proceed to some examples.
1. The Friendly World
Example 0.1. Our model differential equation is
x = ax. 
(1)
You can also write this
dx/dt = ax(t),
and you should think of xft) as a function of time describing some quantity 
whose rate of change is proportional to itself. As such, the solutions of this 
differential equation (1) describe a large variety of systems, for instance
(a) The value of a bank account earning interest at a rate of a percent;
(b) The size of some unfettered population with birth rate a;
(c) The mass of some decaying radioactive substance with rate of change 
a per unit mass. (In this case a is negative.)

Introduction
3
As you probably recall from elementary calculus, and in any case as you 
can check, the function
s(t) = xoea(t~to) 
(2)
satisfies this differential equation, with value xq at to- Thus we say the 
equation has been solved, and almost any information you want can be read 
off from the solution (2). You can predict how much your bank account will 
be worth in 20 years, if the equation were describing compound interest. 
Or, if the equation were describing radioactive decay of carbon 14 in some 
ancient artifact, you may solve for the time when the concentration was 
equal to the concentration in the astmosphere. A
Example 0.1 gives an exaggeratedly simple idea of linear equations. Most 
differential equations are more complicated. The interest rate of a bank 
account, or the birth rate of a population, might vary over time, leading to 
an equation like
x = а(€)х. 
(3)
Other linear differential equations involve higher order derivatives, like
ax" + bxf + ex = 0, 
(4)
which describes the motion of harmonic oscillators (damped, if b ф 0), 
and also of RLC circuits. It has also been used to model the influence of 
government spending on the economy, and a host of other things.
The study of equation (4) is quite a bit more elaborate than the study of 
equation (1), but it still is essentially similar; we will go into it at length in 
Chapter 7. Again this is a success story; the model describes how to tune 
radios and how to build bridges, and is of constant use.
These differential equations (1), (3), and (4) are called linear differential 
equations, where each term is at worst the product of a derivative of x and 
a function of t, and the differential equation is a finite sum of such terms. 
That is, a linear differential equation is of the form
an(t)x^ + ап_1(£)ж(п-1) 4------ 1- a2(t)x” + oti(t)x' + &o(t)x = q(t\ (5)
where x^ means the nth derivative dnx/dtn.
The friendly world of linear differential equations is very accessible and 
has borne much fruit. In view of our next example, it may be interest­
ing to note that the very first mathematical texts we have are Babylonian 
cuneiform tablets from 3000 B.C. giving the value of deposits lent at com­
pound interest; these values are precisely those that we would compute 
by Euler’s method (to be described in Chapter 3) as approximations to 
solutions of xf = ax, the simplest linear equation (1).

4
Introduction
2. The Strange World
It is easy to pinpoint the birth of differential equations: they first appeared 
explicitly (although disguised in geometric language) in 1687 with Sir Isaac 
Newton’s book Philosophiae Naturalis Principia Mathematica (Mathemat­
ical Principles of Natural Philosophy). Newton is usually credited with 
inventing calculus, but we think that accomplishment, great as it is, pales 
by comparison with his discovering that we should focus on the forces to 
which a system responds in order to describe the laws of nature.
In Principia, Newton introduced the following two laws:
(a) A body subject to a force has an acceleration in the same direction as 
the force, which is proportional to the force and inversely proportional 
to the mass.
(F = ma).
(b) Two bodies attract with a force aligned along the line between them, 
which is proportional to the product of their masses and inversely 
proportional to the square of the distance separating them.
(The Universal Law of Gravitation)
and worked out some of their consequences.
These laws combine as follows to form a differential equation, an equation 
for the position of a body in terms of its derivatives:
Example 0.2. Newton’s Equation. Suppose we have a system of two 
bodies with masses and m2 which are free to move under the influence 
of the gravitational forces they exert on each other. At time t their positions 
can be denoted by vectors with respect to the origin, Xi (t) and Хг(^) (Figure 
0.1).
FIGURE 0.1. Representation of two bodies moving in gravitational force field.
The acceleration of the bodies’ motions are therefore the second deriva­
tives, x"(£) and Хз(^) respectively, of the positions with respect to time.

Introduction
5
Combining Newton’s Laws (a) and (b) gives the force on the first body as
= G
тп^гп?
11*2 “Xi ||2
x2 - Xi
J|x2 -Xlll’
this ratio gives the 
unit vector from the 
first body to the second
(6)
and the force on the second body as
772,2X2(0 = G
772-1772,2 
l|xi -x2||2
Xl -x2
J|xi -x2||~
this ratio gives the 
unit vector from the 
second body to the first
(7)
The gravitational constant G of proportionality is universal, i.e., inde­
pendent of the bodies, or their positions.
Equations (6) and (7) form a system of differential equations. To be sure, 
a system is more complicated than a single differential equation, but we 
shall work up to it gradually in Chapters 1-5, and we can still at this point 
discuss several aspects of the system.
Most notably, this system of differential equations is nonlinear; the equa­
tions cannot be written in the form of equation (5), because the denomi­
nators are also functions of the variables xi(t) and x2(t).
As soon as more than two bodies are involved, equations (6) and (7) are 
simply extended, as decribed by
mixiZ(0 = 
for г = 
(8)
l|Xj“Xj|P
to a larger nonlinear system which is today in no sense solved, even though 
it was historically the first differential equation ever considered as such. 
Regard as evidence the surprise of astronomers at finding the braided rings 
of Saturn. These braided rings are a solution of Newton’s equation which 
no one imagined until it was observed, and no one knows whether this sort 
of solution is common or exceptional; in fact no one even knows whether 
planets usually have rings or not.
Nevertheless, Newton’s equation (8) is of great practical importance; for 
instance, essentially nothing else is involved in the guidance of satellites and 
other space missions. Newton was able, in the case where there are precisely 
two bodies, to derive Kepler’s laws describing the orbit of each planet 
around the sun. This derivation can be found in Chapter 6 (Volume II). Its 
success really launched Newton’s approach and completely revolutionized 
scientists’ ways of thinking.
It is instructive to consider what Newton’s approach replaced: the Greek 
and Babylonian theory of epicycles, which received its final form in the 

6
Introduction
Almagest of Ptolemy (2nd century AD). The ancient astronomers had at­
tempted to describe the apparent motion of the planets on the heavenly 
sphere, and had found that epicycles, a kind of compound circular motion, 
were a good tool. Imagine a point moving on a circle, itself the center of a 
circle on which a point is turning, and so on some number of times. Each of 
these points is moving on a (more and more complex) epicycle, as indicated 
in Figure 0.2.
FIGURE 0.2. Epicycles.
It would be quite wrong to ridicule this earlier theory of epicycles; it 
provides quite good approximations to the observed motions (in fact as 
good as you wish, if you push the orders of the epicycles far enough). 
Moreover, if you consider that the sun moves around the earth very nearly 
on a circle (the right point of view to describe the apparent motions as seen 
from the earth), and then that the planets turn around the sun also very 
nearly on circles, epicycles seem a very natural description. Of course, there 
is the bothersome eccentricity of the ellipses, but in fact an ellipse close to 
a circle can be quite well approximated by adding one small epicycle, and 
if you require a better precision, you can always add another. Still, the 
epicycle theory is involved and complicated; but then again the motions of 
the planets really are complicated. In fact, there is no simple description 
of the motions of the planets. ▲
The main point of Newton’s work was the realization that
the forces are simpler than the motions.
There is no more forceful motivation for the theory of differential equations 
than this. Historically, Newton’s spectacular success in describing mechan­
ics by differential equations was a model for what science should be; in fact 
Newton’s approach became the standard against which all other scientific 
theories were measured. And the modus operand! laid out by Newton is 
still largely respected: all basic physical laws are stated as differential equa­
tions, whether it be Maxwell’s equations for electrodynamics, Schrodinger’s 
equation for quantum mechanics, or Einstein’s equation for general relativ­
ity.

Introduction
7
The philosophy of forces being simpler than motions is very general, 
and when you wish to make a model of almost any real system, you will 
want to describe the forces, and derive the motions from them rather than 
describing the motions directly. We will give an example from ecology, but 
it could as well come from economics or chemistry.
Example 0.3. Sharks and sardines. Suppose we have two species, one of 
which preys on the other. We will try to write down a system of equations 
which reflect the assumptions that
(a) The prey population is controlled only by the predators, and in the 
absence of predators would increase exponentially.
(b) The predator population is entirely dependent on the prey, and in its 
absence would decrease exponentially.
Again we shall put this in a more mathematical form. Let x(t) represent 
the size of the prey population as a function of time, and y(t) the size of 
the predator population. Then in the absence of interaction between the 
predators and the prey, the assumptions (a) and (b) could be coded by
x'(t) = ax(t)
with a and b positive constants. If the predators and prey interact in the 
expected way, meetings between them are favorable to the predators and 
deleterious to the prey. The product x(f)y(f) measures the number of meet­
ings of predator and prey (e.g., if the prey were twice as numerous, you 
could expect twice as many meetings; likewise if the predators were twice 
as numerous, you could also expect twice as many meetings). So with c and 
f also positive constants, the system with interaction can be written
x'(t) = ax(t) — cx(fyy(f) 
y'{i) = —by{f) + fx(fyy(t).
(9)
If you have specific values for a, b, c, and /, and if at any time to you 
can count the populations x(to) and у(to), then the equations (9) describe 
exactly how the populations are changing at that instant; you will know 
whether each population is rising or falling, and how steeply.
This model (much simpler to study than Newton’s equation, as you shall 
see in Chapter 6) was proposed by Vito Volterra in the mid-1920’s to 
explain why the percentage of sharks in the Mediterranean Sea increased 
when fishing was cut back during the first World War, and is analyzed in 
some detail in Section 6.3. The modeling process is also discussed there and 
in Section 2.5. 
▲
The equations (9) form another nonlinear system, because of the prod­
ucts x(t)y(t). It will be much easier to extract information about the solu­
tions than to find them explicitly.

8
Introduction
Basically, what all these examples describe is how a system will be pulled 
and pushed in terms of where it is, as opposed to stating explicitly the state 
of the system as a function of time, and that is what every differential 
equation does. To imagine yourself subject to a differential equation: start 
somewhere. There you are tugged in some direction, so you move that way. 
Of course, as you move, the tugging forces change, pulling you in a new 
direction; for your motion to solve the differential equation you must keep 
drifting with and responding to the ambient forces.
The paragraph above gives the idea of a solution of a differential equa­
tion; it is the path of motion under those ambient forces. Finding such 
solutions is an important service mathematicians can perform for other 
scientists. But there are major difficulties in the way. Almost exactly a 
century ago, the French mathematician Poincare showed that solving dif­
ferential equations in the elementary sense of finding formulas for integrals, 
or in the more elaborate sense of finding constants of motion, is sometimes 
impossible. The King of Sweden had offered a prize for “solving” the 3- 
body problem, but Poincare won the prize by showing that it could not be 
done. More recently, largely as a result of experimentation with computers, 
mathematicians have grown conscious that even simple forces can create 
motions that are extremely complex.
As a result, mathematicians studying differential equations have split in 
two groups. One class, the numerical analysts, tries to find good algorithms 
to approximate solutions of differential equations, usually using a computer. 
This is particularly useful in the “short” run, not too far from the starting 
point. The other class of mathematicians practice the qualitative theory, 
trying to describe “in qualitative terms” the evolution of solutions in the 
“long” run, as well as in the short run.
In this book, we have tried constantly to remember that explicit solutions 
are usually impossible, and that techniques which work without them are 
essential. We have tried to ally the quantitative and the qualitative theory, 
mainly by using computer graphics, which allow you to grasp the behavior 
of many solutions of a differential equation at once. That is, although the 
computer programs are purely quantitative methods, the graphics make 
possible qualitative study as well.
We advise the reader, without further waiting, to go to the computer and 
to run the program Planets. Many facts concerning differential equations 
and the difficulties in finding solutions are illustrated by this program, 
including some phenomena that are almost impossible to explain in text.
The Planets program does nothing but solve Newton’s equations of mo­
tion for whatever initial conditions you provide, for up to ten bodies. Try 
it first with the predefined initial conditions KEPLER, to see the elliptic or­
bits which we expect. There are within the program a few other predefined 
initial conditions which have been carefully chosen to provide systems with 
some degree of stability.

Introduction
9
But entering different initial data for 3 or more bodies is quite a different 
matter, and you will see that understanding the solutions of the differential 
equations for Newton’s Laws in these cases cannot be easy. You should try 
experimenting with different masses, positions, and velocities in order to see 
how unusual it is to get a stable system. In fact, the question of whether 
our own solar system is stable is still unanswered, despite the efforts of 
mathematicians like Poincare.
We shall not further analyze systems of differential equations until Chap­
ter 6; Volume II, where we will use the Planets program for computer ex­
ploration and discuss specific data. For now we shall move to Chapter 1 
and begin with what can be said mathematically about the simplest case, 
a single differential equation.

1
Qualitative Methods
A differential equation is an equation involving derivatives, and the order 
of a differential equation is the highest order of derivative that appears in 
it. We shall devote this first volume of our study of differential equations 
to the simplest, the first order equation
where f(t, x) is a continuous function of t and x. We shall consistently 
throughout this text use t as the independent variable and a; as a dependent 
variable, a scheme which easily generalizes to higher dimensional systems.
As you shall see in Volume II, higher order equations such as x" = 
f(t, x,x') can be expressed as a system of first order equations, so this 
beginning indeed underlies the whole subject.
A solution of a differential equation is a differentiable function that sat­
isfies the differential equation. That is, for xf = f(t,x),
и = u(t) is a solution if uf(t) = f(tyu(t)).
We shall behave as if differential equations have solutions and discuss them 
freely; in Chapter 4 we shall set this belief on a firm foundation.
At a point (t,u(tY) on a solution curve, the slope is f(t, u(t)). The core 
of the qualitative methods for studying differential equations is the “slope 
field” or “direction field.”
1.1 Field of Slopes and Sketching of Solutions
A direction field or slope field for a differential equation xf = f(t, x) is the 
direction or slope at every point of the t, ж-plane (or a portion of the plane). 
We shall demonstrate several ways to sketch the slope field by making a 
selection of points and marking each with a short line segment of slope 
calculated by /(t,x) for that point.
1. Grid method. For the differential equation xf = f(t,x) we first think 
(roughly) of a rectangular grid of points (t,x) over the entire t, ж-plane, 
and then determine (roughly) the slope of the solutions through each.
Example 1.1.1. Consider ж' = — tx.

12
1. Qualitative Methods
You can sketch the slope field for the solutions to this particular differ­
ential equation with a few simple observations, such as
(i) If t = 0 or if x = 0, then /(t, x) = 0. So the slope of a solution through 
any point on either axis is zero, and the direction lines (for solutions 
of the differential equation) along both axes are all horizontal.
(ii) The direction lines (and therefore the solution curves) are symmetric 
about the origin and about both axes, because the function /(t, x) 
is antisymmetric about both axes (that is, f changes sign as either 
variable changes sign). Therefore, considering the first quadrant in 
detail gives all the information for the others.
(iii) For fixed positive t, the slopes (negative) get steeper as positive x 
increases.
(iv) For fixed positive ж, the slopes (negative) get steeper as positive t 
increases.
The resulting field of slopes is shown in Figure 1.1.1, with one solution 
to the differential equation drawn, following the slopes. ▲
FIGURE 1.1.1. x' = —tx. Hand sketch of slope field.

1.1. Field of Slopes and Sketching of Solutions
13
Every solution to the differential equation must follow the direction field, 
running tangent to every little slope mark that it grazes. (Drawing solutions 
by hand may take some practice, as you will try in the exercises; a computer 
does it very well.)
Any point in the plane corresponds to an initial condition (to, xq) through 
which you can draw a solution passing through xq at time to­
We shall show in Chapter 2 that the particular equation x' = —tx of 
Example 1.1.1 can be solved analytically by separation of variables, yielding 
solutions of the form x = Хое~г /2. The solutions that can be drawn in 
the slope field (Figures 1.1.1, 1.1.3, and 1.1.4) are indeed the graphs of 
equations of this form, with a different solution for each value of Xq- We 
call this a family of solutions.
In Chapter 4 we shall discuss requirements for existence and uniqueness 
of solutions. For now we shall simply say that graphically the interpreta­
tions of these concepts are as follows:
Existence of solutions means that you can draw and see them on the 
direction field.
Uniqueness means that only one solution can be drawn through any 
given point or set of initial conditions. Uniqueness holds for the 
vast majority of points in our examples. An important implication 
of uniqueness is that solutions will not meet or cross.
In Example 1.1.1 the family consists entirely of unique solutions; all 
except x = 0 approach the £-axis asymptotically, never actually meeting or 
crossing.
isocline for 
isocline for
slope = 1 
slope = 0
solutions to 
differential 
equation x' =f(t,x), 
following slopes = f (t,x)
FIGURE 1.1.2. Isoclines.

14
1. Qualitative Methods
2. Isocline method. Another way (often faster for hand calculation, and 
more quickly enlightening) to construct a slope field for a differential equa­
tion is to find isoclines, which are curves on which the solutions to the 
differential equation have given slope. Set the slope /(t, x) = c; usually for 
each c this equation describes a curve, the isocline, which you might draw 
in a different color to avoid confusing with solutions. Through any point 
on the isocline, a solution to the differential equation crosses the isocline 
with slope c, as shown in Figure 1.1.2. Note that the isocline is simply a 
locus of “equal inclination”; it is not (with rare exceptions) a solution to 
the differential equation.
Example 1.1.2. Consider again the equation of Example 1.1.1, x' = —tx.
The isoclines are found by setting — tx = c, so they are in fact the 
coordinate axes (for c = 0) and a family of hyperbolas (for c / 0). Along 
each hyperbola we draw direction lines of the appropriate slope, as in Figure 
1.1.3. 
▲
FIGURE 1.1.3. x' = — tx. Slope marks on isoclines.

1.1. Field of Slopes and Sketching of Solutions
15
The important thing to realize is that Figure 1.1.3 represents the same 
direction field as Figure 1.1.1, but we have chosen a different selection of 
points at which to mark the slopes.
3. Computer calculation. A slope field can also be illustrated by com­
puter calculation of f^x) for each (t,x) pair in a predetermined grid. 
Figure 1.1.4 shows the result of such a calculation, for the same equation 
as Examples 1.1.1 and 1.1.2, with many solutions drawn on the slope field.
The computer program DiffEq will draw the slope field for any equation 
of the form x' = ffax) that you enter. Then by positioning the cursor at 
any point in the window, you are determining an initial condition, from 
which the computer will draw a solution. (The computer actually draws an 
approximate solution, to be discussed at length in Chapter 3.)
Different programs on different computers may represent the slope field 
in different ways. The Macintosh version of DiffEq makes the familiar little 
slope marks as shown in Figure 1.1.4. The IBM program for DiffEq uses 
color instead of slope marks to code for direction. Different colors mark 
regions of different slope ranges; consequently the boundaries between the

16
1. Qualitative Methods
colors represent isoclines. Figure 1.1.5 gives a sample for the same differ­
ential equation as Figure 1.1.4.
FIGURE 1.1.5. x’ = —tx. Color-coded slopes.
This color-coding method takes a little getting used to, but that comes 
rather quickly; it has some real advantages when dealing with systems of 
differential equations, which we begin to discuss in Chapter 6.
In summary, a direction field is usually drawn either over a grid or by 
isoclines. We shall tend to use the former with the computer and the latter 
for hand drawings, but as you shall see in later examples, you will need 
both. To analyze a direction field you will often need to sketch by hand at 
least a part of it.
At this point, some different examples are in order.
Example 1.1.3. Consider x' = 2t — x.
First we can find the isoclines by setting 2t — x = c. Hence the isoclines 
are straight lines of the form x = 2t — c, and the slope field looks like the 
top half of Figure 1.1.6. Solutions can be drawn in this field, as shown in 
the bottom half of Figure 1.1.6.

1.1. Field of Slopes and Sketching of Solutions
17
FIGURE 1.1.6. x' = 2t — x.

18
1. Qualitative Methods
II 
II 
II 
II 
II
FIGURE 1.1.7. x' = x2 -1.

1.2. Qualitative Description of Solutions
19
The solutions are of the algebraic form x = ke~t+2t—2. You can confirm 
that these are solutions by substituting this function into the differential 
equation. You can also see that x = 2t — 2 is an asymptote for all the 
solutions where к 0. Furthermore, this line itself is a solution (for к = 0), 
one of those rare cases where an isocline is also a solution. ▲
Example 1.1.4. Consider x' = x1 2 — t.
1. Funnels and antifunnels. Usually the main question to ask is “what 
happens as t —> oo?” In the best circumstances, solutions will come in 
classes, all tending to infinity in the same way, perhaps asymptotically to
a curve which can be explicitly given by an equation. This happens in
The isoclines are of the form x2 — t = c, so they are all parabolas. The 
direction field is shown at the top of Figure 1.1.7, and solutions are drawn 
upon it at the bottom. Some solutions fly up, some fall down; exactly one 
exceptional solution does neither and separates the other two behaviors. 
Details of these phenomena will be explored later in Section 1.5. 
▲
The differential equation of Example 1.1.4 and Figure 1.1.7 is of particu­
lar interest because although it looks utterly simple, there are no formulas 
in terms of elementary functions, or even in terms of integrals of elementary 
functions, for the solutions. A proof of this surprising fact is certainly not 
easy, and involves a branch of higher mathematics called Galois theory; 
we do not provide further details at this level. But what it means is that 
solving the equation xf = x2 — t cannot be reduced to computing integrals. 
As you can clearly see, this does not mean that the solutions themselves 
do not exist, only that formulas do not exist.
An important strength of the qualitative method of sketching 
solutions on direction fields is that it lets us see these solutions 
for which there are no formulas, and therefore let us examine 
their behavior.
We shall demonstrate in the remainder of this chapter how to use rough 
graphs of fields of slopes to find more specific information about the so­
lutions. We shall first present definitions and theory, then we shall give 
examples showing the power of these extended techniques.
1.2 Qualitative Description of Solutions
We can now draw slope fields and solutions upon them, as in Figures 1.2.1. 
We need a language with which to discuss these pictures. How can we 
describe them, classify them, distinguish among them?

20
1. Qualitative Methods
the lower halves of each picture in Figure 1.2.1. The solutions that come 
together behave as if they were in a funnel, as shown in Figure 1.2.2.
c. x' = x2 — t2
FIGURE 1.2.1.
Classes of solutions which behave in the same way are often separated by 
solutions with exceptional behavior. The solution x(f) = 0 to the equation 
x' = 2x — x2 (see Figure 1.2.3) is such an exceptional solution, separating 
the solutions which tend to 2 as t —> oc from those which tend to — oc. Such 
solutions often lie in antifunnels, parts of the plane in which solutions fly 

1.2. Qualitative Description of Solutions
21
away from exceptional solutions. An antifunnel is just a backwards funnel, 
but to avoid confusion we adopt the convention of always thinking from left 
to right along the £-axis. Since in many practical applications t represents 
time, this means thinking of going forward in time. Figure 1.2.3 shows a 
funnel on the upper part, an antifunnel on the lower part. Other good 
antifunnels occur in Figure 1.2.1 (the upper half of each picture).
FIGURE 1.2.3. Funnel (in upper part); Antifunnel (in lower part).

22
1. Qualitative Methods
Funnels and antifunnels are related to the stability of individual solu­
tions. If the initial condition (the starting point for drawing a solution) is 
slightly perturbed in the ^-direction, what happens? If a solution is sta­
ble, the perturbed solution is very similar to the original solution, as in a 
funnel. If a solution is unstable, perturbed solutions may fly off in different 
directions, as in an antifunnel.
2. Vertical asymptotes. The question “what happens as t —> oo?” is not 
always the right question to ask about a solution. For one thing, solutions 
may not always be defined for all large t. They may “blow up” in finite 
time, i.e., have a vertical asymptote.
Example 1.2.1. Consider the differential equation x* = x2, shown in Fig­
ure 1.2.1a. It is easy to show that the “functions” x = 1/(0 — t) are solu­
tions. But this “function” isn’t defined at t = O; it tends to oo at t = C, 
and it makes no obvious sense to speak of a solution “going through oo.” 
Thus one should think of the formula as representing two functions, one 
defined for — oo < t < C, and the other for t > C. The first of these is not 
defined for all time, but has a vertical asymptote at t = С. к
We will see many examples of this behavior, and will learn how to locate 
such vertical asymptotes in Section 1.6. All the equations represented in 
Figure 1.2.1 have some solutions admitting vertical asymptotes.
3. Undefined differential equations. Another possibility why it may be 
inappropriate to ask “what happens as t —> oo?” is that a solution may 
land somewhere where the differential equation is not defined. Anytime 
a differential equation xf = f(t,x) has f given by a fraction where the 
denominator sometimes vanishes, you can expect this sort of thing to occur.
Example 1.2.2. Consider the differential equation xf = —t/x.
For every R > 0, the two functions x(fi) = ±д/й2 — t2 are solutions, 
defined for — R < t < R, and representing an upper or lower half circle 
depending on the sign. These solutions just end on the t-axis, where x = 0 
and the equation is not defined. See Figure 1.2.4. A

1.3. Fences
23
FIGURE 1.2.4. x' — —tlx.
Students often think that worrying about domains of definition of func­
tions is just mathematicians splitting hairs; this is not so. Consider the 
equations of motion under gravity (equation (8) of the Introduction). The 
right-hand side is a fraction, and the equations are not defined if x* — 
; of
course, this corresponds to collision. More generally, the differential equa­
tion describing a system will usually be undefined when the system under­
goes some sort of catastrophe.
In order to describe quite precisely these various pictorial behaviors, we 
begin in the next section to formalize these notions.
1.3 Fences
Consider the standard first-order differential equation, 
x' = f(t,x).
On some interval I (an open or closed interval from to to ti, where 
ti might be oo, to might be — сю), we shall formally define funnels and 
antifunnels in terms of fences.
For a given differential equation xf = /(t, x) with solutions x = u(f).

24
1. Qualitative Methods
a “fence” is some other function x = a(t) that channels the 
solutions u(f) in the direction of the slope field.
Definition 1.3.1 (Lower fence). For the differential equation x' = f(t, ж), 
we call a continuous and continuously differentiable function a(t) a lower 
fence if a'(t) < /(t,a(t)) for all t € I.
In other words, for a lower fence o(t), the slopes of the solutions to the 
differential equation cannot be less than the slope of a(t) at the points of 
intersection between o(t) and the solutions, as shown in Figure 1.3.1. The 
fences are dotted, and the slope marks are for solutions to the differential 
equation xf = f(t,x).
FIGURE 1.3.1. Lower fences ot (t) < f(t, «(£)).
3(t)
FIGURE 1.3.2. Upper fences /(*,£(*)) < fi'(t).
P(t)—-

1.3. Fences
25
Definition 1.3.2 (Upper fence). For the differential equation x’ ~ 
f(t,x\ we call a continuous and continuously differentiable function /3(t) 
an upper fence if f(t,/3(ty) < ^(t) for all t € I.
An intuitive idea is that a lower fence pushes solutions up, an upper fence 
pushes solutions down.
Example 1.3.3. Consider xf = x2 — t as in Example 1.1.4. For the entire 
direction field for this differential equation, refer to Figures 1.1.7 and 1.2.1b; 
in our examples (Figure 1.3.3a,b,c) we shall just draw the relevant pieces.
FIGURE 1.3.3a. The line x = —0.8 is a lower fence to the left of the point P on 
the isocline of zero slope and an upper fence to the right.

26
1. Qualitative Methods
FIGURE 1.3.3b. The isocline x2 — t = 0 is an upper fence above the t-axis and 
a lower fence below.
FIGURE 1.3.3c. The isocline x2 — t = 1 is comprised of three fences, divided at 
t = — 1 (where the isocline has infinite slope) and at t = — | (where the isocline 
has slope = 1).

1.3. Fences
27
A fence can be strong, when the inequalities are strict:
a'(t) < f(t,a(t)) or 
< /?'(*),
or weak, when for some values of t equality is realized, so that the best 
statements you can make are
a'(t)< f(t,a(t)) or 
</?'(<)•
Fences can also be porous or nonporous, depending on whether or not 
solutions can sneak through them:
Definition 1.3.4. For a differential equation x' = f(t,x) with solution 
x = u(i),
a lower fence is nonporous if whenever a(t) <u(t), then a(f) < 
u(t) for all t > to in I where u(t) is defined;
an upper fence (3(t) is nonporous if whenever u(t) < /3(t), then u(t) < 
(3(t) for all t > to in I where u(t) is defined.
The reason for the fence terminology is the following result of the above 
definitions: if the fence is nonporous, a solution that gets to the far side of a 
fence will stay on that side. That is, a solution that gets above a nonporous 
lower fence will stay above it, and a solution that gets below a nonporous 
upper fence will stay below. A nonporous fence is like a semipermeable 
membrane in chemistry or biology: a solution to the differential equation 
may cross it in only one above-below direction from left to right, never in 
the opposite direction.
Under reasonable circumstances, all fences will turn out to be nonporous. 
This statement is rather hard to prove, but we shall do so in Chapter 4, 
where exceptions will also be discussed. Meanwhile, with a strong fence we 
can get almost for free the following theorem:
Theorem 1.3.5 (Fence Theorem for strong fences). A strong fence 
for the differential equation xf = f(t,x) is nonporous.
Proof. We shall prove the theorem for a lower fence.
The hypothesis a(to) < u(to) means that at to, the solution u(t) of the 
differential equation is at or above the fence a(t). The fact that a(t) is a 
strong lower fence means that a'(t) < f(t,a(t)).
The conclusion, a(t) < u(t) for all t > to, means that u(t) stays above 
a(t).
i) Suppose first that a(to) < ^(*o). Then suppose the opposite of the 
conclusion, that for some t > to, a(t) > u(t). Let t± be the first t > to such 
that a(t) = u(t), as in Figure 1.3.4. At t15
u'(ti) = /(*1,^1)) = 
> a'(ti),

28
1. Qualitative Methods
by definition of lower fence. If u'(Zi) > then u(t) — a(t) is increasing 
at ti. However this contradicts the fact that u(t) — a(t) is positive to the 
left of £i but supposed to be zero at й.
ii) If a(to) = u(to) and
w'(*o) = f(io,u(to)) = /(io,a(to)) > a'(t0),
then u(t) — a(t) is increasing at to. Therefore the solution first moves above 
the fence for t > to, and after that the first case will apply.
The case for an upper fence is proved analogously. □
FIGURE 1.3.5.

1.3. Fences
29
Note how part i) of the proof of Theorem 1.3.5 requires a strong fence. 
We would not have reached the contradiction if a'(ti) = u'(ti). Figure 1.3.5 
shows how a solution might “sneak through” a weak fence. In Chapter 4 
we shall show how to close this hole (in the fence and in the argument!).
As you shall soon see, we shall often need to construct fences out of pieces, 
so a more inclusive definition requires a(t) and /3(t) to be continuous and 
piecewise differentiable.
A piecewise differentiable function is composed of pieces on 
which the function is continuous and continuously differentiable 
except at finitely many points, and at those points left-hand and 
right-hand derivatives exist though they differ; examples are 
shown in Figure 1.3.6. (See Exercises 1.2-1.4^6 for discussion 
of continuous functions that are not piecewise differentiable.)
FIGURE 1.3.6. Piecewise differentiable functions.
If a(t) or /3(t) are piecewise differentiable, the fence defi­
nitions also require that the inequalities hold for both the left­
hand and right-hand derivatives at the points joining the pieces.
You may well ask what happens to the proof of Theorem 
1.3.5 at discontinuities in the derivative of either fence, but the 
answer is that our argument is a perfectly good one-sided one 
in each case, left-hand for i) and right-hand for ii), so angles in 
the graph of the fence will not matter.
Example 1.3.6. Consider again x' = x2 — t.
A lower fence across the whole plane can be constructed from the half­
line x = — 1 and the lower portion of the parabola shown in part b) of 
Example 1.3.3. This is done in Figure 1.3.7. 
▲

30
1. Qualitative Methods
FIGURE 1.3.7.
But what makes a fence “interesting”? What is the qualitative use of 
fences? In Section 1.4 we shall discuss why we want fences, so you will 
know where to look for them. Then we shall return in Section 1.5 to details 
of how in practice you find fences.
1.4 Funnels and Antifunnels
Fences are especially valuable when there is one of each kind, close together 
(as often happens with isoclines for a slope field), stretching from some to to 
infinity. These determine funnels and antifunnels, and the formal definitions 
provide some extremely useful theorems.
We will want nonporous fences. Recall that the Fence Theorem 1.3.5 
assures us that strong fences are nonporous; weak fences may not be. We 
shall see in Chapter 4 that an additional condition will assure nonporosity 
for a weak fence. For now we shall not worry about it further; you can rest 
assured that all the weak fences in the examples and exercises of Chapter 
1 will turn out to be nonporous. We shall proceed to show what nonporous 
fences can do.
Definition 1.4.1 (Funnel). If for the differential equation x' = f(t,x), 
over some t-inverval I, a(t) is a nonporous lower fence and (3(t) a nonporous 

1.4. Funnels and Antifunnels
31
upper fence, and if a(t) < /?(t), then the set of points (t,#) for t e I with 
a(t) < x < /3(t) is called a funnel.
FIGURE 1.4.1. Funnel.
Once a solution enters a funnel, it stays there. This is the message of 
Theorem 1.4.2, which is especially significant if the funnel narrows as t —> 
00.
Theorem 1.4.2 (Funnel Theorem). Ifa(t) and /3(t) determine a funnel 
for t € I, and if u(t) is a solution to xf = f(t,x) with u(t*) in the funnel 
for some t* E I, then u(t) is in the funnel for all t > t* in I for which u(t) 
zs defined.
Proof. The theorem is an immediate consequence of Definition 1.4.1 and 
the proof of the Fence Theorem 1.3.5, since a nonporous upper fence pre­
vents the solution from escaping from the top of the funnel, and a nonporous 
lower fence prevents escape at the bottom. □
Definition 1.4.3 (Antifunnel). If for the differential equation xf = f(t, x), 
over some t-interval I, a(t) is a nonporous lower fence and (3(t) a nonporous 
upper fence, and if o(t) > /?(t), then the set of points (t,x) for t E I with 
a(t) > x > /?(£) is called an antifunnel.
Solutions are, in general, leaving an antifunnel. But at least one solution 
is trapped inside the antifunnel, as is guaranteed by the following theorem:
Theorem 1.4.4 (Antifunnel Theorem: Existence). If a(t) and fl(t) 
determine an antifunnel for t E I, then there exists a solution u(t) to the 
differential equation xf = f(t,x) with /?(t) < u(t) < a(t) for all tel.

32
1. Qualitative Methods
FIGURE 1.4.2. Antifunnel.
To prove Theorem 1.4.4, the existence of at least one solution in an 
antifunnel, we must wait until Chapter 4. For certain antifunnels, how­
ever, there is another part of the Antifunnel Theorem, uniqueness of an 
exceptional solution that stays in the antifunnel, which is perhaps more 
surprising and which we can prove in a moment as Theorem 1.4.5.
In order to discuss uniqueness of a solution in an antifunnel, we need first 
to examine the quantity df /дх which measures “dispersion” of solutions 
to the differential equation x1 = f(t,x), for which f gives the slope.
The dispersion df /дх, if it exists, measures how fast solutions of the 
differential equation x' = f(t, x) “pull apart,” or how quickly the slope f is 
changing in a vertical direction in the t, ж-plane, as shown in Figure 1.4.3. 
(Recall from multivariable calculus that
df __ H Ж^ + ^-Ж^) 
дх ~ h “o 
h
so for a fixed t, df /дх measures the rate of change with respect to x, that 
is, in a vertical direction in the t, ж-plane.)
If the dispersion df /дх is large and positive, solutions tend 
to fly apart in positive time: If df /дх is large and negative, 
solutions tend to fly apart in negative time: if df/дх is close to 
zero, solutions tend to stay together.
Thus dispersion df /дх measures the stability of the solutions. If solutions 
are pulling apart slowly, then an error in initial conditions is less crucial.
We also have the following result: The distance between two solutions 
x = u\(i) and ж = U2(t) is nondecreasing in a region where df/дх > 0, 
and nonincreasing in a region where df /дх < 0, as we shall show in the 
next proof.

1.4. Funnels and Antifunnels
33
Theorem 1.4.5 (Antifunnel Theorem: Uniqueness). If for the dif­
ferential equation x' = f(t,x), the functions a(t) and /3(t) determine an 
antifunnel for t el, and if the antifunnel is narrowing, with
Um |a(i)-/3(i)| = 0, 
t—
and if df /дх > 0 in the antifunnel, then there exists one and only one 
solution u(t) that stays in the antifunnel.
Proof (of the “only one” part: the existence was proved in Theorem 1.4.4). 
The assumption df /дх > 0 implies that the solutions cannot come together 
as t increases. Indeed, let and be two solutions in the antifunnel, with 
-ui(t) > U2(t). Then
(«i - 
=/(i,ui(t)) -/(t,u2(i)) == / 
(t,u)du>0
Ju2(t) dx
so that the distance between them can never decrease. This is incompatible 
with staying between the graphs of a and (3 which are squeezing together, 
so there cannot be more than one solution that stays in the antifunnel. 
□
Antifunnels are sometimes more important than funnels. Although anti­
funnels correspond to instability, where small changes in initial conditions 
lead to drastic changes in solutions, you may be able to use them to get 
some very specific information:
For example, if the functions a(t) and /3(t) determine a narrowing anti­
funnel such that
Um |a(t) — /3(/)| = 0, 
t—>oo

34
1. Qualitative Methods
you may be able to zero in on an exceptional solution that divides the 
different behaviors, as in Figure 1.4.4.
FIGURE 1.4.4. Exceptional solution in narrowing antifunnel.
The importance of fences, funnels, and antifunnels lies in the 
fact that it is not necessary to solve the differential equation 
to recognize them and use them to give vital information about 
solutions.
We shall now in Section 1.5 go on to examples of their use.
1.5 The Use of Fences, Funnels, and Antifunnels
There are many ways to find fences. Almost any curve you can draw is a 
fence (or a piecewise combination of upper and lower fences), as shown in 
Examples 1.3.3 and 1.3.6. Some isoclines, or parts thereof, make obvious 
fences; some fences come from bounds on f(t, ж); other fences, and these are 
the most important, come from solutions to simpler but similar differential 
equations.
In the following examples we shall illustrate the use of each of these 
methods in finding fences to construct funnels and antifunnels, and we 
shall show the sort of quantitative information that can result.
1. Isoclines. One of the easiest ways to obtain useful fences is to examine 
the isoclines for a given differential equation.
Example 1.5.1. Consider xf = x2 — t, as in Examples 1.1.4, 1.3.3, 1.3.6 
and Exercises 1.2-1.4#1,2.
The isocline x2 - t = 0, for t > 0, x < 0, is a lower fence for this 
differential equation, and the isocline x2 — t = —1, for t > 5/4, x < 0, is an 
upper fence (Exercises 1.5#2), as shown in Figure 1.5.1.

1.5. The Use of Fences, Funnels, and Antifunnels
35
We label the lower fence a(t) = —y/t and the upper fence f3(t) = 
—y/t — 1. Then the functions a(t) and (3(t) determine a funnel that swal­
lows all solutions for t > 5/4, —y/t < x < \/t. (The upper limit for x 
becomes clear in Figure 1.5.2.) As you have shown in Exercise 1.2-1.4#5,
lim |a(t) -/?(t)| = 0.
t—><x>
Therefore, for t —> oo, the funnelled solutions to the differential equation 
behave like a(t) = —y/t.
Furthermore, the isoclines x2 — t = 0 and x2 — t = 1, for t > 0, can be 
written as y(t) = +y/t, 6(t) = +y/t + 1. The curves 7(t) and 6(t) determine 
a narrowing antifunnel for t > 0. Moreover, since df /дх = 2x > 0 in the 
antifunnel, by Theorem 1.4.5 the solution u(i) that stays in the antifunnel 
is unique (Figure 1.5.2, on the next page).
This is a particularly illustrative case of the information obtainable from 
direction fields, funnels and antifunnels:
inside the parabola x2 — t = 0, the slopes are negative so the solutions 
are all decreasing in this region; outside this parabola the slopes are 
positive so all solutions must be increasing there;
there is one exceptional solution u(t), y/t < u(t) < y/t + 1, uniquely 
specified by requiring it to be asymptotic to +y/t for t 00;
all the solutions above the exceptional solution are monotone increas­
ing and have vertical asymptotes both to the left and to the right. 
(The vertical asymptotes need to be proven, which is not so sim­
ple, but we shall encounter typical arguments in Example 1.6.1 and 
Exercises 1.6.);

36
1. Qualitative Methods
all the solutions beneath the exceptional solution have a vertical 
asymptote to the left, increase to a maximum, and then decrease 
and are asymptotic to — y/t.
FIGURE 1.5.2.
Let us elaborate on this last point as follows: When a solution leaves the 
antifunnel given by 7(t), <5(t) at a point (ti,rci), it is forced to enter the 
funnel given by a(t), /?(£), since the piecewise linear function 7?(t) defined
FIGURE 1.5.3. r/(t) = upper fence for solution through (ti.Xi).

1.5. The Use of Fences, Funnels, and Antifunnels
37
and graphed in Figure 1.5.3 is an upper fence. The fence T](t) is the line 
segment of slope 0 from (ti,xri) to (£2,^2) where it first intersects the 
isocline X2 — t = — 1, and the line segment of slope —1 from (^2,^2) to 
(£3, ж3), where it meets the isocline again at the top of the funnel. ▲
2. Bounds on f(t,x). A second source of useful fences is from bounds on 
f(t,x\ which we shall add to isocline fences in the following example:
Example 1.5.2. Consider xf = sintx.
The curves tx = 7?(7r/2) for integer n are the isoclines corresponding to 
slopes 0, 1, and —1. These isoclines are hyperbolas, all with the axes as 
asymptotes, for n Ф 0. Because of symmetry, we shall consider only the 
first quadrant, as shown in Figure 1.5.4.
FIGURE 1.5.4. x' = sintx.
For positive fc, you should confirm that the curves 
ak(t) = 2k^/t and = (2fc - (l/2))7r/t 

38
1. Qualitative Methods
determine an antifunnel, which we shall call the “fcth antifunnel,” in the 
region t > x. There are an infinite number of these antifunnels, and in fact, 
the regions between the antifunnels are funnels, so we get the following 
description of the solutions:
there are exceptional solutions ui, U2, U3,..., one in each of the anti- 
funnels above;
all the other solutions stay between Uk and Uk+i for some k.
We can also derive information about the solutions for t < x, by building 
another fence from the fact that f(t, x) = sintx is bounded. This fence is 
a piecewise differentiable one, constructed as follows, for any point on the 
ж-axis:
(i) start at t = 0 with a slope 1 and go straight in that direction until 
you hit the first isocline for slope 0;
FIGURE 1.5.5. Upper fence for x' = sintx.

1.5. The Use of Fences, Funnels, and Antifunnels
39
(ii) then go horizontally with slope 0 until you hit the next isocline for 
slope 0;
(iii) then go up again with slope 1 until you hit the next isocline for slope 
0;
(iv) continue in this manner, as shown in Figure 1.5.5.
You can show in Exercises 1.5#3b that this “curve” is a weak upper 
fence that meets the line x = t. A solution starting at a point on the ж-axis 
will stay below this fence until it reaches the line x = t, after which it 
will stay below one of the hyperbola fences described above. Therefore no 
solutions can escape the funnels and antifunnels described above.
It can also be shown that for positive /с, the solutions to this differential 
equation have 2k maxima (fc on each side of the ж-axis) and that nonex- 
ceptional solutions in the first quadrant lie in a funnel, which we shall call 

40
1. Qualitative Methods
the “A;th funnel,” described by the curves
orX(t) = (2fc-l)(7r/t) and = (2Л - (1/2))(тг/0
for sufficiently large t (Exercise 1.5#3c). The results of all this analysis are 
shown in Figures 1.5.6 and 1.5.7.
x
FIGURE 1.5.7. x' = sintz. Color-coded slopes.
You have shown in Exercise 1.2-1.4#8 that in the &th antifunnels, df /дх = 
tcostx > 0; in the fcth funnels, df /дх < 0, corresponding to the discussion 
of dispersion in Section 1.4.
For further discussion of this example, see Mills, Weisfeller and Krall, 
Mathematical Monthly, November, 1979. 
▲
3. Solutions to similar equations. A third and most important source of 
fences, often more useful than isoclines, is solutions to similar differential 
equations, which we shall illustrate with Examples 1.5.3 and 1.6.3. These 
are the fences which help us zero in on quantitative results for solutions of 
differential equations, even when we cannot explicitly find those solutions.

1.5. The Use of Fences, Funnels, and Antifunnels
41
Example 1.5.3. Consider
2
x' = l + A^^ = f(t,x\ for A > 0,
an equation we will meet in Volume III, in the analysis of Bessel functions 
for a vibrating membrane. What can you say about the solutions as £ —> oo? 
There are two good equations (because you can solve them and use them 
for fences) with which to compare the given differential equation. Because 
for all x 
n 
one similar equation is
x' = 1, with solution a(t) = t + ci,
and the other similar equation is
If we choose ci = C2 = c, then a(t) and /?(£) determine a narrowing 
antifunnel for t > 0, as graphed in Figure 1.5.8, with the slope marks for 
solutions to the differential equation as indicated according to the inequal­
ity.
FIGURE 1.5.8. Funnel for x' = 1 + A(cos2 x)/t2.
(Actually, according to the inequality, a(t) and /3(t) are weak fences, 
so we are a bit ahead of ourselves with this example, but this is a very 
good example of where we will want to use solutions to similar equations 

42
1. Qualitative Methods
as fences. The Antifunnel Theorem of Chapter 4 will in fact be satisfied in 
this case.)
The antifunnel is narrowing, with
lim |a(t) - /3(t)| = 0,
t—>OO
so we can hope to have one unique solution that remains in the antifunnel.
However
dx
— (A/t2)2cosx sinx = —(A/t2)sin2x
is both positive and negative in this antifunnel, so our present criterion for 
uniqueness is not satisfied. Nevertheless, we will show in Example 4.7 that 
there indeed is for any given c a unique solution in the antifunnel, behaving 
as t —> oo like (t + c). 
▲
See Example 1.6.3 as additional illustration of using similar equations 
for finding fences.
Remark. Pictures can be misleading. This is one reason why we so badly 
need our fence, funnel, and antifunnel theorems.
For instance, look ahead just for a minute to Figure 5.4.6 of Section 5.4. 
There you see three pictures of “solutions” to x' = x2 — t, our equation of 
Examples 1.1.4, 1.3.3, 1.3.6, 1.5.1. You can ignore the fact that the Chapter 
5 pictures are made by three different methods (which will be thoroughly 
discussed in Chapter 3). Just notice that a lot of spurious “junk” appears 
in these pictures, some of which looks like junk, but some of which (in 
the middle) looks like a plausible additional attracting solution in another 
funnel. With the application of the theorems given in Example 1.5.1, we can 
say precisely and without doubt that the solutions approaching x = — y/t 
really belong there, and the others do not.
So, what we are saying is:
You can (and should) use the pictures as a guide to proving 
where the solutions go, but you must apply some method of proof 
(that is, the theorems) to justify a statement that such a guess 
is in fact true.
1.6 Vertical Asymptotes
If \f(t, x) | grows very quickly with respect to x, the solutions to xr = f(t, x) 
will have vertical asymptotes.
Example 1.6.1. Consider x' = kx2, for к > 0.
A computer printout of the solutions looks like Figure 1.6.1.

1.6. Vertical Asymptotes
43
FIGURE 1.6.1. x' = kx2.
Remark. Because the slope x' = /(t, x) has no explicit dependence on t, 
the solutions are horizontal translates of one another.
As you can verify, individual nonzero solutions are u(t) = 1/(C — kt) 
with a vertical asymptote at t = С/к, as in Figure 1.6.2. 
▲
FIGURE 1.6.2. Two solutions to xl = kx2.

44
1. Qualitative Methods
Example 1.6.1 illustrates the fact that a solution need not be defined for 
every t. This happens, for instance, with the following important class of 
differential equations:
Nonzero solutions to the differential equation x' = have 
vertical asymptotes for a > 1.
This is because such equations have (as you can confirm analytically by
separation of variables in the next chapter, Exercise 2.1#lj) nonzero solu­
tions
u(t) = <
[(1 — a)kt + C]1^1
— [(1 — a) kt + C]1^1
C 
(q — l)k
C 
(o — l)k
with constant C. The exponent will be negative if a > 1, and then there 
will be an asymptote at t = —C/(l — a)k.
This fact is useful for recognizing differential equations that may have 
asymptotes. Vertical asymptotes may occur whenever |/(£,ж)| grows at 
least as fast as for a > 1, such as, for instance, functions with terms 
like x3 or ex as dominant terms. Furthermore, we can use functions of the 
form |#|Q for finding fences with vertical asymptotes, as will be shown in 
our next two examples.
The following example proves the existence of some of the vertical asymp­
totes for our favorite example:
Example 1.6.2. Consider again x' = x2 — t, as in Examples 1.1.4, 1.3.3, 
1.3.6, 1.5.1.
There is an antifunnel (Example 1.5.1) determined by
x2 — t = 1 and x2 — t = 0 for t > 0,
with a unique solution u(t) that remains in the antifunnel for all time.
FIGURE 1.6.3. Antifunnel for x' = x2 —t.

1.6. Vertical Asymptotes
45
We shall show that any solution to the differential equation xf = x2 — t 
that lies above u(t) at some t > 0 (i.e., within the shaded region of Figure 
1.6.3 at some point) has a vertical asymptote.
Since x' grows as ж2, the basic plan is to try as a lower fence solutions 
to x' = ж3/2, because the exponent 3/2 is less than 2 but still greater than 
1. Solutions to x' = ж3/2 are of the form
v(<) “ (C -1)2
with a vertical asymptote at t =C.
These solutions v(t) to x' = ж3/2 will be strong lower fences for xf = 
x2 — t only where x2 — t > ж3/2. The boundary of that region is the curve 
x2 — t = ж3/2, as shown in Figure 1.6.4.
FIGURE 1.6.4. Strong lower fence for xf = x2 — t.
We combine Figures 1.6.3 and 1.6.4 in Figure 1.6.5, where we will con­
struct a lower fence in two pieces as follows: Every solution u(t) lying above 
u(t) must first leave the antifunnel by crossing the isocline x2 — t = 1 with 
slope 1 at some point 
For t > ti, the slopes for u(t) continue to
increase, so the lower fence begins for t > t± with the line segment from 
(ti,?z(ti)) having slope 1. The fence then meets the curve x2 — t = ж3/2, at 
some point (£2, #2)- (For t > 0, the curve x2 — t = ж3/2 lies above x2 — t = 1 
and has slope approaching zero as t —> 00; therefore a segment of slope 1 
must cross it.)
Thus u(t) enters the region where the solutions v(t) to xf = ж3/2 are 
strong lower fences. By the Fence Theorem 1.3.5, each solution u(t) must 
now remain above the v(t) with ^(£2) = ^2- Since the v(t) all have vertical 
asymptotes, so must the u(t). A

46
1. Qualitative Methods
FIGURE 1.6.5. Piecewise lower fence for x' = x2 — t.
The analysis of our next example allows us, with given initial conditions, 
to zero in on a particular vertical asymptote for a fence; this therefore gives 
a bound on t for the solution with those initial conditions.
Example 1.6.3. Consider
_ zv.2 I J.2 
X — X "i t •
(1)
This equation (1) can be compared with its simpler (solvable) relatives, 
xf = t2 and x' = x2. Solutions to either of these simpler equations are lower 
fences, because the slope of a solution to equation (1) is greater or equal 
at every point.
To show the powerful implications of this observation, let us see what 
it tells us about the solution to equation (1) through the origin (i.e., the 
solution x = u(t) for which u(0) = 0.) The first simple relation
x' = t2 has solutions oik(t) = (t3/3) + fc,
so a(t) = (t3/3) is a lower fence for t > 0 (a weak lower fence at t = 0), as 
shown in Figure 1.6.6. All solutions to xf = x2 +12 that cross the ж-axis 
above or at 0 lie above this curve a(t).

1.6. Vertical Asymptotes
47
But we can do better than this, because the solutions to equation (1) 
will also lie above another fence, a*(t\ from the second simple relation
x' = x2, which has solutions a*(t) = l/(c - t).
The important thing about a*(t) is that it has a vertical asymptote, so we 
shall be able to put an upper bound on t. The family of curves a*(t) are 
shown in Figure 1.6.7.
FIGURE 1.6.7. Other lower fences for x' — x2 +12.

48
1. Qualitative Methods
None of these curves a*(t) falls below the origin, but we already have a 
lower fence a(t) at the origin. What we now want for the tightest bound on 
t is a*(t) = the first a*(t) that is tangent to a(t). (See Figure 1.6.8 where 
a is the left-hand piece, a* the right-hand piece of the final fence.)
л/З 4/^3
FIGURE 1.6.8. Piecewise lower fence, in bold dashes, with vertical asymptote for 
ж' = x2 4-12.
That is, we want the slope of a*(t), which is x2, to equal the slope of 
a(t), which is t2; this happens when x = t.
from a : x = — = t when t = \/3 = x. 
О
from a* : x = —— at that point means V3 = -—- 
c c-t 
(c -
4
or c = -= « 2.3094.
a/3
Hence c = 4/-\/3 is a vertical asymptote for a fence a* on the right, which 
is the best you can find by these fences.
With this method, the best complete lower fence for the solution to the 
differential equation x' = x2 +12, with initial condition at the origin, is
a(t) = t3/3
0 < t < л/З
a*(t) = 1/((4/д/3) -1) t>%/3

Exercises
49
as shown in Figure 1.6.8. Since the fence has a vertical asymptote, we have 
shown that the solution through the origin is only defined for t < £q, where 
to is a number satisfying
4 
t0< — & 2.3094.
v 3
Exercises 1.1 Slope Fields, Sketching Solutions
Consider the following differential equations:
(a) x' = x 
(b) x* = x — t
(i) Using isoclines, sketch by hand a slope field for each equation. Draw 
the isoclines using different colors, to eliminate confusion with slopes. 
Then draw in some solutions by drawing curves that follow the slopes.
(ii) Confirm by differentiation and substitution that the following func­
tions are actually solutions to the differential equations
for (a): x = Ce* 
for (b): x = t + 1 + Ce*
Consider the following differential equations:
(a) xf = x2 
(b)° x' = x2 - 1
(i) Using isoclines, sketch by hand a slope field for each equation. Draw 
the isoclines using different colors, to eliminate confusion with slopes. 
Then draw in some solutions by drawing curves that follow the slopes.
(ii) Tell in each case which solutions seem to become vertical.
(iii) Confirm by differentiation and substitution that the following func­
tions are actually solutions to the differential equations
for (a): x = 1/(0 — t) 
for (b): x = (1 — Oe2f)/(1 + Ce2t)
(iv) Use the computer programs Analyzer and DiffEq to confirm these 
solutions graphically. Show which values of C lead to which members 
of the family of solutions.
(v) In each case, find another solution that is not given by the formula in 
part (iii). Hint: Look at your drawings of solutions on the direction 
field. This provides a good warning that analytic methods may not 
yield all solutions to a differential equation. It also emphasizes that 
the drawing contains valuable information that might otherwise be 
missed.

50
1. Qualitative Methods
Consider the following differential equations:
(a) x' = — x/t 
(c) i'=x/(2t)
(b) x'—x/t 
(d) x' = (1 — t)/(l + x)
(i) Using isoclines, sketch by hand a slope field for each equation. Indi­
cate clearly where the differential equation is undefined. Draw the iso­
clines using different colors, to eliminate confusion with slopes. Then 
draw in some solutions by drawing curves that follow the slopes.
(ii) Confirm by differentiation (implicit if necessary) and substitution 
that the following equations actually define parts of solutions to these 
differential equations. State carefully which parts of the curves give 
solutions, referring to where the differential equations are undefined.
for (a): x — C/t
for (c): x = C|t|V2
for (b): x = Ct
for (d): (t - I)2 + (rr + I)2 = C2 
1.1#4. (Courtesy Michele Artigue and Veronique Gautheron, Universite 
de Paris VII.) On each of the following eight direction fields,
(i) Sketch the isocline(s) of zero slope.
(ii) Mark the region(s) where the slopes are undefined.
(iii) Sketch in a few solutions.
(iv)° Match the direction fields with the following equations. Each num­
bered equation corresponds to a different one of the lettered graphs.
1. x' = 2
2. x' = t
3. x' = x - t
4. x' = x
5. x' = x/t
6. x' = —t/x
7. x' = (i - 2)/(t - 1)
8. x' = tx2 + t2

Exercises

М И Н И Н

52
1. Qualitative Methods
(Courtesy Michele Artique and Veronique Gautheron, Universite 
de Paris VII.) Determine which of the numbered graphs represents the 
family of solutions to each of the following equations:
(a) x’ = sinta
(b) x' = x2 — 1
(c) xf = 2t + x
(d) x' = (sin t) (sin x)
(e) x1 = x/(t2 — 1)
(f) xf = (sin3t)/(1 — t2)
This time there are more graphs than equations, so more than one graph 
may correspond to one equation, or a graph may not correspond to any of 
the equations. In the latter case, try to guess what the differential equation 
might be, and check your guess with the program DiffEq.

Exercises
53
1.1#6. To gain insight into how solutions fit into the direction fields, con­
sider the following two equations:
/2
(a) x' = x2 — t2 
(b) x' = x2 — -----
1 И-t
With the help of the computer program DiffEq, which makes quick work 
of the more complicated /(£, z)’s, print out direction fields only and try by 
hand to draw in solutions. For part (a) you can (and should) clarify the 
situation by calculating the isoclines directly from the differential equation 
and graphing them (perhaps on a separate piece of paper or in a different 
color) with the proper slope marks for the solutions. You can check your 
final results by allowing the computer to draw in solutions on slope fields. 
Describe the crucial differences in the drawings for (a) and (b).
1.1#7. Symmetry of individual solutions to a differential equation x' = 
f(t,x) occurs when certain conditions are met by f(t, x). For instance,
(a) If /(£,ж) = —/(—£, ж), every pair of symmetric points (t,x) and 
(—x) have slopes that are symmetric with respect to the vertical 
ж-axis, so solutions are symmetric about this axis. Illustrate this fact 
by drawing a direction field and solutions to x' = tx, and to xf = tx2. 
See Figure (a) below.

54
1. Qualitative Methods
But symmetry of individual solutions to a differential equation x' = f(t, x) 
does not necessarily occur when you might expect. When symmetric points 
on the t, ж-plane have the same slope, the resulting drawing of solutions in 
general does not show the same symmetry. For example,
(b) If f(t,x) = /(—t,x), the same pair of points (t,x) and (-t,x) give 
a picture with no symmetry. If you are not convinced, add more like 
pairs of points to this sketch, each pair with the same slopes. You 
could also show the lack of symmetry by drawing the slopes and 
solutions for xf = t2 — x.
(c) Of the other possible permutations of plus and minus signs, show 
which cases lead to symmetric solutions and state with respect to 
what they are symmetric. That is, consider
f(t,x) = f(t,-x); f(t,x) = f(-t,-x)-, 
f(t, x) = -f(t, —x); f(t, x) = 
-x).
1.1#8. A quite different question of symmetry arises in the matter of 
whether the picture of solutions for one differential equation x' = f(t,x) 
corresponds to that for another with a related /, such as xf = — f(t,x). 
The answer is quite likely “no”; for example
(a) Compare the drawings of solutions for x' = tx with those for x' = 
—tx.
But there are times when the answer is “yes”; for example
(b) Compare the drawings of solutions for xf = x2 — t with those for 
x' = t — x2.
This problem serves as a warning that symmetry of solutions to one dif­
ferential equation x' = f(t,x) with respect to another differential equation 
with a related f is not a simple question. You should not assume there 
will be a relation. If you wish to explore this matter further, see the next 
exercise.

Exercises
55
Since you have a computer graphics program DiffEq, you have a 
headstart if you wish to explore some of the symmetry questions raised by 
the previous problem. Consider the differential equation
x1 = x2 + sin x +1 + cos t + 3,
carefully chosen to include both an odd function and an even function in 
each variable (as well as avoiding extra symmetries that could creep in with 
a strictly polynomial function). Print out slope fields with some solutions 
for each of the various sign variants:
(a) x' = /(t, x)
(e) x' = -f(t, x)
(b) x' = f(-t, -x)
(f) x' =
(c) x1 = -f(t, -x)
(g) x' = f(t,-x)
(d) x1 = 
x)
(h) x'= f(-t,x)
The graphs should all be the same size, default is fine. It will be helpful if 
at least some of your initial conditions are the same from graph to graph. 
Then compare the results and conjecture when there might be a relation 
between the solutions of x' = f(t,x) and the graph of one of the sign 
variants. Test your conjecture(s) by holding pairs of computer drawings 
up to the light together. Be careful to match the axes exactly, and see if 
the solutions are the same—you may find that some cases you expected 
to match in fact do not under this close inspection. You can test for the 
various types of symmetry as follows:
with respect to the ж-axis: (flip one drawing upside down) 
with respect to the t-axis: (flip one drawing left to right) 
with respect to the origin: (flip upside down and left to right).
Now that you have honed your list of conjectures, prove which will be true 
in general. This is the sort of procedure to follow for proper analysis of 
pictures: experiment, conjecture, seek proof of the results. The pictures are 
of extreme help in knowing what can be proved, but they do not stand 
alone without proof.
Given a differential equation x' = f(t,x) and some constant c, 
find what can be said when about solutions to ж' = f(t, x) + c. Use the 
“experiment, conjecture, prove” method of the previous exercise.
To become aware of the limitations of the computer program 
DiffEq, use it to graph direction fields and some sample solutions for the 
following equations, for the default window -10 < t < 10, -7.5 < ж < 
7.5, setting the stepsize h = 0.3. Explain what is wrong with each of the 
computer drawings, and tell what you could do to “fix” it.
(а) ж' = sin(t3 - ж3) (b) ж' = (t2/x) - 1 (с) ж'= t(2 - ж)/(£ + 1).
Again using the computer program DiffEq, sketch direction fields 
and then some isoclines and solutions for

56
1. Qualitative Methods
(a) x' = (2 + cost)# — (1/2)#2 — 1
(b) #' = (2 + cost)# — (1/2)#2 — 2.
Notice that many of the solutions sketched in the previous exer­
cises have inflection points. These can be precisely described for a solution 
# = u(t) in the following manner: obtain #" by differentiating /(t, #) with 
respect to t (and substituting for #'). This expression for #" gives infor­
mation at every point (t, #) on concavity and inflection points, which can 
be particularly valuable in hand-sketching. Carry out this process for the 
equation #' = #2 — 1 (direction field and solutions were drawn in Exercise 
l.l#2b).
Use the method described in the previous exercise to derive the 
equation for the locus of inflection points for solutions of #' = #2 — t. Hint: 
it is easier to plot t = g(x)\ label your axes accordingly. Use the computer 
program Analyzer to plot and print this locus.
Confirm that the inflection points of the solutions indeed fall on this locus 
by superimposing your Analyzer picture on a same scale DiffEq picture of 
the direction field and solutions (i.e., holding them up to the light together 
with the same axis orientation—this will mean turning one of the pages 
over and rotating it).
You will get to use the results of this problem in Exercise 1.2-1.4#17.
Use the method described in Exercise 1.1#13 to derive the equa­
tion for the locus of inflection points for solutions of #' = sint#. This time 
part of the locus is not so easy to plot. For various values of t, use the 
computer program Analyzer to find corresponding # values satisfying that 
locus equation. Then mark these (t, #) pairs on a DiffEq picture of the di­
rection field and solutions, and confirm that they indeed mark the inflection 
points for those values of t.
Sketch by hand solutions to #' = ax — bx2 by finding where the 
slope is zero, positive, and negative. Use the second derivative (as in Exer­
cise 1.1#13) to locate the inflection points for the solutions. This equation 
is called the logistic equation; it occurs frequently in applications—we shall 
discuss it further in Section 2.5.
Exercises 1.2-1.4 Qualitative Description of 
Solutions: Fences, Funnels, and Antifunnels
Consider the differential equation
dx q
— = x2 — t. 
dt
For each of the lines # = t, # — 2 and each of the isoclines with c = —2, 
0, 2, show on a drawing which parts are upper fences and which parts are 

Exercises
57
lower fences. Furthermore, show the combination of these curves (or parts 
thereof) that form funnels and antifunnels.
1.2—1.4#2. Identify funnels and antifunnels in these slope fields, drawn 
by isoclines:
t 
>

58
1. Qualitative Methods

Exercises
59
Then look back at the solutions drawn in these fields in Figures 1.1.4, 1.1.6, 
1.1.7.
1.2—1.4#3. For each of the following equations, use Analyzer to draw a 
few relevant isoclines on the same graph and print it out. For each isocline, 
draw on your printout the appropriate slope marks for the solution to the 
differential equation. Shade and identify funnels and antifunnels. You may 
find it helpful to refer back to Exercises 1.1 #1-3 where you drew these 
slope fields from isoclines. (You can use such handsketches for this exercise 
if you prefer.)
(a)
x' = X
(e) x' =
X
(b)
xf — X2
(f) x' = X
1
(c)° x' = X2 — 1
(g) x' = X
2t
(d)
x' — x — t
(h) x' = 1-t
1 + X
In order to partly check your work, you can use the computer program 
DiffEq to print out a slope field with some solutions. Then hold it up to 
the light with your Analyzer printout showing isoclines and some hand­
drawn solutions. The computer-drawn slope field should “fit” the isoclines; 
the extent to which the hand-drawn solutions match the computer-drawn 
solutions is an indication of your skill at following the direction field.
1.2—1.4#4°. Why do we need fences and funnels if the computer pictures 
show us the solutions? The following will show you why a picture alone is 
not enough: Use the computer program DiffEq on our favorite differential 
equation, x' = x2 —t, for 0 < t < 15, -7.5 < x < 7.5, with stepsize h = 0.4. 
You should see spurious solutions that don’t go into the funnel, sometimes 
even appearing to go into a different funnel; yet the analysis of Example 
1.5.1 proves that in fact there is a funnel, but only one. When we get to 
Section 5.4, you will see an explanation of why the erroneous pictures of 
this exercise happen.
1.2-1.4#5. Show that the isoclines for xf = x2 — t (Example 1.5.1) form 
narrowing funnels and antifunnels, that is, that
lim \y/t + c — Vt\ = 0.
t—*oo 1
1.2—1.4#6. Our fence definitions require functions that are continuous 
and piecewise differentiable, as in Figure 1.2.6. Determine which of the 
following continuous functions are piecewise differentiable. State exactly 

60
1. Qualitative Methods
why the others fail, and where they could not be used as a fence:
(a) 
x= |sint| 
(c) rc = tsin(l/t)
(b) x = \/t 
(d) x = ^/|^i
1.2-1.4#7. (a) Can there exist some curve x = such that ^(t) is both 
an upper and a lower fence for xf = on some interval t e [a, b\? If 
so, what can we say about ^(£)?
(b) Can there exist some curve x = ^(t) such that is both a strong 
upper fence and a strong lower fence for x' = f(t, x) on some interval [a, b]? 
If so, what can we say about 'фф?
1.2— 1.4#8. For the following differential equations x' = f(t,x), make 
drawings of the regions in the £, ж-plane where the dispersion df/dx is 
positive and where it is negative. Then with the computer program DiffEq 
sketch some solutions to the differential equation and see that they behave 
as the dispersion predicts.
(a) 
ж' = sin tx 
(b) x'= x3 — x2t
1.2— 1.4#9°. (a) Show that for the differential equation ж' = x — ж2, the 
region |ж| < 1/2 is an antifunnel, and the region l/2<ж<3/2isa funnel.
(b) The funnel and antifunnel found in part (a) are not narrowing. Find 
a narrowing antifunnel containing the solution x(t) = 0, and a narrowing 
funnel containing the solution x(t) = 1. (The first part is very easy, but 
the second is a bit more subtle.) Hint: Consider functions like 1 + (a/t).
1.2— 1.4#10. Consider the differential equation xf = x — x2 + l/t.
(a) Show that the region — 2/t < ж < 0, t > 1 is an antifunnel.
(b) Show that the solution in (a) is the unique solution that stays in the 
antifunnel.
1.2— 1.4#11. Consider further the differential equation ж' = x — x2 + l/t 
of the previous exercise. It is no longer as obvious as in Exercise 1.2-1.4#9 
that there are solutions asymptotic to ж = 1, although it looks likely that 
there is a funnel here for t sufficiently large.
(a) Sketch the slope field around ж = 1 for x' = x — ж2, and then in a 
different color show how the extra term 1/t affects the slope field. It 
should appear that the solutions above ж = 1 will now approach more 
slowly. Where do the solutions go that cross ж = 1? Can they go to 
oo?
(b) Show that ж = 1 is now a lower fence.

Exercises
61
(c) Find an upper fence that is asymptotic to x = 1.
(d) Find a lower fence between the upper fence of (c) and x = 1, showing 
that the solutions are bounded a little bit away from x = 1.
(e) Use the computer program DiffEq with its blowup feature to verify 
these results.
1.2— 1.4#12. Consider the differential equation xf — x — x2 + 2/t.
(a) Use the computer program DiffEq to sketch the slope field and a few 
solutions.
(b) Show that there is a solution asymptotic to x = 0, although we seem 
not to be able to easily state it. Hint: consider the same antifunnel 
as in Exercises 1.2-1.4#10.
1.2— 1.4#13. Consider the differential equation x' = x — x2 + 2/t as in the 
previous problem, but now consider its behavior near x — 1.
(a) What sorts of fences are the curves a(t) = 1 + a/t for 0 < a < oo?
(b) Show that any solution u(f) which at some to satisfies
1 < 'фо) < 1 + 2/Zq
is defined for all t and satisfies lim^oo u(t) = 1.
1.2—1.4#14. Consider the following differential equations, and show how 
the different perturbations affect the behavior compared to x1 — x — x2. 
Use the program DiffEq to draw direction fields with some solutions. Then 
analyze what you can say about what you see. Will there be funnels and 
antifunnels near x = 0 and x = 1 if t gets big enough? (In the default 
window size, the behavior of xf = x — x2 hasn’t “won out” yet; extend the 
t-axis until solutions level out horizontally.)
Explain the behavior for 0 < t < 2. (Try l/(t — 1); is that an upper 
fence? Do all solutions turn up before t = 1?)
(a) xf — x — х2 + 1/\Д 
(b) xr — x — x2 + 1/lnt.
1.2— 1.4#15°. Consider the differential equation xf = —x/t.
(a) Show that there are solutions tending to 0.
(b) Does this behavior change when we add a perturbation? Find out 
what happens near x = 0 for xf — — (x/t)+x2. Start from a computer 
drawing and proceed to look for fences and funnels.
1.2—1.4#16. Consider the differential equation xf = —x + (1 + t)x2. The 
first term on the right indicates a decrease, but the second indicates an 
increase. The factor (1 -h t)...

62
1. Qualitative Methods
(a) Show that for any to > 0, there exists a number a such that if 0 < 
a < e the solution with x(t$) = a tends to 0 at oo.
(b) Show that there is an antifunnel above the t-axis, but that with a 
fence of 1/t you cannot get uniqueness for a solution in the antifunnel. 
Can you invent a better fence?
1.2- 1.4#17. Consider the differential equation x' = x2 —t. Use the results 
of Exercise 1.1 #14 to show that there is a higher lower fence for the funnel 
than x = —y/t.
Exercises 1.5 Uses of Fences, Funnels, and 
Antifunnels
Analyze the following differential equations, using the computer 
program DiffEq to get a general picture, then doing what you can to make 
fences, funnels, and antifunnels that give more precise information about 
the solutions. (The exercises of the previous section give some ideas of what 
to try.)
(a) x' = e* — h 
(e) x' = cos x — t
t2
(b) x' =---- 1 
(f) x' = (t + 1) cos x
x
(c) O’ = -1 + 
(g) X1 = X2 + t2
(d) x' = 
+t2 ~ 1 
(h)° x' = 
■ i ~
С “Г 1
1.5#2. Fill in the steps in Example 1.5.1 to show that for x' = x2 — t, 
(a) the isocline x2 — t = 0 is a lower fence for t > 0,
(b) the isocline x2 — t = —1 is an upper fence for t > 5/4.
1.5#3. Consider x' = sinta, the equation of Example 1.5.2.
(a) 
(i) Show that the piecewise function constructed in Example 1.5.2, 
Figure 1.5.5 is an upper fence.
(ii) Furthermore, show that for any xq = 6, the line x = t + b is a 
weak upper fence, and the line x = — t + b is a weak lower fence.
(b) 
Show why part (a)(i) means that every solution to this differential 
equation gets trapped below one of the hyperbola isoclines. This 

Exercises
63
means to show that the fence constructed in Figure 1.5.5 indeed in­
tersects the diagonal. Hint: show that the slopes of the inclined pieces 
is < 1/2; show also that for fixed x the hyperbolas are equidistant 
horizontally.
(c) 
Show that the nonexceptional solutions (that is, those not in the 
antifunnels) collect in funnels between a£(t) = (2k — 1)(тг/£) and 
/3k(t) = (2k — 1/2) (тг/t). Then show that each of these solutions have 
2k maxima (k on each side of the ж-axis).
(d) 
Show that symmetry gives for every fcth antifunnel another for к = 
—1, —2, —3,... with symmetrical solutions u~k(t) = and that 
by this notation, &k is symmetrical to /?_&, {3k to a_k-
(e) 
Show that &k and /3_£ (from the last part, (c)) for к > 0, t > 0 form 
a narrowing antifunnel. Show that df /дх exists everywhere but is 
both positive and negative in this antifunnel. Then show that there 
are infinitely many solutions that stay in the antifunnel. Why is this 
not a counterexample to the Antifunnel Uniqueness Theorem 1.4.5?
1.5#4. Analyze the following differential equations, using isoclines to sketch 
the direction field and then identifying fences, funnels, and antifunnels to 
give precise information about the solutions:
(a) x' = — sin tx 
(b) x' = cos tx
1.5#5. Sketch some isoclines, slopes, and solutions for the following func­
tions. Locate funnels and antifunnels. Compare the behavior of solutions 
for these equations:
(a) 
x' = t cos x - 1 
(c) x' = t cos x + 2
(b) ж' = t cos ж + 1
1.5#6. Consider the differential equation ж' = ж2 — t2.
(a) Find the isoclines and draw them for c = 0, ±1, ±2. In each quadrant, 
show how some of these isoclines can be used to form funnels and 
antifunnels.
(b) In the first quadrant find an antifunnel A satisfying the Antifunnel 
Uniqueness Theorem 1.4.5. Find in the fourth quadrant a narrowing 
funnel F. Show that every solution leaving the antifunnel A below 
the exceptional solution is forced to enter the funnel F.
(c) Which kinds of symmetry are exhibited by this differential equation, 
considering such facts as /(—t, —x) = /(£,ж)?

64
1. Qualitative Methods
1.5#7. For the equation x1 = 1-------—, show fences and funnels (as in
Example 1.5.3).
1.5#8. Consider = sin(t3 — ж3).
(a) As in Exercise l.l#lla, use the computer to draw solutions with 
stepsize h = 0.3. The picture is not convincing. Change the stepsize 
to get a more reliable picture of the solutions; tell which step you 
have used.
(b) This differential equation can be analyzed using isoclines. You can 
use Analyzer as a “precise” drawing aid for the isoclines. Sketch and 
label some isoclines for slopes of 0, ±1,....
(c) Explain how isoclines can be used to define antifunnels and funnels in 
the first quadrant similar to ones defined for x1 = sintx in Example 
1.5.2. Especially choose antifunnels, so that Theorem 1.4.5 can be 
applied.
1.5#9. For the differential equation x1 — (x2 — t)/(t2 + 1),
(a) Plot the isoclines for slopes 0, —1, 1.
(b) Sketch several solutions.
(c) Find a funnel and an antifunnel.
(d) Show that the solution in the antifunnel is unique.
1.5 #10°. Consider the differential equation x' = x2 /(t2 + 1) — 1.
(a) What kinds of curves are the isoclines? Sketch the isoclines of slope 0 
and —1. Describe the regions where the slope is positive and negative.
(b) Show that the solutions u(f) to the equation with |tz(O)| < 1 are 
defined for all t.
(c) What are the slopes of the asymptotes for the isoclines for slope 
m = (14- л/5)/2? Show that the isocline of slope m and its asymptote 
define an antifunnel. How many solutions are there in this antifunnel?
(d) Sketch the solutions to the equation.
Exercises 1.6 Vertical Asymptotes
i.6 #i. Prove there exist vertical asymptotes for
(a) x' = x3 -1 
(c) x' = x2 - xt
(b) x' = ex + t 
(d) x' = -2x3 + t
(e) x' = ax — bx2

Exercises
65
1.6#2. Consider the differential equation x' — x2 —t2.
(a) You found in Exercises 1.5#6b an antifunnel A in the first quadrant, 
with an exceptional solution trapped inside. Show that every solution 
leaving the antifunnel A above this exceptional solution has a vertical 
asymptote.
(b) Use fences to show that the solution of x' — x2 — t2 through the point 
(0,2) has a vertical asymptote at t = to and find to to one significant 
digit. Confirm your results with a computer drawing, blowing up near 
the asymptote.
1.6#3°. Consider the differential equation xf = x3 — t2.
(a) Show that there is a narrowing antifunnel along x = t2/3.
(b) Show that all solutions above the exceptional solution in the antifun­
nel have vertical asymptotes.
1.6#4. Consider again x' = x2 — t from Example 1.6.2. Show that the 
solutions below the funnel bounded below by x2 — t = 0 for t > 0 have 
vertical asymptotes to the left (by working with negative time).
1.6#5. Consider x' = x2 — t from Example 1.6.2. Find a bound on the ver­
tical asymptotes for the solutions, and confirm your results with a computer 
drawing.
(a) through (0,1) 
(b) through (—1,0).
1.6#6°. By comparing x' = x2 — t with xf — (1/2)#2, show that the 
solution to x' = x2 — t with ж(0) = 1 has a vertical asymptote t = to for 
some to < 2. Confirm your results with a computer drawing.
1.6#7. Consider x1 — x2 + t2 as in Example 1.6.1. Find a bound on 
the vertical asymptotes for the solutions, and confirm your results with a 
computer drawing.
(a) through (1,0) 
(b) through (0,1).

2
Analytic Methods
In this chapter we actually have two themes: methods of solution for dif­
ferential equations that can be solved analytically, and some discussion of 
how such equations and their solutions are used in real-world applications.
The first task is to present traditional methods for analytic solutions 
to differential equations—that is, the non-graphic, non-computer approach 
that can yield actual formulas for solutions to differential equations falling 
into certain convenient classes.
For first order differential equations, xf = f(t,x), there are two classes 
that suffice for solving most of those equations that actually have solutions 
which can be written in elementary terms:
separable equations: 
xf = g(t)h{x)
linear equations: 
x' = p(t)x + q(t).
If x' = f(t,x) does not depend explicitly on x, then x' = g(t), and to 
find the solutions means simply to find the antiderivatives of g(t).
If, on the other hand, x' = f(t, x) does not depend explicitly on t, then 
x' = h(x), and the equation is called autonomous. Every first order au­
tonomous equation is separable, which means it is possibly solvable by a 
single integration.
In Section 2.1 we shall discuss the separable equations, traditionally the 
easiest to attack, but they yield implicit solutions. In Sections 2.2 and 2.3 
are the linear equations, which have the advantage of giving an explicit for­
mula for solutions to equations in this class. Later in Section 2.6 we shall 
discuss another class, the exact equations, which actually include the sepa­
rable equations, and which also produce implicit solutions. Finally, Section 
2.7 introduces power series solutions. The Exercises (such as 2.2-2.3#8,9 
and Miscellaneous Problems #2) explore yet other traditional classes, such 
as Bernoulli equations and homogeneous equations.
These presentations will probably review methods you have seen in pre­
vious courses, but you should find here new perspectives on these methods, 
particularly for the linear equations and exact equations.
Partly because, however, this chapter may be to a great extent a review, 
we also attend here to a second theme of applicability—how differential 
equations arise from modeling real-world situations. In Sections 2.4 and 
2.5 we shall discuss important applications of differential equations that 

68
2. Analytic Methods
also shed a lot of light on the meaning of the linear equations and the 
methods for their solution.
2.1 Separation of Variables
The method of separation of variables applies to the case where xf = f(t,x) 
can be written as
x' = g(t)h(x).
If h(xo) = 0, then u(t) = xq is a solution.
If h(x) 7^ 0, then the variables can be separated by rewriting the equation 
as 
-rr\ = g(t)dt, and 
tlyX )
If you can compute the integrals,
(1)
M r“:
1 g(t)dt = G(t) + CG,
then the solution is
H(x) = G(t) + C, 
(2)
which explicitly or implicitly will define solutions x = u(t).
Remark. This derivation illustrates the beauty of Leibniz’ notation dx/dt 
for a derivative; the same result can be achieved by writing
f Wx}^dt= [ 9^dt-
J h\x) at J
Example 2.1 .1. Let us consider the particularly useful general class of 
equations of the form x' = g(t)x.
For x = 0, u(f) — 0 is a solution.
For x Ф 0,
In |ж| = J g(i)dt 
\x\ = ef3wdt.
If we are given an initial condition u(to) = xq, then the solution is
J 9^d' 
x = u(t) = xoeJto

2.1. Separation of Variables
69
Example 2.1 .2. Consider x' = ax - bx2 a > 0,6 > 0, an autonomous 
equation that arises frequently in population problems, to be discussed in 
Section 2.5.
Since x' = (a — bx)x, two solutions are u(t) = 0 and u(t) = a/b.
For x 0 and x a/b, we get by separation of variables
dx
ax — bx2
which by partial fractions gives for the left side
dx f bdx l.ii I,. , । 
----- H / —; —-Г = - In Ш In a — bx\ 
ax J a(a — bx) a a
and for the right side
J dt — t + C.
Setting these two expressions equal and exponentiating leads to
x
a —bx
= Qeat,
where we write Q as constant instead of a more complicated expression 
involving C, the original constant of integration. Solving for x we get
f aQeat 
bQeat — 1
0
u(t) = <
aQeat 
bQeat + 1
a/b
aQeat
< bQeat - 1
for x < 0
for x = 0
for 0 < x < a/b
for x = a/b
for x > a/b.
This information is confirmed on the next page by Figure 2.1.1, the pic­
ture of solutions in the direction field (as constructed in Exercise 1.1# 16). 
For x < 0 or x > a/b, the independent variable t is restricted, and there 
are vertical asymptotes (as proved in Exercise 1.6#le). 
▲
Remark. In Example 2.1.2, as in Example 1.6.1, the slope does not depend 
explicitly on t, so the solutions from left to right are horizontal translates 
of one another. This is an important aspect of autonomous systems.

70
2. Analytic Methods
FIGURE 2.1.1. x' = ax- bx2.
Example 2.1 .3. Consider x' = (t + 1) cos ж.
Since cos a; = 0 for x an odd multiple of тг/2, some solutions are
uk(t) = (2fc + 1)(тг/2).
FIGURE 2.1.2. x' = (t + 1) cos ж.

2.2. Linear Differential Equations of First Order
71
For other values of ж, the differential equation leads to
dx
cos ж
1, 1 + sin x
- 1П ------ ;----
2 
1 — sin x
t2
= - + t + C 
Z
1 + Sin X _ e2((f2/2)+t+C)
1 — sin x
(3)
Note again that formula (3) states the solution implicitly. The task of 
algebraically solving such a function for x = u(t) or of graphing it is often 
impossible, but qualitative solution sketching from the differential equation 
itself is quite simple and enlightening, as illustrated in Figure 2.1.2. 
▲
Notice that, as shown by Example 2.1.3, it is often far more difficult to 
graph the functions H(x) = G(t) + C produced by separation of variables 
than to treat graphically the original differential equation x' = g(t)h{x). 
The same could even be said for Example 2.1.2, for which the qualitative 
picture of Figure 2.1.1 “says it all” at a glance. Since Example 2.1.2 arises 
from a population problem, as we shall show in Section 2.5, the usual 
question is simply “what happens, for x > 0, as t oo?” The answer to 
that is immediately deducible, with minimum effort, from the qualitative 
picture.
Summary. For separation of variables, if x' — h(x)g(t\ then
1. For all xq such that h(x0) = 0, u(f) = x0 is a solution.
2. For all xq such that h(x0) ф 0, separate variables and solve by
2.2 Linear Differential Equations of First Order
If p(t) and q(f) are two continuous functions defined for a < t < 5, then 
the differential equation
x' = p(t)x + q(t) 
(4)
is the general linear first order equation. It is called homogeneous if q(t) = 
0, nonhomogeneous otherwise. For a given equation of the form (4), the 
equation
x' = p(t)x 
(5)
is called its associated homogeneous equation, or sometimes its complemen­
tary equation.

72
2. Analytic Methods
Note that if a linear first order differential equation is homogeneous, then 
it is separable; as we have seen in Example 2.1.1, the solution x = u(t) of 
equation (5) with ^(to) = x0 is
u(t) = x0e^o p(^dT.
We can use the homogeneous case to help solve the nonhomogeneous case, 
with the following theory which is basic to linear equations of all sorts, 
differential or otherwise. (Here we prove the result for linear differential 
equations.)
Theorem 2.2.1 (Solving nonhomogeneous linear equations). The 
solution to x' = p(t)x + q(t) with initial condition u(to) = xq is
'u(t) = uc(t) +up(t),
the sum of a solution up(t) of the original equation, x' = p(t)x + q(t), and 
the solution uc(t) of the associated homogeneous equation, x' = p(t)x, with 
initial condition uc(to) = xq — up(tn).
Proof. Substitute u(t) into the original differential equation. □
Since equation (6) has given us uc(t), the only other necessity for solving 
nonhomogeneous linear equations is to find a up(t). There are two famous 
methods for doing this: undetermined coefficients, which is often the easiest 
when it can be applied, and variation of parameters, which has the immense 
advantage that it will always work, although it frequently leads to rather 
unpleasant computations. We shall describe the first method here, and the 
second in the following section.
Method of Undetermined Coefficients
For some differential equations x' = p(t)x + q(t), a particular solution up(t) 
can be found by guessing a solution of the appropriate form to work with 
q(t). Let us start with three examples, for which we already know from 
Example 2.1.1 that
uc = xoefp(t)dt = Ce*.
Example 2.2 .2. x' = x + sint.
Let us try for a solution of the form up(t) = a sin t + /3 cos t. Substituting 
this “guess” into the differential equation says that we must have
a cos t — /3 sin t = a sin t + f3 cos t + sin t.
Setting equal coefficients of like terms on the left and on the right implies 
that a = /3 (from the cost terms) and —/3 = a + 1 (from the sint terms). 
This system of two nondifferential equations requires that
a = (3 = -1/2.

2.2. Linear Differential Equations of First Order 
73
Therefore we know up(t) = — | sint — | cost, and the general solution to 
the original differential equation is
u(t) = uc(t) + up{t) = Ce* — - sin t — cos t. ▲
Example 2.2 .3. x' = x +t2.
Let us try for a particular solution of the form
up(t) = a + (3t + 7t2.
To satisfy the differential equation, we must have (by substitution)
/3 + 27* = a + 0t + 7t2 +12,
and the coefficients of the constants, the linear terms, and the quadratic 
terms in turn each produce an equation; the algebraic solution of that 
system of three nondifferential equations requires that
7 = -1, /? = —2, a = -2.
Thus up(t) = — 2 — 2t — t2, and the general solution to the differential 
equation is
u(t) = uc(t) + up(t) = Ce* — 2 — 2t — t2. ▲
Example 2.2 .4. x' = x + e2t.
If you guess up(t) = ae2t and try substituting that into the differential 
equation, you will get
2ae2t = ae2t + e2t,
which implies by the coefficients of e2t that 2q = a + 1, so a = 1.
This gives a general solution to the differential equation of
u(t) = uc(t) + up(f) = Ce* + e2t. ▲
Why did these examples work? Why was it a good idea to “try” the 
functions we did? If we refer to linear algebra (Appendix L in Volume II), 
we can explain in terms of vector spaces.
Proposition 2.2.5. Consider the differential equation x' = p(t)x + q(t). 
Suppose that q(t) is an element of a finite-dimensional vector space V of 
functions f, closed under the operation
i -?(*)] /(*)• 
at

74
2. Analytic Methods
Let /1,... ,fn be a basis of the vector space. Set
x(t) = H----------1-
If this expression is substituted into the equation, the result is a system of 
n ordinary linear equations for the coefficients ai, which may or may not 
admit a solution if d/dt — p(t) is singular.
The fact that the vector space V must be closed under the operation 
f'—pf means that f eV => f' — pf eV.
In Example 2.2.2, the vector space was the space of functions a sin/; + 
/3cost, obviously closed under derivatives f —> f' — f.
In Example 2.2.3, the vector space was the space of polynomials of 
degree at most 2, closed under f —> f' — f.
In Example 2.2.4, the vector space was the space of functions ae2t, 
also closed under derivatives f f' — f.
These examples all happened to have p(t) = 1; the method proceeds in 
similar fashion according to Proposition 2.2.5 for other p(t). The following 
example illustrates how to apply the same proposition in another case.
Example 2.2 .6. txf = x +t2.
If we rewrite this equation as x' = (l/tyx+t, and try a particular solution 
of the form up(t) = a + /3t, we run into trouble because that won’t give a 
vector space closed under f —> f' — pf.
However, we can avoid such difficulty in this case if we stick with the 
equation in its original form, txf = x + t2, then focus on t2 instead of a 
strict q(t) and try for a particular solution of the form
up(t) =& + (3t + yt2.
The natural operator here is f tf' — f for a vector space to which t2 
must belong, and we otherwise proceed as above.
To solve the equation, we must have
/3t + 2yt2 = a + /3t + yt2 + t2,
which implies, by the equations for the coefficients of each type of term, 
that
o = 0, /3 = anything, 7 = 1.
Thus any function of the form t2 + /3t is a particular solution, and
u(t^ = tzc(t) = Ct ”b ^2 ”b /3t — t2 + (С* + /?)/;. A

2.2. Linear Differential Equations of First Order
75
Thus you see that the method of undetermined coefficients is somewhat 
adaptable, and further extensions will be discussed in Exercises 2.2#7 and 
8.
However, life isn’t always so easy. Even when you can find a seemingly 
appropriate vector space of functions to try the method of undetermined 
coefficients, the system of linear equations that you get for the coefficients 
may be singular, and have no solutions. This does not mean that the dif­
ferential equation has no solution, it just means that your guess was bad.
Example 2.2 .7. x' = x + e*.
A reasonable vector space is simply the 1-dimensional vector space of 
functions ae*. But substituting up(t) = ae* gives:
ael = ae* + ef, i.e. a = a + 1,
which has no solution for a, so up(t) = ae* was a bad guess. ▲
In such a case as Example 2.2.7, however, there exists another, bigger 
vector space of the form P(t)ef, for P(t) a polynomial.
Example 2.2 .8. For xf = x + ef, try substituting
up(t) = ae* +
A vector space of these functions also contains q(t) and is closed under 
f —► ff-pf. You can confirm by this method that up(t) = te* is a particular 
solution to the differential equation, and
u(t) = uc(t) + up(t) = (C + t)e*. ▲
Actually, it is only exceptionally that such a large vector space as shown 
in Example 2.2.8 is needed; it is when q(t) include solutions to the as­
sociated homogeneous equation. But those are exactly the cases that are 
important, as we shall discuss further in Volume II, Section 7.7.
The method of undetermined coefficients therefore works, by Proposition 
2.2.5, whenever you can find a sufficiently large /m^e-dimensional vector 
space closed under f ff — pf. The examples illustrate the possibilities. 
For functions q(t) that include terms like tant or Int, another method 
is necessary for finding a particular solution up(t). In the next section we 
present a method that works for any linear equation, and furthermore gives 
some real insight into linear equations.
Summary. For a first order linear differential equation x' = p(t)x 4- q(t), 
the general solution is
x — u(f) = wc(t) + up(f) — Ce^o p^dT up(t), 

76
2. Analytic Methods
where up(i) is a particular solution to the entire nonhomogeneous equation, 
and uc(t) is the solution to the associated homogeneous equation.
One way to find up(t) is by the method of undetermined coefficients, 
which uses an educated guess when q(t) is amenable. See Examples 2.2.2, 
2.2.3, 2.2.4, and 2.2.6.
Another way to find up(t), which always works, is the method of variation 
of parameters, to be presented in the next section.
2.3 Variation of Parameters
Variation of parameters is a technique that always produces a particular 
solution up(t) to a nonhomogeneous linear equation, at least if you can 
compute the necessary integrals. The idea is to
1. Assume a solution of the form of the solution (6) to the associated 
homogeneous equation (5) with the constant xq replaced by a variable v(t):
u(t) = v(t)e^o p(r)dr , 
(7)
with u(to) = u(to) — Xq.
2. Substitute in the non-homogeneous differential equation (4) the as­
sumed solution (7) prepared in the last step:
u'(t) = v'(t)e^o p^dr 4- v(t)p(t)eJ*o p^dr (differentiating (7)) 
f* p(r)dr
= p(t) v(t)eJto +q(t), (from differential equation (4))
u(t)
which, amazingly enough, we can simplify and solve for u'(t):
v'(t) = q(t)e 4 p^dT = 
(8)
3. Solve for the varying parameter v(t) using equation (8) and substitute 
in the assumed solution (7).
As a result of step (2), v(t) is a solution to the differential equation 
xf = r(t), with initial condition v(to) = Xq. Therefore
v(t) = r(s)ds + £o 
Jt0
ds “H Xq,
so, from step (1),
u(t) = e^o p^dT | £ [e ^‘o p(t)<%(s)] ds + x0 
(9)

2.3. Variation of Parameters
77
where xQ = u(to), and the entire {...} is v(t).
This procedure does indeed find solutions for the nonhomogeneous dif­
ferential equation (4), as we shall illustrate in the following example.
Example 2.3 .1. Consider x' = ax + b, with iz(to) = xo and a Ф 0.
This is not just an arbitrary example; it turns out to be an important 
differential equation that we shall use in proving theorems, as we shall see 
in Chapter 4. (Notice that this equation is in fact separable as well as 
linear, so in Exercise 2.1#lc you solve the same equation by separation of 
variables.)
The homogeneous equation is again the equation of Example 2.1.1, so:
1) . Assume iz(t) = v(t)e^fo adr — v(t)ea(t-t°).
2) . Substitution in the differential equation gives
+ av(t)ea^-t0^ = avifye^-^ + b 
v'(t) ~ be~a^~to\ with v(to) = x0.
3) . Solving for v(t) gives
=-( - )e-a^4°)+c, with c = xq + -, 
\а/ 
a
so from step 1),
u(t) =
ea(t-t0)
The method of variation of parameters always works in the same way, 
so you can bypass the steps and simply write the results directly as equa­
tion (9). We shall revisit the equation of Example 2.3.1 and illustrate this 
method.
Example 2.3 .2. Consider again xf = ax + b, with u(to) = xq. In this case,
eJtoaar = ea(t-t0)^
so
u(t) = 
£ be-^-^ds + zo)
_ 1] _|_ XQ
-b 
a
_ ea(t-t0)
= a:oea(t-to) + ( - ) [ео(*-‘о) - 1]. ▲

78
2. Analytic Methods
So far we have obtained equation (9) as one solution to the differential 
equation (4) with initial condition x(to) = xq. In Exercise 2.2-2.3#4 you 
can verify that substitution of (9) in (4) confirms its status as a solution.
We have therefore existence of solutions for every initial condition (to, #o)- 
In Section 4.5 we shall prove uniqueness of this solution, so formula (9) is 
in fact the complete solution to (4).
It is well worth your effort to learn this equation (9), or at least to 
understand it thoroughly. The summary at the end of this section and 
Section 2.4 should both give additional insight.
But first let us proceed with another example.
Example
First,
л 
~ 
.1 
/ ~x +sm^
2.3.3. Consider x =-------------
t
p(t) =
so
•t
It = In — , 
t
and
?(*) = T sint.
So now we are ready to use equation (9):
sin s ds + xq
sin s ds + Xq
— COS t) + Xq
= | { — cos t + cos tp + ffptp }.
c
The constant terms can be grouped together, so there is indeed only 
the one constant overall that is expected in the solution to a first order 
differential equation. ▲
We close this section with a summary of the equations.
Summary. For a first order linear equation x' = p(t)x + g(t), the solution 
with u(to) = xq can be written, by variation of parameters, as
u(t) = e^o p^dr | y* ft0 g(s)] ds | Xq I.
(9, again)

2.4. Bank Accounts and Linear Differential Equations
79
If you first compute
(10)
then the solution with u(to) = xq can be written more simply as
u(t) = 
w(s,to)q(s)ds + Xq
(11)
The quantity w(s,t) defined by equation (10) is sufficiently important 
to the solutions to linear differential equations that it is called the fun­
damental solution. The next section give a real world interpretation for 
the fundamental solution; the concept will later be generalized to higher 
dimensions in Volume II, Chapter 12.
2.4 Bank Accounts and Linear Differential 
Equations
One “universal” way of “understanding” the linear differential equation (4) 
xf = p(t)x + q(t)
is to consider the solutions as the value of a bank account, having a variable 
interest rate p(t); q(t) represents rate of deposits (positive values) and with­
drawals (negative values). Note that the equation representing the value of 
such an account will only be a linear equation if:
1. the interest rate depends only on time, not on the amount deposited, 
and
2. the deposits and withdrawals are also functions of time only, and do 
not depend on the value of the account.
Despite their complication, the equations (9) and (11) for solution of a 
linear first order equation can be understood rather well in these terms. 
Observe that these formulas write the solution as the sum of two terms.
In terms of bank accounts, this is simply the following rather obvious 
fact: the value of your deposit is the sum of what is due to your initial 
deposit, and what is due to subsequent deposits and withdrawals.
We next ask what is represented by the fundamental solution
w(s,t) = e$° p^dr 
(io, again)
defined at the end of the last section?

80
2. Analytic Methods
Recall (Example 2.1.1) that the solutions to the associated homogeneous 
equation xf = p(f)x are
uc(f) = XQe^P^dT = xQw(tQ,t).
Thus w(s,t) has the property that w(s, £)<?($) is the solution, at time t 
of xf = p(t)x with initial condition q(s) at time s.
In terms of bank accounts, w(s, t) measures the value at time t of a unit 
deposit at time s. This is meaningful for all (s, t): If s < t, w(s, t) measures 
the value at time t of a unit deposit at time s. If s > t, w(s,t) measures 
the deposit at time t which would have the value of one unit at time s.
We are now ready to interpret the “bank account equation” in two ways:
1. One possibility is to update each deposit, xq and g(s), forward to 
time t. Then
u(t) = t)xn + w(s, t)q(s)ds.
V 
*^0
uc
(12)
To see that equation (12) is another possible form of (11), notice that
w(s, t)w(to, s) — w(to, t),
from equation (10). Therefore
w(s, t) = w(to, t)(w(to,«)) 1
and we can factor w(to? t) out of the integral in the last term of (12).
2. Another possibility is to date every deposit backward to time to? such 
that the total deposit at time to should have been
жо+ w(s,tQ)q(s)ds 
J to
in order to give the same amount at time t as before, i.e.,
u(t) = w(to, t) xQ + w(s,tQ}q(s)ds 
. Jto 
.
which is exactly the equation (11). Thus the idea of equations (9) and 
(11) resulting from variation of parameters is to take all the deposits 
and withdrawals that are described by the function g(t), and for each 
one to figure out how large a deposit (or overdraft) at to would have 
been required to duplicate the transaction when it occurs at time s.

2.5. Population Models
81
You have to be careful before you add sums of money payable at different 
times. The idea is to avoid adding apples and oranges. Lottery winners 
are well aware of this problem. The states running the lotteries say that 
the grand prize is $2 million, but what they mean is that the winner gets 
$100,000 a year for 20 years. They are adding apples and oranges, or rather 
money payable today to money payable 10 or 15 years hence, neglecting 
the factor w(s, t) by which it must be multiplied.
Example 2.4.1. If interest rates are to be constant at 10% during the next 
20 years, how much did a winner of a $2 million dollar lottery prize win, in 
terms of today’s dollars, assuming that the money is paid out continuously? 
(If the payoff was not continuous, you would use a sum at the end instead 
of an integral; however the continuous assumption gives results close to 
that for a weekly or monthly payoff, which is reasonable.)
Solution. Use one year as the unit of time. The relevant function w(s, t) 
is
w(s,t) = e0-1 (*-«).
Let xq be the desired initial sum equivalent to the prize. Then an account 
started with xq dollars, from which $100,000 is withdrawn every year for 
twenty years will have nothing left, i.e.
e('1)(20) ((-100,000) e-01 Sds + zq) = 0.
This gives, after integrating,
x0 = (100,000)/(0.1) (1 - e“2) « 864,664.
So, although the winner will still get $100,000 a year for 20 years, the 
organizers only had to put rather less than half of their $2 million claim 
into their bank account. ▲
2.5 Population Models
The growth of a population under various conditions is a never-ending 
source of differential equations. Let ^V(t) denote the size of the population 
at time t (a common notation among biologists). Then the very simplest 
possible model for growth is that of a constant per capita growth rate:
Example 2.5 .1. If a population N(t) grows at a constant per capita rate, 
then
1 dN
——— = r = constant. 
(13)
N dt

82
2. Analytic Methods
The constant r is called a fertility coefficient. This equation can be rewritten 
as N' = rTV; the techniques of Chapter 1 and Section 2.1 give the following 
picture, Figure 2.5.1, and the solution N = NQert.
FIGURE 2.5.1. N' = rN.
The effect of r (for r > 0) is to increase the slope of solutions as r 
increases; for a fixed r the equation is autonomous and the solutions are 
horizontal translates. For this problem only the top half-plane is relevant, 
since a negative population has no meaning. ▲
Obviously the model of Example 2.5.1 is too simple to describe for long 
the growth of any real population; other factors will come into play.
Example 2.5 .2. Consider what happens when the basic assumption of 
Example 2.1.1, as formulated in (13), is modified to show a decrease in 
per capita rate of growth due to crowding or competition that is directly 
proportional to N:
Then we can write equation (14) as N' = rN — bN2. This is the logistic 
equation, which we have already solved in Example 2.1.2; the solutions are 
shown again in Figure 2.5.2. (Note that for a real world population problem 
we are only interested in positive values of TV.) See Exercise 2.4-2.5#3 for 
support of this model from U.S. Census data. ▲

2.5. Population Models
83
There are many variations on the theme of population models. One of 
these describes the equations of Exercise 1.1# 12. The form of those equa­
tions is discussed as follows:
Example 2.5 .3. Consider
x' = (2 -I- cost)# - x2 + a(t). 
(15)
This equation does not fall under the classes amenable to our analytic 
methods, but it represents a more complicated population model that we 
would like to discuss and show what you can do with it by the qualitative 
methods of Chapter 1.
A reasonable interpretation of equation (15) is that x represents the size 
of a population that varies with time. Therefore on the right-hand side, the 
first term represents unfettered growth as in Example 2.5.1, with a fertility 
coefficient (2+cost) that varies seasonally but always remains positive. The 
second term represents competition as in Example 2.5.2, with competition 
factor (|). The third term a(t) is a function with respect to time alone, 
representing changes to the population that do not depend directly on its 
present size; e.g., positive values could represent stocking, as in stocking a 
lake with salmon; negative values might represent hunting, in a situation 
where there is plenty of prey for all hunters who come, or some fixed number 
of (presumably successful) hunting licenses are issued each year.

84
2. Analytic Methods
In Exercise 1.1 #12, the values of the function a(t) were constant, at — 1 
and —2, respectively. The interesting results are shown in Figure 2.5.3.
x* = (2 + cos t) x - (1/2) x2 -1 
x* = (2 + cos t) x - (1/2) x2 - 2
FIGURE 2.5.3.
In the first case there are exactly two periodic solutions: there exists a 
funnel around the upper one, which is stable, and an antifunnel around 
the lower one, which is unstable. In the second case, there are no periodic 
solutions: every solution dives off to negative infinity, so no stability is 
possible. Furthermore, as a population size, x = u(t) < 0 makes no sense, 
so note that once u(t) reaches zero, a population is extinct; harvesting was 
too severe for the population to survive.
A good question for further exploration is “what value of the constant 
a will separate these two behaviors?” You can use the computer program 
DiffEq to explore this question; the accuracy of your results is limited only 
by your patience and perseverance (to the limits of the accuracy of the 
computer!). ▲
A population model could be helpful in understanding the linear first 
order differential equation (4) of Section 2.2. In that case, p(t) is a fertility 
rate, and q(t) represents stocking (positive values) or hunting (negative 
values). Again, the equation is linear only if there is to be no decrease in 
fertility due to competition, for instance, and the number killed by hunting 
must depend on factors other than the actual population. If you are more 
interested in populations than in bank accounts, you could reread Section 
2.4 from this point of view.
The word “population” may be interpreted rather broadly to include 
other growth models, and decay models as well. The case of radioactive 
decay is a particularly famous one; the equation is basically the same as 
(13) in the simplest growth model, with a negative rather than positive 
coefficient.

2.6. Exact Differential Equations
85
The rate of disintegration of a radioactive substance is propor­
tional at any instant to the amount of the substance present.
Example 2.5 .4. Carbon dating. The carbon in living matter contains a 
minute proportion of the radioactive isotope C14. This radiocarbon arises 
from cosmic ray bombardment in the upper atmosphere and enters living 
systems by exchange processes, reaching an equilibrium concentration in 
these organisms. This means that in living matter, the amount of C14 is in 
constant ratio to the amount of the stable isotope C12. After the death of 
an organism, exchange stops, and the radiocarbon decreases at the rate of 
one part in 8000 per year.
Therefore carbon dating enables calculation of the moment when an 
organism died, and we set t = the number of years after death. Then 
x(t) = ratio of C14 to C12 satisfies the differential equation
, 1 
x =-------- x,
8000 ’
so, from Example 2.1.1, 
x(t) = x0e~t/800°. 
(16)
A sample use of equation (16) is the following:
Human skeletal fragments showing ancient Neanderthal char­
acteristics are found in a Palestinian cave and are brought to a 
laboratory for carbon dating. Analysis shows that the propor­
tion of C14 to C12 is only 6.24% of the value in living tissue. 
How long ago did this person live?
We are asked to find t when x = 0.0624жд. From equation (16), 
0.0624жо = xoe_f/8000, so
t = -8000 In 0.0624 « 22,400 years,
the number of years before the analysis that death occurred. ▲
Example 2.5.4 discusses a differential equation with a unique solution 
through any set of initial conditions; it illustrates just one of the many 
ways in which this might be done. We shall go on in Chapter 4, in Example 
4.2.3, to study the question of uniqueness and will contrast the behavior of 
this example with another situation.
2.6 Exact Differential Equations
Consider F(t, x), an arbitrary function in two variables, which describes a 
surface in 3-space if you set z = F(t,x). The level curves of this surface 

86
2. Analytic Methods
are the functions given implicitly by
F(t,x) = C. 
(17)
Differentiating equation (17) implicitly gives the following differential 
equation satisfied by these level curves:
dF 
dF
~d^^t,x>)x' + ~dt^’x>> = 0’ 
(18)
and we can rewrite equation (18) as
M(t, x)xf + L(t, x) = 0. 
(19)
We now observe that we might use equation (19) to work the above 
argument backwards. This will be possible for some differential equations, 
which are then called exact differential equations. In order to do this, we 
need the following theorem.
Theorem 2.6.1 (Condition for exactness). Consider a differential 
equation written in the form
M(t, x)xr + L(t, x) = 0,
with Mft, x) and L(t,x) two suitably differentiable functions defined in a 
rectangle R = [a, b] x [c, d\. Then there exists a function F(t,x) such that
dF 
dF
■7— = M(t, x) and -7— = L(t, x) 
dx 
dt
if and only if
(20) 
dt dx
Proof. The necessity of condition (20) is due, from multivariable calculus, 
to the equality of crossed partials (if these functions are continuous):
= *L. 
(a)
dxdt dtdx
To prove the sufficiency of condition (20), pick some point (to,xo) in R 
and set 
t 
x
F(t,x)= /* L(r,XQ)dr+ f M(t,(r)d(T. 
(22)
J to 
J xo
Then
— =M(t,x),

2.6. Exact Differential Equations
87
and
— = L(t, x0) + I a)da
fx 9L
= L(t,x0)+ / — (t,a)da 
Jxq vX
= L(t, з?о) + Ь(Л #) “ 
^o)
= L(t,x). 
□
Remark. The sufficiency derivation in this proof may seem a bit miracu­
lous and the choice of F unmotivated. It is closely related to the statement 
that a force field is conservative, in a simply connected region, if its curl 
vanishes there. A detailed explanation will appear in a related volume on 
differential forms, as a special case of Poincare’s Lemma.
The theoretical procedure for solving an exact differential equation is 
very simple: If Theorem 2.6.1 is satisfied, then the solutions are the func­
tions x = u(t) defined implicitly by F(t, x) = C for various constants C. 
The process of actually finding F(t, x) is illustrated in the following three 
examples.
Example 2.6 .2. Consider xf = 
(which is not defined along the
at И- bx
line at at + bx = 0), which can be written as (at + bx)xf + (ax — ct) = 0. 
This is an exact equation because dM/dt = dL/dx = a. The fact that
M(t, x) = at + bx = dF/dx
implies that
Fm(^^) = atx + 
+ V’W» 
(23)
and the fact that
dF
L(t, x) = ax — ct = — 
ot 
implies that
Fl&x) = axt - 
+ <X^). 
(24)
Equations (23) and (24) can be reconciled to give
F(t,x) = axt + Q)b®2- (|)ct2.
Therefore the solution to this exact differential equation is
F(t,x) = axt + (^)fo2 - (^)ct2 =

88
2. Analytic Methods
which is a description for
hyperbolas, 
if a2 + be > 0;
ellipses, 
if a2 + be < 0.
These observations are confirmed by Figure 2.6.1.
FIGURE 2.6.1. x' = on left, x' = on right. 
v'^'CC 
' 
v“"‘CC
You will find if you try to draw solutions in these direction fields using the 
computer program DiffEq that there may be trouble, as shown in Figure 
2.6.2, wherever the slope gets vertical—precisely along the lines where the 
differential equation is not defined.
This is to be expected, because the solutions to a differential equation are 
functions. What appears to be a hyperbola or an ellipse in the slope field 

2.6. Exact Differential Equations
89
is, as shown in Figure 2.6.3, actually two completely different solutions; one 
above and one below the line where the differential equation is not defined.
FIGURE 2.6.3. Solutions do not cross lines where slopes are undefined.
Example 2.6 .3. Consider
(tcosx + Sx2)xl + sin ж + 2t = 0,
(undefined along tcosrc + Зж2 = 0).
You can confirm that dM/dt = dL/dx. Then the fact that
dF
M(t, x) = t cos x + 3a?2 = ox
implies that
= tsina?-ha?3-h^(t), 
(25)
and the fact that
dF
L(t, x) = sin x -I- 2t = —- 
ot 
implies that
FL(t,x) = tsinrr + t2 + <p(a?). 
(26)
The reconciliation of equations (25) and (26) tells us that
F(t, x) = t sin x + x3 +t2, 
(27)
so the solution to this exact differential equation is
F(t, x) = t sin x + a?3 -I-12 = C.

90
2. Analytic Methods
It is hard to imagine the surface z = F(t, x) described by equation (27) 
standing alone, but using the computer we can easily get a lovely picture of 
its level curves from the direction field and solution curves of the differential 
equation, illustrated in Figure 2.6.4.
Recall that this differential equation is undefined where t cos ж + Зя2 = 0. 
The “level curves” drawn in this direction field are sometimes composed of 
two or three functions, where they cross this locus. In Figure 2.6.4 the third 
function is drawn thicker. (As suggested in the MacMath Documentation 
under DiffEq, you can enter the differential equation as a system of two 
equations, to be further discussed in Section 6.1, to get good computer 
drawings of these level curves.) ▲
The illustration of Example 2.6.3 and Figure 2.6.4 demonstrates a tech­
nique that is useful for helping to visualize the surface described by any 
particular ugly F(t,x): follow the steps of equations (17) through (19), 
and then use the computer to get the direction field on which the solution 
curves of the differential equation are the level curves of the surface.

2.6. Exact Differential Equations 
91
The geometrical interpretation of solutions to exact equations as level 
curves of a function F(t, x) will be very useful in other ways, as you will 
begin to see in Volume II, Chapter 6.
Example 2.6 .4. All separable equations, x' = g(t)h(x), can be written in 
exact form. They can be written as
ssf' “s(1) = °’
and we see that
ot \ h\X) J ox
Therefore, from (22), the solutions are along the level curves of
F(t,x) = [ g(r)dr — f
Jt0 
Jxo
or G(t) — H(x) = — C according to the notation of Section 2.1. A
There is also a relationship between linear equations and exact equa­
tions, which you will explore in Exercise 2.6#2, but perhaps this analogy 
is misleading. Linear and exact equations generalize to higher dimensions 
in completely different ways. The linear equations are the beginning of the 
entire theory of ordinary differential equations, but the exact equations are 
historical tricks that work only for one-dimensional ordinary differential 
equations and don’t generalize at all to the higher dimensional theory of 
ODE’s. (Basically the linear equations generalize to the theory studied in 
Volume II, Chapters 6 and 7 with linear algebra, whereas the exact equa­
tions generalize to total partial differential equations, the Poincare Lemma, 
and the Frobenius Theorem, which occur in the study of differential forms.)
On a more immediately practical level, you should notice that for both 
separable and exact differential equations, the solutions are implicit, and 
more often than not are therefore inconvenient or impossible to solve ex­
plicitly for x = u(t). The linear equations, on the other hand, have the 
advantage of producing explicit solutions, as given by the famous formula 
(9).
Summary. An equation of the form M(t, x)x' + L(t, x) = 0 is exact if
dM dL 
dt dx
If the equation is exact, the solutions are expressed implicitly by
F(t,x) = C,

92
2. Analytic Methods
where
dF 
OF
— — M(t, x) and — = L(t, x). 
ox 
ot
The procedure for finding F(t,x} is as follows:
Holding t constant, integrate M with respect to x, to get Fm (re­
membering that a “constant” in x might be a function in t);
Holding x constant, integrate L with respect to t, to get Fl (remem­
bering that a “constant” in t might be a function in ж);
Finally, reconcile the results of these last two computations to give 
an F(t,x) that satisfies both Fm and Fl-
2.7 Series Solutions of Differential Equations
The method of undetermined coefficients introduced in Section 2.2, can 
also be used to compute power series solutions of the differential equation 
xf = f(t,x), at least if the function f(t,x) is itself given by a power series.
More precisely, suppose we are looking for a solution u(t) such that 
iz(to) = xq. Then set
u(t) = Xq + Ui(t — to) + a2(t ~~ to)2 + • • •
and substitute into the equation. You will then be able to calculate the di 
recursively.
Example 2.7 .1. Find the power series of the solution to x1 = x with 
ж(0) = 1.
Set
x(t) = 1 + d±t + fl2t2 + d^t3 + ... j
then substitution gives
di + 2d2t + 3a3t2 + ... = 1 + d±t + U2t2 + d^t3 + ...
and identifying the coefficients of successive powers of t gives
ai = 1, 
2d2 = fli, i.e. d2 = 1/2 
3a3 = U2? i.e. a3 = 1/6, etc.
Actually, we see that ndn = an-i, so since ai = 1 we get dn = l/(n!), and
x(i)

2.7. Series Solutions of Differential Equations
93
which you should recognize as the power series defining e*. 
▲
The fact that made the computation of Example 2.7.1 work is that each 
coefficient appeared on the left with a lower power of t than it did on the 
right, and as such could be computed from the lower coefficients, which 
had already been computed. This fact is always true, but for nonlinear 
equations it is usually impossible to find a formula for the coefficients. See, 
for example, Exercise 2.7#4.
Example 2.7 .2. Find the power series of the solution of xf = x2 — t, the 
famous equation of Chapter 1, with ж(0) = 1. As in the last example, set
x(t) = 1 + a^t + azt2 + 
+ ...
and substitute. You find
ai 4~ 2azt 4~ 3«з£2 4~... — (1 Я- a\t 4~ azt^ 4~ a^t^ 4~ .. ,)2 — t 
= 1 4~ (2&1 — l)t 4~ (a2 4- 2az)t2
4- (2aiaz 4- 2аз)£3 4- ... ,
leading to
ai = 1 
=> ai = 1
2az = 2ai — 1 
=> 
«2 = 1/2
3a3 = a2 4- 2az 
=> 
«3 = 2/3
4a4 = 2aia2 + 2a3 => a4 = 7/12
Clearly this process could be continued ad infinitum with sufficient ef­
fort; on the other hand, no simple pattern seems to be emerging for the 
coefficients. In particular, it is not clear what the radius of convergence of 
the series is, or even whether the series converges at all. ▲
In the sense of not knowing a radius of convergence, power series so­
lutions are usually pretty useless: unless you know something about how 
they converge, they convey next to no information, and it is usually hard to 
know anything about the convergence unless a pattern for the coefficients 
emerges. A recognizable pattern will usually exist only for linear equations.
In Example 2.7.2, the radius of convergence of the power series cannot be 
infinite, and in fact must be smaller than 2, because we know from Exercise 
1.6#5 that the solution blows up before t = 2. This means that the series
S2nan
diverges, but we do not know how to show this from looking at the recursive 
definition of the coefficients; we can show it only from fence techniques.

94
2. Analytic Methods
The following theorem, which we will not prove (the proof is rather de­
vious, requiring Picard iteration and complex analytic functions) says that 
in fact the power series does converge.
Theorem 2.7.3. If the function f(t, x) is the sum of a convergent power 
series in a neighborhood of (to, xq), then substituting the series
x(f) = a?o + E Gn(* - *o)n
into the differential equation x' = /(t,a?) and equating equal powers oft 
leads to a unique power series, which does converge for t in some neighbor­
hood oftQ, and whose sum is the solution to x' — f(t,x) with x(to) = xq.
Notice the condition that f be given by a power series. If this condition 
is not satisfied, it may be quite possible to determine a power series solution 
of x' — f(t, x) with radius of convergence 0. This will be shown by the next 
example; it is rather lengthy, but carries an important explanation.
Example 2.7.4. Consider the differential equation xf = t2/х — 1, which 
is singular (i.e., not well defined) if x = 0. Determine the power series 
solutions with ж(0) = 0.
As above, set
z(t) = a±t + a2t2 + a$t3 + a^t4 + ...
and substitute in the equation, which tie will write x(xf +1) = t2. We find
{o^t + 
-|- a^t4 + .. .)((1 + ui) + 2a2t + Sa^t2 + ...)== t2. (28)
In particular, ai(l + ai) = 0, leading to two solutions, one with a± = 0 and 
the other with a± = — 1. It turns out that these behave very differently, so 
we will discuss them separately.
(i) The solution with a± = 0. In this case, equation (28) becomes 
(c&2^ H" a^t3 + a^t4 + .. .)(1 + 2a2t + 3ast2 + Aa^t3 + ...) = t2 
and identifying coefficients on both sides of the equation leads to
U2 = 1 
=> a2 = 1
Q3 -|- 2а/ = 0 
U3 —■: 2
a4 + 2азй2 + Загаз = 0 
=> 04 = 10
U5 + 20402 + 3a32 -|- 4U4Q2 = 0 
&5 =: 72
The coefficients appear to alternate in sign, and to grow rather rapidly. 
The following result makes this precise:
The coefficient an has the sign of (-l)n, and \an| > (n - 1)!.

2.7. Series Solutions of Differential Equations
95
The proof is by induction, and this induction is started above. The recursion 
formula can be written
n/2
(Zn — 
“1“ 1) 
i
i=2
if n is even,
(n-l)/2
(Zn = 
“I” 1) 
2
i=2
— (a(n+i)/2)2 if n is odd.
In both cases, the induction hypothesis implies that all the terms on the 
righthand side have the same sign, negative if n is even and positive if n 
is odd, and the factor —(n + 1) then shows that an is positive if n is even 
and negative if n is odd, as claimed. Moreover, since there is no cancelling, 
we have
\an\ > (n + 1)\a2an_11 > (n + l)(n - 2)! > (n - 1)!.
In particular, the power series under discussion has radius of convergence 
equal to 0. The line above gives
an—1
> n -I- 1,
and the result follows from the ratio test.
Just because a power series solves a differential equation does 
not mean that it represents a solution!
(ii) The solution with a± = —1. In this case, equation (28) becomes 
(—t -|- a2t2 Я- a^t3 -|- a^t4 4-.. .)(2a2t 4~ Заз£2 4~ 
,.,) = t2,
which leads to
—2a2 = 1 
=> a2 = —1/2
—Заз 4- 2а2 — 0 
=> аз = 1/6
—4а4 + Заза2 4- 2а2аз = 0 
=> а< 
= —5/48
—5а5 + 4а4а2 + За2 + 2а2а± = 0 => 
а$ = 19/240
This time the coefficients still alternate, but appear to be decreasing in 
absolute value. This is misleading, because the coefficients eventually start 
growing, and the 100th is about 1300. Still, they don’t grow too fast, but 
the proof is pretty tricky. We show that the coefficient an has the sign of 
(_l)n+i, and, for n > 2, |an| < 2n-1/n2, proceeding again by induction.

96
2. Analytic Methods
To start, notice that it is true for <12- Next, observe that the recurrence 
relation for computing the coefficients can be written
-----
n/2
2 aian+l—i
i=2
— n +1
n
if n is even,
(n-l)/2
i—2
П + 1 
2n
(a(n+l)/2)2
if n is odd.
It follows as above that if the signs of the аг alternate for г < n, then all 
terms on the right-hand side have the same sign, which is positive if n is 
odd and negative if n is even. So far, so good. Now, by induction, we find 
if n is even that
1 1 n/2
П + 1 / 
|ага'п+1—i] <
г=2
|®n| —
4(n + 1) 
A’-2 Ara-’-1
n 
i2 (n + 1 — г)2
. n/2 .
4Лп-з2±1 у 1__ 1__ .
n 
i2 (n + 1 - г)2
i=2 
4 
z
We need to bound the sum, and this is done by comparison with an 
integral, as follows:
n/2V-___ -___
i2 (n + 1 — г)2
i=2 v 
z
The first inequality is the one that needs verifying, but we leave it to the 
reader. This bound leads to
2An~2 /_2 n + 1 /____1 
2 ln(n - 1)
n2 \A п у n — 1 n
and we see that in order for the induction to work, it is necessary that A 
be chosen sufficiently large so that the quantity in the outer parentheses 
will have absolute value smaller than 1. This is certainly possible, since the 
sequence in the inner parentheses tends to 1 as n —> oo, and hence has a 
finite upper bound. It is not too hard to show, using a bit of calculus, that 

2.7. Series Solutions of Differential Equations
97
A = 4 actually works. Using the computer program Analyzer, for instance, 
you can show that the greatest value for
/ ж + 1 /1 
+ 2 1п(ж - 1)
\ X \ x — 1 
X
occurs for 5 < x < 6, and that the maximum у is less than 1.57. So the 
smallest A for which the argument will work is approximately 3.14.
The argument for n odd is exactly similar: we find
1 
1 
1 
1 
Г1'2 dx
i2 (n + 1 — i)2 
2 ((n + l)/2)2 < Jr x2(n — x)2 ’
with the last part of the integral, from (n —1)/2 to n/2, bounding the extra 
term in the sum.
In particular, the radius of convergence of this power series is at least 1/A, 
and this power series does represent a solution to the differential equation.
FIGURE 2.7.1. x' = ±--l.

98
2. Analytic Methods
This drastic difference in the behavior of the power series for the two 
different values of requires a bit of explanation. The solutions to the 
differential equation appear in Figure 2.7.1.
In particular, there actually is a unique solution to the differential equa­
tion through (0,0) with slope —1, but with slope 0 there is a unique solution 
to the right of 0, which is continued on the left by a whole “pony tail” of 
solutions. ▲
Pony Tail Behavior and Power Series
If the first power series of Example 2.7.4 had converged, it would have 
picked out one hair of this pony tail as prettier than the others, namely the 
sum of the power series. So a reasonable philosophy is that unless there is 
a good reason to think that a power series represents something particular, 
there is good reason to worry about its convergence. In this case, there is 
no reason for one solution in the pony tail to be distinguished.
You might wonder, considering that the first power series of Example 
2.7.4 does not converge, whether it means anything at all, and it does. It is 
the asymptotic development of the solution to the right, and also of all the 
solutions to the left, which all have the same asymptotic expansion. See 
the Appendix for discussion of asymptotic development.
Exercise 2.7#6 shows that this sort of phenomenon, where multiple so­
lutions are represented by a nonconvergent power series, often occurs in 
funnels. Solutions in funnels often share an asymptotic expansion, which 
does not converge.
The drawing with the pony tail is one that we will return to in Volume 
II, Chapter 9 on bifurcations of planar vector fields; we will see that it is 
quite typical of saddle-node bifurcations.
Summary. A differential equation may have a power series solution of the 
form
u(t) = xq 4- a±(t — to) 4- &2(t ~~ to)2 4-... ,
if you can substitute this expression into the differential equation and 
equate coefficients of like terms to evaluate the a^s.
You must, however, check to see where (if anywhere) the resulting series 
converges.
Exercises 2.1 Separable Equations
Go back and solve by separation of variables wherever possible:
(a) the equation xf = — tx of Example 1.1.1;
(b) the equation xf = kxQ of Section 1.6;

Exercises
99
(c) the equation x' = ax + 6, a very basic equation that you will meet in 
Example 2.2.2 and again in Chapters 3 and 4;
(d) the appropriate parts of Exercises 1.1#1,2,3.
2.1#2°. Solve these differential equations; then use either Analyzer or Diff- 
Eq to get a picture of the solutions:
(a) (1 + x)tx' + (1 — t)x = 0
(b) (1 + ж) — (1 — f)x' = 0
(c) (t2 — xt2)xf + x2 + tx2 = 0
(d) 
(ж — a) + t2x' = 0
(e) 
ж — (t2 — а2)ж' = 0
/гa ch __ 1 + ж2
U dt ~ TTF
(g) sec2 0 tan p d0 + sec2 p tan 0 dp = 0
(h) sec2 0 tan p dp + sec2 p tan 0 d0 = 0
(i) 
Зе* tan ж dt + (1 — e*) sec2 ж dx = 0
(j) (t — x2t)dt + (ж — t2x)dx = 0
See also Exercises 2, Miscellaneous Problems, at the end of the chapter.
Exercises 2.2-2.3 Linear Equations
2.2—2.3# 1. Go back and solve as linear first order differential equations 
wherever possible.
(a) the equation ж' = 2t — ж of Example 1.1.3
(b) the appropriate parts of Exercises 1.1#1,2,3.
2.2— 2.3#2. Go back and solve all those equations that are either separable 
or linear in the following:
(a)-(h) the parts of Exercises 1.5#1.
2.2— 2.3#3. Solve ж' = e* — ж and explain, for different possible values 
of the constant of integration, what happens when t —> oo. Compare your 
results by matching computer drawings (held up to the light) from Analyzer 
(for the equations of the solutions with different values of c) and DiffEq 
(for the direction field and some sample solutions).
2.2— 2.3#4°. Solve the following linear differential equations; then use An­
alyzer or DiffEq to get a picture of the solutions:

100
2. Analytic Methods
(a) x' - —— = (t + l)2 
ь I 1
/IX / 
4” 1
(b) —
(c) (t - t3)xf + (2t2 - 1)ж - at3 = 0
/J4 dx 
1
(d) + ж cost = - sin2t
dt 
2
(e) xf — ^х = e*tn
(f) x H—x = —
v 7 t tn
/ A , 
1
(g) x + X = — 
e*
Z1 4 , 
1 - 2t
(h) x 4--- -r— x -1 = 0
tz
2.2-2.3#5. Verify by direct substitution in the linear differential equation 
x' = p(t)x + q(t) that the boxed formula (9) is a solution.
2.2-2.3#6. Another traditional approach to the linear differential equa­
tion and its solution is as follows: rewrite equation (4) as
- p(t)x = q(t).
Multiply both sides of this equation by e~ J p^dr which is called an 
integrating factor because of what happens next.
The left side of the new equation becomes 
e-fp(r)dr 
=
( dt j dt
Now it is easy to integrate both sides of the new equation with respect 
to t. Finish this process now to arrive at formula (9). You may work with 
indefinite integrals, but notice where the constant of integration will come 
in. This method actually comes under the umbrella of exact equations, the 
subject of Section 2.6; see Exercises 2.6#2.
2.2— 2.3#7. The examples of Section 2.2 are all linear equations (which, 
after all, is the subject of this section). Nevertheless, this is a good place to 
remark that there are certain nonlinear equations to which the method of 
undetermined coefficients can also be applied to give a particular solution. 
For instance, consider
(t — l)x" + (xf)2 + X = t2,

Exercises
101
for which the natural operator on x is
The quantity t2 appears on the right, and we see that on the left if we 
assume a quadratic polynomial, the highest power of t will also be 2. So 
assume a particular solution of the form
up(t) = a + (3t + 7^2,
and proceed to substitute in the original differential equation; then set up 
equations for the coefficients of the powers of x. You will obtain a system 
of nonlinear equations in a, /3, and 7, you will find it can be solved. The 
interesting thing is that there will be different values possible for 7, each 
giving rise to a different particular solution. However, since the differential 
equation is not linear, you will not be able to superpose these particular 
solutions, so the method of undetermined coefficients will not be much help 
at getting a general solution.
2.2— 2.3#8°. Try the method of undetermined coefficients, as in the last 
exercise, on the following differential equation:
(i2 + l)?4(/)2 + h = ^
Discuss the effect of the parameter k.
2.2— 2.3#9. Bernoulli equations are those of the form
x' + P(t)x = Q(t)xn,
where P(t) and Q(t) are continuous functions of t and 0, 
1. These
are important because they can be reduced to linear equations in z(t) by 
a substitution z = x~n+1. Show that this is true.
2.2— 2.3#10°. Use the substitution suggested in the last exercise to solve 
the following Bernoulli equations by transforming them to linear differential 
equations. Use Analyzer or DiffEq to get a picture of the solutions.
(a) x’ + tx = t3x3
(b) (1 — t2)xf — tx — atx2 = 0
(с) Зя2я/ — ax3 — t — 1 = 0
(d) x — x' cos t = x2 cos t(1 — sin t)
See also Exercises 2, Miscellaneous Problems, at the end of the chapter.

102
2. Analytic Methods
Exercises 2.4-2.5 Models Requiring Differential 
Equations
2.4—2.5#1. What rate of interest payable annually is equivalent to 6% 
continuously compounded?
2.4-2.5#2°. Consider the differential equation
dx 
v 1 n .
— = (2 + cost)# — -xz + a(t) 
di/
in the special case where a(t) is a constant (as in Example 2.5.2).
(a) Show that if x = u(t) is a solution then x = u(t + 2%) is also a 
solution.
(b) For a = — 1 there are exactly two periodic solutions,
Ui(t + 2tt) = Ui(t), i = 1,2.
Draw in a funnel around the upper periodic solution and an antifunnel 
around the lower one.
(c) There is no periodic solution for a = —2. By experimenting with the 
computer in the program DiffEq, find the value (to two significant 
digits) of the constant a between —1 and —2, which separates the 
two behaviors: there exists a periodic solution and there is no periodic 
solution.
2.4—2.5#3.
(a) Consider the equation 7V' = rN(1 — ^), which is a form of the logistic 
equation discussed in Example 2.5.1 and solved in Example 2.1.2. 
Confirm that one way of writing the solution is
(b) The formula of part (a) was used successfully by R.L. Pearl and L.J. 
Read (Proceedings of the National Academy of Sciences, 1920, p. 275) 
to demonstrate a rather good fit with the population data of the 
United States gathered in the decennial census from 1790 to 1910. 
Using 1790, 1850, and 1910 as the points by which to evaluate the 
constants they obtained
197,273,000
N =------ ------ ■-------
iy 1 _|_ e-0.03133951
and then calculated a predicted 7V(t) for each of the decades between, 
to compare with the census figures. The results are given in the table, 
with four more decades added by the Dartmouth College Writing 
Group in 1967.

Exercises
103
1 Rounded to the nearest hundred thousand.
Year
Population 
from 
Decimal 
Census
Population 
from
Formula (6)
Error
%Error
1790
3,929,000
3,929,000
0
0.0
1800
5,308,000
5,336,000
28,000
0.5
1810
7,240,000
7,228,000
-12,000
-0.2
1820
9,638,000
9,757,000
119,000
1.2
1830
12,866,000
13,109,000
243,000
1.9
1840
17,069,000
17,506,000
437,000
2.6
1850
23,192,000
23,192,000
0
0.0
1860
31,443,000
30,412,000
-1,031,000
-3.3
1870
38,558,000
39,372,000
814,000
2.1
1880
50,156,000
50,177,000
21,000
0.0
1890
62,948,000
62,769,000
-179,000
-0.3
1900
75,995,000
76,870,000
875,000
1.2
1910
91,972,000
91,972,000
0
0.0
1920
105,711,000
107,559,000
1,848,000
1.7
1930
122,775,000
123,124,000
349,000
0.3
1940
131,669,000
136,653,000
4,984,000
3.8
1950
150,697,000
149,053,000
-1,644,000
-1.1
1960
179,300,00g1
1970
204,000,0001
1980
226,500,00g1
(c) Update and revise the table using the more recent census data. Do you 
think it advisable to change the three base years used to evaluate the 
constants? How much difference would it make? Would it be sufficient 
to change visually computer pictures from Analyzer or DiffEq?
2.4-2.5#4. What is the half-life of C14? (See Example 2.5.3. Half-life is the 
time required for half the amount of the radioactive isotope to disintegrate.)
2.4—2.5#5. At Cro Magnon, France, human skeletal remains were discov­
ered in 1868 in a cave where a railway was being dug. Philip van Doren 
Stern, in a book entitled Prehistoric Europe, from Stone Age Man to the 
Early Greeks (New York: W.W. Norton, 1969), asserts that the best esti­
mates of the age of these remains range from 30,000 to 20,000 B.C. What 
range of laboratory C14 to C12 ratios would be represented by that range 
of dates?
2.4—2.5#6°. A population of bugs on a plate tend to live in a circular 
colony. If N is the number of bugs and n is the per capita growth rate, 
then the Malthusian growth rule states that dN/dt = nN. However, those 

104
2. Analytic Methods
bugs on the perimeter suffer from cold, and they die at a rate propor­
tional to their number, which means that they die at a rate proportional 
to TV1/2. Let this constant of proportionality be r2. Find the differential 
equation satisfied by N. Without solving it, sketch some solutions. Is there 
an equilibrium? If so, is it stable?
2.4—2.5#7. By another of Newton’s laws, the rate of cooling of some body 
in air is proportional to the difference between the temperature of the body 
and the temperature of the air. If the temperature of the air is 20° C and 
the body cools for 20 minutes from 100° to 60° C, how long will it take for 
its temperature to drop to 30° C?
2.4-2.5#8. Nutrients flow into a cell at a constant rate of R molecules 
per unit time and leave it at a rate proportional to the concentration, with 
constant of proportionality K. Let N be the concentration at time t. Then 
the mathematical description of the rate of change of nutrients in the above 
process is
that is, the rate of change of N is equal to the rate at which nutrients 
are entering the cell minus the rate at which they are leaving. Will the 
concentration of nutrients reach an equilibrium? If so, what is it and is it 
stable? Explain, using a graph of the solutions to this equation.
2.4—2.5#9. Water flows into a conical tank at a rate of k± units of volume 
per unit time. Water evaporates from the tank at a rate proportional to 
V2/3, where V is the volume of water in the tank. Let the constant of 
proportionality be A;2. Find the differential equation satisfied by V. Without 
solving it, sketch some solutions. Is there an equilibrium? Is it stable?
2.4—2.5#10. Water containing 2 oz. of pollutant/gal flows through a treat­
ment tank at a rate of 500 gal/min. In the tank, the treatment removes 2% 
of the pollutant per minute, and the water is thoroughly stirred. The tank 
holds 10,000 gal. of water. On the day the treatment plant opens, the tank 
is filled with pure water. Find the function which gives the concentration 
of pollutant in the outflow.
2.4—2.5#11. At time t = 0, two tanks each contain 100 gallons of brine, the 
concentration of which then is one half pound of salt per gallon. Pure water 
is piped into the first tank at 2 gal/min, and the mixture, kept uniform 
by stirring, is piped into the second tank at 2 gal/min. The mixture in the 
second tank, again kept uniform by stirring, is piped away at 1 gal/min. 
How much salt is in the water leaving the second tank at any time t > 0?
2.4-2.5#12. The following model predicts glucose concentration in the 
body after glucose infusion: Infusion is the process of admitting a substance 
into the veins at a steady rate (this is what happens during intravenous 
feeding from a hanging bottle by a hospital bed). As glucose is admitted, 

Exercises
105
there is a drop in the concentration of free glucose (brought about mainly 
by its combination with phosphorous); the concentration will decrease at a 
rate proportional to the amount of glucose. Denote by G the concentration 
of glucose, by A the amount of glucose admitted (in mg/min), and by В 
the volume of liquid in the body (in the blood vessels). Find whether and 
how the glucose concentration reaches an equilibrium level.
2.4-2.5#13. A criticism of the model of the last exercise is that it as­
sumes a constant volume of liquid in the body. However, since the human 
body contains about 8 pt of blood, infusion of a pint of glucose solution 
would change this volume significantly. How would you change this model 
to account for variable volume? I.e., how would you change the differential 
equation? Will this affect your answer about an equilbrium level? How? 
What are the limitations of this model? (Aside from the fact you may have 
a differential equation which is hideous to solve or analyze, what criticisms 
or limitations do you see physically to the variable volume idea?) What 
sort of questions might you ask of a doctor or a biologist in order to work 
further on this problem?
2.4—2.5#14. A spherical raindrop evaporates at a rate proportional to its 
surface area. Find a formula for its volume V as a function of time, and 
solve this differential equation.
Exercises 2.6 Exact Equations
2.6#1°. Solve the following differential equations that are exact:
(a) (t2 + x)dt + (t — 2x)dx = 0
(b) (x — 3t2)dt — (4ж — f)dx = 0
(с) (ж3 — t^x' = x
X2 
1 1 
Г 1 
/2
(d) 7Г~^2 "7 dt+ --7Г-Ц2 ^ = 0
L(t- x)2 
tj 
[a? 
(t-xf2
(e) 2(3tx2 + 2i)dt - 3(2t2a? + x2)dx = 0
,,, t dt + (2t + x)dx
W (t + хУ = °
2.6#2. Show that the linear equation x' = p(t)x+q(t), multiplied through­
out by efp(T)dT is exact. The quantity efp^dr is called an integrating fac­
tor because it makes the equation exact and able to be “integrated.” (This 
idea was presented in different format as Exercises 2.2-2.3#6.)
2.6#3. Using the introductory idea of Section 2.6, use the computer pro­
gram DiffEq to draw some of the level curves for the following surfaces 
z = F(t, x):

106
2. Analytic Methods
(b) z = sin(x2 +12) — tx
(c) z = (t2 +x2)/t3
(d) z = etx+1 - cos(t + x)
Exercises 2.7 Series Solutions
2.7#1. For the equation x’ = xt +12 sin t,
(a) Use the method of undetermined coefficients to find the power series 
solution.
(b) Write a general term for the coefficients.
(c) Give the radius of convergence for the power series. Hint: Use the 
ratio test.
(d) Check your results with a computer drawing from DiffEq.
2.7#2. In the manner of the previous exercise, find power series solutions 
for the following differential equation (which are also solvable by other 
methods, so you can check your results). It is not always possible to get a 
nice formula for the coefficients, though, so it may not be easy to find the 
radius of convergence of a solution. When this happens, see if you can use 
computer drawings from DiffEq to help.
(a) x' — 3t2x, with ж(0) = 2
(b) x1 = (x — l)/t3 with x(l) = 0
(c)° (1 + t)x' — kx = 0, with ж(0) = 1. You should recognize this series.
2.7#3. Consider the first order differential equation xf =
(a) Show how to find the first four successive derivatives (and recursively 
how to find more) from the differential equation, using implicit differ­
entiation and substitution. That is, find the functions Fn such that
x^n\i) = Fn(t) x, x').
One reason for doing this is to find inflection points as in Exercises 
1.1#13-16. Another reason is shown in the next part.

Exercises
107
(b) The derivatives from part (a) allow us to find the Taylor series (The­
orem A3.1 of the Appendix) for the solutions x(i):
x(t) = x(to) + x'(to)(t - to) + Q) x"(to)(t - to)2 
+ ...+(^)o:(n)(to)(t-to)n + ...
If you need to find the solution only for a particular initial con­
dition, and if the derivatives are sufficiently easy to calculate from 
xf = /(£,ж), then you may find this a more convenient route to the 
series for that particular solution than the method of undetermined 
coefficients.
Use this Taylor series method to find the solution for xf = x2 through 
ж(0) = 1, and determine the radius of convergence of the resulting 
series.
2.7#4. Find the first five terms of the power series solution of x' = x2 +t2 
with ж(0) = 1. Do this problem both by the Taylor series method of the 
previous exercise and by the method of undetermined coefficients from this 
section. Which do you find easiest?
2.7#5. Using any method you like, find the first three terms of the power 
series solution of x' = sinta with ж(0) = тг/2.
2.7#6. In the manner of Example 2.7.4, study the behavior of the differ­
ential equation x' = — x/t2 at the origin of the t, ж-plane. A power series 
solution is appropriate and okay in this case, because there is a special 
“nicest” element of the “pony tail.” Use the computer program DiffEq to 
demonstrate.
2.7#7°. Power series solutions can often be helpful for higher order differ­
ential equations. The method of undetermined coefficients applies exactly 
as for a first order equation; you simply have to take more derivatives be­
fore you substitute in the differential equation, and the resulting relations 
between the coefficients involve more ordinary equations. Keep in mind 
that we expect n arbitrary constants for an nth order differential equation. 
Find power series solutions to the following differential equations, with the 
given conditions:
(a) Find a formula for the coefficients of a power series solution to ж" + 
ж = 0.
(i) Find a solution satisfying ж(0) = 0, ж'(0) = 1.
(ii) Find a solution satisfying ж(0) = 1, ж'(0) = 0.
(Pretend that you never heard of sine and cosine.)

108
2. Analytic Methods
(b) x" + xt = 0, with ж(0) = 0, ж'(0) = 1. This is an equation that has 
no solution in terms of elementary functions, yet you can (and must) 
show that this particular solution indeed converges for every t.
(c) x” + tx' — x = 0, with ж(0) = 1, ж'(0) = 0. Find a recursion formula 
for the coefficients of the power series for x. Estimate the value of 
x(0.5) to within 0.01 and show that your estimate is this good.
(d) x" = 2txf + 4x, with ж(0) = 0, я/(0) = 1. Express your answer in 
terms of elementary functions.
(e) x" + (2/t)a/ + x = 0, with ж(0) = 1, ж'(0) = 0. Express your answer 
in terms of elementary functions.
(f) xm —tx"+x = 0, with ж(0) = 1, ж'(0) = -1, ж"(0) = 0. This equation 
indeed has a power series solution. Find the terms through x4.
2.7#8. Find power series solutions for the following equations:
(a) xf = x2
(b) x,f = xxf
(c) x' = x sin t
(d) x1 = zsint — (1 + t)2
Exercises 2 Miscellaneous Problems
2misc.#l.
(b)
(c)
(d)
(i) Solve the following linear or separable differential equations, 
(a) (1 — t2)#' = x - 1
x' — tx = 3t + e* 
x1 = 3t2(x + 2) 
2е*х2х' = t + 2 
xf = t2x + t 
xf = (ж - 2t)/(2x - t), with ж(1) = 2
x’ = — + ttan| 4 I Hint: Substitute у = x2g(x) 
t \t /
sintf ] - xcost + sin2t, with ж(тг/2) = тг/2. 
\ at J
(f)
(g)
(h)
(ii) Graph the slope field and some solutions for each part of (i). You 
may find the computer program DiffEq a help. If any solutions stand 
out, identify them, both by formula and on your picture.

Exercises
109
2misc.#2°. A certain class of nonlinear first order differential equations 
is traditionally called “homogeneous” in a very particular sense that is 
not related to our use of the word homogeneous with regard to linear 
equations, in Section 2.2. A nonlinear differential equation x' = /(£,#) is 
called “homogeneous” if can be written as a function of (x/t).
(i) Show that if indeed you can write xf = f(x/t), then the substitution 
v = x/t will always cause the variables v and t to separate, thus 
giving an equation that is easy to solve if the integrals exist.
(ii) Solve the following differential equations, which are “homogeneous” 
as defined in this exercise, by using the method of substitution pre­
sented in (i):
(a) (x - f)dt + (x + t)dx = 0
(b) (t + x)dt + tdx = 0
(c) (t + x)dt + (# - f)dx = 0
(d) t dx — x dt = Vt2 + x2dt
(e) (&r + 
+ (5z + 7t)dx = 0
(f) (2y/st - s)dt + tds = 0

3
Numerical Methods
We have been emphasizing that most differential equations do not have so­
lutions that can be written in elementary terms. Despite this, the computer 
program DiffEq does draw something that purports to be an approximation 
to a solution, and you should wonder how.
The answer is by numerical methods, by which the computer approxi­
mates a solution step by step. Numerical solutions of differential equations 
are of such importance for applied mathematics that there are many books 
on the subject, but the basic ideas are simple. Most methods tell you to 
“follow your nose,” but the fancier ones do some “sniffing ahead.”
In this chapter we explain the schemes used in our computer drawings, 
but we give only an introduction to the subject of numerical approximation. 
We begin in Section 3.1 with Euler’s method, followed in Section 3.2 with 
some other methods that are numerically superior to Euler’s method but 
for which the theory is more cumbersome.
Next we try to throw additional light on these computational methods by 
an experimental approach for analyzing the error that occurs in numerical 
approximation. This error is due to two sources: truncation of the Taylor 
series for a solution (with the degree of the truncation caused by the method 
of numerical approximation), and the limitations of finite accuracy (due to 
computing on actual machines). In Section 3.3 we analyze the differences in 
errors that occur using the different methods, corresponding to the different 
truncations of the Taylor series. In Section 3.4 we discuss the finite accuracy 
effects of rounding down, up, or round.
In Section 3.5 we finish the experimental and computational discussion 
with other practical considerations. Later in Section 4.6 we will return to 
the theoretical side of numerical methods and show that, at least for the 
simpler methods, we can indeed justify bounds of the form illustrated in 
our experiments.
3.1 Euler’s Method
Euler’s method for approximating solutions of the differential equation x' = 
f(t, x) can be summed up by the instruction “follow your nose.”
Suppose you are at some point (to,^o), representing initial conditions. 
At this point, the differential equation specifies some slope /(to?^o)- As t 
increases by a small step h, you can move along the tangent line in the

112
3. Numerical Methods
FIGURE 3.1.1. Euler’s method. Single step, starting at (to,#o)-
direction of that slope to
(ti,a:i) = (to + h,j:o + h/(to,^o)),
as shown in Figure 3.1.1.
This is based on the Fundamental Theorem of Calculus, in the form
ph
x(to + h) = a?(t0) + / xf(t0 + s)ds.
Jo
If the step size h is small and if the slope is not changing too drastically 
near (to, #o), the value x± will be close to u(ti), where x = u(f) denotes the 
solution through (to,#o)«
The Euler approximate solution between the two points (to,#o) and 
(ti,a?i) is the straight line segment between those points.
The Euler approximate solution can be extended to additional points in 
a piecewise linear fashion. You can start from (ti, ), using the slope given 
by /(ti,a?i), to get
(*2,^2) = (ti + h,#i + h/(ti,#i)).
In like manner, you can then use (£2,^2) to get (£3,^3), and so on. Figure 
3.1.2 shows the result of using the Euler method over three successive steps.
It seems reasonable to expect that a smaller step, such as h/2, will give a 
closer approximation to the solution, that is, we might expect an improve­
ment such as shown in Figure 3.1.3, p. 114, for the interval to to to + ЗЛ.
Thus in an Euler approximation for a given stepsize h, we move through 
the following sequence of points:
(to, #0)
(ti,a?i) with = xq + h/(to,#o),
(tn,a?n) with xn — xn_L + hfffn-^Xn-i).

3.1. Euler’s Method
113
FIGURE 3.1.2. Euler approximate solution for three steps, stepsize h.
A more formal mathematical statement of Euler’s method is the follow­
ing:
Definition 3.1.1. Consider the differential equation x' = f(t,x) with f 
a function defined in some rectangle R = [a, 6] x [c,d]. Choose a point 
(to,^o) £ R and a given stepsize h. Define a sequence of points (tnixn)
recursively by
tn — tn_± 4“ h —
►
Xn = ^n—1 “I” hf^tn—1> X^n—1) ,
(1)
as long as (tn, жп) € R. Then the Euler approximate solution Uh(t) through 
(to,£o) is the piecewise linear function joining all the (tn,xn), where each 
piece has formula
'U'h(t) = “b 
tn}f {tn^ £n)
for t G \tn, tn.y.i].
Definition 3.1.1 gives an approximate solution moving to the right if h is 
positive, and to the left if h is negative.

114
3. Numerical Methods
FIGURE 3.1.3. Smaller-step Euler approximate solution.
The idea underlying this method of numerical approximation is so in­
tuitive that it is hard to find to whom to attribute it. The first formal 
description is generally attributed to Euler, and the first proof that as the 
step gets small the approximate solution does indeed converge to a solution 
is due to Cauchy. But the method was used without comment by Newton in 
the very first book using differential equations. Furthermore, this algorithm 
gives approximate solutions to xr = гж, which match calculations dating 
back to the Babylonians of 2000 B.C., as we shall discuss in Evample 3.1.4.
First, however, let us pause to look at a sample calculation illustrating 
the use of the equations (1).
Example 3.1 .2. For x' = sin(ta), start at (£q,#o) = (0,3) and construct 
an Euler approximate solution to the right with step size h = 0.1. An 
organized calculation for the xn's proceeds as follows, moving from left to 
right along each row in turn:

3.1. Euler’s Method
115
Table 3.1.1
tn
Xn
f (tn^ Xn)
Xn-j-1 — Xn “h hf(tn, Xn)
to = 0.0
xq = 3.000
sin[(0)(3)] = 0
X! =3+(.l)(0) = 3.000
ti =0.1
xi = 3.000
sin[(.l)(3)] = .296
x2 = 3 + (.1)(.296) = 3.030
t2 = 0.2
X2 = 3.030
sin[(.2)(3.03)] = .570
x3 = 3.030 + (.l)(.57O) = 3.087
t3 = 0.3
xs = 3.087
sin[(.3)(3.087)] = .799
X4 = 3.087 + (.1)(.799) = 3.167
t4 = 0.4
X4 = 3.167
continue in this 
manner
▲
Programming a computer for Euler’s method allows extremely fast cal­
culation, and expanded accuracy without effort (up to the limits of the 
machine).
Example 3.1 .3. For the equation of Example 3.1.2, we show a computer 
tabulation listing the results of tn and xn (without showing the intermedi­
ate steps), with a smaller stepsize, h = 0.05. You can see how the numerical 
results are refined by a smaller stepsize.
Table 3.1.2. x' = sin(tr)
tn
0.00
3.00000
0.05
3.00000
0.10
3.00747
0.15
3.02228
0.20
3.04418
0.25
3.07278
0.30
3.10752
0.35
3.14767
0.40
3.19227 ▲
The Euler method provides the simplest scheme to study the essentials 
of numerical approximation to solutions of differential equations. The next 
two examples are familiar cases where you will see that we already know 
the Euler method from previous experience, and furthermore we can see 
that when the stepsize h —> 0, these Euler approximate solutions converge 
to solutions to the differential equation in question.
Example 3.1 .4. Apply Euler’s method to the differential equation that 
represents bank interest continuously compounded,
xf = rx, where x(t) = amount of savings.

116
3. Numerical Methods
(The interest rate r is annual if t is measured in years.) To apply Euler’s 
method, let h = 1/n, where n is the number of compounding periods per 
year. Then
r 
f 
r \
X1 — Xq H—Xq = I 1 H— )^o> 
n 
\ 
n J
/ 
\ 2
r 
/ 
r \
#2 = #1 H----- ^1 =114------I Xq,
n 
\ 
nJ
This xn is the value of the savings account after n periods of compound 
interest; the Euler’s method formula corresponds precisely to the tables 
used by banks to calculate interest compounded at regular intervals! In 
fact, the earliest mathematical writings, those of the Babylonians 4000 
years ago, are tables of interest that match these calculations.
For continuous compounding, over one year, Euler’s method indicates 
that savings will grow to
i 
r \
lim I Ц— 
xq.
n—>oo \ 
nJ
For continuous compounding over one year we also know that the analytic 
solution to x’ = rx is xoer. So when we will prove in Section 4.5 that Euler’s 
method converges to the solution to the differential equation, we will be 
proving a famous formula that you probably already know:
er = lim f 1 + — . A 
n—>oo \ 
n J
Example 3.1 .5. Consider the differential equation x1 = g(f), which has no 
explicit dependence on x. From elementary calculus, with initial condition 
w(t0) = xq, we know the solution is the function defined by
u(t) = x0 + [ g(s)ds. 
(2)
J to
Apply to x' = g(t) Euler’s method with stepsize h= (t — to)/n, starting 
at u(to) = x0. This gives, for any t >
n—1
«h(i) = zq + 
(3)
i=0
For xq = 0, equation (3) is precisely a Riemann sum for numerical inte­
gration of a function g(t). This Riemann sum is illustrated for a positive

3.1. Euler’s Method
117
function by the area of the shaded rectangles in Figure 3.1.4. Each rectangle 
has base h and lies above [ti, from the t-axis to 
▲
FIGURE 3.1.4. Riemann sum.
Numerical Methods as Approximation of an
Integral
As shown in Example 3.1.5 by equation (2), Euler’s method amounts to 
approximating the integral, of g(t). In the particular Rieman sum of that 
example, we are approximating the height of each vertical strip under g(t) 
on an interval 
by the value
ть = g(ti) value at left endpoint.
This is often not a good approximation. Two reasonable improvements in 
most cases are obtained by using other values or averages of g(t) in each 
subinterval:
тм = g((ti + £г+1)/2) value at midpoint;
тпт = (l)b(^i) + X^i+i)] average of values at endpoints.
In the computation of integrals, these improvements are called the mid­
point Riemann sum and the trapezoidal rule, respectively.
A fancier method to approximate an integral or get a slope for the ap­
proximate solution to the differential equation xf = g(t) is Simpson’s Rule, 
using a weighted average of values of g(t) at three points in each subinterval:
ms = 
+ ti_|_i)/2) + 0(^+1)],
as taught in first year calculus. There is a unique parabola passing through 
the three points (ti,Xi), (ti + h/2,g(ti + h/2), and (ti + h,g(ti + Д)), as 
shown in Figure 3.1.5. Simpson’s Rule gives exactly the area beneath that 
parabola.

118
3. Numerical Methods
(tj+1 ,xi+l)
[<—
ti 
(t| + h/2) ti+h =ti+1
FIGURE 3.1.5. Simpson’s Rule.
Remark. The formula for Simpson’s Rule given in most textbooks uses 
intervals of length from left or right to midpoint, which in our notation is 
h/2. This explains why their formulas have thirds instead of sixths.
Example 3.1.5 and the subsequent discussion are more than a happy 
coincidence. In fact, it is precisely the approximation of integrals that has 
motivated and produced the numerical methods we are discussing in this 
book.
When slope /(t, x) depends on x as well as on t, there are smarter ways 
than Euler’s of approximating solutions to differential equations, corre­
sponding to the improved schemes for approximating integrals: midpoint 
Euler, trapezoidal rule, and Simpson’s rule. We shall study two of these in 
the next section, and the third in Exercise 3.1-3.2#7.
3.2 Better Numerical Methods
Two other numerical methods for solving a differential equation x' = /(t, x) 
are based on the same idea as Euler’s method, in that using intervals of 
step size Д,
= ti 4- h and Xi+i = Xi + hm, where m = slope.
For Euler’s method we simply use the slope, f(ti,Xi), available at the 
point where we begin to “follow our noses,” the left endpoint of the interval. 
For fancier methods we “sniff ahead,” and then can do a better job of 
“following.”
1. Midpoint Euler. For the midpoint Euler method (also called modified 
Euler) we use the slope тпм at the midpoint of the segment we would have 
obtained with Euler’s method, as shown in Figure 3.2.1.
This method takes into account how the slope is changing over the in­
terval, and as we shall see, it converges to a solution considerably more 
quickly than the straight Euler method.

3.2. Better Numerical Methods
119
x
tj tj + h/2
FIGURE 3.2.1. Midpoint slope = тпм = f(tt + Xi + ^f(ti,Xi)).
If the midpoint Euler method is used in the case xf = g(t), it reduces 
exactly to the midpoint Riemann sum mentioned at the end of the previous 
section.
2. Runge-Kutta. The Runge-Kutta numerical method converges consid­
erably more rapidly than the Euler methods, and is what was used to make 
your DiffEq programs. The method was developed by two German math­
ematicians, C. Runge and W. Kutta, at the end of the nineteenth century. 
Without discussing the complexities of how these gentlemen arrived at their 
conclusions, we hereby describe the most commonly used fourth-order ver­
sion, where the Runge-Kutta “slope” тцк is a weighted average of four 
slopes:
mr = f(ti,Xi)
m2 = f(ti + |, Xi + |mi)
m3 = f(ti + , Xi + |m2)
m4 = f(ti + h, Xi + hm3) 
ttirk = | (mi + 2m2 + 2m3 + m4).
slope at beginning of interval
slope at midpoint of a segment 
with slope mi
slope at midpoint of a segment
with slope m2
slope at end of a segment
with slope
The Runge-Kutta method makes a linear combination of four slopes, 
illustrated in Figure 3.2.2, which you might think of as follows: mi is the 
Euler’s method slope, m2 is the midpoint Euler slope, m3 “corrects the 
shot,” and m4 brings in the slope at the right-hand end of the interval. 
The weighted average тдк is used to calculate x^i = Xi + hm.

120
3. Numerical Methods
FIGURE 3.2.2. Runge—Kutta makes a linear combination of these four slopes 
using mRK = (I) (mi + 2m2 + 2m3 + m4).
A special case of the Runge-Kutta method is the following: if x' = g(t\ 
then the slope depends only on t, not on ж, so m2 = m3 (see Figure 3.2.2) 
and
mRK = -(mi + 4m2 + m4) = ms, 
о
exactly Simpson’s rule for numerical integration, as discussed in Section 
3.1. Simpson’s rule, in fact, was the original motivation for the fourth order 
Runge-Kutta scheme.
We compare these three numerical methods—Euler’s, midpoint Euler, 
and Runge-Kutta—in Examples 3.2.1 and 3.2.2, using two different presen­
tations. In the first we fix the stepsize and follow the approximate solution 
through a number of steps for each method.

3.2. Better Numerical Methods
121
Example 3.2.1. We return to xf = sinta, with ж(0) = 3, and stepsize 
h = 0.1, and tabulate the computations in Table 3.2.1.
Table 3.2.1. x' = sin ire.
tn
Euler
Midpoint Euler 
xn
Runge-Kutta
0.0
3.00000
3.00000
3.00000
0.1
3.00000
3.01494
3.01492
0.2
3.02955
3.05884
3.05874
0.3
3.08650
3.12859
3.12829
0.4
3.16642
3.21812
3.21744
0.5
3.26183
3.31761
3.31637
0.6
3.36164
3.41368
3.41185
0.7
3.45185
3.49163
3.48947
0.8
3.51819
3.53946
3.53746
0.9
3.55031
3.55144
3.55003
1.0
3.54495
3.52867
3.52803
1.1
3.50570
3.47694
3.47698
1.2
3.44016
3.40379
3.40433
1.3
3.35674
3.31643
3.31729
1.4
3.26276
3.22081
3.22187
1.5
3.16380
3.12145
3.12263
1.6
3.06386
3.02157
3.02285
1.7
2.96565
2.92340
2.92479
1.8
2.87102
2.82844
2.82998
1.9
2.78122
2.73772
2.73946
2.0
2.69713
2.65196
2.65396 ▲

122
3. Numerical Methods
In the second version we fix the final value of tf and calculate Xf = 
Uh(tf), for different numbers of steps on the interval
Example 3.2.2. We return to x1 = sinta, approximating ж(2), with ж(0) = 
3. On each line the stepsize is half that on the line above, so the number 
of steps is 2N and h = (tf — tQ)/2N.
Table 3.2.2. xf = sin tx
No. of steps
Euler
Midpoint Euler
Runge-Kutta
1
3.00000
3.28224
3.00186 = uh(tf)
2
3.14112
3.24403
2.66813
4
2.84253
2.61378
2.65370
8
2.75703
2.64049
2.65387
16
2.70750
2.65078
2.65396
32
2.68128
2.65321
2.65397
64
2.66776
2.65379
2.65397
128
2.66090
2.65393
2.65397
256
2.65745
2.65396
2.65397
512
2.65571
2.65397
2.65397
1024
2.65484
2.65397
2.65397
2048
2.65441
2.65397
2.65397
4096
2.65419
2.65397
2.65397
8192
2.65408
2.65397
2.65397
16384
2.65403
2.65397
2.65397 
▲
You can see in this example that as the number of steps increase, and 
the stepsize h —> 0, the values for Xf soon seem to approach a limit. If 
the computer worked with infinite precision, these values for Xf would 
converge, although sometimes not too quickly, to the actual solution to the 
differential equation, as we shall prove in Section 4.5.
You can also see how the midpoint Euler method approaches this limit 
considerably sooner than the Euler method, and how the Runge-Kutta 
method approaches the limit considerably sooner than midpoint Euler.
Summary. We summarize with three computer programs that calculate 
one step of the approximate solution to a differential equation, by the 
methods presented in Sections 3.1 and 3.2. The programs are written in 
the computer language Pascal, but it is not necessary that you know this 
language to read the sequences of equations that are the center of each 
program.

3.2. Better Numerical Methods
123
Table 3.2.3
Procedure StepEuler (var t,x, foreal);
Euler’s method
Begin
x := x + h * slope(t, ж);
t := t 4- h;
end;
Procedure StepMid(var t, ж, /i:real);
var ml,tl,zl:real;
begin
tl := t + Л/2; xl := x + (Л,/2) * slope(t, ж); 
ml := slope(tl,a;l);
t := t + h;
x := x + h * ml;
end:
midpoint Euler
Procedure StepRK(var t, ж, foreal);
var tl, zl, ж2, жЗ, ml, m2, m3, m4, m:real;
begin
ml := slope(t, ж);
tl := t 4- h/2] xl := x 4- ml * fo/2;
m2 := slope(tl,xl);
x2 := x 4- m2 * h/2, 
m3 := slope(tl,a;2);
t := t 4- h\ x3 := x 4- h * m3;
m4 := slope(t, x3);
m := (ml + 2* m2+ 2* m3 4- m4)/6;
x := x 4- h * m]
end;
Runge-Kutta
There exist many other numerical methods for approximating solutions 
to differential equations. A few others are introduced in the exercises; ref­
erences are listed at the end of this volume. In this text proper we concen­
trate only on the three methods already presented; our purpose is to show 
as clearly as possible what is going on and what needs to be considered, so 
that you can evaluate other methods for your particular circumstances.
However, we cannot close this section without discussing two other meth­
ods. The first is very natural, but seldom used in computers, because it 
requires the evaluation of high order derivatives; this method may become 
more practical as symbolic differentiators become more common. The sec­
ond is of such importance in practice that it cannot reasonably be omitted.

124
3. Numerical Methods
The “Naive” Taylor Series Method
We saw in Chapter 2 that there exists a unique proper series solution 
x = u(t) for xf = f(t,x) with u(tQ) = x0. In fact, u(t) is the Taylor series 
(as presented in Theorem A3.1 in the Appendix), so we can write it in that 
fashion and show each coefficient in terms of /(£,ж):
u(t) = u(t0) + u'(t0)(t - t0) + (l/2)u"(t0)(t - *o)2 + ...
= Xo + /i/(to^o) +(h2/l)[df/dt + f(df/dx)]to,Xo + ...
slope
Euler’s method
Let Pn(t0, xq; t) be terms of this series up to degree n, the nth degree Taylor 
polynomial of the solution x = u(t). Then the “naive” method (which we 
will see in the proofs of Chapter 4 to be of order n) can be found as follows:
ti-i-i — h
•Ei+l = Pnifii
Comment 1. The case n = 1 is exactly Euler’s method. The cases n = 2 
and n = 4 have the same “precision” as midpoint Euler and Runge-Kutta 
respectively, in a manner to be discussed below.
Comment 2. An attempt to solve Exercise 2.7#5, x' = sinfcr, will show 
the reader the main weakness of this numerical scheme: the computation 
of the polynomials Pn(to,xo\t) can be quite cumbersome, even for n = 
4. Nevertheless, in those cases where the coefficients can be expressed as 
reasonably simple recursion formulas, a Taylor series method of fairly high 
degree, perhaps 20, may be a very good choice. See Exercise 3.3#6.
One way of understanding the Runge-Kutta method is that, as a func­
tion of h, the function v(t) = v(to + h) = xq + hmto>XO)h has the same 
4th degree Taylor polynomial as the solution u(t) to xf = f(t,x) with 
x{to) = xq. However, finding v(i) only requires evaluating f at 4 points, 
and not computing the partial derivatives of f up to order 4. So you can 
think of Runge-Kutta as a substitute for the naive 4th order method, a 
substitute that is usually much easier to implement.
The computation required to show the equivalence of Runge-Kutta to 
a 4th degree Taylor polynomial is quite long, but the analogous statement 
that midpoint Euler is equivalent to the 2nd degree Taylor polynomial is 
quite feasible to prove (Exercise 3.1-3.2#9).
Implicit Methods
There is a whole class of numerical methods which are particularly well 
adapted to solving differential equations which arise from models like dis­
cretizations of the heat equation. We will show only one example of such 

3.3. Analysis of Error, According to Approximation Method
125
a method. It appears at first completely unreasonable, but the analysis in 
Section 5.4 should convince the reader that it might well be useful anyway.
The implicit Euler method consists of choosing a step h, and setting
= t<i 4- h and 
“b «^14-1 )•
Note that the second expression above is not a formula for but an 
equation for , i.e., it expresses Xi+i implicitly. To carry out the method, 
this equation must be solved at each step. There is in general no formula 
for such solutions, but a variety of schemes exist to approximate solutions 
of equations, most of which are some variant of Newton’s method, which 
will be discussed in Section 5.3. As the reader will find, these methods are 
always a little dicey. So the scheme appears a priori of little interest: each 
step requires the numerical solution of an equation, with all the attendant 
possible problems. The reader is referred to Examples 5.4.2, part (d) to 
see why it is useful anyway: the other methods may be simpler but they 
have their problems too. The implicit method avoids a breakdown of the 
numerical method when the stepsize gets large.
3.3 Analysis of Error, According to 
Approximation Method
For a differential equation with exact solution x = u(t) and a particular 
numerical approximation, xn = Uh(tn), the actual error at the nth step,
ВД) — ^(*n) 'U'h(j'n)>
depends on both the number of steps n and on the stepsize h.
This actual error in a numerical approximation has two sources: one 
source is the method of approximation, which tells how the Taylor series 
for the actual solution x = u(t) has been truncated; that is what we shall 
discuss in this section, and this is what contributes the greatest effect on 
error. The other source of error is the finite numerical accuracy of the 
calculation, which depends on the computing machine and its method of 
rounding decimals; this we shall discuss in the subsequent section.
Examples 3.3.1 and 3.3.2 compute for our three numerical methods— 
Euler, midpoint Euler, and Runge-Kutta approximate solutions Uh(tf) — 
Xf, for fixed tf, to a differential equation xf = f(t,x) that can be solved 
analytically. In these examples, the number of steps in the interval [t0, tf] 
varies as 2N, from N = 0 to N = 13, so that h = (tf - t0)/2N. In other 
words, this arrangement of setting tf makes the number of steps N directly 
related to stepsize h, and error can be studied as an effect of h alone.
For each value of N, and for each method, we list both the computation 
of Uh(tf) = Xf and the actual error E(h). Note that we have written E(h) 

126
3. Numerical Methods
with only five significant digits, because E is the difference of two numbers 
that are quite close, so additional digits would be meaningless.
Example 3.3 .1. For the equation x' = ж, find #(2), with #(0) = 1. By 
separation of variables, the exact solution is found to be u(t) = so 
u(2) = e2 « 7.38905609893065.
Table 3.3.1. Actual Error E(h) = u(tf) — Uh(tf) for xf = x
No. of 
steps
Euler
Midpoint Euler
Runge-Kutta
1
3.00000000000000
4.3891 x 10°
5.00000000000000
2.3891 x 10°
7.000000000000 = uh(tf) 
3.8906 x 10-1 = E(h)
2
4.00000000000000
3.3891 x 10°
6.25000000000000
1.1391 x 10°
7.33506944444444
5.3987 x IO-2
4
5.06250000000000
2.3266 x 10°
6.97290039062500
4.1616 x 10"1
7.38397032396005
5.0858 x IO-3
8
5.96046447753906
1.4286 x 10°
7.26224718998853
1.2681 x 10-1
7.38866527357286
3.9083 x 10~4
16
6.58325017202742
8.0581 x 10-1
7.35408290311116
3.4973 x IO-2
7.38902900289220
2.7096X10-5
32
6.95866675721881
4.3039 x 10“1
7.37988036635186
9.1757 x IO"3
7.38905431509387
1.7838 x 10~6
64
7.16627615278822
2.2278 x IO"1
7.38670685035460
2.3492 x 10"3
7.38905598450266
1.1443 x IO"7
128
7.27566979312842
1.1339 x 10"1
7.38846180262122
5.9430 x 10“4
7.38905609168522
7.2454 x 10-9
256
7.33185059874104
5.7206 x IO"2
7.38890664780243
1.4945 x 10"4
7.38905609847485
4.5580 x IO"10
512
7.36032355326928
2.8733 x 10~2
7.38901862627635
3.7473 x IO-5
7.38905609890206
2.8586 x 10"11
1024
7.37465716034184
1.4399 x 10“2
7.38904671701835
9.3819 x 10“6
7.38905609892886
1.7897 x 10-12
2048
7.38184843588050
7.2077 x 10“3
7.38905375173306
2.3472 x 10“6
7.38905609893053
1.1546 x 10-13
4096
7.38545021553901
3.6059 x 10“3
7.38905551191629
5.8701 x IO"7
7.38905609893065
-1.776 x IO'15
8192
7.38725264383889
1.8035 x IO"3
7.38905595215017
1.4678 x IO"7
7.38905609893063
1.6875 x 10~14 (*)
16384
7.38815424298207
9.0186 x 10~4
7.38905606223213
3.6699 x 10“8
7.38905609893059
6.4837 x IO"14 (*) ▲

3.3. Analysis of Error, According to Approximation Method
127
Example 3.3 .2. For the equation x1 = x2sin^, finding ж(тг), with ж(0) = 
0.3. By separation of variables, the actual solution is found to be u(t) = 
l/(cost + C), which gives u(tt) = 0.75.
Table 3.3.2. Actual Error E(h) = u(tf) - Uh(tf) for x' = x2sin*
No. of 
steps
Euler
Midpoint Euler
Runge-Kutta
1
.300000000000000
4.5000 x 10-1
.582743338823081
1.6726 x 10"1
.59825123558439 = uh(tf) 
1.5175 x 10-1 = E(h)
2
.441371669411541
3.0863 x 10"1
.706815009470264
4.3185 x IO"2
.735925509628509
1.4074 x 10“2
4
.556745307152106
1.9325 x IO-1
.735645886225717
1.4354 x IO"2
.749522811050199
4.7719 x 10"4
8
.634637578818475
1.1536 x 10"1
.745355944609261
4.6441 x 10-3
.749984292895033
1.5707 x IO-5
16
.684465875258715
6.5534 x IO"2
.748779351548380
1.2206 x 10“3
.749999711167907
2.8883 x 10“7
32
.714500155244513
3.5500 x 10“2
.749695668237506
3.0433 x 10“4
.750000010105716
-1.011 x 10~8
64
.731422748597318
1.8577 x 10“2
.749924661781383
7.5338 x IO"5
.750000001620961
-1.621 x 10-9
128
.740481836801215
9.5182 x IO"3
.749981303141273
1.8697 x IO"5
.750000000133992
-1.340 x IO"10
256
.745180317032169
4.8197 x 10"3
.749995345938535
4.6541 x IO"6
.750000000009427
-9.427 x IO-12
512
.747574578840291
2.4254 x 10-3
.749998839192566
1.1608 x 10“6
.750000000000615
-6.153 x 10"13
1024
.748783339005838
1.2167 x 10“3
.749999710148590
2.8985 x IO"7
.7500000000000036
-3.608 x 10~14
2048
.749390674843791
6.0933 x 10-4
.749999927581687
7.2418 x IO"8
.7499999999999986
1.4211 x 10-14
4096
.749695087867540
3.0491 x 10-4
.749999981900960
1.8099 x IO"8
.7499999999999919
8.1157 x 10"14 (*)
.7500000000000086
-8.560 x 10“14 (*)
.7500000000000008
-7.661 x 10-15 (*) ▲
8192
.749847481433661
1.5252 x 10-4
.749999995476055
4.5239 x 10-9
16384
.749923725077763
7.6275 x IO”5
.749999998869095
1.1309 x IO"9
Notice that, as expected, in general the error decreases as you run down 
each column. However, something funny (*) can happen in the lower right­
hand corner of both Examples 3.3.1 and 3.3.2 under the Runge—Kutta 
method, with the error and the last digits. Actually, the same funny busi­
ness can eventually happen under any method that converges. The fact 
that the error begins to increase again, and even to wobble, is due to the 
finiteness of the computation, to be discussed at length in Section 3.4.

128
3. Numerical Methods
The overall behavior of E(h) (ignoring the final wobbly (*) values) is 
different for each of the three methods. Considering Examples 3.3.1 and 
3.3.2 as numerical experiments, it seems that Runge-Kutta is much better 
than midpoint Euler, which in turn seems much better than straight Euler; 
the better methods converge more quickly and give smaller errors for a 
given N.
It is not obvious from the tables what makes the difference. However, the 
fact, which we will support by numerical experiments (in this section) and 
Taylor series approximation (to come later, in Section 4.6), is that the error 
E(Ji) varies as a power of h, where that power is 1, 2, and 4 respectively 
in these methods. That is, we claim
for Euler’s method, 
for midpoint Euler, 
for Runge-Kutta,
E(h) « CEh, '
E(K) « CMh2, ► 
E{h) « CRKh\ ,
(4)
These formulas (4) produce the graphs for E versus h that are plotted 
in Figure 3.3.1, with the constants CE, Cm> Crk ignored in order to focus 
on the powers of h.
FIGURE 3.3.1. Claimed relationship between E(h) and h.

3.3. Analysis of Error, According to Approximation Method
129
E(h)
h
FIGURE 3.3.2. E(h) versus h for x' = x.
In fact, Figure 3.3.2, which is a computer plot of E(fi) versus h for 
Example 3.3.1, gives a picture visually supporting the above claim. (To 
graph the actual values of the example, the scale must be skewed so that 
the 45° line for Euler’s method in Figure 3.3.1 appears to be much steeper 
in Figure 3.3.2.)
Because of the formulas (4) for E in terms of powers of h, Euler’s method 
is called a first order method, midpoint Euler is a second order method, and 
our version of Runge-Kutta is a fourth order method. There are in fact 
methods of all orders, and there are entire texts and courses on numerical 
methods. Our selection is just an introduction.
As the order of a numerical approximation increases, accuracy increases; 
for small h, h4 is a much smaller error than h. But more computations 
are involved for each step of a higher order method, so each step of a 
higher order method is more costly to compute. We need to strike a balance 
between number of steps and cost per step; fourth order Runge-Kutta is 
often the practical choice. Later, in Section 4.6 of the next chapter, we shall 
give rigorous estimates that the errors are indeed bounded by such terms.
Meanwhile, if it is true that for order p
E(K) « Chp, 
(5)
as in formulas (4), how can we exhibit this claim quantitatively in our 
numerical experiments? Using (5), on h and on h/2, we get
In \E(h) | - In |E(h/2)| « p In h - p In h + p In 2 = p In 2, 
which says that the order of the error,
In|£(h)| — In |Д(Ь/2)| ~ 
(6)
ln2

130
3. Numerical Methods
approaches an integer as expected. This is evidence that E(h) « Chp.
In the following Examples 3.3.3 and 3.3.4, we have tabulated the “or­
der” p from the expression (6), using the values of E(h) as tabulated in the 
earlier examples for the same differential equations. (Note that this exper­
imental “order” calculation cannot appear until the second case, because 
it represents a comparison of each case with the previous one and can only 
be computed after two cases.)
Example 3.3 .3. We return to x' = ж, approximating x(2), with ж(0) = 1. 
The values of u(2), u^(2), and Eh are tabulated in Example 3.3.1.
Table 3.3.3. Order of Error p = —!'—- ■ 71
P 
In 2
Number
Midpoint
Runge-
of steps
Euler
Euler
Kutta
2
0.373
1.069
2.849
= “order”
4
0.543
1.453
3.408
8
0.704
1.714
3.702
16
0.826
1.858
3.850
32
0.905
1.930
3.925
64
0.950
1.966
3.962
128
0.974
1.983
3.981
256
0.987
1.992
3.991
“order”
«4
512
0.993
1.996
3.995
1024
0.997
1.998
3.998
2048
0.998
1.999
3.954_
4.96
0.999
1.999
“order”
6.022
«2
“order”
8192
1.000
“order”
2.000
-3.248
«?
« 1
16384
1.000
2.000
-1.942
▲

3.3. Analysis of Error, According to Approximation Method
In Example 3.3.3 we see the predicted tendency to
131
1 in the first column (Euler’s method)
2 in the second column (midpoint Euler)
3 in the third column (Runge-Kutta).
Again something funny happens in the lower right due to finite accuracy. 
We see the same phenomena in Example 3.3.4.
Example 3.3.4. We return to x' = x2sint, approximating ж(тг) with 
a?(0) = 0.3. The values of ?z(tt), ^(tt), and Eh are tabulated in Exam­
ple 3.3.2.
\n\E(h)\-ln\E(h/2)\
m2
Table 3.3.4. “Order” of Error p = 
1 
'
Number 
of steps
Euler
Midpoint
Euler
Runge- 
Kutta
2
0.544
1.953
3.431 = “order”
4
0.675
1.589
4.882
8
0.744
1.628
4.925
16
0.816
1.928
5.765
32
0.884
2.004
4.837
64
0.934
2.014
2.640
128
0.965
2.011
3.597
256
0.982
2.006
3.829
512
0.991
2.003
3.937 “order”
«4
1024
0.995
2.002
4.092
“order”
2048
0.998
2.001
«2 
1.344
“order”
4096
0.999 “order”
2.000
-2.514
к 1
8192
0.999
2.000
-0.077
16384
1.000
2.000
3.482 
▲

132
3. Numerical Methods
The form of the errors predicted in equations (4) and (5) is indeed true 
in general, as illustrated in Examples 3.3.3 and 3.3.4. However, there are 
exceptions:
In some cases the error E can be even smaller and can look like higher 
orders of h than we would expect. This phenomenon is nicely demonstrated 
in Example 3.3.5, where the solution is symmetric about the line t = тг; 
such symmetry could reasonably lead to cancellation of dominant terms 
in the error (Exercise 3.3#7). So the errors for the midpoint Euler and 
Runge-Kutta methods vary as h3 and h5 respectively, instead of h2 and 
h4.
Example 3.3.5. We return to x' = x2sint, but change tf from 7Г to 2тг. 
To find the actual solution tt(27r), recall that u(t) = l/(cost + C), which 
is periodic with period 2тг. So without even calculating C, we know that 
71(2тг) = u(0) = 0.3 for the actual solution when t = 2тг. As in Examples 
3.3.2 and 3.3.4, the computer can calculate г^(2тг), E(h\ and “order” p; 
we tabulate here only the final result, p.
Table 3.3.5. “Order” of Error p = ln ~ ln
In2
No. of
Midpoint
Runge-
steps
Euler
Euler
Kutta
2
-112.096
-53.649
-162.414
= “order’
4
-51.397
2.223
1.915
8
1.201
5.040
5.693
16
0.831
3.992
5.088
32
0.868
3.338
5.016
64
0.917
3.097
5.004
“order”
«5
128
0.953
3.025
5.001
256
0.975
3.006
5.000
512
0.987
3.002
5.023.
1024
0.993
3.000
“order”
4.344
2048
0.997
3.000
« 3
0.279
4096
0.998
“order”
3.001
-0.236
8192
0.999
« 1
2.935.
-2.309
16384
1.000
3.921
-0.336
▲

3.4. Finite Accuracy
133
Compare Examples 3.3.4 and 3.3.5; both numerically solve the same 
differential equation. The first is over an interval on which the solution is 
not symmetric, so the expected orders 2 and 4 are observed for midpoint 
Euler and Runge-Kutta respectively. But the second is over an interval on 
which the solution is symmetric, so the observed order appears as 3 and 5 
for midpoint Euler and Runge-Kutta respectively.
Another exception to the error predictions of equations (4) and (5) are 
cases where the partial derivatives of /(t, x) are unbounded or do not exist. 
As we shall see in the next chapter, bounds on the error depend on the 
partial derivatives of order up to the order of the method. We shall present 
an example when we discuss estimating error in Section 4.6.
Meanwhile you are forewarned that such exceptions as these exist to the 
general rule E(h) « Chp.
3.4 Finite Accuracy
In practical computations, computers work with “real numbers” of finite 
accuracy. This is usually not a problem now that computers standardly 
calculate to 14 decimal places (IBM-PC with 8087 co-processor) or 18 dec­
imal places (Macintosh). Nevertheless we shall discuss the effects of the 
finiteness of computation, and if you wish to explore these ideas, the pro­
gram NumMeths is set up to artificially compute to fewer decimal places 
and allow you to more readily see the effects.
It is the phenomenon of round-off that affects finite accuracy, and the 
effects are different depending on whether the rounding is consistently 
“down” (towards —oo), “up” (towards +oo), or “round” (to the nearest 
grid point). We shall discuss these cases separately.
Rounding Down (or Up)
We will give evidence in Example 3.4.1 that if a computer systematically 
rounds down (or up), the error E(h) will behave like
+ 
(7)
The first term, with order p according to the chosen method of numeri­
cal approximation, has been discussed in Section 3.3; we shall proceed to 
explain where a term like Съ/h might come from.
Typically, a number on a computer is something like 1.06209867E02, 
and the numbers between 1.06209867E02 and 1.06209868E02 simply do 
not exist. (Actually, the computer does this with bits, or numbers in base 
2, but this will not affect the discussion.) The number of bits available

134
3. Numerical Methods
depends on the computer; typically, it might be
24 bits, about 6 decimal digits
32 bits, about 8 decimal digits
52 bits, about 14 decimal digits
64 bits, about 18 decimal digits
standard single precision
Apple II Basic
standard double precision; IBM-PC 
standard Macintosh numerics.
Clearly, it can make no sense to use a stepsize smaller than the smallest 
available increment, but we will show in this section that there are good 
reasons to stick to much longer stepsizes.
Consider the computer’s “plane,” which consists of an array of dots, 
spaced A apart. Even in floating point systems, this is true locally. For 
example, in standard double precision you can expect roughly that for 
numbers
order 1 A « 10“16,
order .01 
A « 10“18,
order 1000 A « 10-13.
Suppose we apply Euler’s method to xf = f(t,x) starting at (£о,жо). We 
should land at the point with
ti = to + h, x1=x0-\- hf(t0, ж0),
but the computer will have to choose a point (ti, x±) which is a point of 
its grid, as shown in Figure 3.4.1. Optimistically, we might hope that the 
computer will choose the closest point (in the jargon, make an error of half 
a bit in the last place). This hope is justified on the IBM-PC with 8087 
coprocessor, and you can choose your rounding on the Macintosh; other 
machines (including nlainframes) might not make a smart choice.
Note in Figure 3.4.1, that the difference between the slope of the segment 
Uh of the “real” Euler’s method and the segment Uh actually computed by 
the computer is of order Д/fo, with a constant in front which might be 
about 0.5 in the best case.
FIGURE 3.4.1. Grid of computer coordinates with spacing A.

3.4. Finite Accuracy
135
Thus the computer systematically makes an error like Д/Л at each step, 
and moreover guesses low each time if it always rounds down. It seems 
reasonable that such errors will contribute a term like Cz/h to E(h\ and 
we will see evidence that this is so in Example 3.4.1 and Exercise 3.4# 1. 
We will prove in Section 4.6 that a bound of this form is correct.
Remark. If instead of rounding down, the computer rounds to the nearest 
available number, there will probably be cancellation between the errors 
in the successive steps, and the contribution to E(h) of round-off error is 
both smaller and harder to understand. We postpone that discussion to the 
second part of this section.
Returning to the case of rounding down (or up), we now have
E(h) = C!hp + ^ 
(7, again)
which looks like Figure 3.4.2.
FIGURE 3.4.2.

136
3. Numerical Methods
We observe that if the error is going to behave as in equation (7), then 
only for a small range of h’s will both terms be observable. For h large, the 
first term will swamp out the second; for h sufficiently small, the second 
will swamp out the first. Thus, if we print out as in Example 3.3.3 the 
“order” quantity
ln|E(h)| -ln|E(h/2)|
ln2 
~P’
we should see for large h's much the same as we saw in Section 3.3, namely 
a number close to the integer that gives the order of the method, but for 
small h's, we should see a number close to the integer —1, and this number 
should appear sooner if the number of bits you are using is small. Moreover, 
this dominance of —1 should occur much sooner for Runge-Kutta than 
for midpoint Euler, and much sooner for midpoint Euler than for Euler, 
because in each case more calculations are required for the second method. 
This is indeed what experiments demonstrate.
Example 3.4.1 has been computed using the program NumMeths where 
the user can decide with how many digits the computer should compute. For 
example 18 bits gives about five significant figures. The columns list for each 
number of steps the same quantities, E(h) and “order,” that were fisted in 
Example 3.3.1 using 52 bits, but the computation of E(h) is rounded down 
to 18 bits. Compare the calculation of E(h) in the two cases; with fewer 
bits, E(h) becomes more different as h decreases.

3.4. Finite Accuracy
137
Example 3.4.1. We return to xf = ж, approximating ж(2), with ж(0) = 1. 
However, this time we round down using only 18 bits, which gives approx­
imately five decimal digits.
„ ртп 
1п|ВД|-1п|ВД2)|
Table 3.4.1. “Order” of Error p = 
1 
, o
m2
for xf = x (when rounded down) _____________________
No. of
steps
Euler
Midpoint Euler
Runge-Kutta
1
4.3891x10°
2.3891x10°
3.8906X10”1
=E(h)
2
3.3891x10°
1.1391x10°
5.4004 xlO-2
=E(h)
0.373
1.069
2.849
= “order”
4
2.3266x10°
4.1616X10”1
5.0992 xlO-3
0.543
1.453
3.405
8
1.4286x10°
1.2685 xlO”1
4.6052 xlO-3
“order”
0.704
1.714
3.469,
«4?
16
8.0591 x 10“1
3.5098X10-2
1.8586xl0”4
0.826
1.854
1.309
32
4.3062X10”1
9.5242 xlO-3
“order” 2.9267xl0”4
*
0.904
1.882_
«2?
-0.655
64
2.2335 xlO”1
2.9935 xlO-3
7.0466 xlO-4
0.947
1.670
-1.268
128
1.1457X10-1
“order”
1.7728xl0~3
*
1.2235X10-3
0.963
«1
0.756
-0.796
256
5.9695 xlO”2
2.7799 xlO-3
2.6883 xlO-3
0.940
-0.649
-1.136
512
3.3969 xlO-2
5.1450X10-3
5.0534X10-3
0.813
-0.888
-0.911
1024
2.4371xlO-2
*
1.0577xl0”2
1.0577X10”2
0.479
-1.040
-1.066
2048
2.8338 xlO-2
2.1197xl0'2
2.1197X10-2
“order”
-0.218
-1.003
-1.003
«-1
4096
4.5413X10-2
4.1919X10-2
4.1919X10”2
-0.680
-0.984
“order”
-0.984
8192
8.5437xl0~2
8.3575 xlO-2
«-1
8.3575 xlO”2
-0.912
“order”
-0.995
-0.995
16384
1.6709X10"1
«-1
1.6623 xlO”1
1.6623 xlO”1
*Ъ.Т 1 • 
-
-0.968
-0.992
-0.992
▲
* Notice that the smallest error (in boldface) for each method falls somewhere
between the cases with “order” p and the cases with “order” « -1.

138
3. Numerical Methods
If you wish to explore this phenomenon of rounding down, we leave it as 
Exercise 3.4# 1 to construct a similar analysis for the equation xf = x2 sint 
of Examples 3.3.2 and 3.3.4.
Rounding Round
Most respectable modern computers do not round consistently down (or 
up); they round to the nearest grid-point. As a result, the bound C/h 
discussed in the previous subsection for the error contributed by finite 
calculation is exaggeratedly pessimistic. The round-off errors tend to cancel 
when rounding round, as we shall attempt to explain after a couple more 
examples.
Example 3.4.2 (and Exercise 3.4#lb if you wish to pursue this study) 
illustrate the smaller effects due to rounding round rather than consistently 
in one direction. The computer program NumMeths allows you to choose 
the number of bits, and whether you wish to round down, up, or round. 
As in the previous example, the two lines of information for each case give 
the values of the actual error E(h) and the “order.”
Example 3.4.2. We return to x' = ж, approximating ж(2), with ж(0) = 1. 
However this time we round the calculations to 18 bits, approximately five 
decimal digits.

3.4. Finite Accuracy
139
T , , U9 “Ord₽r” nf Frrnr n 
ln WI “ ln
lable o.4.2. Order oi Error p =------------:—-------------
r 
In 2
for x' = x (when rounded round)
No. of
steps
Euler
Midpoint Euler
Runge-Kutta
1
4.3891x10°
2.3891x10°
3.8906 xlO-1
= Eh
2
3.3891x10°
1.1391x10°
5.3973 xlO-2
= Eh
0.373
1.069
2.850
= “order”
4
2.3266x10°
4.1616X10-1
5.0992 xlO-3
0.543
1.453
3.404
8
1.4286x10°
1.2682X10-1
3.9948 xlO"4
0.704
1.714
3.674
“order”
16
8.0582 x 10“1
3.5006 xUT 2
4.8531 xlO-5
«4
0.826
1.857
3.041_
32
4.3042 x 10“1
9.1428X10"3
-1.250X10-5
0.905
1.937
1.956
64
2.2284X10"1
2.3068X10"3
3.3272 xlO-5
0.950
1.987
-1.412
128
1.1345X10"1
6.1311X10-4
“order”
3.3272 xlO-5
0.974
1.912
«2
0.000
256
5.7147X10"2
1.5534X10"4
7.9048 xlO-5
0.989
1.982.
-1.248
512
2.8796X10-2
-3.329X10"4
-3.329X10-4
“order”
0.989
“order”
-1.100
-2.074
~?
1024
1.4254X10"2
«1
- 1.804X10"4
“order” -2.566 xlO-4
1.014
0.884
~?
0.375
2048
7.5406 xl0~3
2.0112X10"4
2.0112X10-4
0.919
-0.157
0.352
4096
3.5733X10"3
1.8013X10"5
1.8013X10-5
1.077-
3.481
3.481
8192
2.2153X10"3
2.4689X10"4
2.4689 xl0~4
0.690
“order”
-3.777
random
-3.777
random
16384
2.2916X10"3
~?
1.3608X10"3
1.3608X10-3
-0.049
-2.462
-2.462
▲

140
3. Numerical Methods
Example 3.4.2 shows very nicely the most peculiar effects of rounding 
round. The exponents of h in the error do not appear to follow any pat­
terns, quite unlike the systematic — l’s that we found for rounding down. 
In fact, rounding round is best described by a random process, which we 
shall describe in two stages.
1. Integrating in a noisy environment: random walk. Consider the differ­
ential equation x' = g(t), the solution of which,
u(t) =xq+ g(s)ds, 
J to
is an ordinary integral. Suppose this integral is computed by any of the 
approximations discussed at the end of Section 3.1, with n steps. Then 
round-off error just adds something like Si = ±A at each step to whatever 
would be computed if the arithmetic were exact, and we may think of the 
signs as random (in the absence of any good reason to think otherwise). 
Thus the cumulative round-off error
n 
Ел 
2=1
can be thought of probabilistically. For instance, you might think of a ran­
dom walk where you toss a coin n times, and move each time A to the right 
if the toss comes out heads, and A to the left if it comes out tails.
What are the reasonable questions to ask about the cumulative error? 
It is perfectly reasonable to ask for its average (over all random walks 
(si,...,sn) with n steps), but this is obviously zero since it is as often 
negative as positive. More interesting is the average of the absolute value, 
but this turns out to be very hard to compute. Almost as good is the 
square root of the average of the squares (which, because the mean is 0, 
is the standard deviation from statistical theory), and this turns out to be 
quite easy to study for a random walk.
Proposition 3.4.3. The standard deviation of the random walk ofn steps 
of length A, with equal probability of moving right or left, is Ay/n.
Proof. There are 2n possible such random walks, so the expression to be 
evaluated is
Г 1 
/ n \ 2-i 1/2
у Y. fc'.) 
■ 
(8)
L random walks (ei ...En) хг=1
Fortunately this computation is much easier than it seems at first glance. 
If the square is expanded, there are terms like ej and terms like 8i8j. The 
key point of the proof is that the terms like CiCj cancel, when summed over 
all random walks: for half the random walks they give +Д2 and for half 
—A2, as can easily be verified.

3.4. Finite Accuracy 
141
Thus our expression (8) becomes
г i 
/ n \ 
11/* 2
there are 2n possible random walks
In probabilistic jargon, the cancellation of the cross terms follows from 
the fact that the £i are independent, identically distributed, with mean 0. In 
terms of coin tosses, this means that the zth and jth toss are independent, 
each is the same experiment as the other, and that each of these experiments 
has the same probability of sending you to the left or to the right. This is 
all that is really used in the proof; the assumption that the £i are exactly 
dzA is unnecessary, fortunately, since in our case it isn’t true. □
The random walk result was fairly easy, but lies at the beginning of a 
whole chapter of probability theory, on which we will not touch. Instead, 
we will move on from xf = g(t) to the more difficult stochastic problem of 
solving x' — f(t,x), where f depends on ж as well as t. We begin with the 
most important case, the simple linear equation xf = a(t)x, and start with 
the simplest subcase, where a(t) is a constant, a.
2. Solving x' = ax in a noisy environment. We were discussing above 
the special differential equation x' = g(t), with solutions given by indefinite 
integrals. Errors committed during the solution are not amplified, and so 
errors committed at the end can cancel those committed at the beginning. 
It is quite unclear whether anything similar should be true for a differential 
equation like
x = ax
where errors committed near the initial time to are amplified by a factor 
of ea^~to\ and will swamp out an error made at the end.
There is an intuitive way of understanding how the round-off error affects 
the numerical solution of xf = ax with ж(0) = xq, closely related to the 
discussion of the nonhomogeneous linear equation in Sections 2.2 and 2.3. 
Think that you have a bank account with interest a and initial deposit xQ 
at time 0. At regular intervals, make either a deposit or a withdrawal of 
A, choosing whether to make a deposit or a withdrawal at random. How 
should you expect these random deposits and withdrawals to affect the 
value of the account? The equation becomes x' = a(x + random deposits).
The variation of parameters formula of Section 2.3 is just the right tool 
to study this question. Looking back at the discussion in Section 2.3, we 
can see what to substitute in the parentheses of equation (8) for the stan­
dard deviation. Thus for xf = ax, the standard deviation at time t to be
± у 
Уд2 
= дуй.
L random walks (ei...€n) 
г=1
for each random walk

142
3. Numerical Methods
evaluated is
Г i 
/ n 
\ 2“I 1/2
- e"‘ jr 52 
(52 «““‘M
L random walks (ei...en) хг=1 
' J
with Si = (if)/n if there were n deposits or withdrawals. Again, upon 
expanding the square, we find terms like
£2e-2aSi anj 
.
Just as before, the cross terms cancel in the sum. So the standard deviation 
is
The term
can be approximated for n large by observing that the sum on the right­
hand side is a Riemann sum for
^(1 -e-).
So the standard deviation is about
/P2at _ i \ V2
\ 2at J
In particular, we find the term y/n again, and we see that the amplifica­
tion of errors due to the exponential growth of the solutions just changes 
the constants in front of y/n by a factor independent of the step, but which 
becomes very large when at becomes large.
This probability analysis says that if is the approximate solution 
of xf = f(t,x) = ax computed with perfect arithmetic, then the values 
computed by rounding will be randomly distributed around this approxi­
mate solution, with standard deviation С/Vh, where C is usually a small 
constant, of order A. This random distribution is illustrated in Figure 3.4.3.
The dots of Figure 3.4.3 represent the computed values of most 
of these values lie closer to Uh(tf} than the standard deviation.
The discussion above was for the case of constant coefficients a, but the 
diligent reader will observe that it goes through without substantial change 
even if the constant a is replaced by a function a(f) (see Exercise 3.4#3).
In fact, the analysis above is more or less true in general, not just for 
linear equations, but exploration of that issue must be postponed until we 
discuss linearization of nonlinear functions, in Volume II, Chapter 8.

3.5. What To Do in Practice
143
3.5 What To Do in Practice
The examples of Sections 3.3 and 3.4 were made from differential equations 
whose solutions were analytically known, so that errors could be precisely 
analyzed. Of course, you will be interested in understanding errors primar­
ily when the solution is not known ahead of time. The object of this section 
is to see how the discussion of the previous sections can help make good 
choices of method and of step-length.
Suppose we have a differential equation x’ = /(t,x) and that we are 
interested in finding a solution u(t) for t € [to,ty] with u(to) = Then it 
is useful to focus on the value of t in that interval for which you would expect 
the approximate error to be the worst, usually tf, and use the program that 
calculates errors and “orders” for
A first, naive (and not so bad) approach, quite adequate if nothing rides 
on the outcome, is to compute solutions for a succession of numbers-of- 
steps, for instance halving the step each time, and see whether the approx­

144
3. Numerical Methods
imate solutions u^tf) appear to converge. Then take as a guess for the 
exact solution u(tf) whatever they appear to converge to.
This procedure can be dangerous: for one thing, we know that the solu­
tions should not go on converging forever: round-off error should creep up 
on us sometime. Another possible danger is that the convergence we see 
may not be the real convergence; it may be that for much shorter steps 
the method converges to something altogether different. It may also be 
that with the precision with which we are calculating, such short steps are 
unavailable.
How can we guard against such dangers? One way, of course, is to obtain 
realistic error bounds, but they are usually extremely difficult to find. We 
present another approach, realistic in practice.
Our basic assumption is that an approximate solution Uh(tf\ which 
equals the actual solution u(tf) plus the error E*(h), behaves like
~ Cq + C^hp 
(7, again)
where p is the order of the method, and h is small, but not so small that 
round-off errors dominate. Both Co and Ci are unknown; in fact, Co is the 
actual value we are looking for. How can we guess what step size h 
is optimal?
Suppose as above that we can find approximate solutions
uh(tf) = uN(tf) with h =
Then we look at the difference Dn between successive approximations,
Dn = uN(tf) - 
« C1(2~pN - 2-^)|to - tf\P
— Ci2~Np(l — 2p)\to — tf\p,
and the ratio
ln|Djv-i| — ln|Dw| ln\DN-1/DN\ 
..
Rn----------- Ь2 = ta2 P- W
This argument, similar to the one in Section 3.3, has the advantage that 
the numbers Dn can be computed without knowing the exact solution. As 
before, we expect the equivalence in the relation (9) to be destroyed by 
round-off error as soon as that becomes significant.
So one possible approach to the problem is to tabulate values of UN^tf) 
with the corresponding Rn- Note that the Rn won’t show up until the 
third case, since each Rn requires ^tv-i, ^tv-2-
Find, if it exists, the range of steps for which Rn is roughly the expected 
p. In this range, the solutions are converging as they should; the convergence 
might still be phony, but this is unlikely. Then the optimal precision is 
probably achieved by the shortest step-length in that range, or perhaps the 
next shorter step-length below the range.

3.5. What To Do in Practice
145
The reason for saying maybe the next shorter is the following: recall from 
equation (7) of Section 3.4 that the error
ВД«С'1/гр + ^, 
n
which gives graphs for E(h) of the form shown in Figure 3.5.1. (Of course 
we do not know the values of the constants, but the behavior we wish to 
point out is not dependent on the values of the C^s.)
FIGURE 3.5.1.
As we move down numerical tables with increasing number N of steps 
and decreasing step size h, we move left along the curve for error. What 
we observe in Figure 3.5.1 is that the minimum for the error E(h) occurs, 
moving left, soon after the error curve bends away from the dotted curve 
representing C\hp.
The following Examples 3.5.1 and 3.5.2 show that for two differential 
equations not solvable in elementary terms, we observe the expected phe­
nomena. The second line of the printout for each N does, in an appropriate 
range, tend to orders 1, 2, and 4 as we would expect.
We shall continue the discussion following a look at the examples, which 
were computed with the Macmath program NumMeths. The “curve-fitting” 
mentioned at the bottom of the printouts is explained in this later discus­
sion.

146
3. Numerical Methods
Example 3.5 .1. Consider again x' = sinfcr, the equation of Examples 
1.5.2; 3.1.2,3; 3.2.1,2. We start at to = 0, x§ — 3, and end at tf = 2 
we approximate 
using the full precision (64 bits) available on the
Macintosh.
Table 3.5.1. xf = sintx
No. of
Steps 
Euler
Midpoint Euler
Runge-Kutta
1 3.00000
3.28224001
3.00186186264321
=UN(tf)
2 
3.14112
3.24403094
2.66813472585461
4 2.84253
2.61377795
2.65369780703221
-0.158
-4.129
4.531
= Rn
8 2.75702
2.64048731
2.65387019514893
0.881
4.645
6.388
16 2.70749
2.65078298
2.65396429900677
0.788
1.375
0.873
32 2.68127
2.65320805
2.65397098850082
0.918
2.086
3.814
64 2.65776
2.65378538
2.65397141423697
0.956
2.071
3.974
128 2.66090
2.65392555
2.65397144082086
0.978
2.042
4.001
256 2.65744
2.65396005
2.65397144247765
order
0.989
2.023
4.004
«4
512 2.65571
2.65396860
order 2.65397144258098
0.995
2.012
«2 
4.003
1024 
2.65484
2.65397073
2.65397144258743
0.997
2.006
4.001
2048 2.65440
order 
2.65397126
2.65397144258784
0.999
«1 
2.003
3.997-
4096 
2.65418
2.65397139
2.65397144258787
*
0.999
2.002
3.680
8192 2.65408
2.65397143
2.65397144258788
1.000
2.001
1.757
Curve fitting:
CF interpolation using method R and steps from 7 to 13 , is 
2.65397144258792E+000 + -2.96461702367259E-002 h*
CF interpolation using method M and steps from 6 to 13 , is
2.65397156047038E+000 + -1.90511410080302E-001 h2
CF interpolation using method E and steps from 9 to 13 , is
2.65397177402202E+000 + 4.45219184726699E-001 h1

3.5. What To Do in Practice
147
Example 3.5 .2. Consider again x' = x2 — t, the famous equation of Ex­
amples 1.1.4, 1.3.3, 1.3.6, and 1.5.1. We start at to = 0, xq = 1, and at 
tf = 1 we approximate u^(ty), using the full precision (64 bits) available 
on the Macintosh.
Table 3.5.2. x' = x2 - t
No. of 
Steps
Euler
Midpoint Euler
Runge-Kutta
1
2.0000
2.75000
5.2760823567
=UN(tf)
2
2.3750
3.73888
7.2621789573
4
2.9654
5.19816
8.7088441194
-1.364
-1.308
-0.789
= Rn
8
3.8159
6.87271
9.2426947716
0.183
0.548
2.685
16
4.9058
8.22320
9.3421536298
-0.358
0.310
2.424
32
6.1089
8.95433
9.3529005110
-0.143
0.885
3.210
64
7.2220
9.23519
9.3537418047
0.112
1.380
3.675
128
8.0808
9.32167
9.3537987807
0.374
1.699
3.884
256
8.6467
9.34546
9.3538024380
0.602
1.862
3.962
512
8.9791
9.35168
9.3538026686
order
0.768
1.937
3.987
«4
1024
9.1606
order
9.35326
order
9.3538026831
0.873
^1
1.970
^2
3.995
2048
9.2556
9.35366
9.3538026840
*
-0.933
— 1.986
3.998
Curve fitting:
CF interpolation using method R and steps from 8 to 11 , is 
9.35380268407649E+000 + -1.05668505587010E+003 h4
CF interpolation using method M and steps from 9 to 11 , is
9.35379926109397E+000 + -5.55196762347348E+002 h2
CF interpolation using method E and steps from 9 to 11 , is
9.34643244726150E+000 + -1.88376521685730E+002 h1
Note: The Euler interpolation is not very reliable since Rn is not yet very close 
to 1. Yet note also that despite this fact, the interpolation estimate is much closer 
to the solution than any of the Euler values calculated so far! ▲

148
3. Numerical Methods
On the basis of the above discussion of the range of steps for which 
Rn is roughly the expected p, we expect that the * approximations are, 
in each of the two preceding examples, the best obtainable by our three 
methods—Euler, midpoint Euler, and Runge-Kutta.
If the range does not exist, then with the precision with which you are 
computing, the method has not had a chance to converge, and you must 
use more steps, go to higher precision, or use a method of higher order. 
A shorter step-length is of course easiest, but it will not help if round-off 
error becomes predominant before convergence has really occurred.
If an appropriate range does exist, and if it includes several values of N, 
then it is reasonable to try fitting a curve of the form
Co + Ci^
to the values of wjv(ty) in that range, for instance by the method of least 
squares. The value for Cq = Uh(tfi) is then the best guess available for the 
solution of the differential equation using that method, and the quality of 
the fit gives an estimate on the possible error.
In Examples 3.5.1 and 3.5.2 this curve-fitting has been done by the com­
puter, with the results listed at the bottom of each printout.
None of this analysis using the difference of successive approximations 
is quite rigorous, but an equation has to be quite nasty for it not to work. 
Rigorous error bounds will be studied in Chapter 4; unfortunately, useful 
bounds are hard to get. Usually bounds, even if you can find them, are 
wildly pessimistic.
Summary. For a first order differential equation x' = f(t,x) with initial 
condition (to,xo) and a desired interval [to,ty], choose the value of t in 
that interval for which you would expect the approximate solution to be 
the worst, usually tf.
A succession wjv(ty) of numerical approximations can be made using step 
size h = For each TV, calculate
Dn = ux(tf) — UN-l(tf)
and
ln|Djv_i/.E>jv| 
Rn =-------h?2 P-
Find, if it exists, the range of steps for which Rn is roughly the expected 
p, and expect the best value of izjv(ty) to be that with the shortest step 
length in that range, or perhaps the next shorter below that range.
If you desire more precision, use curve fitting, such as least squares, on 
the values uN(tf) in that range of steps. The result should give an estimated 
u(tf) and an estimate on the error.
Then you can compute Uh(t) over the entire interval using the 
stepsize you found best for Uh(tfi).

Exercises
149
Exercises 3.1-3.2 Basic Numerical Methods
3.1- 3.2#1°. This exercise is not at all as trivial as it might seem at first 
glance, and it will teach you a great deal about the various numerical 
methods while keeping the computation to a minimum. (You will see that 
it’s already a job just to keep all of the “formulas” straight, using proper 
tiS and XiS at each step, and you’ll be able to figure out how you have to 
organize the necessary information.) We use a ridiculously large stepsize in 
order that you can really see the results.
Consider x1 = x, starting at to = 0, xq = 1.
(a) Using stepsize h = 1, calculate by hand an approximate solution for 
two steps, for each of the three numerical methods: Euler, midpoint 
Euler, and Runge-Kutta.
(b) Solve the equation analytically and calculate the exact solution for 
ti = 1 and t2 = 2.
(c) Turn a sheet of graph paper sideways and mark off three graphs 
as shown, with large scale units (one inch units are good, with a 
horizontal domain from 0 to 2 and a vertical range from 0 to 1 to 8, 
for each graph).
о
о
О
Euler
midpoint Euler
Runge-Kutta
Using one graph for each of the three methods, construct the approx­
imate solution graphically. That is, use the graph paper to make lines 
of exactly the proper slopes, as calculated in (a). For midpoint Euler 
and Runge-Kutta, make dotted lines for the trial runs showing the 

150
3. Numerical Methods
actual slope marks at beginning, midpoints, and endpoint, as in Fig­
ures 3.2.1 and 3.2.2. Confirm that the final slope for each step indeed 
looks like an average of the intermediate slopes from the trial runs. 
The graphical values for Xi and X2 should be close to the numerical 
approximation values calculated in part (a), and you should add for 
comparison the exact values calculated in part (b).
You may be surprised how well Runge-Kutta makes this approximation 
in two (very large) steps. This exercise was designed to emphasize graphi­
cally how the “better” methods indeed get closer to the true solution, by 
adjusting the slope rather than the stepsize.
3.1— 3.2#2. Consider x' = x2 -1, with t0 = 0, xQ = 1.
(a) With ж(0) = 1 and h = |, calculate the forward (for positive h) and 
backward (for negative h) Euler approximate solution x = Uh(t) in 
the interval — 1 < t < 1.
(b) Make a drawing of the graph of Uh(t) as found in part (a). Determine 
whether the approximate solution you have just formed is above or 
below the real solution.
(c) With ж(0) = 1 and step h = |, calculate by hand (that is, with­
out a program; you may certainly use a calculator to carry out the 
operations) the approximate solution and Uh<X) by
(i) Midpoint Euler;
(ii) Runge-Kutta.
Add and label these points on your graph of part (b).
3.1—3.2#3. Prove that for x' = x2 — t, with initial condition ж(0) = 1, 
that the forward Euler approximate solution, for any stepsize h, is a lower 
fence.
3.1-3.2#4. Actually work out, numerically, the first three steps of Euler’s 
method and of Midpoint Euler for the differential equation x' = x2 — t2, 
ж(0) = 0, with stepsize h = 0.1.
3.1- 3.2#5. Euler’s method can be run forwards or backwards in time, ac­
cording to whether the stepsize h is positive or negative respectively. How­
ever, Euler’s method run backwards is not the inverse of Euler’s method 
running forward: running Euler’s method backwards starting at (to,^o) 
does not usually lead toa(Li,x_i) such that if you start Euler’s method 
at (£-1,ж_1), you will get back to (fo>#o)-
(a) Verify this fact for x' = x2 -1, with ж(0) = 1, h = |. Go forward one 
step, then backwards one step. You should not return to 1.
(b) Make a sketch to show why this is true.

Exercises
151
3.1—3.2#6. A special case of the Runge-Kutta method is for x1 = g(t). In 
this case the slope of Runge-Kutta reduces from
mRK = 
+ 2m2 + 2m3 + m4)
о
to
niRK = 
+ 4m2 + m4).
О
Show why this is true.
3.1— 3.2#7°. Another numerical method, called the improved Euler method, 
is defined as follows for a given stepsize h\
For n = 1,2,... set tn+i =tn + h
_  
। JL f (^1? ^n) H” /(^n+1 , ^71 H” hf (tn> 
— xn । O' 
- -------------------------------- .
(a) Explain on a drawing how xn+i is defined from xn.
(b) For the differential equations for the form = g(t) the improved 
Euler gives a numerical method for integration, another one with 
which you should already be familiar. Explain the method and give 
its name.
3.1— 3.2#8°. For the differential equation x' = x2, with initial condition 
z(0) = 2,
(a) Calculate (by hand, calculator, or computer) an Euler approxima­
tion with stepsize h = 0.1 to find a?(l). Does anything seem wrong 
numerically?
(b) Calculate a Runge-Kutta approximation with stepsize h = 0.1 to 
find a?(l). Does this agree at all with part (a)? Now do you think 
something might be wrong?
(c) To help find out what is going on, solve the equation analytically. 
State why parts (a) and (b) are both wrong, and why you might have 
missed the realization there was any problem.
(d) How could you avoid this problem? Do you think changing the step­
size would help? If so, try finding x(l) with a smaller stepsize, and 
see if you can solve the problem. If not, can you explain why?
(e) This problem could be helped by a numerical method with a built-in 
(automatic) variable stepsize; that is, as the slope gets steeper, the 
stepsize is made smaller; e.g., make stepsize inversely proportional to 
slope. Explain how this can help.

152
3. Numerical Methods
(f) Yet another suggestion is to design the numerical method to take 
the step h along the approximate solution, instead of along t axis. 
Explain how this can help.
3.1—3.2#9. For x' = /(t,x) with x(to) = ж0, consider the approximate 
solution
= Uh(to + h)=x0 + hmto,XOth,
with the midpoint Euler slope 
f I to + 77
as a function of h. Show that Uh(t) has the same 2nd degree Taylor poly­
nomial as the exact solution x = u(t). That is, expand Uh(t) about (t0,^0), 
expand u(t) about u(£0)«
3.1— 3.2#10°. Let the points 
(ti,#i),... be constructed by the
implicit Euler method, as introduced at the end of Section 3.2. That is, for 
x' = /(t,x), use
— t{ “h h and 
"b tlf 
> «^i+l) •
This second expression is only implicit in a?i+i, and must be solved at each 
step to find a?i+i before proceeding to the next step. For the following four 
equations, write (t^Xi) for 0 < i < 3, and (t0,^0) = (0,1)- The equations 
are listed in order of increasing difficulty: the first can be done by hand, the 
second requires a calculator, and the third and fourth are best done using 
something like Analyzer to solve the cubic and transcendental equations 
which appear.
(a) x' = 4 — x
(b) xf = 4ж — x2
(c) x1 = 4ж — ж3
(d) xf = sin(ta).
3.1— 3.2#11. We saw in Example 3.1.4 that if Uh is Euler’s approximation 
to the solution of xf = x with initial condition ж(0) = xq, then Uhff) = 
(1 + at least if t is an integral multiple of h.
(a) Find an analogous formula for the midpoint Euler method.
(b) Evaluate your formula at t = 1 with h = 0.1. How far is the answer 
* from the number (e).
(c) Find an analogous formula for Runge-Kutta.

Exercises
153
(d) Again evaluate your formula at t = 1 with h = 0.1, and compare the 
value with (e).
(e) What other name, in the case of this particular equation, would be 
appropriate for the Midpoint Euler and Runge-Kutta calculations?
Exercises 3.3 Error Due to Approximation Method
3.3#1 Consider the equation x' = ж, solved for 0 < t < 2 with ж(0) = 1, 
as in Example 3.3.1.
(a) Using Table 3.3.1, show that the error for Euler’s method does indeed 
behave like
E(Ji) « CEh.
That is, show that the ratio E(li)/h stabilizes as h gets small. Find 
an estimate of the constant CE.
For this equation, the constant can be evaluated theoretically. As can be 
shown from Example 3.1.4, the Euler approximation Uh(t) with step h gives
Ил(2) = (1 + Л)2/\
(b) Find the Taylor polynomial to first degree of (1 + h)2^h, by the fol- 
lowing procedure:
(i) Writing (1 + h)2/h as e(2/^) ln(i+^)-
(ii) Expanding ln(l + Л) in its Taylor polynomial;
(iii) Use the Taylor polynomial of the exponential function, after 
carefully factoring out the constant term of the exponent.
(c) Find CE exactly from part (b), as lim/l_,0(^/^)- Compare with the 
estimate in part (a).
3.3^2°. (harder) Consider the differential equation xf = x as in the last 
exercise, and let Uh(t) be the approximate solution using midpoint Euler 
with uh(ty = 1-
(a) Again using Table 3.3.1, show that E(h) « CMh2 for some constant 
Cm, and estimate Cm-
(b) Use the explicit formulae from Exercise 3.1-3.2#11 to find an asymp­
totic development (see Appendix) of the error at t = 2; i.e., find an 
asymptotic development of the form
uh(2) = e2 + CMh2 + o(/i2),
and evaluate the constant Cm-

154
3. Numerical Methods
(c) Compare your theoretical value in part (b) with your estimate in part 
(a).
3.3#3. (lengthier) Again consider the differential equation x1 = x, and do 
the same steps as the last two exercises for the Runge-Kutta method. Con­
ceptually this is not harder than the problems above, but the computations 
are a bit awesome.
3.3#4. From Table 3.3.2 in Example 3.3.2, verify that the errors for Euler’s 
method, Midpoint and Runge-Kutta do appear to behave as
E(h) « CEh 
E(h) « CMh2 
E(h) « CRKh4
for Euler’s method, 
for midpoint Euler and 
for Runge-Kutta 
and find approximate values for CE, См^ Crk-
3.3#5. For the following equations, run the program NumMeths over the 
given ranges, with a number of steps from 22 to 210 (or 212 if you have the 
patience, since the speed diminishes as number of steps increases). Observe 
which of these experiments give the predicted orders. For those cases where 
the “order” is unusual, try to find the reason why.
(a) xf = —tx
(b) xr = x2 — t
(c) x' = t2
(d) x' = t
0 < t < 2 
0<t<2 
0<t < 1 
0< t < 1
xo = 2
жо = 0.5
x0 = 1
xo = 1
3.3#6°. Consider x' = 1/(ж + t), where successive derivatives are not 
difficult to compute, and the exact analytic solution can be found as well.
(a) Calculate the 6th order Taylor series for the solution.
(b) Solve the equation analytically.
(c) Starting at ж(0) = 1, compare the exact solution, the truncated sixth 
degree Taylor polynomial, and the fourth order Runge-Kutta approx­
imation for two steps, with h = 0.1. Which approximation gives the 
better answer? Which is least costly to compute?
(d) Repeat part (c) for ж(0) = 0. Explain any differences.
(e) Repeat part (c) for ж(0) = 2. Explain any differences.
3.3#7. Consider Example 3.3.5: xf = x2sint, approximating ж(2тг) with 
ж(0) = 0.3. Show why symmetry about the line t = 7Г leads to cancellation 
of dominant terms in the error E(h).
3.3#8°. Let xf = /(t^x) be a differential equation, let u(f) be the solution 
with initial condition x(t^) = ж0, and let uh(t) be the Euler approximation 

Exercises
155
to this solution. In this chapter we have given numerical evidence that the 
error
E(h) = и(^) - uh(ti)
should have an asymptotic development of the form E(h) = Ch + o(h), and 
in Exercise 3.3# 1 this is proved for the differential equation xf = x.
(a) 
Show that if the equation is simply x' = f(t), then
ад = №)-лда/2+о(л).
(b) Show that if the equation is linear; i.e., xf = g(t)x, then the error 
also has the form E(Ji) = Ch + o(h\ where this time the constant is 
given by the expression
E(/i) = 
[5'(s) + (5(s))2]ds + o(/z).
(c) Show that this expression agrees with that found in Exercise 3.1- 
3.2#U.
(d) Set tQ = 0, and find a number £i > 0 such that Euler’s method as 
applied to x' = —tx has order greater than 1. Use the computer to 
see what the order actually is.
(e) The formula in (b) tells us that the error in Euler’s method will be 
of the form Ch + o(Ji) for practically any function g(t). In order to 
make this fail, we must find a sufficiently nasty function g(t) such 
that (git))2 does not have a finite integral. Show that g(t) = l/|£|a 
is such a function if 0 < a < 1.
(f) Show that nevertheless the differential equation x' = x/|£|Q has unique, 
continuous solutions through every point if 0 < a < 1.
(g) Use the program Numerical Methods to study this equation for a — 
0.3 and a = 0.8, using Euler’s method, for — 1 < t < 1 and x(—1) = 1. 
Can you explain what you observe?
Exercises 3.4 Error Due to Finite Machines
3.4#1°. For xf = £2sin£, the equation of Examples 3.3.2 and 3.3.4, con­
struct an analysis like that of Examples 3.4.1 and 3.4.2. That is,
(a) 
make a calculation using only 18 bits, rounding down;
(b) make a calculation using only 18 bits, rounding round;

156
3. Numerical Methods
(c) for each of parts (a) and (b), state the range of stepsizes giving the 
proper “order”; also compare the magnitude of errors in parts (a) and 
(b).
3.4#2. Run the program NumMeths, for number of steps: 21 to 210, for 
the differential equation
x =x, 0 < t < 2, z(0) = 0.5,
under all of the following conditions (a total of 18 runs):
method: Euler, midpoint Euler, and Runge-Kutta 
number of bits: 10, 35, and 64.
rounding: down, and round.
For each printout
(a) Describe the region in which the terms coming from integration error 
and those commonly from round-off error are dominant; and
(b) Say which step length gave the most accurate value and why.
3.4#3. Show that the analysis of Section 3.4.2, “solving xf = ax in a noisy 
environment,” follows through in the case x' = a(t)x, for a a function of t.
Exercises 3.5 What To Do in Practice
3.5#1. For the following differential equations, solve numerically by vari­
ous methods. Use 20 bits so that you will see the effects of roundoff error. 
Find how closely your approximate solution near the bottom of a range of 
good numbers of steps (i.e., those with the proper order appearing) com­
pares with an exact analytic solution (as found in Exercises 2.1#2b,d,f 
respectively).
(a) x' = (1 + a;)/(l - t)
(b) (x — a) + t2x' = 0
(c) x' = (1 + a;2)/(l +1?)
0<t<2
1 <t<2 
0 < t < 1
x(0) = 3 
x(l) = 2e 
x(0) = 3.
3.5#2. For the following differential equations, which are not easy or pos­
sible to solve analytically, solve numerically by midpoint Euler and Runge- 
Kutta. Use 20 bits so that you will see the effects of roundoff error.
(a) x' — sin (ж2 — t)
(b) x' = sin(a;2 — t)
(c) x' - x2 — t
(d) x' = l/(a:2 +1 + 1)
0<t<27r
0 < t < 2?r
0 < t < 5
0 < t < 2
x(0) = 0 
x(0) = 0.7 
a;(0) = 0 
ar(O) = 0.

4
Fundamental Inequality, 
Existence, and Uniqueness
To begin, we must emphasize why we need to get theoretical.
If we have a direction field for a differential equation, we need to know 
whether through a given point (i.e., with a given initial condition) there is 
a solution. Furthermore, if a solution does exist through such a point, when 
can we be sure it is the only one? These are the questions of existence and 
uniqueness, and they are not trivial.
Chapters on “existence and uniqueness” are usually viewed as absolutely 
essential and central by mathematicians, and absolutely useless or meaning­
less by everyone else. Students in particular have traditionally considered 
these chapters as a prime example of a mathematician talking to himself.
But we shall demonstrate the need for these concepts (in Sections 4.1 
and 4.2) and develop useful conditions (in Sections 4.3 and 4.4) that will 
guarantee existence and uniqueness (in Section 4.5).
Then in Section 4.6 we use these results to bound the error E(h) arising 
in numerical approximations of solutions, as discussed in Section 3.3. And 
finally in Section 4.7 we can prove general versions of the fence, funnel, and 
antifunnel theorems introduced in Sections 1.3 and 1.4.
4.1 Existence: Approximation and Error Estimate
Existence theorems fall in two classes, constructive and non-constructive. 
People wishing to use mathematics are probably right to disdain the non­
constructive proofs; they correctly think that proving that something exists 
but giving no way of finding it will not help them much.
Constructive existence proofs are quite a different matter. They almost 
always consist of two parts: an approximation procedure and an error esti­
mate. In other words,
constructive existence proofs give you a recipe for finding an 
approximation to what you want, and a formula for estimating 
how good an approximation it is.
The existence is then proved by taking a sequence of better and better 
approximations, and showing that they converge using the error estimate.

158
4. Fundamental Inequality, Existence, and Uniqueness
Historically, the approximation procedure was mainly a theoretical tool. 
In most cases, carrying out an approximation involved an amount of com­
putation that could only be handled by hand if the result were of sufficiently 
great interest to warrant the time and tedium, and in particular could not 
be carried out in a classroom. As a result, teachers were often careless 
about emphasizing the actual computability of their approximations, and 
the students then naturally felt that the entire subject was a prime example 
of mathematicians being impractical.
However in this day of easy computer access, a constructive proof of 
existence and uniqueness can be provided that will be useful as well as 
instructional.
In Chapter 3 we introduced the approximation procedures, which lead 
to existence. We finish in Section 4.4 with the error estimate, in the form 
of the Fundamental Inequality, which will immediately give uniqueness as 
well.
However, we should mention here that numerical accuracy of approxi­
mation is not the only problem to watch out for. Numerical methods are 
an iterative process, which will be discussed at length in Chapter 5. The 
pictures which result from numerical approximation do indeed demonstrate 
existence, but they can be misleading.
Peek ahead to Figure 5.4.6 for our favorite equation xr = x2 — t. A 
slight increase in stepsize h for the approximate solutions that are drawn 
has led to spurious solutions. This drawing emphasizes the need for the 
fence, funnel, and antifunnel theorems to be given in Section 4.7. With 
these theorems we can prove which portions of the pictures are correct 
(the funnel and antifunnel on bottom and top respectively of the parabola 
x2 = t) and recognize whether an apparent solution is in fact spurious 
(the seeming funnel above the lower curve) or incorrect (the jagged ones, 
or those that drop down below the parabola). Later, in Chapter 5, we’ll 
discuss how to control the stepsize to avoid misleading pictures, but what 
you must remember (now and always) is that an apparent feature must be 
proved to be there before you can be sure.
4.2 Uniqueness: The Leaking Bucket Versus 
Radioactive Decay
You might, take the attitude: “O.K., we can see on the computer screen that 
differential equations have solutions, determined by the initial conditions. 
So I believe in existence—let’s get on to something practical.”
It isn’t quite that simple: for some equations, even some of practical 
importance, uniqueness does not hold. That is, there may be more than 
one solution for a given initial condition. Here we want to discuss this 
situation. What does non-uniqueness mean? How can you tell when this

4.2 . Uniqueness: The Leaking Bucket Versus Radioactive Decay 159 
will be a problem?
A good example is the leaky bucket (Examples 4.2.1, 3 and 4.3.6), as 
contrasted with radioactive decay (Examples 4.2.2, 4 and 4.3.5).
Example 4.2 .1. Leaky bucket. Consider a bucket with a hole in the bottom, 
as in Figure 4.2.1.
FIGURE 4.2.1. Leaky bucket.
If at a given time you see the bucket empty, can you figure out when (if 
ever) it was full? No, you (obviously) cannot!
We shall see (Example 4.2.3) that there simply is not a unique solution to 
the differential equation that states this problem. In order to find out why 
we have non-uniqueness and what is special about f(t,#) in this example, 
we will develop its differential equation.
To study the rate at which the water level drops, we need a physical 
assumption about the velocity with which the water leaves the hole. Phys­
ically the following model may be an oversimplification for the situation in 
question, but it certainly is reasonable to assume that the velocity v(t) will 
depend on the height h(t) of the water remaining in the bucket at time t. 
After all, the water will flow faster when the bucket is full than when it 
is nearly empty (the greater depth of water exerts more pressure to push 
water out of the hole).
The volume of water leaving the hole equals the volume of water lost in 
the bucket, so
av(t) = Ah'(t), 
(1)
where a = the cross-section of the hole and A = the cross-section of the 
bucket.
Furthermore, if one assumes no energy loss, then the potential energy 
lost at the top when a small amount of water has left the bucket must 
equal the kinetic energy of an equal amount of water leaving the bottom 

160
4. Fundamental Inequality, Existence, and Uniqueness
of the bucket through the hole. That is, if p is the density of the water, the 
mass of water in question is (Ah)Ap, and
[^h)Ap]gh =
SO
v2 = 2p/i. 
(2)
Combining equations (1) and (2),
or
(fc'(Z))2 = 2g
h' = -CVh,
(3)
The level of water in the bucket obeys (approximately) this equation (3), 
cited in physics as Torricelli’s Law. Some specific examples are given in 
Exercises 4.1-4.2#2,3.
We shall study the differential equation (3) as x' = —Сд/ж, for 0 < x < 1, 
where x = 0 corresponds to an empty bucket and x = 1 to a full bucket.
By separation of variables,
dt.
and the solution with u(0) = 1 (full bucket) is
C2Z v
4 
“ te)
u(t) = <
for 0 < t < te
(4)
10
for t > tei
where te = 2/C, the time required for the bucket to go from full to empty. 
You get to verify this solution in Exercises 4.1-4.2#1, where you will also 
verify that it is indeed differentiable—even at te—and that it does satisfy 
the differential equation. The graph of this full bucket solution occurs in 
Figure 4.2.2, the rightmost solution.
This solution for a full bucket initial condition u(0) = 1 uniquely de­
termines the height u(t) of the water at any time t > 0. The problem of 
non-uniqueness arises when we look backwards from an empty bucket initial 
condition.
For example, an empty bucket initial condition u(te) = 0 could have re­
sulted from a full bucket at any t < 0. Figure 4.2.2 shows some solutions for 
this case. The highlighted curve corresponds to the full bucket solution (4) 
with u(0) = 1. The empty bucket solutions for u(te) = 0 are all horizontal 

4.2. Uniqueness: The Leaking Bucket Versus Radioactive Decay 161 
translates (to the left) of that full bucket solution that reaches the 2-axis 
at t — tg. Ж
FIGURE 4.2.2. x' = -Cy/x.
Contrast the leaky bucket equation, x' = — Cy[x of Example 4.2.1, with 
the deceptively similar equation x' = —Cx of Example 4.2.2.
Example 4.2 .2. Radioactive decay. Consider the disintegration of a ra­
dioactive substance over time, as discussed in Section 2.5. The differential 
equation is
xf = — Cx, with C > 0, 
(5)
and initial condition u(to) = xq] the solution is
u(t) = x0e-c^~to\ 
(6)
Uniqueness holds for the solutions (6) to this differential equation (5), 
and working backwards from any initial condition is no problem, at least 
mathematically. For instance, for t < 1,000,000 years, then carbon dating, 
as discussed in Example 2.5.3, can determine, from the ratio of x/xq in 
a given sample of dead organic matter, exactly when that organism died. 
(From the physical point of view, for large t the right-hand side of (6) be­
comes zero within the limits of experimental error; this difficulty is explored 
in Exercise 4.1-4.2#4.) 
▲
So, from current data the radioactive decay equation can tell you ex­
actly what happened a given number of years ago, but the bucket equation 
cannot. We need a mathematical criterion to tell why these differential 

162
4. Fundamental Inequality, Existence, and Uniqueness
equations behave in such different ways. We shall prepare for this crite­
rion by looking more closely at f(t,x) for both differential equations in 
Examples 4.2.1 and 4.2.2.
In order to emphasize that the criterion does not involve any funny busi­
ness along the boundary of the domain, we shall extend these equations to 
the entire t, ж-plane. Since the bucket equation (3) requires an extension of 
the form xf = —С, we shall extend the radioactive decay equation (5) 
to x' = —С|ж|.
Example 4.2 .3. Extended leaky bucket equation. Consider again the equa­
tion of Example 4.2.1,
x' = — Су/Щ, with C > 0. 
(7)
The computer solutions are shown in Figure 4.2.3.
Analytically, one solution to equation (7) is u(f) = 0, and from separation 
of variables (first for x > 0 and then for x < 0) we get the following general 
solution:
C2
— (t - ti)2 for t < ti
u(t) = < 0 
f°r
for t >
where ti and t2 are the points where a solution meets and leaves (respec­
tively, from left to right), the t-axis, as shown in Figure 4.2.4, and where 
possibly ti = —oo and/or t2 = oo.

4.2. Uniqueness: The Leaking Bucket Versus Radioactive Decay
163
FIGURE 4.2.4. One solution for xr = —
If xq / 0, then for any initial condition
ti = to + (2/C)y/x^ (for xq > 0),
t2 = t0 - (2/C)V-^o (for xQ < 0).
Note that in Figure 4.2.3 and 4.2.4 the solutions for x > 0 and for x < 0 
each show only half the parabola that you might have expected from the 
formula for the solutions—the half with non-positive slope, as required by 
the differential equation (7).
The big point illustrated in Figure 4.2.3 is that all the solutions to (7) 
meet or cross the t-axis, which is also a solution, illustrating graphically 
that there is no uniqueness in any region including the t-axis. For example, 
Figure 4.2.5 shows some of the infinite number of solutions for a given 
initial condition with xQ above the axis—there is uniqueness only until the 
solution meets the axis.
Furthermore, the t-axis is a weak lower fence a(t), because it is true that 
= /(t,0) = 0 = a'(t).
This illustrates the failure of the Fence Theorem 1.3.5 for weak fences, 
because solutions can slip right through, as in Figure 4.2.5.

164
4. Fundamental Inequality, Existence, and Uniqueness
We shall return to this example in Sections 4.3 and 4.7, where a single 
reason for these difficulties will become apparent. A
Example 4.2 .4. Extended radioactive decay equation. Consider again the 
equation of Example 4.2.2,
x' = — С|я?|, with C > 0.
(8)
Again, u(t) = 0 is a solution, and from separation of variables (first for 
x > 0 and then for x < 0) we get the following general solution:
u(f) = <
XQe *°)
0 
xoec^~to>)
for Xq > 0
for xq = 0
for a?o < 0-
The solutions to the extended radioactive decay equation (8) are shown 
in Figure 4.2.6.

4.3. The Lipschitz Condition
165
FIGURE 4.2.6. x' = -C\x\.
The difference between this example and the last is that this time we do 
have uniqueness; although u(t) = 0 is a solution to (8), the other solutions 
never meet or cross it. (You may not see that in the picture, but look at 
the formulas in the analytic solution!) ▲
We now have an example (4.2.1 and 3) with non-uniqueness, and another 
(4.2.2 and 4) with uniqueness. We shall proceed in the next section to 
examine the key to the difference.
4.3 The Lipschitz Condition
What makes the difference between the leaky bucket and radioactive decay 
examples is that Example 4.2.2 admits a Lipschitz constant while Example 
4.2.1 does not. This quantity is named for Rudolf Otto Sigismund Lipschitz, 
a German mathematician who in the mid-19th century simplified and clari­
fied Augustin-Louis Cauchy’s original work, earlier in the same century, on 
existence and uniqueness of solutions to differential equations. Lipschitz’ 
work provides a more general condition for existence and uniqueness than 
the requirement that df /дх be continuous, as quoted in most elementary 
texts.
Definition 4.3.1. A number К is a Lipschitz constant with respect to x 
for a function f(t, x) defined on a region A of R2 (the t, ж-plane) if 
f(t,x2)\<K\xi~x2\, 
(9)

166
4. Fundamental Inequality, Existence, and Uniqueness
for all (t,^i), (£,#2) in A. We call this inequality a Lipschitz condition in 
x.
A differential equation x' = f(t, x) admits a Lipschitz condition if the 
function f admits a Lipschitz condition. What makes a Lipschitz condition 
important is that
the Lipschitz constant К bounds the rate at which solutions can 
pull apart,
as the following computation shows. If in the region A, ui(t) and uz(t) are 
two solutions to the differential equation xf = f(t,x), then they pull apart 
at a rate
|(ui-u2)'(t)| = |/(t,Ui(t)) -/(t,U2(t))| < K|ui(t) -U2(t)|.
So in practice we will want the smallest possible value for К.
As we will see, such a number К also controls for numerical solutions of 
a differential equation the rate at which errors compound.
The existence of a Lipschitz condition is often very easy to ascertain, if 
the function in question is continuously differentiable.
Theorem 4.3.2. If on a rectangle R = [a, b] x [c, d\ a function f(t,x) is 
differentiable in x with continuous derivative df /дх, then f(t,x) satisfies 
a Lipschitz condition in x with best possible Lipschitz constant equal to the 
maximum value of \df/dx\ achieved in R. That is,
df
К = sup — 
(t,x)eR dx
(tl , X1 )
FIGURE 4.3.1.
(ti,x2)
f (ti,x*)

4.3. The Lipschitz Condition
167
Proof. If throughout R the function f(t, x) is continuous and differentiable 
in x, the Mean Value Theorem says that
- f(t,X2) =
(rri -x2), 
(M*)
where x\ < x* < х%, as shown in Figure 4.3.1.
Furthermore, since R is closed and bounded, if df /дх is continuous, the 
number К — sup \df /дх\ is finite. □
Example 4.3 .3. The differential equation xr = ax 4- b has Lipschitz con­
stant К = \a\ everywhere. ▲
Example 4.3 .4. Consider x' = sintx for 0 < t < 3, — 5 < x < 5. Since
— (sintx) — t costa, 
ox
sup -(state) < |i| < 3.
This value is realized at t = 3 and cos3rr = 1, or x — 0, so 3 is the best 
Lipschitz constant on this rectangle. ▲
The equation x' = ax 4- b of Example 4.3.3 is the archetypal example. 
We will use the Fundamental Inequality in Section 4.4 to compare all other 
differential equations against this one because we know its explicit solution 
and its Lipschitz constant.
We shall see in the rest of this chapter that the Lipschitz condition is the 
key hypothesis for theorems giving uniqueness, fences and funnels, and the 
Fundamental Inequality. Since the Lipschitz condition is what is useful in 
proofs, we have isolated that rather than differentiability as the criterion 
for uniqueness.
Usually our functions will satisfy a Lipschitz condition because, by The­
orem 4.3.2, they are continuously differentiable with respect to x. However 
f(t, x) may satisfy a Lipschitz condition even if f(t,x) is not differentiable 
in x, as the following examples show. Satisfying a Lipschitz condition sim­
ply means satisfying equation (9), and that equation by itself does not 
require a differentiable function.
Our familiar Examples 4.2.2,4 and 4.2.1,3 will give us a good look at the 
role of the Lipschitz condition.
Example 4.3 .5. Extended radioactive decay equation, continued. 
For xf = — C|a;|,
dx
-C
not defined
C
for x > 0 
for x = 0 
for x < 0.

168
4. Fundamental Inequality, Existence, and Uniqueness
The function = — С|ж| is not differentiable in x at x = 0, but as 
ж —> 0, \df /дх\ = C, as shown in Figure 4.3.2 (which is a graph of /(£,ж) 
versus x).
FIGURE 4.3.2. f versus x for xf = —С|ж| — f(t,x).
However, the following statement is true for f(t, x) = —С|ж|:
\f(t, Xl) - 
= C|(-|xi| + |«2|)| < 
-Ж2|-
Hence even though this /(t, x) is not differentiable at x = 0, it nevertheless 
does satisfy a Lipschitz condition in the whole t, ж-plane, with Lipschitz 
constant К = C. A
FIGURE 4.3.3. f versus x for x' = -Cy/x = f(t,x).

4.4. The Fundamental Inequality
169
The function f(t, x) = —Су/Щ is not differentiable in x at x = 0, but 
it also, as we shall see, does not satisfy a Lipschitz condition in any region 
which contains a point (t, 0). This is because as x —> 0, \df/dx\ —> oo, as 
shown in Figure 4.3.3 (another graph in f(t,x) versus x).
Therefore there can be no finite Lipschitz constant К. 
▲
Thus we now have the essential difference between the leaky bucket sit­
uation of Example 4.2.1 and the radioactive decay situation of Example 
4.2.2. In the first case, there is no Lipschitz condition in any region A in­
cluding the /-axis, and there is no uniqueness; in the second case, there is 
a Lipschitz condition throughout the entire plane, and there is uniqueness. 
We shall actually prove the theorem in Section 4.5.
Because of Theorem 4.3.2, in our Examples 4.3.6 and 4.3.5 we only 
needed to make actual calculations for К around x = 0, where df /dx 
is not differentiable. Throughout the rest of the t,x-plane in both cases, a 
local Lipschitz condition holds.
Definition 4.3.7. A function f(t,x) defined on an open subset U of R2 is 
locally Lipschitz if about every point there exists a neighborhood on which 
f is Lipschitz.
For a function /, where the partial derivative df /дх is continuous, then 
f is locally Lipschitz, as a consequence of Theorem 4.3.2.
We shall show subsequently in Theorem 4.5.1 that the existence of a 
local Lipschitz condition is sufficient to assure uniqueness. Example 4.2.2 
in the previous section, shows clearly that without a Lipschitz condition, 
you may not have uniqueness, as this example does not.
We shall proceed in the remainder of this chapter to show how the Lip­
schitz condition is used in proving the important theorems.
4.4 The Fundamental Inequality
The inequality that we shall state in this section and prove in the next 
contains most of the general theory of differential equations. We will show 
that two functions Ui and u2 which both approximately solve the equation, 
and which have approximately the same value at some to, are close.
Let R = [a, 5] x [c, d] be a rectangle in the t, ж-plane, as shown in Figure 
4.4.1. Consider the differential equation xf = f{t,x), where f is a continu­
ous function satisfying a Lipschitz condition with respect to x in R. That 
is,
|/(t, rci) - f(t, x2)| < 
- z2|
for all t e [a, b] and Zi, x2 € [c, d].

170
4. Fundamental Inequality, Existence, and Uniqueness
Suppose u\(t) and uz(t) are two piecewise differentiable functions, the 
graphs of which lie in R and which are approximate solutions to the differ­
ential equation in the sense that for nonnegative numbers £1 and £2,
l«'i(t)-/(t,«i(t))| <Ei, and |i4(t) - /(t,u2(t))| < £2,
for all t e [a, b].
Suppose furthermore that ui(t) and U2(t) have approximately the same 
value at some to 6 [a, 6]. That is, for some nonnegative number 6,
|ui(to) -«2(to)I <
Figure 4.4.1 shows how all these conditions relate to the slope field for 
xf = f(t,x).
The diagram of Figure 4.4.1 is not the most general possible, which 
could include approximate solutions that cross and/or are piecewise dif­
ferentiable, but it gives the clearest picture for our current purpose. In 
particular, this diagram sums up all the hypotheses for the following major 
result.

4.4. The Fundamental Inequality
171
Theorem 4.4.1 (Fundamental Inequality). If, on a rectangle R = 
[a, b] x [c, d], the differential equation x' = f(t,x) satisfies a Lipschitz con­
dition with respect to x, with Lipschitz constant К 0, and if u± (i) and 
?i2(i) are two continuous, piecewise differentiable, functions satisfying
Iw'lW - /(*, «l(*))l < 
1*4(0 -/(^«2(i))| < e2
for all t E [a, b] at which ui(t) and u2(i) are differentiable; and if for some 
to E [a, b]
|ui(*o) - u2(f0)| < <5;
then for all t E [a, b],
\ui(t) - u2(t)\ < SeK^ *o1 + j(eK,f *°l — 1),
(10)
where e = Ei -h e2.
Before we proceed to the proof, let us study the result. The boxed for­
mula (10) is the Fundamental Inequality, and it certainly looks formidable. 
Nevertheless, looking at it carefully will make it seem friendlier. First, no­
tice that it does say the and u2 remain close, or at least it gives a bound 
on |ui — u2|, over the entire interval [a, b], not just at to.
Second, notice that the bound is the sum of two terms, one having to do 
with 6 and the other with e. You can think of the first as the contribution 
of the difference or error in the initial conditions, and the second error in 
solving the equation numerically.
Both terms have an exponential with a К in the exponent, this means 
that the bounds on all errors grow exponentially in time, with the Lipschitz 
constant К controlling the growth rate. Note that this exponential growth 
with respect to t is in contrast to the error discussion in Chapter 3 for fixed 
t, there error functions are polynomials in h.
From some points of view, the Fundamental Inequality (10) is a wonderful 
tool. First, it gives existence and uniqueness of solutions to differential 
equations, as we shall prove in Section 4.5.
Second, even if a differential equation itself is only known approximately, 
the Fundamental Inequality tells us that an approximate solution to an 
approximate equation is an approximate solution to the real equation. This 
happens, for instance, in mathematical modeling when the coefficients that 
appear in a differential equation describing some situation are measured 
experimentally and as such are never known exactly. See Exercise 4.4#6.
And third, the Fundamental Inequality will hold even if a real differential 
equation is stochastic. This is the situation where the differential equation 
that is written down corresponds to an ideal system, but in reality the 

172
4. Fundamental Inequality, Existence, and Uniqueness
system is constantly disturbed by random (or at least unknown) noise. 
Essentially all real systems are of this type.
On the other hand, although the Fundamental Inequality gives all the 
wonderful results cited above, it is quite discouraging. Exponential growth 
is very rapid, which means we should not trust numerical approximations 
over long periods of time. The fact that the Fundamental Inequality cor­
rectly describes the behavior of differential equations in bad cases is a large 
part of why a theory of such equations is necessary: numerical methods are 
inherently untrustworthy, and anything that you might guess using them, 
especially if it concerns long term behavior, must be checked by other meth­
ods.
Proving the Fundamental Inequality
The Fundamental Inequality makes a statement about the error approxi­
mation E(t) = |ui(t) — ^2(^)1 for all t, given its value at to- So we have
F(t0) < 6,
and we want to prove that for all t,
(10, again)
This is reminiscent of a fence situation, because we know that at to, 
E(t) < right-hand side of (10), and we want to prove that E(t) stays below 
the right-hand expression for all t > to, and then (which must be proved 
separately) for all t < t0- So indeed we shall use a fence to prove it!
Proof. Any continuous piecewise differentiable function u(t) can be ap­
proximated, together with its derivative, by a piecewise linear function 
v(t), simply by replacing its graph with line segments between a sequence 
of points along the graph, and being sure to include among that sequence of 
points all those points of discontinuity of the derivative u'(t), as in Figure 
4.4.2.
FIGURE 4.4.2. Piecewise linear approximation v(t).
So, for any pair of approximate solutions (t) and U2 (t) and any positive
number 77, there exist piecewise linear functions and V2(t) such that 
\Vi(t) -Ui(t)\<rj

4.4. The Fundamental Inequality
173
and
wherever both Ui(t) and Vi(t) are differentiable. Let
7(t) = |vi(t) - v2(t)|;
the function 7(t) is continuous and piecewise linear.
Then
< 1^'iW 
+2^7
< -/(t,W2(t))| + б1 +б2 +2//
< К|tzi(t) — w2(t)| + 6i + 62 + 2т/
< #{|тл(t) — ^2(t)| + 277} + 61 + 62 + 2т/
< ^7(^+6^,
where 6^ = 61 + 62 + 2т/(1 + K).
You should justify these steps as Exercise 4.4#1; each step is important, 
but not quite obvious. The first inequality is true with left-hand derivatives 
and right-hand derivatives at points where they differ. The result of this 
sequence of steps,
7'(t) < K7(t) + en, 
says that the piecewise linear curve 7(f) is a strong lower fence for another 
differential equation,
x' = Kx + 6^, with solution x = u(t). 
(11)
We can apply the Fence Theorem 1.3.5 for t > to, and for К / 0, we have 
solved equation (11) as a linear equation in Example 2.3.1
w(t) = 
(eK(t-to) - 1).
If we reverse time and solve for the case where t < to, we can replace this 
equation by
w(t) = xoeK^~to^ + 
(ек\^о\ - 1).
Because we have 7(to) < 
= 6 + 277, we know that 7(t) will remain
below the solution w(t) with w(to) = Xq = 6^, so
Mt) - u2(t)| < 
+ (^) 
- 1),
|«i(t) - «2(t)| < 
+ (|) 
- 1) + 2r). 
(12)

174
4. Fundamental Inequality, Existence, and Uniqueness
Equation (12) holds for any rj > 0, so in the limit as 77 —> 0, which implies 
that St? —> s and 6^ 
6, this is the desired conclusion of the Fundamental
Inequality.
Solving x1 = Kx + e for the case where К = 0 and reapplying the Fence 
Theorem 1.3.5, is left to the reader in Exercise 4.4#2. 
□
4.5 Existence and Uniqueness
The Fundamental Inequality
- «2(^)1 < бе*'1*-*01 + (j0 
v- 1), 
(10, again)
gives uniqueness of solutions, the fact that on a direction field wherever 
uniqueness holds two solutions cannot cross.
Theorem 4.5.1 (Uniqueness). Consider the differential equation xf = 
f(t, x), where f is a function satisfying a Lipschitz condition with respect to 
x on a rectangle R = [a, b] x [c, d] in the t, x-plane. Then for any given initial 
condition (to, ^o), if there exists a solution, there is exactly one solution u(t) 
with u(to) = xq.
Proof. Apply the Fundamental Inequality of Theorem 4.4.1. If two different 
solutions passed through the same point, their 6 would be zero. But their 
e’s would also be zero because they were both actual solutions. Therefore, 
the difference between such solutions would have to be zero. □
More generally, if the function f(t,x) in Theorem 4.5.1 is locally Lips­
chitz, then locally xf = f(t,x) will have uniqueness. E.g., the leaky bucket 
equation of Example 4.2.1 will have uniqueness of solutions for any (t,x) 
with x Ф 0.
Please note, however, that lack of a Lipschitz condition does not neces­
sarily mean that we have no uniqueness of solutions. Good examples are 
x' = y/\x —1\ and xf = 
1 (Exercises 4.5#1,2).
The Fundamental Inequality also will give, in Theorems 4.5.5 and 4.5.6, 
existence of solutions to differential equations, as well as the fact that our 
approximation schemes converge to solutions.
You should observe that the statement of the Fundamental Inequality 
makes no reference to actual solutions of differential equations, that is, 
to continuous functions u(t) that satisfy xf = f(t,x). Rather it refers to 
functions uh(t) that approximately solve the differential equation in the 
sense that the slope error, |iz^(t) — f(t,Uh(tf)\, is small.
It is reasonable to hope that such functions iz^(t) approximate solutions, 
and this is true under appropriate circumstances. But to make such a state­
ment meaningful, we would need to know that solutions exist. Instead, we 

4.5. Existence and Uniqueness
175
intend to use the Fundamental Inequality to prove existence of solutions, so 
we must use a different approach, by means of the following three theorems.
Theorem 4.5.2 (Bound on slope error, Euler’s method). Consider 
the differential equation x' = /(£, x), where f is a continuous function on a 
rectangle R= [a, b] x [c, d] in the t, x-plane. Let Uh be the Euler approximate 
solution with step h. Then
(i) for every h there is an eh such that Uh satisfies
\u'h(t) - f(t,Uh(t))\ <Eh
at any point where Uh is differentiable (and the inequality holds for 
left- and right-hand derivatives elsewhere);
(ii) —> 0 as h —> 0;
(iii) if furthermore f is a function on R with continuous derivatives with 
respect to x and t, with the following bounds over R:
sup |/| < M;
df
SUP5F < P; sup 3/
dx <K,
then there is a specific bound on
К(t) - f(t,uh(t))\ < h(P + KM).
Proof. Parts (i) and (ii) are not difficult to prove, using the concept of 
uniform continuity, but we choose not to do so here. Such a proof is non­
constructive (that is, it does not lead to a formula for computing e^), and 
therefore is not in the spirit of this book.
We proceed with the proof of Part (iii). Over a single interval of the 
Euler approximation, that is, for ti <t <
l<(*) - 
Uh(t))\ = \ f(ti,Xi) - f(t,Uh(t))\
< 
- f(t,Xi)\ + \f(t,Xi) - f(t,uh(t))\
< 
|t - ti\P + \uh(t) - Xi\K
<hP + hMK
= h(P + MK) = eh.
The bounds P, К, M are introduced by two applications of the Mean 
Value Theorem. Thus we have an explicit expression for an upper bound of 
Ch, for any differentiable 
□
Example 4.5.3. Consider x' = sin tx, first discussed in Example 1.5.2.
Later in Example 3.2.1 we found at tf = 2 the Euler approximate solution

176
4. Fundamental Inequality, Existence, and Uniqueness
through to = 0) 
= 3. Here we shall calculate the bound on slope
error appropriate for that example.
First we need a rectangle R on which to work, which is often the hardest 
part in calculating 6^ that is, given an interval for t, we need to find an 
interval for x that will contain the solutions in question.
For this differential equation we already know that t G [0,2], and that 
xq = 3; we can also see that the maximum slope is -1-1 and the minimum 
slope is —1; therefore we know that Uh(tf) cannot move further than 3±2, 
so Uh(t) will stay within R = [0,2] x [1,5]. See Figure 4.5.1. We can now 
calculate the various bounds over R = [0,2] x [1,5]:
df 
sup^
Of 
sup at
= sup |tcos(ta)| < 2 = К
= sup |rccos(tj;)| < 5 = P
sup \f\ = SUPI sin(ta)| < 1 = M.
t=2
FIGURE 4.5.1. Rectangle for bounding slope error with Euler’s method.

4.5. Existence and Uniqueness
177
Therefore, by Theorem 4.5.2,
Eh < h(P + MK) < 7h. 
▲ 
(13)
We can use the bound Eh on slope error to bound the actual error 
E(h). The Fundamental Inequality (11) says that through a given point 
(t0, ж0), the actual error between the solution and an approximate solution 
is bounded as follows:
E(h) = |u(t) - Uh(t)| < |- 1). 
(14)
Equation (14) shows that the bounds on the actual error E depend on the 
slope error bound Eh as a factor in an expression involving exponentials and 
a constant K. So the main thing to understand about numerical estimates 
is that:
You cannot escape the exponential term in bounds on actual 
error \u(t) — Uh(f)|. All you can affect is Eh, a bound on the 
slope error \u'h(t) - f(t,uh(t))\.
Furthermore, these formulas (13) and (14) for bounds on Eh and E(Ji) 
respectively are just that—only bounds, and overly pessimistic bounds at 
that; they are wildly overdone.
Example 4.5.4. Consider again xf = sintx, with to = 0, Xq = 3, tf = 2 
as in Example 4.5.3, where К = 2 and Eh = 7 h. Substituting these values 
into the estimate (14) gives
E(h) < ^(е^1‘-‘о1 - 1) < ^(e2<2) - 1) « 191 h.
To compare this with an actual computation, we go back to Example 
3.5.1 where, using the interpolated “solution” as we find the actual 
constant of proportionality between E(h) and h is more like 0.45 than 191. 
▲
Theorem 4.5.5 (Convergence of Euler approximations). Consider 
the differential equation xf = f(t,x), where f is a continuous function 
satisfying a Lipschitz condition in x with constant к on a rectangle R = 
[a,b] x [c, d] in the t, x-plane. If Uh and Uk are two Euler approximate 
solutions, with steps h and к respectively, having graphs in R, and having 
the same initial conditions
Uh(t0) = Ufc(to) = я?0,
then for all t € [a, 6],
-1),

178
4. Fundamental Inequality, Existence, and Uniqueness
where £h and £k both go to zero as h and к go to zero. Moreover, if df /dt 
and df /дх exist and are bounded as in Theorem 4.5.2, then,
\uh(t) - uk(t)\ < —MK(h + к) (ек1‘-*о1 _ 1).
Proof. The first part follows immediately from Theorem 4.4.1 (the Fun­
damental Inequality) and the first part of Theorem 4.5.2, since
6 = |M*o) ~ ^fc(^o)| = 0,
because Uh(tQ) = Uk(to) = xQ.
The second part follows from the second part of Theorem 4.5.2, choosing
eh = (P + MK)h, ек = {Р + МК)к, £ = £h + £k' 
□
We now have all the ingredients for a proof of existence of solutions. 
Theorem 4.5.5 shows that the Euler approximations converge as the step 
tends to 0. Indeed, it says that once your step gets sufficiently small, the 
Euler approximations change very little. For instance, you can choose a 
sufficiently small step so that the first n digits always agree, where n is as 
large an integer as you like.
We have never met a student who doubted that if the Euler approximate 
solutions converge, they do in fact converge to a solution; in that sense the 
next theorem is really for the deeply skeptical, and we suggest skipping the 
proof in a first reading, for it is messy and not particularly illuminating. 
Still, if you think about it, you will see that the Euler approximate solutions 
with smaller and smaller step have angles at more and more points, so it 
is not quite obvious that the limit should be differentiable at all.
Theorem 4.5.6 (Existence of solutions). Consider the differential equa­
tion x' = f(t,x), where f is a continuously differentiable function on a 
rectangle R = [a, b] x [c, d] in the t,x-plane. If the Euler approximate solu­
tions Uh, with step h and initial condition Uh(t$) = x$, have graphs which 
lie in R for sufficiently small h, then for all t 6 [a, b],
(i) The limit u(t) = lim^_>o exists;
(ii) The function u(t) is differentiable, and is a solution of xf = f(t,x).
Proof. Part (i), the convergence, comes from Theorem 4.5.5.
For part (ii) (that convergence is to a solution to the differential equation, 
and that the result is differentiable) we need to prove a standard epsilon­
delta statement, for which we shall use s* and 6* to avoid confusion with 
our use of £ and 6 in the Fundamental Inequality. This is provided by the 
following lemma:

4.5. Existence and Uniqueness
179
Lemma. For all t with a <t <b, for all e* > 0, there exists a 6* > 0 such 
that if \tj\ < S*, then
u(t + t?) “ u(f)
Л
/(*,«(*))
« slope of Euler 
approximation
slope of solution to 
differential equation
Proof of Lemma. Choose s* > 0; we will show that 6* = 2(p+mk) works. 
Since by Part (i)
u(t + 77) - u(t) - rjf(t, u(ty) = lira + 77) - Uh(t) - T]f(t, uh(t))], 
h—>0
we can work with Uh rather than with u. By the definition of
(Sr) =
\arl 7 (t+„)
where, as illustrated in Figure 4.5.2, ti is the t-coordinate of the left-hand 
end of the line segment that (t + 77, uh(t + 77)) is on, at the points where uh 
is differentiable. So
fjuh(t + Tj) -Uh(t) ~T}f(t,Uh(t))] = 
- f(t,uh(t))\.
FIGURE 4.5.2. Between grid points of Euler approximation.
As in the proof of Theorem 4.5.2,
uh(ti)) - f(t,uh(t))\ <\t-ti\(P+ MK).

180
4. Fundamental Inequality, Existence, and Uniqueness
Since if |ту| < 6* then |t -1,| < 6* + h, if you take
2(P + MK) ’
you find
-^[uh(t + r]) -uh(t) -7?/(t,uh(t))] < e*.
at all points where Uh is differentiable, if \tj\ < ё* and h < £*.
Since the quantity in brackets is a continuous, piecewise differentiable 
function of tj which vanishes when tj = 0, this shows that
1«л(* +»?) - Uh(t) - uft(t))| < e*|7?|
if 77 < <5* and h < 6*, by the Mean Value Theorem. Take the limit as h 
goes to zero, to get
|u(t + 7/) -U(t) -7//(t,u(t))| < £*|7?|.
Dividing this by 77 gives the desired result. □
By proving the lemma, we have proved the theorem. □
Remark. We have only proved Theorem 4.5.5 when f is differentiable, 
because everything depended on Theorem 4.5.2, proved only for f differen­
tiable. Theorems 4.5.2,5,6 however are all true, even without that restric­
tion, but the proofs are nonconstructive.
4.6 Bounds for Slope Error for Other Numerical 
Methods
Now it is time to discuss in general the qualitative aspects of errors in 
numerical methods, keeping in mind the quantitative results exhibited in 
Chapter 3. This section is devoted to understanding how the slope error 
bound depends on the step h for each of the Euler, midpoint Euler and 
Runge-Kutta methods.
We have presented in Section 3.3 pretty convincing evidence that actual 
error
E(h) « Ce^ 
for Euler’s method,
E(h) « СмЪ? 
for midpoint Euler,
E(H) « CRxh4 
for Runge-Kutta.
Now we shall discuss the fact that there exist constants Be, Bm, and 
Brk (different from the Ce, Cm, and Crk above) such that the slope 

4.6. Bounds for Slope Error for Other Numerical Methods
181
error is bounded by
eh = В Eh 
for Euler’s Method,
eh = ВмЬ? 
for midpoint Euler,
eh = BRKh4 
for Runge-Kutta.
The first was proved as Theorem 4.5.2; the second we shall now present 
(with a lengthier proof) as Theorem 4.6.1; for the third we shall omit 
an even more complicated proof in favor of the compelling experimental 
evidence cited above for E(h) « CrkIi\ recalling that
E(h) = \u(t) - uh(t)\ < 
- 1). 
(10, again)
For further reading, see References for numerical methods at the end of 
this volume.
Theorem 4.6.1 (Bound on slope error, midpoint Euler method). 
Consider the differential equation x' = f(t,x), where f is a continuously 
differentiable function on a rectangle R = [a, b] x [c, d] in the t, x-plane. 
Consider also the midpoint Euler approximate solution Uh, with step h. 
Then there is an £h such that Uh satisfies
K(0 - 
<£h
at any point where Uh is differentiable {or has left- and right-hand deriva­
tives elsewhere), and 
—> 0 as h —> 0.
Furthermore, if f is a function on R with continuous derivatives up to 
order two with respect to x and t, then there is a constant Bm such that
K(t) - 
< BMh2.
This computation is not too difficult if you don’t insist on knowing BM, 
which is a fairly elaborate combination of sup’s of the second order partial 
derivatives.
Proof. First we need to decide just what the mid-point approximation is. 
Of course we know what it is at the grid-points, as shown in Figure 4.6.1, 
but we can take any piecewise differentiable function joining them that we 
like. Segments of straight lines are the easiest choice, but you cannot get a 
slope error of order at most h2 that way. Our choice will be the quadratic 
function having slope fit^xt) at (t^xf) and passing through a?i+i), 
also shown in Figure 4.6.1. These properties do specify a unique function
Vh(*) = %i + f(ti, Xi)(t - ti) + a(t - ti)2 for ti < t < ti+i.
By the definition of the midpoint approximation scheme,
•Ei+l — Xi -|- hf
Xi + уЯ^Хг

182
4. Fundamental Inequality, Existence, and Uniqueness
FIGURE 4.6.1. Between grid points of midpoint Euler approximation. Quadratic 
function Vh(t).
so setting
gives
Vh(ti + h) = xi+i
a =
■ f 
h 
h 
\ 
"
f I ti + x, Xi + Xi) ) - f(ti, Xi) /h.
\ 
А 
A 
/
(15)
Now that we know what Vh(t) is, we need to evaluate
I4W -f(t, ^W)l-
We only need to do this on one segment of the graph of and without 
loss of generality we may assume that (t^xi) = (0,0).
First let us evaluate a to first order in h (you may refer to the Appendix 
on Asymptotic Development). Suppose that
/(t, x) = a + bt + ex + ri(t, ж),
where a+bt+cx is the Taylor polynomial of f at (0,0) so that the remainder 
ri(t,a?) satisfies
|ri(t,x)| < Ci(t2 +x2)
for a constant ci that can be evaluated in terms of the second partial 
derivatives of f. Substituting in equation (15) above, we find
a = |(5 + ac) + r2(fo),
where |r2 (Л) | < c2|fc| with c2 = (ci + a)/4, so that

4.7. General Fence, Funnel, and Antifunnel Theorems 
183
\v'h(t) - /(t,^(t))| = [a+ (6 + ac)t + 2tr2(ft)J
. x 
, second order \
— a + bt + c at+ . 
. ,
у terms mt J
= t (first order terms in h) + (second order terms in t).
Now on the segment of interest, t < h, so there exists a constant Bm 
such that
KW - 
< BMh2.
The constant Bm can be evaluated in terms of the partial derivatives of f 
up to order two. In particular, Bm does not depend on the segment under 
consideration. □
It is unclear from this derivation whether a wigglier curve joining grid 
points might give a higher order dependence on h; however, looking at the 
actual evidence in Section 3.3 shows that it won’t.
Although we shall not take space to prove it, the Runge-Kutta method 
admits the slope error bound
1*4 W “ №*^W)I BRKh\
In practice, only the bounds for Be (= P + MK by Theorem 4.5.2) can 
actually be evaluated (and even then only in special cases). Bounds for the 
constants BM and Brk are also sups of various partial derivatives of /, but 
partial derivatives usually become complicated so fast that the evaluation 
of their maxima is untractable. In any case, the bounds given by the theory 
are just that: bounds, and will usually be very pessimistic.
4.7 General Fence, Funnel, and Antifunnel
Theorems
Once the Fundamental Inequality is proved, everything about fences, fun­
nels, and antifunnels becomes more or less easy. We have actually used all 
the theorems in Chapter 1, but we can now prove them in greater general­
ity. The basic result of the Fundamental Inequality is to extend Theorem 
1.3.5, which was for strong fences, to weak fences by adding a Lipschitz 
condition.
Theorem 4.7.1 (Fence Theorem). Consider the differential equation 
xf = f(t,x) with f a function defined in some region A in R2 and satisfying 
a Lipschitz condition with respect to x in A. Then any fence (strong or 
weak) is nonporous in A.
We shall prove this theorem for the case of a lower fence, a(t). The case 
of an upper fence would proceed similarly.

184
4. Fundamental Inequality, Existence, and Uniqueness
As we saw in Chapter 1, nonporosity is assured if the weak inequalities 
(<) are replaced by strong inequalities (<) as in Theorem 1.3.5. On the 
other hand, a fence could fail to be nonporous if we use weak inequalities 
and fail to impose a Lipschitz condition, as in the leaky bucket of Example 
4.2.1.
The idea of the proof is to modify the situation slightly so that the weak 
inequalities become strong. The first temptation is to tilt a slightly. But 
a moment’s reflection will show that this will probably not work, since in 
that case the tilted a will have a graph (top half of Figure 4.7.1) in parts 
of A in which we know absolutely nothing about the slopes /, since the 
restriction of the hypothesis is only for f along a. Instead we shall tilt the 
slopes f (bottom half of Figure 4.7.1).
Proof. Let /e(Z, x) = f(t,x) + e. Then q is a strong lower fence for the 
differential equation x' = f£(t,x) if e > 0. Moreover, f£(t,x) still satisfies 
a Lipschitz condition with respect to x (in fact with the same constant as 
for /)•
Let u£(t) be the solution of x' = f£(t,x) with the same initial value 
u£(a) = u(a). We showed in Theorem 4.5.5 that u£(t) exists and is unique 
because the assumption of a Lipschitz condition for f implies the Fun­
damental Inequality. The solution u£(t) to the differential equation x' = 
f£(t,x) is an approximate solution to xf = f(t,x\ so
Ht)-U£(«)|< (|)(eK|t~a|-i),

4.7. General Fence, Funnel, and Antifunnel Theorems 
185
and in particular for every fixed t we have that u£(f) —* u(t) as e —* 0.
Since a(t) < u£(t) for all e > 0 and all t > to, we have a(t) < u(t) for all 
t. □
This proof is just the first instance of the benefits to be reaped by think­
ing of a solution to one differential equation as approximate solutions to 
another which we understand better.
Corollary 4.7.2 (Funnel Theorem). Let a(t) and a(t) < f3(t) be 
two fences defined fort e [a, 6), where b might be infinite, defining a funnel 
for the differential equation x' = f(t,x). Furthermore, let f(t,x) satisfy a 
Lipschitz condition in the funnel.
Then any solution x = u(t) that starts in the funnel at t = a remains in 
the funnel for all t e [a, 6).
P (t) = upper fence
- a (t) = lower fence
FIGURE 4.7.2. Funnel.
Proof. The result follows immediately from Definition 1.4.1 and Theorem 
4.7.1. 
□
Theorem 4.7.3 (Antifunnel Theorem; Existence). Let a(t) and /3(t), 
(3(t) < a(t), be two fences defined for t G [a, b), where b might be infi­
nite, that bound an antifunnel for the differential equation x' = f(t,x). 
Furthermore, let f(t, x) satisfy a Lipschitz condition in the antifunnel.
Then there exists a solution x = u(t) that remains in the antifunnel for 
all t G [a, b) where u(t) is defined.
a (t) = lower fence
-x-x-x-
— p (t) = upper fence
FIGURE 4.7.3. Antifunnel.

186
4. Fundamental Inequality, Existence, and Uniqueness
Proof. For any s € [a, b), consider the solutions vs(t) and T]s(t) to xf = 
f(t,x) satisfying vs(s) = a(s) and t/s(s) = /3(s), as shown in Figure 4.7.4.
FIGURE 4.7.4. Backwards funnel inside antifunnel.
Using the funnel theorem backwards (i.e., reversing time), we see that 
vs(t) and T]s(t) are defined for t e [a, s], and satisfy
< 7?s(f) < i/e(t) < a(t)
for t E [a, $]. Let Is = [?js(a), i/s(a)], as labelled in Figure 4.7.4. As s —> b, 
if there is a Lipschitz condition to guarantee uniqueness of the individual 
rjs(t) and vs(t), the intervals Is form a nested family of closed intervals 
(Figure 4.7.5). It is a basic and early theorem of advanced calculus that for 
a nested family of closed intervals, their common intersection must contain 
at least one point xq. (See Exercise 4.7#4.)

4.7. General Fence, Funnel, and Antifunnel Theorems
187
The solution u(t) to x' = f(t,x) with u(a) = xq is a solution which stays 
in the antifunnel. Indeed, for any s G [a, b), we have that ^(t) and 
form a backwards funnel for a < t < s and that u(t) starts inside it, so u(t) 
is defined for all t G [a, s] and satisfies
£(*) < rjs(t) < u(t) < vs(t) < a(t)
for t G [a,s]. Since this is true for any s G [a,b), the result follows. □
Theorem 4.7.3 is particularly useful when b = oo, or if f is not defined 
when t = b.
The really interesting results about antifunnels are the ones which give 
properties which ensure that the solutions which stay in them are unique. 
We will give two such properties; the first is a special case of the second, 
but is so much easier to prove that it seems worthwhile to isolate it.
Theorem 4.7.4 (First uniqueness criterion for antifunnels). Let 
aft) and /3(t), /3(t) < aft), be two fences defined for t G [a,b) that bound 
an antifunnel for the differential equation xf = fft,x). Let f(t,x) satisfy 
a Lipschitz condition in the antifunnel. Furthermore, let the antifunnel be 
narrowing, with
lim(a(t) - 
= 0.
t—>0
If df /дх > 0 in the antifunnel, then there is a unique solution that stays 
in the antifunnel.
Proof. Theorem 4.7.4 is identical to Theorem 1.4.5, where the uniqueness 
was already proved; Theorem 4.7.3 provides the existence. □
However, this first criterion for uniqueness does not give us all we can 
get. It is perfectly possible for an antifunnel to contain several solutions, 
for instance, an antifunnel may contain a funnel. (See Exercise 4.7#5.)
FIGURE 4.7.6. Funnel inside an antifunnel.
But in many important cases, antifunnels do contain unique solutions, 
without the condition df /дх > 0 being satisfied, as we will presently see 
in Example 4.7.6 and also in Exercise 4.7#6. The following theorem gives 
a less restrictive criterion for uniqueness:

188
4. Fundamental Inequality, Existence, and Uniqueness
Theorem 4.7.5 (Second uniqueness criterion for antifunnels). Let 
a(t) and /?(£), &(t) < o(t), be two fences defined for t € [a, b) that bound 
an antifunnel for the differential equation x' = f(t,x). Let f(t,x) satisfy 
a Lipschitz condition in the antifunnel. Furthermore, let the antifunnel be 
narrowing, with
lim(a(t) - 0(i)) = 0. 
t—*b
If (df /dx)(t,x) > w(f) in the antifunnel, where w(t) is a function satis­
fying
fb
I wts^ds > —oo, 
J a
then there is a unique solution which stays in the antifunnel.
Note that the first uniqueness criterion is a special case of the second, 
with w(t) = 0, since J’J’Ods = 0 > —oo.
Proof. Again, existence of solutions that stay in the antifunnel is provided 
by Theorem 4.7.3; let ui(t) and u2(t) be two such solutions, with ui(t) > 
u2(t). Then as above,
(ui-u2)'(*) = f^u^tf)- f(t,u2(t)) = 
Ju2(t) Vх
>w(t)(u! — U2)(t)
so that the difference between the solutions in the narrowing antifunnel is 
a function 7(t) = (u± — u2)(t) satisfying
?'(*) > w(t)7(t).
As such, 7(t) is an upper fence for the differential equation x' = w(t)x, 
and, choosing the solution with x = v(t) such that v(a) = 7(a),
7(t) > 7(a)e^* w^ds.
Here we can see that if w > 0, solutions ui and u2 pull apart; requiring 
the exponent faw(s)ds > —00 gives a bound on how fast the solutions 
can pull together yet still have uniqueness in the antifunnel. Because the 
antifunnel is narrowing, we have y(t) —> 0. Thus if w(s)ds > —00, we 
must have 7(a) = 0. This is the desired conclusion. □
A further weakening of the uniqueness criterion of Theorems 4.7.4 and 
4.7.5 is given in Exercise 4.7#3. (Third uniqueness criterion for antifun­
nels.)
We will finish this chapter with an example to show how much more 
powerful the second antifunnel criterion for uniqueness is than the first.

Exercises
189
Example 4.7.6. Consider the differential equation of Example 1.5.3 (which 
we will later meet in Volume III when studying Bessel functions):
xf = 1 + (A/t2) cos2 x,
with A a positive constant.
We have shown that for each C the two curves
д
a(t) = t + C and (3(t) = t + C— —
bound an antifunnel.
Of course,
[1-|-(A/t2) cos2 x] = —(A/t2) sin 2x
is both positive and negative in the antifunnel, but
— (A/t2)sin2x > —A/t2
and
(-A/t2)dt = -A > —oo,
so the solution in the antifunnel is unique. ▲
Exercises 4.1-4.2 Uniqueness Examples
4.1— 4.2#1.
(a) Derive by separation of variables the explicit solution to xf = —C\fx 
for the physical situation of the leaky bucket in Example 4.2.1. That 
is, show that indeed the solution is
c2
x=—(t —to)2 if t < to, 0 if t > to-
(b) Graph these solutions in the t,x-plane for different values of to-
(c) Verify that these solutions are differentiable everywhere, especially at 
the bottoms of the parabolas.
(d) Verify that the above solution (a) indeed satisfies the differential equa­
tion everywhere.
(e) Show how (b), despite (c) and (d), shows the nonuniqueness of solu­
tions along x = 0.

190
4. Fundamental Inequality, Existence, and Uniqueness
4.1-4.2#2. A right circular cylinder of radius 10 ft and height 20 ft is 
filled with water. A small circular hole in the bottom is of 1-in diameter. 
How long will it take for the tank to empty?
4.1—4.2#3°. During what time T will the water flow out of an opening 
0.5 cm2 at the bottom of a conic funnel 10 cm high, with the vertex angle 
6 = 60°?
Exercises 4.3 Lipschitz Condition
4.3#1. Find Lipschitz constants with respect to x for the indicated func­
tion in indicated regions:
(a) /(t, x) = x2 — t 
0 < t < 2, — 1 < я < 0
(b) /(t,x} = sin(ta) 
0 < t < 3, 0 < ж < 5
4.3#2°. For the differential equation x' = cos(a;2 +t) = /(t, ж),
(a) Compute df /дх.
(b) Where in the square —10 < t < 10, —10 < x < 10 is df /дх > 5? 
Where in the square is df /дх < —5?
(c) Sketch the field of slopes and of solutions (perhaps with the com­
puter), and mark the regions found above.
4.3#3. Find a Lipschitz constant for
(a) x1 — t2e~x on [—5,5] x [—5,5]
(b) x' = |2z3| on [-2,2] x [-2,2]
(c) c' = -zarcsina;, for 0 < t < 2, -| < X < I
(d) xf = (2 + cost)x + 5-^/prf + ^x, for 0 < t < тг, 1 < x < 2
(e) x' = e* sin t cos ж, for 0 < t < тг, 1 < x < 2
(f) xf = e* sin(t + | tant) cos ж, for 0 < t < тг, 0 < ж < тг
Exercises 4.4 Fundamental Inequality
4.4#1. Consider in the proof of the Fundamental Inequality (10) the string 
of inequalities showing that 7'(t) < Ky(t) 4- Justify each step.
4.4#2. The Fundamental Inequality is derived under the assumption that 
the Lipschitz constant К is different from 0.

Exercises
191
(a) Which functions /(t, x) can have Lipschitz constant К = 0?
(b) Derive the Fundamental Inequality in the case where К = 0.
(c) Confirm, by computing ^irn^ (ек1ж~ж°1 — 1) that the Fundamental 
Inequality for К = 0 corresponds to the limit as К —> 0 of the 
Fundamental Inequality for К / 0.
4.4#3.
(a) Consider the very simple differential equation x' = f(t). What is the 
best Lipschitz constant?
(b) Compute ^im^ (eKl®”®o| _ 1).
(c) Use parts (a) and (b) to tell what the Fundamental Inequality says 
about the functions ui(f) that satisfy |u<(t) - f(t)\ < e.
4.4#4°. Suppose we have set up a differential equation, x1 = f (t, x) based 
on some physical system, but in so doing we have ignored some small forces. 
So we believe the actual physical system is governed by the differential 
equation xf = f(t, x} + g(t, x), where all we know about g is that \g(t, ж)| < 
0.1.
Suppose f has Lipschitz constant 2, and u(t) is a solution to x' = f(t, x) 
with u(0) = 0 and u(5) = 17. If the physical system starts off with initial 
condition x = 0 ± 0.03, what can we say about the x value of the physical 
system at t = 5? Hint: Use the Fundamental Inequality.
4.4#5. Show that the right-hand side of the Fundamental Inequality could 
not be made to grow any more slowly than it does (which is exponentially). 
In the jargon we say, “Show that the estimate is sharp.”
4.4#6. Derive the changes to the Fundamental Inequality in a mathemati­
cal modeling situation where the coefficients in a differential equation have 
been derived experimentally rather than exactly.
That is, suppose a real system is described by a function x(i) satisfying 
the differential equation x' = f(t,x), where f (the forces of the system) 
should be measured. Measuring the forces in the system leads to the dif­
ferential equation x' = /i(t,x). If r] is the error of the measurements, then
1/1 (t,ж) -f(t,x)\ < 7).
Suppose u(t) is the “predicted” solution, i.e., the solution of x' = /i(t,#) 
with initial condition u(0) = x0. Suppose also that v(t) is the observed 
solution, when the system is prepared in state xq at time to- Find a bound 
for

192 
4. Fundamental Inequality, Existence, and Uniqueness
Exercises 4.5 Existence and Uniqueness Theorems
4.5#1°.
(a) Solve the differential equation x’ = 
& for fc / 0 explicitly.
(b) Show that /(t,#) = 
+ fc does not satisfy a Lipschitz condition
in x in a rectangle R = [a, b] x [—1,1]. What do you learn from this?
4.5#2. Consider the differential equation xf = y/\x -1\ + c where c is a 
constant. Make the following coordinate change: у = x — t, r = t. Write 
down the corresponding differential equation for y: dy/dr = f(r,y). For 
which values of c do we have uniqueness of solutions?
4.5#3. (a) Consider the differential equation xf = 
+1 as in Exercise
4.5#1. Let (to,xo) = (0,-3). There is a unique solution through (0,-3). 
For x < 0 the solution is
-2УЙ + 2 1п(УЙ + 1) = t + (—2д/3 + 2 ln(V3 + 1)).
So the solution crosses the t-axis for t = 2д/3 — 2 ln(\/3 + 1). Let tf = 1. 
Use the program Numerical Methods to calculate the errors and orders for 
Euler, Midpoint Euler, and Runge-Kutta with different numbers of steps. 
Graph the stepsize versus solution. Hint: Calculate the exact solution using 
the Analyzer program in “reverse.”
(b) Choose another final time, tf > 2л/3 — 21og(\/3 + 1). Calculate the 
exact solution u(tf) with the same initial condition as before, (£o,#o) = 
(0,-3). Use again the program Numerical Methods for Euler, Midpoint 
Euler, and Runge-Kutta with different numbers of steps to calculate the 
errors and orders. Why is the behavior different from part (a)?
4.5#4. Show (without needing to exactly evaluate the coefficients) that 
the Taylor series method described in Section 3.2 is in fact of order n.
4.5#5°. Consider Clairaut’s differential equation, x = txf — (a/)2/2, which 
is quite different from those we have been studying because the derivative 
is squared.
(a) Show that the straight lines x = Ct — C2/2 are solutions.
(b) Show that the parabola x = t2/2 is another solution.
(c) Show that the lines in part (a) are all tangent to the parabola in (b).
(d) Show why the ordinary existence and uniqueness theorem (for explicit 
differential equations) is not satisfied by this equation. That is, show 
where the hypotheses fail.

Exercises
193
4.5 #6.
(a) Describe the solutions of x = tx' + (a/)2. Hint: Look at the previous 
exercise.
(b) Can you describe the solutions of x = tx' — where f is any 
function?
Exercises 4.6 Bound on Slope Error
4.6 #1. For each of the following differential equations, find a step h such 
that if you solve the equations using Euler’s method and the given initial 
condition, you can be sure that your solution at the given point is cor­
rect to three significant digits. (You have calculated Lipschitz constants 
in Exercises 4.3#2, but you should worry about whether the approximate 
solutions stay in the region in which those computations were valid.) 
(a)° x' = x2 - t ж(0) = 0; ж(2) = ?
(b) xf = sin(ta) ж(0) = 5; ж(3) = ?
4.6#2. Let Uh(t) be the Midpoint Euler approximation to the solution of 
x' = —X, with initial condition 1^(0) = 1.
(a) Show that 0 < Uh(t) < 1 for t > 0 and h < 1.
(b) Using the same technique as in Theorem 4.6.1, find a function C(t) 
such that
\uh(t) — е^\ < C(t)h2.
(c) How short should you choose h to guarantee that ^(1) approximates 
1 /e to five significant digits?
4.6#3°. Let Uh(t) be the Runge-Kutta approximation to the solution of 
x' = —x, with initial condition Uh(fi) = 1.
(a) Show that 0 < Uh(t) < 1 for t > 0 and h < 1.
(b) Find a function C(t) such that |uh(t) — 
< C(t)h4. This requires
a bit of ingenuity. Let (£$,#$) and Яг+i) be two successive grid 
points of Uh(t). First find a formula for in terms of x^ and h. 
This formula should suggest a curve joining the grid points; the result 
follows from evaluating the slope error of this curve, and applying the 
Fundamental Inequality.
(c) How short should you choose h to guarantee that 1x^(1) approximates 
1 /e to five significant digits? Check this using the program Numerical 
Methods.

194
4. Fundamental Inequality, Existence, and Uniqueness
4.6#4. In the text, we say that the slope errors which occur when approx­
imating solutions of x' = /(£,#) by the three methods we have discussed 
can be bounded by expressions of the form Chp, where p is the order of 
the method and C can be evaluated in terms of partial derivatives of f up 
to some order, which depends on the method. Use the program Numerical 
Methods on the following equations, for — 1 < t < 1 and x(—1) = 0, to 
discover how many derivatives are needed for such a bound to exist.
(a) x’ = \x + t\
(b) xf = — (x + t)\x + t\
(c) x' = -(x + f)y/\x + t\
(d) x’ = -(x + 
+
4.6#5. Using the program Numerical Methods, find the order of our three 
methods on the two differential equations x' = .8|ж — t\ and x' = .5|ж — t\ 
for — 1 < t < 1 and x{—1) = 0. Can you explain when and why Runge- 
Kutta turns out to be less reliable here than, say, midpoint Euler? Hint: 
Make a sketch, using isoclines of slope 0, 1, and —1. Observe in which cases 
the approximate solution crosses x = t, and explain why that makes a 
difference.
4.6#6. For the differential equation x' = o|x — tl3/2, as in the previous 
exercise, we can expect some surprises if the approximate solution Uh(t) 
crosses t = 0. Using the program Numerical Methods, try some different 
values of a that show different orders of some methods for — 1 < t < 1 and 
x(—1) = 0. (Note: the computations will be faster if you enter |ж — £|3/2 as 
|ж —1\д/|ж — t|, since the computer can then avoid computing logarithms.)
4.6#7. The following exercise shows that Euler’s method does not neces­
sarily converge to a solution if the differential equation does not satisfy a 
Lipschitz condition. Consider
x' = |ж|-3/4ж + t sin^y^ with ж(0) = 0.
Consider the Euler approximations un(t) with h = (n + |) \ as do.
(a) Use the computer program DiffEq for n = 10, 11, 12, and 13, and 
observe that the approximations do not appear to converge. (Note: 
It is necessary for to and xq to be close, but not equal, to zero. 
This can be arranged if you “uncenter” the domains; e.g., —.5 to 
.5000000000001.)
(b) Show that if n is even,
un(h) = 0;

Exercises
195
un(2h) = h2;
«„(ЗЛ) > 
> i(3fe)3/=.
2 lb
(c) Furthermore (for n even) show there exists a constant c > 3h such 
that part (a) implies we also have
un(irh) > -^-(тг/г)3/2 for ЗЛ < Tvh < c.
16
Hint: Reason by induction, showing that
/(тгЛ,ип(тгЛ)) > (un(7r/l))1/4 - irh > i(7r/i)3/8 - irh > -F(tt/i)3/8 
Zj 
JLU
and note that for t < c we have
Jl/3/8 > 
jL/3/2 ]
10 
dt\16 
)'
(d) Reason similarly for n odd, showing that then
un(idi) < — ^(тг/г)3/2 for ЗЛ < irh < c.
16
Conclude that the approximate solutions un(t) do not tend to any 
limit as n —> oo.
(e) Verify that the equation does not satisfy a Lipschitz condition.
Exercises 4.7 General Fence, Funnel, and 
Antifunnel Theorems
4.7#1°. Consider x' = cos(t -F x).
(a) Find funnels and antifunnels (they need not be narrowing). (Warn­
ing: the follow-up questions we ask assume that you found the same 
funnels and antifunnels that we did. If you find different ones, you 
may not be able to answer some of those questions.)
(b) Which of your funnels and antifunnels are strong and which are weak? 
For those that are weak, show which of the weak funnel and antifunnel 
theorems of Chapter 4 apply.
(c) For the antifunnels, show at least one solution which never leaves the 
antifunnel (big hint: the fences which form a funnel (resp. antifunnel) 
are considered part of the funnel (resp. antifunnel)).

196
4. Fundamental Inequality, Existence, and Uniqueness
(d) By drawing a computer solution with DiffEq, show that the solutions 
tend to be squeezing together, as if they were falling into a narrowing 
funnel. Even if your funnels are not narrowing, you may still be able 
to explain their behavior. Give it a try. Also, even if your antifunnels 
are not narrowing, you may be able to show that only one solution 
remains in an antifunnel for all time.
4.7#2. Consider x' = cos(#ef). Find narrowing funnels and antifunnels.
4.7#3. Let o(t), (3(t) be defined for t > t0, a(t) > (3(t), and suppose that 
the region t > to, (3(t) < x < a(t) is an antifunnel. Suppose there exists a 
function w(t) such that w(t) < (df/dx)(f x) for all x with /3(t) < x < a(t), 
and that
a(t) - (3(t)
~yt—0 as t oo.
w(s)ds
eJto
Then show there exists a unique solution in the antifunnel, thus replacing 
the restriction of Theorem 4.7.5 that an antifunnel narrow to get unique­
ness.
Remark. You can consider the numerator a — /3 as the squeeze on the 
antifunnel, and the denominator e^o w^ds the squeeze on the solutions.
Hint: Suppose ui(t) > uz(f) are solutions in the antifunnel and define 
7(t) = ui(t) — U2(t). Show that y(t) is an upper fence for the differential 
equation xf = w(t)x. Use the explicit solution of x' = w(f)x with xf (to) = 
7(to) to show 7(^0) = 0.
4.7#4. Prove, using decimal notation, that for a nested family of closed 
intervals, their common intersection must contain at least one point xq. 
This confirms a fact used in the proof of Theorem 4.7.3.
4.7#5. Consider the equation xf = sinta, and refer back to the analysis 
of Example 1.5.2 where an antifunnel is formed by Qfc(t) and Show 
that the first uniqueness criterion of Theorem 4.7.4 is insufficient to prevent 
two (or more) solutions from getting together fast enough to stay in the 
antifunnel.

5
Iteration
In this chapter we will study iteration in one dimension. (Iteration in two 
dimensions is far more complicated and comprises an important chapter 
in Volume IL) “Iterating” a function /(x) consists of the following simple 
process:
Start with a seed xo, and consider the sequence
xi = f(x0), x2 = /(xi), x3 = /(ж2),... . 
(1)
The essence of iteration is to use the last output as the next 
input.
The sequence (1) is called the orbit of xo, and the kinds of questions we 
will ask are:
What happens in the long run for a particular seed?
How does “what happens” depend on the seed?
How does “what happens” depend on the function /(x)?
How does “what happens” depend on parameters within / (x)?
Such questions are similar to the kinds of questions asked about differ­
ential equations, and indeed the two subjects are very closely related, as 
we hope to show in this chapter. In some sense, a differential equation de­
scribes evolution in continuous time, and an iteration describes evolution 
in discrete time. The continuous dependent variable t that has appeared 
in earlier chapters has here been “discretized” and appears as an index: Xi 
means the value of x after i units of “t.”
Both differential equations and iteration are included in the area of math­
ematics called dynamical systems. In this book, when we say we will con­
sider a function /(x) as a dynamical system, we mean that we will iterate 
it.
The continuous-discrete dichotomy suggests one way in which iteration 
is related to differential equations. Since all the numerical methods we have 
seen, and in fact just about all the methods in existence, consist of “dis­
cretizing” time, we might expect that numerical methods are simply itera­
tions. This is true, and is the real reason for this chapter: mathematicians 
are still trying to understand what numerical methods actually do.

198
5. Iteration
Iteration arises in contexts other than differential equations. We have 
seen that computers find numerical methods for differential equations ex­
tremely congenial; a truer statement is that computers find iteration very 
congenial. As a result, a great many other fundamental algorithms of math­
ematics are simply iterations. This includes the most popular of them all, 
Newton’s method for solving equations (nondifferential), to be described 
in Section 5.3. Other iterative algorithms occur in linear algebra, such as 
Jacobi’s method and the QR method, discussed in Volume II, Appendices 
L8 and L7 respectively.
Even though iteration is not traditionally a part of the differential equa­
tions curriculum, in this day of interaction between computers and mathe­
matics, it would be a mistake to ignore this important topic which is closely 
related to differential equations.
As you read on in this chapter, you will realize that iteration is more 
complicated than anything we have studied so far, and problems involving 
order versus chaos rapidly come to the fore. It is rather surprising that it­
eration, which looks easier at first view than differential equations, is really 
not so. The source of increased difficulty with iteration is that there are no 
simple analogs of the fence and funnel theorems, which require conditions 
only on the fences.
In Section 5.1 we shall consider how to represent graphically and analyze 
a function f(x) under iteration. In the remaining sections we shall examine 
some specific iterative systems: In Section 5.2 we show how the famous 
logistic model behaves very differently in the cases where population growth 
is amenable to a difference equation rather than a differential equation. 
Section 5.3 delves into the complications of Newton’s method for calculation 
of roots, the iterative scheme that is used within all our MacMath computer 
programs.
Section 5.4 examines numerical methods for differential equations. In 
Section 5.5 we look closely at periodic differential equations. This is where 
we introduce the work of Henri Poincare, who in the late 1890’s was the 
first mathematician to have established and exploited the connection be­
tween iteration and differential equations. His theory of Poincare sections 
revolutionized differential equations, and this whole book consists largely of 
an attempt to bring Poincare’s ideas to the undergraduate audience. Here 
we will give the first introduction to Poincare sections, but the subject 
only becomes really serious in higher dimensions; that will have to wait for 
Volume II.
Finally, in Section 5.6 we offer as diversion a peek at the delights and 
rewards of iterating in the complex numbers. This is one direction of current 
research action in dynamical systems.

5.1. Iteration: Representation and Analysis
199
5.1 Iteration: Representation and Analysis
The Iterates of a Function
We need a notation for iteration of a function /(ж). We will denote the nth
iterate of f by
f°n =
n functions f
so that
fon(x) = f(...(f(x))...),
is the n-fold composition of /(ж) with itself.
Examples 5.1.1.
If f(x) = ax, 
/°1 (ж) = ax 
/о2(ж) = a(ax) 
fo3(x) = а(а(аж))
If f(x) = x2 + c, 
/°1 (ж) = x2 + c 
/о2(ж) = (ж2 + c)2 + c 
/о3(ж) = ((ж2 + с)2 + с)2 + с,
Remark. Many authors simply write fn for the nth iterate, which invites 
confusion with powers; consequently we will always use the composition 
symbol. As you can see in the first example, we have /оп(ж) = anx, whereas 
fn(x) = anxn\ in the second example there is not even a formula for /оп(ж), 
but fn(x) = (ж2 + c)n.
Remark. Writing the nth iterate in closed form, that is, as an explicit 
function of n, is analogous to solving a differential equation in terms of 
elementary functions. (For example, for ж' = ax we can write the solution 
x(f) = еа*ж(0).) Such closed form is definitely not always possible, despite 
the fact that our examples might seduce you into thinking it is.
The idea of iteration is very simple, but the results are not, largely be­
cause the iterates fon can be very complicated. For instance, we shall look 
long and hard at the quadratic polynomial /(ж) = ж2 + c, the second ex­
ample of 5.1.1. You might reasonably think that everything there is to say 
about quadratic polynomials was said, long ago, and is easy. But note that 
for a quadratic, the nth iterate fon is a polynomial of degree 2n, so that 
studying quadratic polynomials as dynamical systems involves the study of 
infinitely many polynomials, of arbitrarily high degree. This is the source 
of the great complication of iteration.
Example 5.1.2. The computer program Cascade will automatically graph 
for you any iterate of ж2 + c. Figure 5.1.1 on the next page represents the 
16th iterate of ж2 + c, for c = —1.39. The function /°16 is of degree 216, so 
this picture only begins to show how complicated it is. ▲

200
5. Iteration
16th Iterate of x2 -1.39
FIGURE 5.1.1. /о16(ж) for /(ж) = x2 - 1.39.
Time Series
For the remainder of this chapter we will be iterating mappings f: R —* R 
(except for Section 5.6, where we will show some examples of iterations of 
mappings f: C —* C).
There are many ways of representing an iterative process. Probably the 
simplest to understand is a time series. This simply means plotting the 
points (n, /On(^o)), perhaps joining them. We already have a program that 
will do this for us, and it is good old DiffEq applied as follows:
Suppose we want to iterate a function f.
1. Enter the differential equation x' = f(x) - x.
2. Apply Euler’s method, using step h = 1.
These two steps yield xn+± = xn + (/(#n) — xn) = f(xn).
Remark. Euler’s method can be run forwards or backwards in time. How­
ever, Euler’s method run backwards is not the inverse of Euler’s method 
running forward: running Euler’s method backwards starting at xq does not 
usually lead to an x_i such that = xq. (See Exercise 3.1-3.2#5.) 
Consequently, the part of the orbit run backwards from your initial condi­
tion is not relevant to the function you are trying to iterate.
Let us give a few examples of such iterations.

5.1. Iteration: Representation and Analysis 
201
Examples 5.1.3. Time series for x2 + c, c = —1, —1.3, —1.8.
If c = — 1, then the orbit of —1 is, from iterating x2 — 1,
xQ = -1, X1 = (-1)2 -1 = 0, x2 = О2 - 1 = -1,... .
These results can be summarized by the formula
(-!)»+! -1 
n~ 2
We see this repetitive pattern, a cycle of period two, in Figure 5.1.2.
Xq = -1
n= 90
Iteration of x2 -1. Note the cycle of period 2. 
x
FIGURE 5.1.2. Time series for x2 — 1.
If c = —1.3, iteration produces a cycle of period four, as in Figure 5.1.3.
x
Iteration of x2 -1.3. Note the eventual cycle of period 4.
FIGURE 5.1.3. Time series for x2 — 1.3.

202
5. Iteration
If c = —1.8, the orbit appears to be chaotic, meaning simply “without 
apparent order,” as shown by the time series in Figure 5.1.4. 
▲
Iteration of x2 -1.801. Note that the orbit looks chaotic.
FIGURE 5.1.4. Time series for x2 — 1.8.
Graphic Iteration
It is also quite easy to iterate a function f(x) graphically, and the computer 
program Analyzer will do it for you. The process is as follows:
Begin with some xq. (The ^-coordinate is irrelevant; the computer program 
uses (a?o,0).) Then
(i) Go vertically to the graph of f: (a?o,/(zo))
(ii) Go horizontally to the diagonal: (/(a?o), /(^o)) = (^i,^i)
To iterate you repeat the construction, going vertically to (xi, then 
horizontally to (/(#г),/(^J) = (a^+i,a?i+i), and so on.
Example 5.1.4. See Figure 5.1.5.

5.1. Iteration: Representation and Analysis
203
FIGURE 5.1.5. Graphic iteration of x2 — 2 for xq = 1.86.
Fixed Points
If you play with Analyzer a bit, you will see that the intersections (xf,Xf) 
of the graph of /(ж) with the diagonal are important. Such points Xf are 
called fixed points of /, because /(27) = Xf.
You should think of fixed points as representing equilibrium behavior, 
which is usually classified as stable or unstable. Fixed points can be classi­
fied as “attracting” (corresponding to stable equilibrium), “repelling” (cor­
responding to unstable equilibrium), or “indifferent” (leading to various 
other behaviors).
A fixed point Xf is attracting if there is some neighborhood of Xf which 
is attracted to xf, i.e., if xq is in that neighborhood, then the orbit of xq 
is a sequence converging to xq. See Figure 5.1.6.
The fixed point Xf is called repelling if there is some neighborhood of Xf 
such that the orbit of any point xq in that neighborhood eventually leaves 
that neighborhood (though it might still later return).

204
5. Iteration
FIGURE 5.1.6. Attracting and repelling fixed points.
The slope m of the graph of f at (ж/, жу) determines the type of a fixed 
point, as follows: A fixed point Xf is
attracting 
repelling 
indifferent 
superattracting
if \m\ < 1;
if \m\ > 1;
if \m\ — 1;
if m = 0.
This derivative criterion for classifying fixed points can be seen from a 
Taylor expansion of /(ж), centered at xf. Setting ж = ж/ + h gives as the 
Taylor polynomial of /,
/(ж) = f(xf + ti) = Xf + mh + higher order terms in Д,
so the deviation from Xf is simply multiplied (to first order) by m, and the 
number m is called the multiplier of f at the fixed point Xf. Theoretical 
details are spelled out in Exercises 5.1#3-5, but you can directly observe 
the following.
As soon as you are sufficiently close to Xf for the linear approximation to 
be valid, the point Xf will repel you if \m\ > 1, and suck you in if |m| < 1. 
By far the strongest attracting situation occurs when m = 0, when the 
first order term in h disappears entirely, which is the reason for the term 
superattracting.
If m = ±1, a fixed point is called indifferent because many things can 
happen, depending on higher terms in the Taylor polynomial of f. We will 
explore some possibilities in Exercises 5.1 #2.

5.1. Iteration: Representation and Analysis
205
The graphic behavior of an iteration attracted to or repelled by a fixed 
point depends on the sign of the multiplier, as illustrated in the following:
Example 5.1.5. The polynomial x2 — 1.2 has two fixed points, at
Xf ~ 1.7041... and at Xf « —.7041 ... .
Both fixed points are repelling, since the derivatives 2x at these points are 
approximately 3.4083... and —1.4083... respectively. Figure 5.1.7 shows 
them both, with some orbits escaping from them. Note particularly the 
“outward spiralling” behavior of the graph of the orbits near a fixed point 
with negative multiplier < — 1.
repelling fixed point with 
multiplier m > 1 orbits are, 
repelled on both sides, / 
do not spiral. x j
repelling fixed point 
with multiplier m < -1 
Note the orbit being 
repelled by spiralling 
away.
Iteration of x2 -1.2
FIGURE 5.1.7. Repelling fixed points; different behaviors according to sign of 
multiplier.
Exercise 5.1#8 asks you to explore a question you should ask about 
Figure 5.1.7. What is going on between the two fixed points if both are re­
pelling? Although the explanation belongs in the next subsection, guessing 
and experimenting with the computer provides valuable insight. ▲
Example 5.1.6. A function like —0.15ж5 + 0.4ж + 0.7ж3 + 0.05 exhibits 
various iterative behaviors, depending on the values of the multiplier at 
each fixed point, as shown in Figure 5.1.8 on the next page. ▲

206
5. Iteration
FIGURE 5.1.8. Attracting fixed points; different behaviors according to multi­
plier.
Periodic Points
Periodic cycles are closely related to fixed points. A collection of distinct 
points {жо,Ж1,... ,жп_1} forms a cycle of length exactly n, if
f(xi)=Xi+i, for i = 0,1,2,... , n — 2, and f(xn_i) = xq.
This is equivalent to saying that each point of the cycle is a fixed point of 
the nth iterate /on, i.e., that /on(xi) = and is not a fixed point of any 
lower iterate of f. The elements of the periodic cycle are called periodic 
points of period (or order) n.
Fixed points are a special case of periodic points, of period 1.
A cycle is attracting or repelling if each of its points is an attracting 
or repelling fixed point of fon. In particular, if the seed of an orbit is 
sufficiently close to a point of an attractive cycle, then the orbit will tend 
to a periodic sequence, with successive terms closer and closer to a point 
of the cycle.
Examples 5.1.7. Iteration of ж2 —1.75 gives the two-cycle .5 —> —1.5 
.5,
as shown in the top half of Figure 5.1.9.
Iteration of x2 — 1.75488... gives the three-cycle 0 
—1.75488 ... —>
1.3247 ... —> 0, as shown in the bottom half of Figure 5.1.9.

5.1. Iteration: Representation and Analysis
207
FIGURE 5.1.9. Periodic cycles

208
5. Iteration
The derivative criterion for a cycle being attracting or repelling comes 
from the chain rule. Indeed, the derivative of fon at жо is
(ГП)'W = /'(/(• . • (/(xo) ...)••••• /'(/W) • /'W
But xq — xnj and this is true for any point of the cycle, so we get
mn = (Гп)Ы = (Гп)Ы = ■■■ = (ГпУ^п-1),
showing that the derivative of fon is the same at all points of the cycle 
(you can fill in the steps in Exercise 5.1 #9); this number mn is called the 
multiplier of the cycle.
If \mn\ < 1, then points near a point of the cycle will be attracted to 
the cycle, which means that their orbits will tend to periodic sequences 
of period n, with successive terms closer and closer to successive elements 
of the cycle. If \mn\ > 1, then points beginning nearby fly off, and their 
long-term behavior is not governed by that cycle.
Examples 5.1.8. For f(x) = x2 + c, ff(xo) = 2xq, and (/on)' = 
2nxox± • • • xn-±. We return to the cycles of Examples 5.1.7. For x2 — 1.75, 
m2 = (J02)' = 4x0^1 = 4(.5)(—1.5) = —3, predicting a repelling cycle order 
2. For x2 - 1.7548..., m3 = (/°3)' = 8zozi = 8(0)(-1.75488.. .)(1.3247 
...) = 0, predicting a (super)attracting cycle order 3. The behavior of the 
cycle shows up in the graphical representation when the seed is not an 
element of the cycle, as shown in 5.1.10. 
▲
It might seem that periodic points are sort of exotic and rare, but the 
opposite is the case. There are usually zillions of periodic cycles; periodic 
points xp are the roots of the equation
f°n И = x.
If f is a polynomial of degree d, this is an equation of degree dn, with in 
general dn roots. Of course many of the roots xp may be complex, which 
will not correspond to a periodic cycle on the graph of /(ж), but many may 
also be real.
The first object of any analysis of an iterative system is to try to locate, 
among the zillions of periodic points or cycles, the attractive ones, because 
they are the only ones we can see. For specific equations, the program 
Analyzer can be an immense help by locating them graphically. Algebraic 
calculations proceed as in the next example, where we deal with a whole 
parameter family of equations.

5.1. Iteration: Representation and Analysis
209
FIGURE 5.1.10. Repelling and attracting cycles.

210
5. Iteration
Example 5.1.9. Let us look at the periodic points of period 2 of x2 + c. 
They are the solutions of the polynomial equation
(ж2 + с)2 + c - x — 0, 
(2)
which is a fourth degree polynomial. However, the fixed points are also 
solutions of this equation, and they are the roots of the polynomial
(ж2 + c) — x = 0,
which must therefore divide polynomial (2). Hence by long division, the 
points of period exactly 2 are the solutions of
ж2 + ж + с+1 = 0.
We leave it to the reader to verify (as Exercises 
the following
facts:
(a) The periodic points xp of period 2 are real if c < —3/4.
(b) The multiplier m2 of the period 2 cycle is 4(c +1).
(c) The cycle of period 2 is attracting if and only if —5/4 < c < —3/4. 
▲
Comparison of Different Representations
All of the graphical representations of iteration are in common use among 
applied mathematicians making iterative models, so it is useful to gain 
some familiarity with each and an understanding of the relation between 
different representations.
Examples 5.1.10. A time series and an iteration picture, if made to the 
same scale, can be lined up to show a one-to-one correspondence. This is 
particularly clear for the case of a cycle, as in Figure 5.1.11 where the 
correspondence is shown by the horizontal dotted lines. ▲

5.1. Iteration: Representation and Analysis
211
FIGURE 5.1.11. Iterating x2 — 1.62541; cycle order five. Iteration matched with 
time series.
Example 5.1.11. For an equation that iterates to give a cycle of order n, 
the iteration picture showing that cycle can be graphically lined up with 
a picture of the nth iterate, again to show a one-to-one correspondence, 
between the points on the cycle in the first case and the points where the 
nth iterate in the second case is tangent to the diagonal, as in Figure 5.1.12 
on the next page.
The two places where the nth iterate crosses the diagonal are fixed points 
of the nth iterate, i.e. periodic points of period n. Both are repelling. ▲
There is nothing magic about showing the first correspondence side- 
by-side and the second above-and-below. We just point out that you can 
work either horizontally or vertically, because of the role of the diagonal 
in graphical iteration.
Exercise 5.1 #13 ask you to explore these comparisons for other functions.

212
5. Iteration
FIGURE 5.1.12. Iterating x2 — 1.62541; cycle order five. Iteration matched with 
fixed points of 5th iterate.
Changes of Variables in Iterative Systems
We have deliberately underplayed changes of variables in the earlier chap­
ters on differential equations. It is much easier, at least at first, to compute 
blindly rather than to try to understand geometrically what the computa­
tions mean, and we have tried to steer away from formal manipulation of 
symbols. Changes of variable in iterative systems are easier to understand. 
Each field of mathematics has its own scheme for “change of variables”; 
the one relevant here is conjugation, which is different, for instance, from 
the change of variables scheme encountered in integration. Conjugation 
proceeds as follows:
If f: X —> X is a mapping we wish to iterate, and Y —> X is a 1-1 
correspondence between elements of Y and elements of X, then there ought 
to be a mapping g: Y —> Y which “corresponds” to f under <p.

5.1. Iteration: Representation and Analysis
213
It is not really surprising, but it is important, to realize that
9 = <P-1 ° f ° W 
(3)
we say that p conjugates f to g.
Example 5.1.12 (Nonmathematical). Let X be the set of English nouns, 
Y be the set of French nouns, and the French-English dictionary. Let f 
be the “English plural” function, which associates to any English noun its 
plural. Then the corresponding function g in French is the “French plural” 
function. To go through the formula (3), start for instance with
“oeil”, which the French-English dictionary p will translate 
to “eye”; the English plural function f will transform this to 
“eyes”. We now need to return to French using the English- 
French dictionary <^-1; when applied to “eyes” this gives “yeux”, 
the French plural of “oeil”, and the desired function g. ▲
Example 5.1.13 (Mathematical). Let /(ж) = 2ж — ж2. We can rewrite this 
as
= (l-z)2-
If we now set <р(ж) = 1 — x, we see that g(y) = y2 “corresponds” to f under 
¥>. This can be computed out as follows to show that g — о f о
У 1-» 7 2(1 — y) — (1 — y)2 = 1 — y2 _ l-(l-y2)=y2. A
Equation (3) is exactly the same formula that occurs when doing changes 
of bases for linear transformations (Volume II, Appendix L2). Example 
5.1.13 brings out the real purpose of changes of variables: things are often 
simpler in some other variable than the one which seems natural. In Volume 
II, we will see this in a different context in Chapter 6; when studying the 
central force problem we will pass to polar coordinates, where things will 
simplify. In Chapter 7, the passage to coordinates with respect to a basis 
of eigenvectors will be the main theme.
One way of seeing that formula (3) is an extremely useful relation to 
require between f and g is to see that if p conjugates f to g, then p also 
conjugates fon to gon. A conjugation diagram like Figure 5.1.13 shows how 
the process can go on and on under iteration.
Another important aspect of conjugacy is to see that it preserves quali­
tative behavior of orbits under iteration. If у is a fixed or periodic point of 
g, then x = p(y) is a fixed (or periodic) point of /, so that periodic points 
of f and g “correspond” under p, and moreover attracting (resp. repelling) 
periodic points correspond to attracting (resp. repelling) points.

214
5. Iteration
g 
f
Y----------- ------------- ► X
g 
’
I 
I
I 
I
FIGURE 5.1.13. Conjugation of f to g. g = ср-1 о f о ip.
Furthermore, if <p and <£-1 are differentiable, the multipliers at corre­
sponding points are equal. This is proved in Exercise 5.1# 16.
See Exercises 5.1#15-19 for further exploration of conjugation.
5.2 The Logistic Model and Quadratic
Polynomials
Second degree polynomials are about the simplest mappings next to linear 
ones we can think of. In this section we will study the iteration of quadratic 
polynomials as functions of a real variable; in Section 5.6 we will give some 
hints of what happens for quadratic polynomials of a complex variable.
Mathematical applications in physics, chemistry and engineering only 
occasionally give rise to iterative systems as models of real systems; these 
topics are usually modelled by differential equations. We will motivate the 
iteration of quadratic polynomials from an ecological model: in ecology, 
iterative systems are fairly common, and the logistic model to be described 
below is the granddaddy of them all.
As we will see in Sections 5.4 and 5.5, the logistic model is also useful in 
understanding some differential equations.
Population Growth and the Logistic Model
Suppose we have some species, whose population goes from generation to 
generation (you might think of gypsy moths, or any of many other species 

5.1. The Logistic Model and Quadratic Polynomials 
215
of insects which live one year, existing only as eggs during the winter). We 
will denote the population of the nth generation by p(n); to say that the 
population of this species can be described by an iterative system in one 
variable is to say that there is a function f such that
p(n + l) = /(p(n)).
This can also be expressed by saying that the population in any genera­
tion depends only on the population in the previous generation.
A first possible such model is
p(n + 1) = (1 + OL)p(n) 
(4)
where a is a number (presumably positive if the species is to survive) 
describing the fertility rate of the species.
This simplest model (4) is linear, like that of Example 5.1.1, and it is 
easy to solve explicitly:
p(n) = (l + a)np(0).
(Solving an iterative system means giving a formula for xn in terms of n and 
#0, as above for p(n) = xn.) In particular, if a > 0 the population grows 
exponentially. This model describes growth only under ideal circumstances, 
for instance in a laboratory trying to raise a maximal number of a particular 
species, isolated from any disturbing influences. It can never describe for 
long a natural system, which is always subject to such factors as crowding, 
competition, and disease.
A more realistic model than (4) takes crowding into account. Assume that 
there is some “stable” population ps which the environment can support, 
and that the fertility rate depends on how far the population is from ps. 
One possible way to model this is
p(n-hl)= fl + a——-(П))^р(п). 
(5)
It is more convenient to use the stable population as the unit of popula­
tion, i.e., to set q(n) =p(n)/ps. Then equation (5) becomes
q(n + 1) = (1 + a)q(n) - aq(n)2. 
(6)
This equation (6) is called the logistic model. You should observe how 
similar this discrete logistic model is to the differential equations population 
model
xf = a(x — x2) 
(7)
studied in Chapter 2.5. In fact, Euler’s method (Chapter 3.1) with step 
h = 1 applied to the differential equation (7) leads exactly to the iteration 
of (6) (Exercise 5.2#2).

216
5. Iteration
Back in Section 2.5 we found that the differential equation predicted 
something quite reasonable, namely that whatever the initial population 
(assumed positive), the population tended to the stable population. It may 
well come as a surprise that the iterative model has enormously more com­
plicated behavior, especially if the fertility is large.
More precisely, if the fertility is “small,” so that 0 < a < 2, and if the 
initial population is “small,” meaning 0 < q(0) < 1+1/a, then the sequence 
q(n) will be attracted to the stable population qs = 1, as was the case for 
the differential equation (Exercise 5.2#3).
In fact, in this case of “small” fertility, q8 = 1 is a fixed point of (6), for 
any q, with multiplier m = (1 - a). In particular, qs = 1 is an attracting 
fixed point if and only if 0 < a < 2 (Exercise 5.2#4).
But as soon as a > 2 the fixed point q3 = 1 becomes repelling, and 
can attract nothing. We will show in detail what happens for a = 1 and 
a = 3; but the analysis we will make cannot be carried out for other values 
of a, and playing with a computer for a while is the best way to gain 
insight into this remarkable iterative system. It is even more remarkable if 
you notice that all this complicated behavior results just from iterating a 
second degree polynomial, which is about the simplest thing you can think 
of beyond a polynomial of degree one.
Example 5.2.1. Analysis of logistic model (6) if a = 1. A change of 
variables will simplify the problem. Let us center q at qs = 1, i.e., set 
x(n) = 1 — q(n). Then the mapping
q(n + 1) = (1 + a)g(n) — aq(ri)2 
(6, again)
with a = 1 becomes 
x(n + 1) = x(n)2. 
(8)
Equation (8) is very easy to analyze:
if |ж| < 1 then x(n) tends to 0 (very fast);
if |ж| > 1 then x(n) tends to infinity (very fast);
if x = 1 it stays there;
if x = — 1 then it goes to 1 and stays there after that. ▲
Example 5.2.2. Analysis of logistic model (6) if a = 3. Again a change 
of variables helps, using the particular change (to be explained two pages 
hence) which gives the simplest result: an iteration of form x2 + c. Setting 
x(n) = 2 - 3g(n), we find the logistic equation (6) becomes
x(n + 1) = x(n)2 — 2. 
(9)
Even in this simpler form (9), it still isn’t easy to see what is going on. 
Furthermore, there could be a little problem here, which is explored in 

5.1. The Logistic Model and Quadratic Polynomials
217
Exercises 5.2#5. However, if |ж| < 2, we can set x = 2cos(0) and transform 
equation (9) into
2cos(0(n + 1)) = 4cos(0(n))2 — 2,
which (remembering the formula for cos 20) becomes
0(n + l) = 20(n). 
(10)
This last clever change of variables, x = 2 cos 0, permits us to view graph­
ically the transformation law on ж as in Figure 5.2.1.
FIGURE 5.2.1. Circular representation of iteration after change of variables.
The interpretation of Figure 5.2.1 is as follows, using a circle of radius 
2: Given x (between —2 and 2), find a point on the circle with x as its first 
coordinate; then double the polar angle of the point (i.e., use equation (10)). 
The first coordinate of the resulting point is ж2—2 (working backwards from 
equation (10) to equation (9)). 
▲
It should be clear from this description that the iterative system of Ex­
ample 5.2.2 with a = 3 behaves in a completely different way from that of 
Example 5.2.1 with a = 1. In particular, in the most recent case there is 
no ж value which attracts others (especially not ж = — 1, which corresponds 
to q = 1). The values of ж just seem to jump around rather randomly, at 
least from the point of view of someone who sees only the ж-values rather 
than the entire “circular” pattern of iteration. This observation leads to 
the following diversion:
Example 5.2.3. Pseudo-random number generator. Actually, the function 
ж2 — 2, which was derived in Example 5.2.2, can be used to make quite a 
good random number generator. Start with some random жо in [—2,2], your 
seed, and write a sequence of 0’s and 1’s by checking whether the orbit of 

218
5. Iteration
xq is positive or negative. Here is a PASCAL implementation, which will 
print out an apparently random sequence of “true” and “false.”
PROGRAM PSEUDORANDOM;
VAR x: real;
BEGIN
Writeln(‘enter a seed x with — 2 < x < 2’); Readln(rr);
REPEAT writein (x > 0); x := x*x — 2 UNTIL keypressed;
END.
Remark. The computer program evaluates x > 0 (just as it would evaluate 
x + 2), only it evaluates it as a “boolean” i.e., as true or false.
A random number generator is difficult to make. There should be no 
obvious pattern among the digits, and the results should meet statistical 
tests, such as giving equal numbers of 0’s and 1’s overall. By empirical (and 
theoretical) reasons, this little program based on iteration seems to work. 
▲
Note again the drastic difference in iterating the logistic model (6) be­
tween the complete predictability of the case a = 1 and the random char­
acter of the case a = 3.
Examples 5.2.1 and 5.2.2 are misleadingly simple, or perhaps we should 
say misleadingly analyzable. We will resort to the computer to see what 
happens in greater generality for different values of a.
A Change of Variables.
In both Examples 5.2.1 and 5.2.2, we made a change of variables. Actually, 
this change can be made in general. If we set
1 + a
x = —------aq,
the iterative system (6) when changed to the x variable becomes
2 
1 + a (1 + a)2 _ 2
Xn+i — xn + 
2 
xn “b
Thus we can always transform the logistic model to the simple 
quadratic x2 + c.
Exercise 5.2#lb asks you to show that setting
1 + a
x = —------aq,
is equivalent to a conjugation (end of Section 5.1) with
1 + a x
' a
We will usually study the iteration of x2 + c, because the formulae are 
simpler; and then we can change variables back as necessary.

5.1. The Logistic Model and Quadratic Polynomials 
219
The Cascade Picture.
For a feeling of what happens in general to the logistic model (7) of iter­
ation, for different values of c, the program Cascade is quite enlightening. 
Choose some interval of values of c. The program will then fit 200 values 
of c in that interval, along the vertical axis. Starting at the top of the c 
scale, for each value of c the program will iterate x2 + c a total of к +t 
times, starting at 0. It will discard the first к (unmarked) iterates, and plot 
horizontally in the ж-direction the next t (marked) iterates, where к and t 
are numbers you can choose. The idea in discarding the first к iterates is 
to “get rid of the noise”; if the sequence is going to settle down to some 
particular behavior, we hope it will have done so after к iterates. In prac­
tice, к — 50 is adequate for large-scale pictures, but blow-ups require larger 
values of к and t. Larger к makes for cleaner pictures at any stage.
The picture we get for — 2 < c < 1/4, with к = 50 and t = 50 is given as 
Figure 5.2.2. It certainly looks very complicated.
FIGURE 5.2.2. Cascade of bifurcations.
For —3/4 < c < 1/4, there is one point on each horizontal line. There 
should be 50 (= ^); what happened? All fifty got marked on the same point, 
which is an attracting fixed point. Similarly, for —5/4 < c < —3/4, all fifty 
points are marked at just two points, which are the points of an attracting 
cycle of period 2. Between these two behaviors, at c = —3/4, we say that 
the cycle bifurcates, which means splitting in two.
At c = —5/4, this cycle bifurcates again, to give you an attractive cycle 
of period 4, which later bifurcates to give a cycle of period 8, and so on.

220
5. Iteration
All these points of bifurcation accumulate to some value c approximately 
equal to —1.40116 .... This value of c is called the Feigenbaum point, after 
physicist Mitchell Feigenbaum who in the late 1970’s produced reams of 
experimental evidence that such cascades are universal.
After the Feigenbaum point, the picture looks like a mess, with however 
occasional windows of order, the open white horizontal spaces. The most 
noticeable such window occurs near c = —7/4. There we see an attractive 
cycle of period 3, which itself bifurcates to one of period 6, then 12, and so 
on.
Actually, there are infinitely many such windows; if you start blowing up 
in any region, you will find some, with periodic points of higher and higher 
period, and each one ending in a cascade of bifurcations, with the period 
going up by multiplication by powers of 2 (Exercises 5.2#6 and 7).
Playing with the program for a while will convince you that there are 
windows all over the place; in mathematical language, that the windows 
are dense. The computer certainly suggests that this is true, but it has not 
been proved, despite an enormous amount of effort.
Consider Figure 5.2.3, a blowup from Figure 5.2.2 of the “window” near 
c = —1.75. Note the resemblance of each of the 3 branches with the entire 
original drawing. In fact all windows are made up this way, with some 
number of “tree-like” structures coming down, each resembling the whole. 
As we will see in Section 5.3, this structure comes up for many families of 
mappings besides quadratics.
FIGURE 5.2.3. Blowup of cascade of bifurcations across window near c — -1.75.
+ 1.44

5.1. The Logistic Model and Quadratic Polynomials
221
FIGURE 5.2.4. Further blowup of leftmost portion of Figure 5.2.3.
Figure 5.2.4 is a blowup of the left-hand branch of Figure 5.2.3. Note 
that this blowup has a cascade of bifurcations, and a window of period 
3 (which actually is part of a cycle of period 9), exactly like the total 
picture of Figure 5.2.2. The horizontal scales are different in Figures 5.2.2 
and 5.2.4, but one can show that in fact the structures (windows and their 
associated periods, as well as the order in which they appear) coincide down 
to the finest detail. Even if you scale them correctly, they will still not quite 
superpose (but they come amazingly close).
The real justification for the Cascade program described above is the 
following theorem:
Theorem 5.2.4. If the polynomial x2 + c has an attractive cycle, then the 
cycle attracts 0. In particular, x2 + c can have at most one attractive cycle.
There does not seem to be an easy proof of Theorem 5.2.4. It was found at 
the beginning of this century by a Frenchman, Pierre Fatou, who proceeded 
to lay the foundations, from the point of view of complex analysis, of the 
subject which we will go into in Section 5.6, more particularly in connection 
with Theorem 5.6.6. So we shall accept it without proof and discuss what 
it tells us.
The significance of the point 0 in Theorem 5.2.4 is that it is a critical 
point (point where the derivative vanishes) of x2 + c. In fact, 0 is the only 
critical point of x2+c. More generally, when iterating polynomials of higher 

222
5. Iteration
degree d, each attractive cycle must attract a critical point, and so there 
are at most d — 1 attractive cycles.
The relationship between Theorems 5.2.4 and the computer program is 
the following:
If there is an attractive cycle for x2 + c, the Cascade program 
will find it, and for each cycle, the cascade will have a window 
very much like the ones above.
The Cascade picture relates exactly to the time series picture and the 
Analyzer iteration picture, as shown in the following example and explored 
further in Exercises 5.2#8.
Example 5.2.5. See Figure 5.2.5. 
▲
FIGURE 5.2.5. Iterating x2 - 1.30; cycle order four. Iteration matched with 
cascade of bifurcations and time series.

5.3. Newton’s Method
223
In the twentieth century, especially since 1980 when computer graphics 
became easily accessible, a great deal of effort has gone into understand­
ing the iteration of quadratics. The subject is now fairly well understood. 
The detailed description of the “islands of stability,” in which orbits are at­
tracted to attractive cycles, has been explored and described in great detail 
by such eminent contemporary researchers as Feigenbaum, John Milnor, 
and Dennis Sullivan, as well as others to be mentioned later. Section 5.6 
discusses further the iteration of quadratic polynomials, for the case where 
a real number x is generalized to a complex number z. A list of references 
is provided at the end of the this volume.
5.3 Newton’s Method
Solving equations is something you have to do all the time, and anything 
more than a quadratic equation is apt to be impossible to solve “exactly.” 
So “solving an equation” usually means finding an algorithm which ap­
proximates the roots. The most common algorithm is an iterative scheme: 
Newton’s method. Newton’s method is used as a subroutine in practically 
every computational algorithm—it is one of the real nuts and bolts of ap­
plied mathematics.
Our purpose in this section is first to show the complications that are 
inherent in an ostensibly simple method, and second to show a surprising 
relation to ideas discussed in the previous section. We shall first discuss, 
with theorems, the orderly results of Newton’s method; then we shall ex­
amine the global disorder that also results.
If f is a function (say of one real variable, though we will be able to 
generalize in Volume II, Ch. 13), then the iterative system for Newton’s 
method is
= (11)
The main point to observe about the defining equation (11) for Newton’s 
method is that xq is a root of the equation f(x) = 0 if and only if xq is a 
fixed point of Nf.
Moreover, if /'(^o) 
0 (the general case), then Nj(xo) = 0, which
means that Nf has the roots of f as superattracting fixed points. This 
means that Nf converges extremely fast when it converges: the number of 
correct decimals doubles at each iteration.
In the subsections that follow, we shall examine various aspects of New­
ton’s method.

224
5. Iteration
Geometric Interpretation of Newton’s Method
It is quite easy to see geometrically what Newton’s method does. At the 
point xq, substitute for f its linear approximation
LXo(x) = f(x0) + f'(xo)(x - a?0),
(12)
and look for a root of LXo, namely
x = Xq
/(ftp)
/'W
This is illustrated by Figure 5.3.1 (which you probably have seen before). 
Given an initial guess xq, Newton’s method draws a tangent line to /(ж), 
intersecting the ж-axis at Xi; the process is repeated from Ж1, converging 
on the right-hand root of /(ж). A second example is provided in the same 
drawing, starting at yoj in this case Newton’s method converges to the left­
hand root (rather than to the closest root, because of the shallow slope of 
the initial tangent line).
Notice that this geometric idea goes a long way towards justifying the 
principle that if you start near a root then Newton’s method should con­
verge to the root, since the linear approximation LXo is close to f near 
ж0.
Local Behavior of Newton’s Method
The following major theorem provides most of the theory concerning New­
ton’s method. It is a fairly hard theorem, and actually hardly worth the 
work, if we were only interested in one variable. Indeed, in one real variable, 
for roots where the graph of the function is not tangent to the ж-axis, there 
is the bisection method (as in the Analyzer program), which works more 

5.3. Newton’s Method
225
slowly and more surely than Newton’s method. But the bisection method 
does not generalize to higher dimensions, whereas both Newton’s method 
and Theorem 5.3.1 below do.
Theorem 5.3.1. Let f be a function of a single variable and suppose that 
/'(^o) / 0. Define
. -fM 
к°~7ы’
3?i — xq И- ho,
Jo = [^1 - |/lo|,®l + l^ol], 
M = sup |/"(ж)|.
xEJq 
If
f(x0)M
then the equation f(x) = 0 has a unique solution in Jo, and Newton’s 
method with initial guess x$ converges to it.
rYл
l<---------- h0-------->l<-----------h0------- >1
*0 
*2 «I
%* h/1
FIGURE 5.3.2.
Proof. Let hi = —-777—4-. 
f Ы
We need to estimate hi, and /(xi) and /'(aq)
along the way. First, using the triangle inequality,
|ГЫ1>1ГЫ|-№1)-ГЫ1
> |/'(®o)| -Лф1 -Xol
> I/'WI -
> |l/W-
(13)
Estimating /(aq) is trickier, and involves an integration by parts:

226
5. Iteration
f(*i)
= -
Xq
x)f"(x) dx
(xi — X
Make the change of variables thQ = xi - x, and the formula above gives
I/(H)| < Mlhol2 
Jo 
*
These two inequalities (13) and (14) tell us what we need, namely
(14)
and
/(ah) 
/'(xi)
/'Ы2
M|h0|2
2
|hp| 2
2 |/'(*o)|
Iftol
2
f(x0) 
/'(*o)2
f(xi)M
< M
2
= M
If we define x2 = xi + hi and Ji = [x2 - |hi|, x2 4- |/ii|], we see that 
Ji C Jo, and that the same requirements are satisfied by xi, hi and Ji 
as were satisfied by xq, ho and Jo- As such, we can continue by induction, 
defining
, 
/(Sfc) 
j 
.
hfc = ~y>(Xfc) and xk+i=xk + hk.
All the points xk lie in Jo, and they form a convergent sequence, since 
the series ho + hi + ... converges. The limit
Xf = lim xk
is a root of /(x) = 0, since
/(Sfc) + /'(it) (Jfc+i -Jfc) -* 0.
continuous bounded 
—»0
□
Theorem 5.3.1 apparently gives only geometric convergence for Newton’s 
method, from hk+i < hk/2. This would mean that you get one more correct 
digit in base 2 for each iteration, or that it takes a bit more than 3 iterations 
to get an extra digit in base 10. The truth is much better, and we very nearly 
have it proved.
Theorem 5.3.2. Under the conditions of Theorem 5.3.1, Newton’s method 
leads to quadratic convergence:
|hfc+i| <
M 
\fW\ Ы2.

5.3. Newton’s Method
227
Proof. This is obtained by dividing inequality (14) by |/'(xi)|, then using 
inequality (13). Generalize to get statement for fcth step. □
Example 5.3.3. Consider the equation x3 — 2x — 5 = 0.
Taking xq = 2, we find /(я?о) = —1, /Ч^о) = Ю, ho = -1, and Jq = 
[2,2.2]. The second derivative of the polynomial is 6т, whose supremum on 
Jo is M = 13.2.
Since |M/(to)/(/4^o)2| = -132 < .5, Theorem 5.3.1 guarantees that 
there is a root in the interval [2,2.2], and that Newton’s method starting 
at xq = 2 will converge to it. ▲
Global Behavior of Newton’s Method
You pay for the speed at which Newton’s method converges by the uncer­
tainty of what the method will converge to: if you do not start sufficiently 
close to the desired root (whatever that means), the method may converge 
to any root, or fail to converge at all. We shall illustrate this statement in 
the next subsection, especially Examples 5.3.6-5.3.8, but we begin with a 
simpler case, in fact very nearly the only case in which Newton’s method 
is actually understood as a dynamical system.
Example 5.3.4. Quadratic equations and square roots. Suppose we want 
to solve the quadratic equation
/(t) = (x — d)[x — 6) = 0
by Newton’s method. Of course, this is a ridiculous thing to do, since we 
already know the roots, namely a and b. However, if we want to understand 
how Newton’s method works, it is best to start with a simple example.
In this case, Newton’s method as defined by equation (11) gives
, T , x x2 — (a + b)x + ab x2 — ab
N/ W = 1-------2z — (a + b) = 2.--a + l,)'
There is no trouble in seeing that a and b are indeed superattracting fixed 
points, but in this case it is possible to analyze the system completely, by 
a clever change of variables.
/X by-a 
x =
Then we can show (below) that
ip-1 oNfo <p(y) = y2 = g(y) 
(15)
Remark. The mapping sends у = 0 and у = oo to x = a and x = b.

228
5. Iteration
The proof of equation (15) is a “simple” computation (Exercise 5.3#8a). 
Note that 
and we have
<?(y) = у2 
->
J.y’
3?
Computing away, we find
, by-a ; 
~ab = by2-a > 
~Q = 2
v У - 1 
- (a + 6) У2 - 1 
—b V ’
which indeed proves equation (15). This fact allows us to understand every­
thing about Newton’s method for quadratic polynomials, by simply chang­
ing to the variable у where everything is known because g(y) = y2. We 
leave it to the reader (Exercise 5.3#8b) to verify the following:
If you start Newton’s method to the right of (a + b)/2, you will 
converge to the larger root, and if you start to the left, you will 
converge to the smaller root. If you attempt to start at (a+6)/2, 
Newton’s method does not converge at all.
To find the square root of a real number a we need to solve the equation 
x2 — a = 0, for a > 0. Newton’s method (11) in this case consists of iterating
Na(x) =x- (x2 — a) If a 
--------- L = - ( x + -
2x 
2 \ 
x
By Exercise 5.3#8b you have shown that if the initial x$ is chosen pos­
itive, Newton’s method will converge to the positive square root of a; and 
if xq is chosen negative, the method will lead to the negative square root.
Disorder in Newton’s Method
Various unpleasant things may cause difficulty when applying Newton’s 
method. For instance,
(a) Newton’s method does not work if the tangent at f(x$) is horizontal.

5.3. Newton’s Method
229
Example 5.3.5. See Figure 5.3.3. A
FIGURE 5.3.3. Horizontal tangent at xq.
(b) Newton’s method may converge, but to a different root than expected.
Example 5.3.6. Figure 5.3.4 shows an orbit which converges not to the 
root closest to the initial point, but to the farthest. ▲
FIGURE 5.3.4. Convergence to a further root.
(c) Newton’s method does not work, because the iteration does some­
thing else, such as being cyclic of order > 1, or being attached to a 
cycle of period > 1, or being “chaotic.” ▲

230
5. Iteration
Example 5.3.7. For ж3 — x ± л/2/2, 0 is part of an attracting cycle period 
2.
FIGURE 5.3.5. Newton’s method in a cycle.
For quadratic polynomials, only difficulty (a) can occur. Cubic polyno­
mials are a much richer hunting ground for strange behavior.
Example 5.3.8. Consider f(x) — x3 — x ± c, for which we already know 
the roots are 0 and ±1. Nevertheless this is a good example for analyzing 
the convergence of Newton’s method for different values of xq.
Newton’s method (11) gives
)\r / \ 
Ж3 - Ж 2Ж3
= 
.3^-1 =3^T-
We will find the values of ж о that fall into each of the above classes:
(a) f'(x) = 3x2 — 1 = 0 when x = ±1/л/3 « ±.57735, so for these values 
of x0 Newton’s method fails to converge because of the horizontal 
tangent, as shown in the example of Figure 5.3.3.
(b) A cycle order 2 will occur, by symmetry, where Nf(x) = —x.
2ж3 
1
—------= —x when x = 0, x = ±-7= ~ ±.44721.
Зж2 - 1 
’ 
x/5
The point a?o = 0 is a fixed point, which does converge under Newton’s 
method, but the others form a cycle of order 2 that does not converge 
to a root, similar to that shown in the example of Figure 5.3.5.
(c) As shown in the example of Figure 5.3.4, if xq = —.5, Newton’s 
method iterates to neither of the closer roots, but to ±1. There are, 
in fact, whole intervals of values of x$ for which Newton’s method 
behaves like this, and we can further analyze this phenomenon.

5.3. Newton’s Method
231
For an initial guess xq chosen from the intervals (—1/\/3, —1/\/5) and 
(1/л/б, 1/л/З) , the picture of what happens under Newton’s method is com­
plicated: In Figure 5.3.6 all those initial values that eventually iterate to 
the right-hand root (r = +1) are “colored” with a thick line; those that 
converge to the left-hand root (r = —1) are “colored” in normal thickness; 
those that converge to the center root (r = 0) are “colored” in white. Those 
points xq for which Newton’s method does not converge to any root at all 
are indicated by the tic marks; there are more and more of them as xq 
approaches the inside endpoint of the complicated intervals. ▲
FIGURE 5.3.6. Convergence of Newton’s method for different xq.
Example 5.3.8 is just the tip of the iceberg as to the delicacy in certain 
regions about how close xq is to the root being sought by Newton’s method. 
If you have a picture of the graph of the function to which you wish to apply 
Newton’s method, you may be able to choose xq wisely; if not, try to get 
as close as you can—a good choice would be a place where /(ж) changes 
sign. If your result is somewhere far from what you expected, the picture 
may be like Figure 5.3.6.
Let us now study from a different point of view the behavior under 
iteration of cubic polynomials. The program Cascade will plot for you the 
orbit of 0 under iteration by Newton’s method for the polynomial x3 + 
ax + 1. (Exercise 5.3#4b asks you about the generality of this expression, 
to show that any cubic polynomial can be written this way with just one 
parameter if you make an appropriate change of variables.) Just as in the 
last section for Figure 5.2.2, the cascade of bifurcations for quadratics, you 
can choose a vertical range for a, how many (£) points to mark and how 
many (A;) not to mark; only the function to iterate has changed. The picture 
you get if you let a vary from —2 to —1, for к = 20 and £ — 100, is shown 
in Figure 5.3.7.

232
5. Iteration
The cubic polynomial in question changes its number of roots at a0 = 
—(3/2) ^2 « —1.88988..., where it has a single and a double real root. For 
a > a0 there is just one real root, and for a < a0 there are 3 real roots. 
This is shown in Figure 5.3.7 by the following:
We see a fairly steady “line” to the left in the region a > a0, which is 
the single real root, and which frequently does attract 0 (one of the critical 
points and the starting point for iteration on each line of this picture). Gaps 
in this “line” represent values of a for which 0 is not being attracted to this 
root. For a < a0, there is another perfectly steady “line”, representing the 
fact that one of the two new roots (that appear when a < Qq) consistently 
attracts 0. However, in the middle of the picture (where there are gaps in 
these “lines”) there appear to be more complicated regions, where 0 does 
something other than being attracted to the root, and we see a blowup of 
such a region below in Figure 5.3.8.
Figure 5.3.8 is strikingly similar to Figure 5.2.2 for straight (nonNewton) 
iteration of quadratics, and in fact reproduces it in full detail. This appear­
ance of the “cascade picture” in settings other than quadratic polynomials 
is a large part of why the cascade of bifurcations has attracted so much 
attention recently. It appears to be a “universal drawing,” an archetypal 
figure which can be expected to show up in many different settings. This 
is an instance of the universality of the cascade picture investigated by 
Feigenbaum.

5.3. Newton’s Method
233
FIGURE 5.3.8. A cascade of bifurcations for x3 4- ax 4- 1.
different x
FIGURE 5.3.9. Time series for x3 — 1.28ж 4-1, for different xq.
There are actually infinitely many other blowups of Figure 5.3.7 which 
will show the same picture (in fact infinitely many blowups even within the 
region already blown up).
Figure 5.3.9 is a superposition of several time series pictures for five dif­
ferent values of for a = —1.28, which occurs in the middle of Figure 
5.3.7. We see some orbits being attracted (very fast) to the root, and an­
other (the middle value of Xq) being attracted to an attracting cycle of 
period 4. (You can tell that it is period 4 rather than period 2 by looking 

234
5. Iteration
carefully at the heights of the wiggles; you will see that there are two dif­
ferent heights.) Incidentally, why should the period be 4 when this value of 
a, in the blowup, appears to be in a region of period 2? The answer is in 
the overall picture, Figure 5.3.8, where you can see that Figure 5.3.9 covers 
just one of two cascades of bifurcation at that level of a.
Figure 5.3.9 exhibits one of the difficulties for Newton’s method, namely 
the existence of an attracting cycle other than the roots. Figure 5.3.10 
shows another possibility, for a = —1.295: an orbit moving more or less 
chaotically (from one of the top two values of xq\ it’s a bit too jumbled to 
distinguish which), but still never getting near the root (which is reached 
fairly quickly from the other two values of xq).
Generalization of Newton’s Method to More 
Variables
Newton’s method in several variables is an even more essential tool in sev­
eral variables than it is in one variable, and also far more poorly understood. 
The bare bones of this generalization are as follows:
Newton’s method as expressed by equation (11) at the beginning of this 
Section 5.3 transfers exactly to higher dimensions, where both f and x are 
vector quantities. If f: Rn —> Rn is a function and we look for a solution of 
f(0) = 0 near xq, we may replace f by its linear approximation
Wx) = f(x0) + (dxof)(x - x0) 
(16)
where dXof is the (n x n)-matrix of partial derivatives of the components 
of f evaluated at xq.

5.4. Numerical Methods as Iterative Systems
235
Example 5.3.9. In Volume II, Section 8.1, we shall use this scheme in R2,
with
and
g(x,y)
this equation (16) would become
У(^о,Уо)
g(xo, yo)
X — Xq
У-Уо
xo
If dXof is invertible, the equation
LXo(x) = 0
has the unique root
xi=x0-(dXof) 1(f(x0)) = V/(x0).
Exactly as in the single variable case, you may expect that if xo is chosen 
sufficiently near a root of f, then the sequence of iterates of Nf starting at 
x0 will converge to the root.
5.4 Numerical Methods as Iterative Systems
In Section 3.3-3.5, we discussed at some length the question of what hap­
pens to numerical methods if we fix the independent variable t and decrease 
stepsize h = At? Now we want to discuss a quite different aspect: what 
happens if we fix stepsize h and increase the independent variable t? It is 
really in order to study this second question that the present chapter is 
included.
At heart, the kind of thing we want to discuss is illustrated by the two 
pictures on the next page as Figures 5.4.1 and 5.4.2.
Figure 5.4.1 was obtained by “solving” x' = x2 — t2, using DiffEq with 
Runge-Kutta, —10 < t < 10, and stepsize h — 0.3. The middle of the 
picture is reasonable, but the jagged “solutions curves” to the left and 
right are wrong; the solutions of the differential equation do not look like 
that. In fact, they look like the picture in Figure 5.4.2.

236
5. Iteration
FIGURE 5.4.1. Some solutions for x' = x2 — t2 by Runge—Kutta with h = 0.3.
Figure 5.4.2 is obtained from the same program (DiffEq with Runge- 
Kutta and —10 < t < 10), changing only the stepsize, to h = 0.03. It is 
easy to show that the funnel and antifunnel are really there, so that at least 
these properties of the actual differential equation are accurately reflected 
by the drawing. In reality, if solutions are drawn for larger t’s, even with 
this stepsize the solutions in the funnel will eventually become jagged junk. 
You can right now experiment, to find for which t this occurs; after having 
read the section you should be able to predict it without the computer. 
Exercise 5.4#6 will ask you to do so.

5.4. Numerical Methods as Iterative Systems
237
Where did the jaggles come from? We will explore this question in this 
section and see that it comes from a lack of “numerical stability" of the 
computation (not a lack of stability of the solution itself, but of the numer­
ical scheme for approximating solutions of differential equations).
A first observation is that each of the numerical methods we have dis­
cussed may be interpreted as an iterative system. Consider for instance 
Euler’s method applied to x' = f(t,x):
tn+1 — tn + h
^n+l := 
“I” hf(tn,Xn).
The successive points (tn,xn) are the orbit of (^o?^o) under iteration of 
the mapping Fh' R2 —> R2 given by
t
X
Fh
t + h 
Fh
where Fh = x 4- hf(t, x).
We shall study the behavior of the second coordinate of Fh.
We definitely do want to study the nonautonomous case, but first we will 
see what happens for autonomous equations. If the differential equation is 
autonomous, of the form
ж' = /(ж),
then, for Euler, 
Fh(x) = x + hf(x). 
(17)
Actually, for autonomous systems it is also true of the other methods 
under discussion that there exists an analogous function Fh for the sec­
ond coordinate of Fh that does not depend on t, although the formulas 
corresponding to equation (17) are more complicated (Exercises 5.4#1):
Midpoint Euler gives Fh(x) = x + hf(x + ^/(ж));
Runge-Kutta gives the rather awesome formula
Fh(a:) = a;+^(/(a;) + 2/(a:+^/(a:)^ + 2/(a; + |/(a: + 
+
f (ж + hf (a: + 
(a; + 
)) •
The following example, although simple and completely analyzable, is 
still the most important example in understanding numerical stability.
Example 5.4.1. We will try various numerical methods for the archetyp­
ical linear differential equation x' = -ax, with a > 0. Since the solutions 
of this equation are x(t) = e~at, all solutions go monotonically to 0, faster 
and faster as a gets larger.

238
5. Iteration
A perfect numerical method with step h would give
Fh(a;) = e~ahx.
Now let us examine what in fact our actual numerical methods yield:
(a) Euler’s method with step h comes down to iterating
Fh(x) = x — ahx — (1 — ah)x.
If h is very small, then 1 — ah is quite close to e“ah, in fact it is the 
value of the Taylor polynomial approximation of degree 1 (Appendix A3). 
If h is larger, things are not so good.
Since the nth iterate Еоп(ж) = (1 — ah)71 ж, the orbit of x will go to zero 
if and only if |(1 — ah)\ < 1, which will occur if h < 2/a. Moreover, the 
orbit of x under Fh
will go monotonically to 0 if h < l/o,
will land on 0 in one move and stay there if h = 1/a,
will tend to zero, oscillating above and beneath zero iil/a < h < 2/a.
So we see that only if the step h is chosen smaller than 1/a does the nu­
merical method reproduce even the qualitative behavior of the differential 
equation; the numerical method does not even give solutions which tend to 
0 at all if h > 2/a. к
Euler’s method applied to xf = -ax, as in Example 5.4.1, exhibits “con­
ditional stability,11 i.e., the stability of approximate solutions depends on 
the stepsize. Note the peculiar feature that the more strongly 0 attracts 
solutions, the shorter the step must be for the numerical method to reflect 
this attraction. This phenomenon, known as “stiffness11 is at the root of 
many difficulties with numerical methods. We shall see this particularly in 
Volume III when we try to solve analogs of the heat equation. These are 
the situations in which implicit methods are particularly useful, as shown 
below in part (d).
Example 5.4.2. Let us try other methods on x' = —ax.
(b) Midpoint Euler, applied to the same equation with stepsize h, amounts 
to iterating
Fh(x) = x + h[ -a(x + -(-ax) J J = (1 - ah 4--- — J x.
We can analyze this system, exactly as above. For h small, we now see 
the second degree Taylor polynomial (again, see Appendix A3), and can 
expect the numerical approximation to be quite good. As you can confirm 

5.4. Numerical Methods as Iterative Systems
239
in Exercise 5.4#2a, we find that all solutions tend monotonically to 0 if 
0 < h < 2/a, but if h > 2/a, the orbit goes to oo.
(c) Runge-Kutta applied to the same equation with stepsize Д, amounts 
to iterating
/ 
h2cy2
ад= 1-сл+^-
hPo? h4a4 \
Exercise 5.4#2b asks you to analyze the iteration of this function and find 
for what h do orbits tend to 0? You will find a wider region of conditional 
stability.
(d) Implicit Euler applies the equation
З^г+1 — “И 
? *^г+1) —
which gives
#i+i = Xi/(1 + ah).
The function 1/(1 + ah) has the Taylor series
1 — ah + a2h2 — a3h3 + ... ,
and for small h is close to 1 — ah. Since the quadratic term is different from 
the quadratic term of the Taylor series, one expects that this method will 
have the same order as Euler’s method, i.e., order 1. The implicit method’s 
great advantage comes from the fact that for all positive h, we have
0 < 1/(1 + ah) < 1.
Thus for all step lengths, all solutions tend monotonically to 0, as they 
should. So for this equation, this implicit method is much better than the 
explicit ones; there will never be any jagged “solutions.” ▲
Examples 5.4.1 and 5.4.2 are of course exceptionally simple; in general 
we cannot hope to analyze iterative numerical methods so simply.
Example 5.4.3. Now we will examine how the simplest numerical method 
deals with
x' = x — x2. 
(18)
Euler’s method leads to iterating
Fh(x) = x + h(x - x2), 
(19)
a second degree polynomial. It is not too hard to show (Exercise 5.4#3a) 
that by choosing h appropriately and making a change of variables, you 
can get any quadratic polynomial this way.

240
5. Iteration
So studying the solutions of equation (18) by Euler’s method is just 
another way of studying iteration of quadratic polynomials. We will not 
repeat Section 5.2, but will apply the insight gained there to study the 
differential equation (18).
First, observe that x = 1 is a solution of the differential equation, which 
in fact attracts nearby solutions. Also observe, from equation (19), that 
x = 1 is a fixed point of Fh- If we set x = 1 + и and write Fh in terms of 
u, we find
Fh(l + u) = 1 + ((1 — h)u — hu2). 
(20)
From the form of equation (20) we can see that x = 1 (or и = 0) is an 
attracting fixed point if and only if |1 — h\ < 1, i.e., if and only if 0 < h < 2.
Figures 5.4.3 and 5.4.4 represents solutions, through several different 
starting points each, of equation (18) under Euler’s method (for steps h = 
1.95 above and h = 2.1 below). Please observe that in the upper drawing 
the approximate solutions are all attracted to the actual solution x = 1, 
whereas in the lower drawing they are not, but instead are all attracted to 
a cycle of order 2, which represents some sort of “spurious solution.” ▲
FIGURE 5.4.4. Iterating x' = x - x2 by Euler’s method, for stepsize 2.10.

5.4. Numerical Methods as Iterative Systems
241
Example 5.4.4. Now we will try a nonautonomous equation:
x' — —tx.
This is the equation of Examples 1.1.1 and 1.1.2, and the exact solutions 
are
x^Ce^.
Figure 5.4.5 shows various approximate solutions, all for stepsize h = 0.3, 
for the three numerical methods we have been using. We observe that all 
three methods appear to behave properly, giving solutions which tend to 0 
as they should, for small t. But suddenly, as t becomes large (about 5 for 
Euler, 6.6 for Midpoint Euler, 9.3 for Runge-Kutta), solutions stop tending 
to 0; in fact, the solution 0 loses its stability, and solutions fly away from
t = о
t = 15
FIGURE 5.4.5. Solving x' = —tx by different methods, for fixed stepsize.
Let us see what the theory predicts. For Euler’s method, we are iterating 
^n+l — “b h( 
~ *^n(l 
^п)-

242
5. Iteration
In order for the sequence (жп) to be tending to 0, we must have 
\l-htn\<l,
i.e., tn < 2/h. For h = .3, this means that the solution 0 should lose its 
stability at t = 6.666... .
For Midpoint Euler, the formula for xn is a bit more formidable:
/ h2 , (h3 
\ 
h2t„\
•^n+i жп I 1 
4- tn I —— h 1 4 
— j.
Unlike the case of Euler’s method, this does not become negative. This 
explains the different kinds of loss of stability: Euler’s method oscillates 
around the correct solution, but midpoint Euler pulls away on one side. As 
soon as tn becomes large, the quadratic term dominates. For stability we 
must have (Exercise 5.4#4a) 
, h2 * fh3
1 
2 +<n( 4
h2t2n
2
for h reasonably small this is true, again, when tn is smaller than about 
2/h.
There is a similar formula for Runge-Kutta, but it is long, and we will 
not develop it here. It appears from the drawing that the loss of stability 
occurs at about t = 9.3. 
▲
Example 5.4.5. For implicit Euler we get the following from x' = — tx, 
with tn = nh:
^n+i == 
4~ hf(tn+i, Жп+i)
Thus
^n+l(l 4" ^n+l^) = ^n,
and 
ч
- ( 
1 A
Ж"+1 ~ 
\ Ц-(П + l)h2 / ’
which goes monotonically to zero. Thus no jaggies will appear in drawings 
of the solutions by implicit Euler, regardless of the stepsize h. ▲
Example 5.4.6. We will return to x' = x2 — t, the ubiquitous equation of 
Chapter 1. The question we want to investigate is “Do numerical approx­
imations remain in the funnel?” Figure 5.4.6 shows much of the answer. 
All three pictures are for the region 0 < t < 15, and for Euler’s method, 
Midpoint Euler and Runge-Kutta with stepsize h = 0.4.
The computer picture clearly shows that the funnel loses its stability, at 
approximately t = 6 for Euler’s method and midpoint Euler, and somewhat 
later for Runge-Kutta.

5.4. Numerical Methods as Iterative Systems
243
t = 0
t = 25
FIGURE 5.4.6. Iterating x' = x2 — t by different methods, for fixed stepsize.
We will analyze this phenomenon in a moment, but first observe that 
Midpoint Euler develops a really unpleasant behavior: a perfectly reason­
able looking but spurious solution appears, designed to fool people who 
trust computers. (We know it is spurious from our analysis in Chapter 1.)
It is not quite obvious how to analyze Figure 5.4.6; the answer is to 

244
5. Iteration
linearize around a real solution. Suppose u(t) is a solution in the funnel, 
and suppose that another solution is u(t) + v(t), where we will assume that 
v(t) is small. Then, substituting x = и + v into the differential equation, 
we get
(u + v)f(t) = (u(t) + v(t))2 - t = (u(t))2 + 2u(fyv(t) + (v(t))2 - t.
Since u(t) alone also satisfies the differential equation, this can be rewrit­
ten
v'(t) = 2u(t)v(t) + (v(t))2.
Now we will make two approximations. First, we will neglect (v(t))2, 
which if v(t) is small should be very small. Second, we will replace u(t) by 
—Vi, since this is a good approximation to curves in the funnel. In the final 
analysis, we see that v(t) very nearly solves the linear equation
vf = -2Vtv, 
(21)
which is very similar to Example 5.4.3. A similar analysis can be performed, 
showing that the zero solution of (21), solved by Euler’s method with step 
h, loses its stability at t = 1/h2. In our case, this is t = 6.25, in agreement 
with the picture (Exercise 5.4#5a).
A similar analysis can also be carried out for Midpoint Euler, and gives 
precisely the same result. Stability is lost when t = 6.25. The authors do not 
know an explanation of the spurious solution, or whether this is a common 
or exceptional phenomenon.
If we had applied an implicit method to xf = x2 — t, no jaggies would 
have appeared. You can prove this in Exercise 5.4#5b. 
▲
Example 5.4.7. We now return to Figures 5.4.1 and 5.4.2, at the beginning 
of the section, for the differential equation
x=x2-t2.
Exactly as in Example 5.4.6, if we let u(t) be a solution in the funnel 
and u(t) + v(t) a perturbation of it, then v(t) approximately solves
v' = 2u(t)v(t),
and since u(t) & -t, we find that v(t) approximately solves
v = —2tv. 
(22)
Equation (22) is almost that of Example 5.4.4, up to the factor of 2. So 
if we use a change of variables s = \/2t, with w(s) = v(t), then
wz(s) = —sw. 
(23)

5.5. Periodic Differential Equations
245
which is exactly the equation of Example 5.4.4. Since under Runge-Kutta 
with stepsize h = 0.3 solutions of (23) lose their stability at s « 9.3 and 
t = s/y/2, we can expect solutions of
x = x2 — t2
close to u(i) = —t, solved by Runge-Kutta with stepsize h = 0.3, to lose 
their stability at « 9.3/V2 « 6.6.
This agrees with Figure 5.4.1, its t-coordinate runs from —7.5 to 7.5.
Exercise 5.4#6 asks you to perform a similar analysis for Figure 5.4.2 
where h = 0.03, and to confirm your calculation by a computer drawing 
over an appropriate t domain. ▲
We conclude this section on conditional stability of numerical approx­
imation with the remark that there exist higher order implicit methods 
of numerical approximation that have the precision of higher order meth­
ods like Runge-Kutta, but still have the stability of implicit Euler. However 
these methods are far more complicated and beyond the scope of this book. 
Consult the references listed at the end of this volume.
5.5 Periodic Differential Equations
This section is much closer to differential equations than the previous sec­
tions of this chapter. It reduces a surprisingly large class of differential 
equations to straight and simple iteration as described in Section 5.1. Pe­
riodic differential equations provide a first instance of the main way in 
which iterations arise when studying differential equations: Poincare sec­
tions. This technique was introduced into the subject by Henri Poincare, 
about 1890, and has remained an essential tool. The particular case we will 
study is very simple, but still brings out some of the main ideas. This will 
be investigated in a more substantial setting in Volume II, Chapter 13 on 
iteration in two dimensions.
By a periodic differential equation we shall mean throughout this section 
x'= f(t,x) 
(24)
where /(t, x) will be assumed to be locally Lipschitz with respect to ж, and, 
most importantly, periodic of period T with respect to t.
The periodicity of f means that /(t, x) = f(t+T, x) for all t, x. Examples 
of such functions are
xf — cos(#2 +1), which is periodic of period 2тг, or
x' = #frac(t), where frac(t) is defined to be the fractional 
part of t, which is periodic of period 1.

246
5. Iteration
x’ = 1 
1
x1 = g(x) J
The term “periodic differential equations” sounds more specialized than 
it is, as you can see from additional examples like
both of which are periodic with any finite period.
Remark. It is important to note that the hypothesis that f is periodic 
does not mean that the solutions of x' = f(t, x) are periodic. It does mean 
that if you translate the whole field of slopes in the (t, x) plane to the right 
by T (or nT, where n is any integer), you find the same field as what you 
started with. Equivalently, if is a solution, then the translate ^(t-nT} 
is also a solution for any integer n (but not in general the same solution).
Example 5.5.1. The differential equations studied in Example 2.5.2 are 
periodic. (See Figure 5.5.1.)
x' = (2 + cos t) x - (1/2) x2 - 1 
x' = (2 + cos t) x - (1/2) x2 - 2
FIGURE 5.5.1. Periodic differential equations.
Particularly in the second drawing it is easy to see that while the direction 
field repeats with period 2тг, the individual solutions 7(t) are not periodic. 
However y(t — 2п7г) is indeed also a solution, and one that has the same 
shape as 7(f). 
▲
To make the connection of periodic differential equations with iteration, 
we need to define a new function, called a one period later mapping, the 
simplest, one-dimensional Poincare section.
Definition 5.5.2. Consider the differential equation x' = f(t,x), with 
solutions x = 7(t). Let 7£Co(t) be the solution with initial condition ж0 at 

5.5. Periodic Differential Equations
247
to, i.e., so that 7®o(to) = xq. Then define the “one period later mapping” 
Pto as that which associates to xo the value of 7a,0(t) after one period, so 
that
Ло(«о) = yx0(to + T).
The map Pf0 depends on the choice of to and tells what happens to the 
solutions 7(f) for each xq.
Pt0 is constructed as follows, as illustrated in Figure 5.5.2:
FIGURE 5.5.2. Construction of Р*о(жо) for periodic differential equation xf = 
f(t,x) with solutions x = 7(2).
(i) choose to;
(ii) move vertically to x0 at tQ;
(iii) for every value xq for which the solution 7®0(t) makes it all the way 
across to to + T, the endpoints xq = 7®o(to) and x± = 7®o(to + T) 
give a point (xo,^i) of the graph of Pt0.
In Figure 5.5.2 points 1, 2, 3, and 4 belong to the domain of Pt0; the 
other values of xq do not. Also, as a consequence of the construction, you 
can note that
Pt0+T = Pt0,
because the direction field at to + T simply repeats that at to- Exercise 
5.5# 1 asks you to show this fact directly from the definition, as a good 
way to master the role of all the subscripts therein.
For linear equations, Pt0 is a linear function Pt0(x) = ax + 6, for appro­
priate a and b (Exercise 5.5#8). For separable differential equations, the 
graph of Pt0 is sometimes a segment of the diagonal (Exercises 5.5#3c and 
d, 5, and 6).

248
5. Iteration
The computer program 1-D Periodic Equations will compute and draw 
these maps Pto, but we warn you that it is quite slow because each in­
dividual point on Pto is computed by following one entire solution on the 
direction field, which you already know takes several seconds. You can min­
imize this time by choosing a window for the ж-range that only includes 
points in the domain of J*o, i.e., those points for which the solutions make 
it all the way across. Exercises 5.5^3 and 4 ask you to do this for the 
functions of Example 5.5.1.
For Figure 5.5.3 the program 1-D Periodic Equations has plotted for 
another differential equation, xf = sintcos(x +1), the period mapping Pt0 
which is shown on the left; the solutions over one period are shown on 
the right, without the slope marks in the direction field. You should follow 
carefully (Exercise 5.5#2) the relation between the left and right drawings.
One period later map Pt0
Solutions over one period T = 2т.
Many features can be seen to correspond on the two drawings in Figure 
5.5.3, more particularly attracting fixed points of Pto and attracting peri­
odic solutions of the differential equation. Actually, the correspondence is 
very close; the point we want to make is that
Iterating Pto is much the same as solving the differential equa­
tion xf = f(t,x).

5.5. Periodic Differential Equations
249
At least any information about “what happens in the long run” can be 
discovered by iterating Pto.
The reason is straightforward. Pick any jjq, and set
a?i = Pt0(rco), x2 = Pt0(xi),....
Then the solution 7x0(^) of d = /(^, ж) gives
7я0(*о + т) = жь 7Жо(Л) + 2Т) = ж2,...,7хо(^о + пТ) = xn.
In other words, iterating Pto is just like solving the equation one period 
further.
Example 5.5.3. Consider the differential equation
xf = (ж2 — l)(cost + 1). 
(25)
Again, this equation (25) can be solved, to give
1 (x — 1 \
x In —— I = sint +1 + C, 
(26)
2 
\ж +1/ 
v 7
and the solution (26) can be solved explicitly for x in terms of t:
1 _J_ (72e2(t+smt)
The solution 7Жо (t) with 7Жо (0) = xq is then
Л, M = (X° + 
~ l)e2(t+sint)
7x0 W (x0 + 1) - (x0 - l)e2(t+sin *) ’ 
( ’
so that
PtM ^o(2?r)-Жо(1_е4эт) + (1 + е47г). 
(29)
The graph of Pto is a hyperbola. Actually, it is only the left-hand branch 
of the hyperbola, the right-hand branch corresponds to solutions which “go 
through infinity”; you can check (Exercise 5.5#9) that if
e47r+i
X° > е4тг-1 ’ 
(30)
then the formula (28) for 7Жо does not correspond to a genuine solution of 
the differential equation, but goes to infinity at some t, 0 < t < 2тг. Figure 
5.5.4 shows this graph, together with several orbits. Observe that Pto has 
two fixed points, ±1, corresponding to the two constant solutions of the 
differential equation. Every point x < 1 iterates to —1, and every point

250
5. Iteration
x > 1 leads to an increasing sequence which eventually lands in the region 
(30) where Pto, as given by equation (29), is not defined. ▲
Example 5.5.4. Consider the following differential equation periodic with 
period 2тг:
xf = (sint + a)x — (3ecostx2. 
(31)
This is a “Bernoulli equation” (Exercise 2.3#9). The Bernoulli “trick” 
involves setting x = 1/u, transforming equation (31) to
uf sint + a: /3ecost
и2 и u2 ’
whereby multiplying by — u2 gives
u' = -(sint + a)n + /3ecost. 
(32)
Equation (32) is a linear equation, which can be solved by variation of 
parameters, to give
u(t) = u(0)ecost-1-at + /? / e^t-coss-a(t-s)ecossds 
Jo
cost—1—at । P 
a
In particular, we find
u(2tt) = e“27rau(0) + ^(1 - e-2™) - 4u(0) + B, 
,at _  
at
— u(0)e

5.5. Periodic Differential Equations
251
where A and В are the constants
A = e~2?ra, В = — (1 - e~2™).
a
Now going back to the variable x = 1/u, and as in Definition 5.5.2 calling 
7z0(t) the solution with 7zo(0) = xq, we find
Pt0 (*o) = yxo (2тг) = (л/а.о) + ^ = A^°Bxo ■ 
(33)
Thus Pto(#o) = xq/(A + Bxq). Actually again this is not quite true. Only 
the right-hand branch of the hyperbola (33) belongs to the graph of Pto; 
Pto(^o) is undefined for x$ < —A/В. Indeed, in those cases the solution 
we have written down goes through oo, and is not really a solution of the 
differential equation on the entire interval 0 < t < 2тг.
From the point of view of iteration, there are three very different oases 
to consider: A > 1, A = 1, and A < 1; which correspond to a positive, zero 
or negative. We will only consider the case В positive, which corresponds 
to (3 > 0.
FIGURE 5.5.5. For x' — (sint + a)x — f3ecostx2, graphs of Pt0 for different values 
of A^e'2™.
Graphs of Pto in these three cases are shown in Figure 5.5.5. We see (as 
you can verify in Exercise 5.5#10) that
If A > 1, 0 is an attracting fixed point and there is a negative repelling 
fixed point;
For A = 1, 0 is an indifferent fixed point, attracting points to the 
right and repelling points to the left;
If A < 1, 0 is a repelling fixed point and there is a positive attracting 
fixed point.

252
5. Iteration
Exercise 5.5# 11 asks you to show with graphs how these statements 
about Pf0 translate into statements about the solutions to the differential 
equation. A
Properties of the Function Pto
(a) The function Pto is defined in some interval (a, b), where either end­
point can be infinite.
(b) On this interval Pto is continuous and monotone increasing.
Any function g satisfying the two properties (a) and (b) is indeed the 
period-mapping function Pto for some differential equation, so there is not 
much more to say in general. In Exercise 5.5# 12 you are asked to prove 
this if g is defined on all of R and is onto. The general case is not much 
harder, but a lot messier.
Functions satisfying (a) and (b) are especially simple under iteration. 
In general, such a function (in fact any differentiable function) will have a 
graph which will intersect the diagonal in points ... ,x_i, tq, a?i,... and 
in each interval (x^Xi+i), the graph will lie either above or below the 
diagonal.
Proposition 5.5.5. If the graph of Pt0 in an interval (яг,Яг+1) between 
intersection points Xi with the diagonal lies above (resp. below) the diagonal, 
then for any x 6 [xi,Xi+i], the sequence
X, Pt0(x), Pt0(Pt0(x)),...
increases monotonically to 2?i+i (resp. decreases monotonically to xi).
Proof.' Since Pt0 is monotone increasing by (b), the sequence
x, PtQ(x), Pfo(Pto(a:)),...
is either monotone increasing, monotone decreasing or constant; indeed, 
Pto being monotone increasing means that
x < у => Ft0(x) < Ft0(y). 
(34)
Apply inequality (34) to the first two terms of the sequence:
if x < Pto(x) then Pto(a:) < Pto(Pto(xy), 
if x > Pto(x) then Pto(x) > Pto(Pto(x)).
The first of these alternatives occur if x 6 (a:*, а?г-ы) and the graph of Pto 
is above the diagonal in that interval; the second alternative occurs if the 
graph is below.

5.5. Periodic Differential Equations
253
Similarly, if x G (xi,xi+1), then Pto(x) G (xi,xi+1) by applying (34) to 
x and Xi or #i+i, so the entire sequence is contained in the interval, and in 
particular remains in the domain of Pto.
Thus the sequence is monotone and bounded, so it must tend to some 
limit. The limit must be in the interval [жг5^г+1]5 and it must be a fixed 
point of Pto (Exercise 5.5# 13). So it must be Xi (if the sequence is decreas­
ing) or Жг+i (if the sequence is increasing). □
Note the phrase “in general” preceding Proposition 5.5.5. It is really 
justified; the only way it can be wrong is if the graph of Pto intersects the 
diagonal in something other than discrete points, most likely in intervals, 
as in Exercises 5.5#3c, 5 and 6. We leave the very easy description of what 
happens in that case to the reader.
Thus to understand Pto under iteration, all you need to do is 
to find the fixed points of Pt0 and classify the intervals between 
them as above or beneath the diagonal.
Example 5.5.6. Consider the equation
x! = (2 + cos(£))z — f — a,
a = 1.3 — 
a = 1.32— 
a= 1.34— 
a= 1.36- 
a= 1.366 
a = 1.368 
a= 1.37- 
a= 1.38-
(.5,.5)
(.25,.25)
pto
FIGURE 5.5.6. Graphs of Pt0 for x' = (2 + cos(t))z — (l/2)x2 — a.
which was introduced in Examples 2.5.2 and 5.5.1. In this case, no formula 
for Pto can be found in elementary terms. However, the graph of Pto can 
still be drawn from the graphs of solutions, and Pt0 helps to understand 
the strange change in the behavior of the solutions for different values of 
a. In Figure 5.5.6 we have plotted the function Pto for various values of a 
between 1.3 and 1.38.

254
5. Iteration
The change of behavior for this differential equation occurs because the 
graph of Pto moves as follows: first there are two fixed points, then as 
a increases they coalesce into a single fixed point, then they disappear 
altogether. ▲
Example 5.5.6 illustrates again how changes in a parameter cause changes 
of behavior of an iteration. As in Section 5.2, this phenomenon is called 
bifurcation. In Volume II, Chapter 9 we will further discuss bifurcation 
behavior for differential equations. Meanwhile we give another example:
Example 5.5.7. Figure 5.5.7 illustrates how, as a result of small parame­
ter changes and the resulting bifurcations of fixed points, the same initial 
condition can lead to an orbit attracted by an altogether different fixed 
point. It comes from the differential equation
x' = cos(#2 + sin 27vt) — a
for various values of a as shown, and gives some idea of the sort of compli­
cation which may arise. However, there is nothing exceptional about this 
example; it is actually rather typical.
Exercise 5.5#14 will ask you to predict and verify the implications of 
Figure 5.5.7 for the solutions to this differential equation. ▲
The properties (a) and (b) make these iterative systems very simple for a 
one-dimensional differential equation. Unfortunately, when the dimension 
increases, there isn’t anything so simple to say.

5.6. Iterating in One Complex Dimension
255
Furthermore, it is the monotonicity of Pto in the examples in this section 
that makes them simple. If you look back at Section 5.1, you will see that 
non-monotone functions, even in one dimension, can make for any number 
of complications.
5.6 Iterating in One Complex Dimension
Although this section is neither directly related to differential equations 
nor essential for understanding the basic iteration techniques underlying 
the applications to differential equations, it is easy to discuss complex it­
eration at this point. Iterating complex variables is at the heart of cur­
rent research in dynamical systems. This research leads to the most beau­
tiful pictures, which are no longer seen only by mathematicians. These 
computer-generated images currently appear on magazine covers and as 
art on walls. Because the explanation of how the pictures are made is so 
accessible, we cannot resist giving a peek at this topic, right here where 
we have already laid all the groundwork in Sections 5.1-5.3. We hope that 
this introduction will inspire you to experiment and read further.
Iteration of maps f: Rn —> Rn is more complicated than iteration of maps 
/:R —> R. An important chapter in Volume II will be devoted to such 
things. However, iteration of appropriate maps /:С —> C often behaves 
mathematically more like mappings in one than in two dimensions; this is 
particularly the case for polynomials. Further, there are many aspects of 
iteration which are simpler in the complex domain than in the real, and 
whose behavior sheds important light on the behavior of iteration of real 
numbers.
Not only does iteration of polynomials in the complex plane give rise to 
beautiful pictures; it is also often easier than iteration in the reals. Easy 
examples arise like the counting of periodic points. By the Fundamental 
Theorem of Algebra a polynomial of degree d always has d complex roots 
counted with multiplicity, whereas no such simple statement is true about 
real roots. For instance, every complex quadratic polynomial z2 + c has 
2P — 2 complex periodic points of period exactly p, counting multiplicity, 
for every prime number p (Exercise 5.6#9). For a real polynomial x2 -he, it 
can be quite difficult to see how many real periodic points of a given period 
it has: see Example 5.1.9 for the case p = 2. In some cases, questions about 
real polynomials cannot at the moment be answered without considering 
the polynomials as complex.
Example 5.6.1. Under iteration by x2 + c, for how many real values of c 
is 0 periodic of period p, for some prime number p? Since the orbit of 0 is

256
5. Iteration
Яо(с) = 0 
a?i(c) = c 
a?2(c) = с2 + c 
z3(c) = (c2 + c)2 + c 
ж4(с) = ((c2 + c)2 + c)2 + c
this question is simply: how many real roots does the equation xp{c) = 0 
have? This is an equation of degree 2P~1, hence it has 2P-1 roots (counted 
with multiplicity). Note that c = 0 is one of the roots, but in that case 
z = 0 is a fixed point, hence not of exact period p. The point z = 0 is of 
period exactly p for the 2P-1 - 1 other roots.
But how many of these roots are real? The answer is (2P-1 - l)/p for 
all primes p 2. Two proofs of this fact are known; both rely on complex 
polynomials, and rather heavy duty complex analysis, far beyond the scope 
of this book. We simply ask you in Exercise 5.6#10 to verify this for p = 3, 
5, 7, 11. 
▲
For this entire section, we return to the basic class of functions studied 
for real variables in Sections 5.1 and 5.2—quadratic polynomials of the form 
x2 + c. As has been shown (Exercises 5.1#19), studying x2 + c in effect 
allows us to study iteration of all quadratic polynomials.
If c is complex, then x must also be complex in order for iteration to 
make sense. Since the letter z is usually used to denote a complex variable, 
we will henceforth write quadratic polynomials as
z2 + c, where both z and c may be complex numbers.
All the previous real examples are of course special cases.
Bounded Versus Unbounded Orbits
Experience has shown that the right question to ask about a polynomial 
(viewed as a dynamical system) is: Which points have bounded orbits?
Example 5.6.2. If c is real and you ask which real values of z have bounded 
orbits under z2 + c, the answer is as follows (shown in Exercise 5.1# 10):
(a) If c > 1/4 there are no points with bounded orbits;
(b) If — 2 < c < 1/4, the points of the interval
[-(1 + Vl^)/2, (1 + y/l^Tc)/2]
have bounded orbits, and no others.
(c) If c < —2, there is a Cantor set of points with bounded orbits. (Cantor 
sets are elaborated in a later subsection.)

5.6. Iterating in One Complex Dimension
257
For illustrations of these cases, you can look back to some of the pic­
tures for iterating real variables x2 + c in Section 5.1. Figure 5.1.10 has 
two examples of case (b); both of the orbits shown are bounded. In the 
first case, c = — 1.75, the points with bounded orbits form the interval 
[—1.9142..., —1.9142 ...]. An example of an unbounded orbit is provided 
by the rightmost orbit of Figure 5.1.6. 
▲
In the complex plane, the sets of points with bounded orbits are usually 
much more complicated than intervals, but quite easy to draw using a com­
puter. However, we will need a different type of picture, since the variable 
z requires two dimensions on a graph.
Pictures in the z-Plane
If we choose a particular value of c, we can make a picture showing what 
happens to the orbit of each point z as follows: Consider some grid of 
points in the complex plane, and for each point, take that value for zq and 
iterate it a certain number of times (like 100, or 300). If the orbit remains 
bounded for that number of iterates, color the point black; if the orbit 
is unbounded, leave the point white. (For z2 + c there is a practical test 
for knowing whether an orbit is unbounded: whenever an iterate exceeds 
|c| + 1 in absolute value, the successive iterates will go off toward infinity— 
Exercise 5.6#4.) The resulting black set will be an approximation to the 
set of points with bounded orbits, for that value of c.
The set of points with bounded orbits under iteration of Pc = z2 + c (i.e., 
the set of points approximated by the black points of the drawing described 
above) is denoted by
Kc={z\(z2 + c)°n/>oo}.
Pc
The variable z is called the dynamical variable (i.e., the variable which is 
being iterated), so Kc, which lies in the z-plane, is a subset of the dynamical 
plane (to be distinguished from pictures in the c-plane or parameter plane, 
which will be discussed later). The name Julia set was given to the boundary 
of a set Kc, in honor of a French mathematician, Gaston Julia. Julia and 
his teacher, Pierre Fatou, did the seminal work in this field. Technically 
the sets of points with bounded orbits are called “filled-in” Julia sets, but 
we shall call them simply Julia sets, since we will not need to discuss the 
boundaries separately.
Examples 5.6.3. See the pictures shown in Figures 5.6.1 and 5.6.2 on the 
next page, all printed to scale inside a square —2 to 2 on a side.
If c = 0, then the Julia set Kq is the unit disk. Confirm by iterating z2.
If c = —2, then the Julia set K~2 is the interval [—2,2] (Exercise 5.6#11).

258
5. Iteration
If c = —1, the Julia set K_i is far more complicated. This is a good set 
to explore, as in Exercise 5.6#6, in order to learn about the dynamics of 
complex iteration—they are full of surprises for the newcomer. ▲
FIGURE 5.6.1. The simplest Julia sets, in the г-plane, iterating z2 4- c for c = 0, 
c = -2.
The only geometrically simple Julia sets are Kq and К-2- All the others 
are “fractal.” This word does not appear to have a precise definition, but 
is meant to suggest that the sets have structure at all scales. Some other 
Julia sets are pictured in Examples 5.6.4.
Examples 5.6.4. (See Figure 5.6.3 on the next page.) ▲

5.6. Iterating in One Complex Dimension
259
FIGURE 5.6.3. More typical Julia sets, in the 2-plane.
Most Julia sets cannot be more than roughly sketched by hand, but we 
fortunately live in the age of computer graphics where there are several 
kinds of programs that draw them easily on personal computers. See the 
references at the end of this section for software to help you sample other 
Kc’s.
With the infinite variety of patterns for Julia sets, it is not surprising 
that a great deal of study has gone into being able to predict what sort of 

260
5. Iteration
picture will appear for a given value of c. Fatou and Julia proved a num­
ber of theorems about these sets back in 1905-1920, long before computer 
graphics enabled anybody to see accurate pictures. We state them here 
without proofs, which can be found in the references.
Theorem 5.6.5. Consider the Julia set Kc = {z | (г2 + c)on -/+ oo}.
(i) Kc is connected if and only if О e Kc.
(ii) Kc is a Cantor set if О Kc.
(See below for a definition and discussion of Cantor sets.)
What makes the value zq = 0 special? It is the unique critical point 
for z2 + c, the value that gives zero derivative. If we wrote our quadratic 
polynomials in the form az(l — z), then the place to start would be the 
critical point of that function, zq = 1/2.
One of the main discoveries of Fatou and Julia is that the dynamical 
properties of a polynomial are largely controlled by the behavior of the 
critical points. One example is Theorem 5.6.5. Another is the following 
result:
Theorem 5.6.6. If a polynomial has an attractive cycle, a critical point 
will be attracted to it.
For instance a quadratic polynomial always has infinitely many periodic 
cycles, but at most one can ever be attractive, since there is only one critical 
point to be attracted. A cubic polynomial can have at most two attractive 
cycles, since it has at most two critical points, and so on. No algebraic proof 
of this algebraic fact has ever been found.
The dominant behavior of critical points is what makes complex analytic 
dynamics in one variable so amenable to study. There is no theorem for 
iteration in R2 corresponding to Theorems 5.6.5 or 5.6.6. As far as we know, 
there is no particularly interesting class of points to iterate in general in 
Rn.
Consequences of Theorems 5.6.5 and 5.6.6 may be seen looking back at 
Examples 5.6.3 and 5.6.4; if you iterate z2 + c for any of the given values 
of c, starting at z — 0, and look at what happens to the orbit, you will find 
bounded orbits and corresponding connected ATc’s, for all except A0.3125 
and Jf-o.75+o.ii- Theorem 5.6.5 tells us that these last two Julia sets are 
Cantor sets, which we shall finally explain.
Cantor Sets
Georg Cantor was a German mathematician of the late nineteenth century. 
In the course of investigations of Fourier series, Cantor developed world­
shaking new theories of infinity and invented the famous set that bears his 

5.6. Iterating in One Complex Dimension
261
name. These results were considered most surprising and pathological at 
the time, though since then they have been found to occur naturally in 
many branches of mathematics.
A Cantor set X C Rn is a closed and bounded subset which is 
totally disconnected and simultaneously without isolated points.
The first property says that the only connected subsets are points; the 
second says that every neighborhood of every point of X contains other 
points of X. These two properties may seem contradictory: the first says 
that points are never together and the other says that they are never alone.
Benoit Mandelbrot of IBM’s Thomas Watson Research Center, the fathor 
of “fractal geometry,” has suggested calling these sets Cantor dusts. This 
name seems quite auspicious: it is tempting to think that a dust is made 
up of specks, but that no speck is ever isolated from other specks.
The compatibility of the requirements above is shown by the following 
example, illustrated by Figure 5.6.4.
i nte rval
2
3 
1
виви^вим first step
.2 ...
2_ 
]_ 
8_
3 
9 
9 
1. 
,
< second step
.20... 
.22...
мм м м third step
.200... .202... 
.220... .222 ...
and SO on
FIGURE 5.6.4. Construction of the Cantor set.
The Cantor set, the one originally considered by Cantor, is the 
set X C R made up of those numbers in [0,1] which can be 
written in base 3 without using the digit 1.
The set X can be constructed as follows: From [0,1] begin by removing 
the middle third (|, |). All the numbers in the interval (|, |) are not in 
X, since they will all use the digit “1” in the first place after the “decimal” 
point. Next remove the middle thirds of both intervals remaining, which 
means that the intervals (|, |) and (§, |) are also excluded, those numbers 
that will use the digit “1” in the second place. Then remove the middle 

262
5. Iteration
thirds of the four intervals remaining, then remove the middle thirds of the 
eight intervals remaining, and so on.
You are asked in Exercise 5.6# 13 to show that this set indeed satisfies 
the stated requirements for a Cantor set, and to consider the endpoints of 
the excluded intervals.
Example 5.6.7. The sets Kc can be constructed in a remarkably similar 
way when c is real and c < — 2. Actually, we will construct only the real 
part of Kc\ but it turns out that in this case the whole set is contained 
in the real axis. This fact appears to require some complex analysis in its 
proof, so the reader will have to take it on faith. The iteration graph of Pc 
looks like Figure 5.6.5.
FIGURE 5.6.5. Construction of a Cantor set of points with bounded orbits.
In particular, since c < -2, the graph dips below the square
{(x,!/)| |a:|,|l/| </?},
where (3 = (1 + Vl — 4c)/2 is the rightmost fixed point. Clearly the points 
x e R with |ж| > (3 escape to oo. But then so do the values of x in the 
“dip,” which form an interval in the middle of [—/3,/3\. In particular, 
О G Ii, hence does escape. Now Ii has two inverse images one 
each of the two intervals of [—/3, /3] — I±. Now each of the intervals I2 and 
I2 has two inverse images, so there are four intervals I3, one in each of 
the four pieces of [-/3, /3] - Д - Г2 - I2. This process of going back to two 
inverse images of each interval can continue indefinitely.
The intervals being removed are not in this case the middle thirds, and it 
is not quite clear that no intervals remain when they have all been removed. 
In fact, it is true but we omit the proof, which is rather hard. A
Cantor dusts need not lie on a line.

5.6. Iterating in One Complex Dimension
263
Example 5.6.8. Consider the polynomial z2 + 2. In Exercise 5.6#5 you 
are asked to show that all points z with |z| > 2 escape to oo. The inverse 
image of the circle |z| = 2 is a figure eight, touching the original circle at 
±2i, and the polynomial is a 1-1 map of each lobe onto the disc of radius 2. 
Thus in each lobe you see another figure eight, the inside of which consists 
of the points which take two iterations to escape from the disc of radius 
2. Now inside of each lobe of these there is another figure eight, and so 
on. Again, it is true but not easy to prove that the diameters of the figure 
eights tend to zero. If the reader will take that on faith, then the Julia set 
is clearly a Cantor dust: Any two points of the Julia set will eventually 
be in different lobes of some figure eight, hence can never be in the same 
connected part of the Julia set; on the other hand, any neighborhood of 
any point will necessarily contain whole figure eights, hence must contain 
infinitely many points of the Julia set. ▲
(Cantor Dust) 
Construction of K2
FIGURE 5.6.6. The Julia set JC2, showing the inverse image contours that bound 
and define it.

264
5. Iteration
The Picture in the Complex c-Plane
In the late 1970’s, Mandelbrot (among others) looked in the parameter space 
(that is, the complex c-plane) for quadratic polynomials of form z2 + c and 
plotted computer pictures of those values of c for which the orbit of 0 is 
bounded, after a large finite number of iterations. The computer algorithm 
is the same as described for the construction of Julia sets, to iterate z2 + c 
at each point of some grid, but this time it is c that is determined by the 
given point, and the starting value for z is what is fixed, at 0.
The set M that resulted from Mandelbrot’s experiment, shown in Figure 
5.6.7, is called the Mandelbrot set. Studying the Mandelbrot set is like 
studying all quadratic polynomials at once.
FIGURE 5.6.7. The Mandelbrot set, in the c-plane; the values of complex c such 
that iterating z2 + c from zq = 0 gives bounded orbits.

5.6. Iterating in One Complex Dimension 
265
The Mandelbrot set is marvelously complicated. The closer we zoom in 
on the boundary, the more detail appears, as illustrated in Figure 5.6.8, 
on pp. 266-267. Notice how jumbled and intertwined are the regions of 
bounded orbits (black) and the regions of unbounded orbits (white). Yet 
with the advent of high speed and high resolution computer graphics, we 
are able, quite easily, to explore this set.
Iteration pictures such as these are made more detailed by using contour 
lines representing “escape times,” telling how many iterations are neces­
sary before the result exceeds the value that assures the orbit will not be 
bounded. The outer contour is a circle radius 2; outside this circle every 
point has “escaped” before the iteration even begins. The next contour in is 
the inverse image of the first contour circle; points between these contours 
escape in one iteration. The next contour is the inverse image of the sec­
ond; points between this contour and the last will escape in two iterations, 
and so on. The pictures in Figure 5.6.8 in fact show only the contours for 
escape times; these contours get denser and denser as they get closer to the 
boundary of M, so they actually outline it.

266
5. Iteration
FIGURE 5.6.8. Zooming in on the boundary of the Mandelbrot set, in the c- 
plane. Each region that is blown up in the next picture is marked with a black 
box and arrow.

5.6. Iterating in One Complex Dimension
267
FIGURE 5.6.8. (cont.) The bands represent how many iterations are necessary 
for those points to escape beyond the circle radius 2, where they are sure to go 
to infinity. In the first frame they are numbered accordingly.

268
5. Iteration
Self-Similarity in the z-Plane and the c-Plane
Some people think that self-similar sets (those that repeatedly look the 
same no matter how closely you look, such as the limit of the sequence 
of figures shown in Figure 5.6.9) are complicated. The famous self-similar 
Koch snowflake is constructed simply by repeatedly replacing each straight 
line segment with W
FIGURE 5.6.9. Construction of the Koch snowflake, a self-similar figure.
The Julia sets Kc are roughly self-similar. If you blow up around any 
point, you keep seeing the same thing, as in Figure 5.6.10 on pp. 270-271.
But the Mandelbrot set is far more complicated than the self-similar sets 
of Figures 5.6.9 and 5.6.10, by being not self-similar. Look back at the blow­
ups of the Mandelbrot set and compare Figure 5.6.7 with Figure 5.6.8. As 
you zoom in on the boundary in the first picture, it gets more complicated 
rather than simpler, and although similar shapes may appear, they are 
different in detail at every point. In fact, the Mandelbrot set exhibits in 
various places all the shapes of all the possible associated Kc’s.

5.6. Iterating in One Complex Dimension
Universality of the Mandelbrot Set
269
The Mandelbrot set is of intense mathematical interest, among other rea­
sons because it is universal. Somewhat distorted copies of M show up in pa­
rameter space pictures for iteration of all sorts of nonquadratic functions— 
higher degree polynomials, rational functions (including Newton’s method 
in the complex plane), and even transcendental functions (like Asinz).
Despite its complication, the Mandelbrot set is now quite well under­
stood. The first result in this direction is that it is connected (those “is­
lands” off the edges are actually connected to the whole set by thin “fila­
ments”). Further, a complete description of how the “islands” are connected 
up by the “filaments” has been found, but the description is not easy. The 
interested reader can get a flavor of how this is done in the book by Peitgen 
and Richter. See the References.
To tie things together a bit, one should realize that the part of M on the 
real axis is essentially the cascade picture of Section 5.2.
Indeed, the cascade picture is also obtained by letting c vary (only on 
the real axis) and plotting for each value of c the orbit of 0. The windows 
of order correspond to regions in which the critical point is attracted to 
an attractive cycle, the order of which can be read off from the cascade 
picture. These windows correspond to islands of the Mandelbrot set along 
the real axis, as Figure 5.6.11 onp. 272 illustrates.
Note: This particular drawing was computed with no unmarked points, 
that is, with К = 0. You might want to contrast it with Figure 5.2.2. The 
white spaces are in the same places, but in Figure 5.6.11, the branches are 
greatly thickened.
The universality of the cascade picture exhibited in Sections 5.2 and 5.3 
is simply a real form of the universality of M.
Further discussion of iteration in the complex plane is beyond the scope 
of this text. References for further reading are listed at the end of this 
volume.

270
5. Iteration
FIGURE 5.6.10. The Julia set for c = -0.2+0.75г and successive blow-ups. Each 
region that is blown up in the next picture is marked with a black box and arrow.

5.6. Iterating in One Complex Dimension
271
FIGURE 5.6.10. (cont.)

272
5. Iteration
FIGURE 5.6.11. Relation between the Mandelbrot set and the cascade picture.

Exercises
273
Exercises 5.1 Iterating Graphically
5.1 #1. Analyze each of the following functions under iteration. Find the 
fixed points and determine the classification of each fixed point (attracting, 
superattracting, or repelling). Indicate all cycles of order two. Graph the 
equation and then iterate it by hand for a representative selection of Жо’в. 
Finally, use Analyzer to confirm your handwritten iteration and to quali­
tatively determine and indicate regions on the ж-axis for which all initial 
points converge to the same attracting fixed point.
(a) /(*) = ~ix +1
(e) /(x) = 3
(b) /(®) =
(f)
/(a?) = a:2
(c) /(®) = 2a: + 1
(g) /(a:) = a:3
(d) /(*) = -2a: + 1
(Ю /(a:) = 1/x
5.1 #2. To see that anything can happen with an indifferent fixed point, 
analyze as in 5.1# 1 the behavior of the following equations under iteration:
(a) f(x) = e x - 1 
(d) f{x) = ln(l - x)
(b) /(ж) = -ж 
(e) /(ж) = ж + e x*
(с) /(ж) = - tan ж (f) /(ж) = 0.5ж + 0.5ж2 + 0.125 
5.1#3.
(a) Prove, using Taylor series expansion, that the difference between a 
superattracting fixed point, жв, and an attracting fixed point, жа, is 
the fact that, for sufficiently small |w|,
xn = f(xa + u) = xa + Mu, 0 < \M| < 1,
while
xn = f(xs + u)=xs± Nup, p>2.
Hence, for sufficiently small |u|, solutions will converge faster (i.e., 
with fewer iterations) to xs than to xa.
(b) Compare Exercises 5.1#le,f,g with Exercises 5.1#la,b. How does this 
comparison confirm the preceding result?

274
5. Iteration
5.1#4°.
(a) Use a Taylor series expansion to prove: If at some fixed point tq of 
a function to be iterated /(x), /'(xq) = 1 and /"(xo) > 0, then in a 
sufficiently small neighborhood of xq, points to the left of xq will be 
attracted to tq under iteration by /(x), and points to the right will 
be repelled. Confirm your result with
/(®) = |a;3 + |. 
о о
(b) What will happen in the neighborhood of tq if /"(xo) < 0? Confirm 
your result with
f(x) = ln(l + x).
(c) Show that anything can happen in the neighborhood of tq if /"(xo) = 
0 by iterating the following functions:
(i) /(x) = sinx
(ii) =
(iii) /(a:) = e"-l-^-^-x4
2 О
2 
3
(iv) /(x) = e’-l-^-^-+x4
(d) (harder) What will happen in the neighborhood of xq if f(x) = — 1? 
Confirm your results with f(x) = —g(x), where g(x) is any of the 
functions from (a), (b) and (c). Hint: Write the Taylor polynomial of 
/°2 to degree 3.
5.1#5. Near a point t0, we define the triangular regions formed by the 
lines x — xq = ±(y — /(xq)), as shown.
Consider a function f(x) with fixed point at xq.
(a) If /(x) lies entirely within the gray regions, justify with an e — 6 
type argument (rather than a differentiability argument) that tq is 
an attracting fixed point.
(b) Show that if f(x) is a differentiable function with /'(xq) < 1, then 
f(x) indeed lies in the gray region specified.

Exercises
275
(c) Show that if f(x) is a differentiable function with /'(ж0) > 1? then xq 
is a repelling fixed point. Note that this is a little harder, and that it 
does not prove f(x) will never come back to xq, only that xq repels 
near neighbors.
5.1#6. Analyze the following functions under iteration, using your results 
from the previous exercises. First try to predict the behavior and then use 
Analyzer to confirm your predictions.
(а) /(ж) = x + cos ж
(b) f(x) = x + sin ж
x^
(c) f(x) = cos X + — - 1
(d) f(x) = 1п|ж + 1|
(e) /(ж) = x + e~x
(f) /(ж) = x - e~x2
( 
x2 
x& 
x^ 
\
(g) /(*) = - 
+
\ 
2 
о 
24 
J
x2
(h) /(ж) = ex - 1 - 
- 2ж - x3
5.1#7. For the following functions, find the fixed points under iteration 
and classify them as attracting or repelling. Use the computer to confirm 
your results.
(a)° f (ж) = ж4 — Зж2 + Зж
7 
1
(b) f(x) = x* - -x2 + -x
5.1#8.
(a) Refer to Figures 5.1.6 and 5.1.7, iterating ж2 — 0.7 and ж2 — 1.2 re­
spectively. Tell why, when they look at first glance so similar, they 
exhibit such different behavior under iteration.
(b) Referring again to Figure 5.1.7 and Example 5.5, iterating ж2 — 1.2, 
determine what happens between the two fixed points, since both are 
repelling. Try this by computer experiment.
5.1 #9. Show that the multiplier mn for a cycle order n is the same at all 
points of the cycle. That is, fill in the necessary steps in the text derivation.
5.1#10°. For Example 5.1.9 iterating ж2 + c,

276
5. Iteration
(a) Find for which values of c there are, respectively, two fixed points, a 
single fixed point, and no fixed points.
(b) Sketch graphs for the three cases in part (a) showing how the diagonal 
line x = у intersects the parabola x  4- c in three possible ways as the 
parabola moves from above the diagonal to below it.
(i)
2
(c) 
Find for which values of c there will be an attracting fixed point.
(d) Show that c G [—2, 
, all orbits starting in the interval
(i) Use Analyzer to make computer drawings for iterating them, and
identify the order of any cycles that appear.
Ic = -hl + Vl —4c), +|(1 + VI - 4c) 
Z 
z
are bounded, and all others are unbounded.
(e) 
Show that the orbit of 0 is bounded if and only if c G [—2, 
.
The results of this exercise are used in Section 5.6, Example 5.6.2.
5.1#11. For iterating x2 4- c,
(a) verify the following facts about the period two cycle:
(i) The periodic points of period 2 are real if c < — |.
(ii) The multiplier m2 of the period 2 cycle is = 4(c 4-1).
(iii) The cycle of period 2 is attracting if and only if — | <c< —
(b) Illustrate the possibilities in part (a) graphically by sketching the 
various parabolas and the diagonal for each, then tracing out some 
appropriate iterations.
5.1#12. For each of the following functions, in the manner of Exercise 
5.1#10b, analyze the various possible behaviors of the function under iter­
ation for different values of the parameter a. That is, determine (graphically 
or numerically) which values of a will leads to different possible behaviors. 
Then for each a, sketch the curve, the diagonal, and the iterative behavior.
(a) f(x) = 1 - (x - a)2
(b) f(x) — x  — x + a
3
5.1#13. For each of the following functions,
(a) x  - 1.75 
(d) x  - 1.30 
(g) x  - 1.41739
2
2
2
(b) x  - 1.76 
(e) x  - 1.35 
(h) x  - 1.45
2
2
2
(c) x  + l 
(f) x  - 1.39 
(i) x  — 1.69
2
2
2

Exercises
277
(ii) Use DiffEq to make time series drawings for iterating them, and add 
by hand the actual points. For cycles of an odd order or of no order, 
you may need to use a ruler to determine where the time series line 
bends, as in Example 5.1.10, or refer to a computer listing on the 
points on the trajectory.
(iii) Use Cascade to graph, with the diagonal, the nth iterate of a function 
that has a cycle order n. (The other purposes of the Cascade program 
will come up in Section 5.2.)
(iv) Match up the results of parts (i), (ii), and (iii) as in Examples 5.1.10 
and 5.1.11. (If your scales are different for the different programs, 
your matching lines won’t be horizontal and vertical, but you can 
still show the correspondences.)
5.1#14°. Match the numbered Analyzer iteration pictures with the let­
tered time series pictures:

278
5. Iteration
7)

Exercises
279
e)
«<®ии
f)

280
5. Iteration
5.1#15. For the function of Example 5.1.12, /(ж) = 2ж — ж2, analyze 
the iteration and identify fixed points and cycles. Then use the conjugate 
mapping <р(ж) = 1 — ж to get р(ж) = ж2. Likewise analyze р(ж) and show 
that in fact the qualitative (attracting or repelling) behavior of the orbits 
is preserved.
5.1#16. Prove the following statement from the end of Section 5.1: If a 
conjugate mapping 92 and its inverse 92”1 are differentiable, the multipliers 
at corresponding points are equal.
5.1# 17. For /(ж) = 2ж2 — 1, find the multipliers for cycles of any period 
n. Hint: Use the results of the preceding exercise, which works everywhere 
except at the fixed points, where the conjugate mapping is not differen­
tiable.
5.1#18°. For /(ж) = 2ж and ip(x) = ж3,
(a) Find the function р(ж) which is conjugate to f by cp, i.e.,
(b) Show that the multipliers of f and g are different.
(c) Why does (b) not contradict the statement proved in Exercise 5.1#16?
5.1#19. For a general quadratic function /(ж) = аж2 + 6ж + d, with а / 0, 
show that a conjugate mapping <£>(ж) = ax + /3 can always be found such 
that /(ж) is conjugate to the simpler quadratic map, р(ж) = ж2 + c. That 
is, show how to choose a, /3, and c such that 92 о f = g о which is another 
way to state equation (3) in Section 5.1.7.
(6)
Exercises 5.2 Logistic Model and Quadratic 
Polynomials
5.2#1. For the logistic model we study equation (6) 
q(n 4-1) = (1 + a)q(n) - aq{n)2
which may be considered as iterating the quadratic polynomial
Q(x) = (1 + a)x - ax2.
(a) Show that the conjugate mapping <р(ж) = ёх+/3 that transforms this 
quadratic to the simpler form P(x) = a:2 + c (as in Exercise 5.1#19) 
can be written
1 + a x
(b) Show that the change of variables x = (14- a)/2 - aq in equation (6) 
is equivalent to the conjugation mapping you found in part (a).

Exercises
281
5.2#2. Show that another way to look at equation (6) is as an application 
of Euler’s method, with stepsize h = 1, to the differential equation x' = 
a(x — x2} studied in Section 2.5.
5.2#3°. Show that q(n) of equation (6) will be attracted to the stable 
population q8 = 1 if the fertility a is “small,” 0 < a < 1, and the initial 
population is “small,” 0 < g(0) < 14- as is the case with the correspond­
ing differential equation.
5.2#4. (harder) Show that qs of the preceding exercise is an attracting 
fixed point if and only if 0 < a < 2.
5.2#5°. If f:X —> X and g:Y —> Y are two maps, and <p-.X —> Y is a 
mapping which is 1-1 and onto such that 9? о f = g о <^, then everything 
dynamical about (X, /) gets carries into something with exactly the same 
dynamical properties for (Y, g). When <p is not 1-1 and onto, this is no 
longer quite true. Consider, for example,
X = the unit circle in С, /(г) = г2;
Y = the interval [—1,1], g(x) = 2x2 — 1;
<p the mapping <p from X to Y given by <р(егв) = cos#.
The accompanying figure shows two of the mappings: f (from the circle 
X to itself) and ip (from the circle X to the interval Y)
(a) Show that ip о f = g о ip.
(b) Show that <p is not 1-1.
(c) Find the fixed points of f and g. Do they correspond under <p?
(d) Show that both f and g have 2 periodic points of period exactly 2, 
and find them. Do they correspond under yrt
(e) Same as (d) for period three, in which case there are six points. 
Hint: in this case the periodic points for g are the numbers cos 2fc?r/7, 
к = 1,2,3 and cos2fc?r/9, к = 1,2,4.

282
5. Iteration
(f) Same as (d) for any period n, and describe how they correspond to 
the periodic points of g.
5.2#6.
\a) Make a Cascade picture and by locating “windows,” find values of 
c that will give cycles orders 3, 4, and 5. Verify your results with 
Analyzer iteration pictures at those values.
(b) By suitable blowups of the cascade picture around values giving cycles 
orders 3 and 4, find different values of c that will give copies of order 
12.
5.2#7.
(a) Use Analyzer’s root-finding capability and Theorem 5.1 to find the 
values of c for which you will find real periodic points of order 3, 4, 5, 
and 7. That is, find the roots of equations like ((0 + с)2 + с)2 + c = 0.
(b) Verify this fact experimentally by making either a time series or an 
Analyzer iteration picture for each value of c so found.
(c) Further verify your results by locating suitable “windows” at the 
given values of c on the Cascade picture or a suitable blowup of it. 
Mark the overall cascade picture with tickmarks on the vertical axis 
and labels showing what order cycles will appear for the particular 
values of c found in part (a).
5.2#8. For the following functions, make Analyzer iteration pictures and 
match them up to a Cascade picture, as in Example 5.14.
(a) x2 - 1.48 
(b) x2 - 1.77 
(c) x2 - 1.76
5.2#9. Consider the polynomial p(x) = 2x — ex2, where c is a number not 
equal to zero.
(a) What are the fixed points of p(x)l
(b) Show that p(x) is conjugate to q{x) = x2.
(c) What initial values are attracted under p(x) to 1/c?
(d) Suppose you knew 7Г to 1000 significant digits. If you set xq = 0.3, 
how many iterations of p(x) would you need to compute l/тг to 1000 
digits? Compare this with the number of steps in long division nec­
essary to achieve 1000 digits.
Remark. This is the algorithm actually used in computers to compute 
inverses to high precision.

Exercises
283
Exercises 5.3 Newton’s Method
5.3#1. Write the first five iterates of Newton’s method for calculating the 
roots of
(a) x2 — 23 = 0 starting at xq = 5;
(b) x3 + x - 1 = 0 starting at xQ = 1;
(c) sin x - | = 0 starting at ж0 = 1.
5.3#2. Notice that if xq = 1, Exercise 5.3#lc does not converge very well 
under five steps of Newton’s method. Describe why this is so, first graphing 
the function sin x — (x/2) with the Analyzer program, and then discussing 
the behavior of the iteration of Newton’s method.
5.3#3°. Let Pb be the polynomial z3 — z 4- 5, and Nb the corresponding 
Newton’s method.
(a) Show that there exists b such that Ль(Аь(0)) = 0.
(b) What happens to points z near 0 if you iterate Newton’s method 
starting at z?
Hint: Compute -^-(Nb(Nb(z)). 
dz
5.3#4.
(a) Show that Newton’s method for finding roots of x3 + ax +1 amounts 
to iterating (2ж3 — 1)/(Зж2 + a), as in the Cascade picture of Figure 
5.3.7.
(b) Show that any cubic polynomial ax3 + bx2 + ex + d can be written in 
the form x3 + ax +1 if you make the appropriate change of variables, 
which you should state.
5.3#5. Show that if p(z) = (z — a)(z — 6) is a quadratic polynomial, with 
a 7^ 6, then the sequence 
converges to a if \z — a| < \z — b\.
Hint: Make the change of variables и = (z — a)/(z — b).
5.3^6. Try to understand what Newton’s method does for the polynomial 
x2 + 1 on the real axis. Hint: Try setting x = arctan 0.
5.3#7. Refer to the end of Example 5.3.4, about using Newton’s method 
to find square roots. For x2 — a = 0, you will get
,r z x 1 / a
Na(x) = - Ж + - 
2 \ x
x Ф 0, a > 0.

284
5. Iteration
(a) Draw the graph of Na, showing that it is a hyperbola.
(b) Show that the square roots ±^5 are superattracting fixed points for 
Na.
(c) Show that for every positive xG, Newton’s method will converge to a 
positive root (i.e., xn —> ^/a) and that for every negative xq, Newton’s 
method will converge to a negative root (i.e., xn —> —y/a).
(d) Show that ±y/a are the only cycles.
5.3#8. Refer to Example 5.3.4 for using Newton’s method to find the roots 
to (x — a)(x — b) = 0 (!!)
(a) Prove equation (15) in Example 5.3.4 (a “simple” computation with 
plenty of chances for errors).
(b) Verify that if x > (a + b)/2, you will converge to the larger root;
if xq < (a + b)/2, you will converge to the smaller root;
if xq = (a + b)/2, you will converge to no root at all.
5.3#9°. You might like to see how differently
behaves from the Na of Exercise 5.3#7 (if under iteration you hit xn — 0, 
just stop):
(a) Draw the graph of Ta (it is again a hyperbola). Show that there are 
no fixed points. But find a cycle of period 2 and show that the cycle 
is repelling by showing that (Г о T\(xq) > 1.
(b) Why is (T о T)'(^o) = (To Т)'(ж1)? Or, in general, why is for a 
periodic cycle, x^x-^ ... ,xt = xq, (To... о T)f(xi) the same for 
i = 0,1,2,..., к -1?
Note: The functions Ta(x) arise if you study Newton’s iteration for x2 + 
a = 0 with positive a. One can show that every complex number with 
positive real part converges to у/a under iteration with Na, while every 
complex number with negative real part converges to —у/a under iteration. 
If x is purely imaginary, then Na(x) is purely imaginary with 1т^(ж) = 
Ta(Imx).
5.3#10. Prove that each root r of an equation f(x) = 0 is a superattracting 
fixed point of Newton’s equation, provided that /'(r) 7^ 0 and f"(r) exists.

Exercises
285
5.3#11°.
(a) Let r be a root of f(x) = 0 and let x = r + e, e 0. Derive an 
expression for f(x - e) using a Taylor polynomial with e2 as highest 
order s-term. Assume f is twice differentiable.
(b) Assuming f'(xi) / 0 and f is doubly differentiable, use your result 
from part (a) to show that
f"(c)
Nffa) -r = 
~ r)2, c € {ki - £,Xi] U [xt,xt - e]}.
(c) Using the result from part (b), prove the assertion that if Xi = r + e 
and |e| is small enough, the number of correct decimal digits will 
approximately double after each iteration.
(Remember N correct decimal digits => |error| < |10“N.)
(d) y/3 = 1.732050808. Using a calculator choose an initial x > 0 and 
iterate 7У3(:г) = |(я + 5), Newton’s equation for x2 — 3 = 0. Indicate 
the doubling of the digit accuracy. Why is e always “sufficiently small” 
in this case?
(e) Try x = e x also. That is, find roots of f(x) = x — e x.
5.3#12. For each of the following equations, express as f(x) = 0; then find 
each Nf(x) and analyze the iterations of each Nf(x) using Analyzer (or the 
Analyzer component of the Cascade program with the Newton’s method 
option). Then use Analyzer to graph each f(x). Draw the linear approxi­
mations (tangents to the graph of /(ж)) to demonstrate why solutions in a 
certain region converge to a certain root.
(a) x2 — x — 6 = 0 
(b) x3 — x = 0 
(c) x = e~x.
5.3#13. Use the Analyzer component to the Cascade program with the 
Newton’s method option to find the roots of each of the following equations:
(a) a:4 + |a;3 - ^-x = 1. 
z z
(b) x5 — a:4 + x2 = 15a:3 — 38a; — 24.
(c) 2a;8 - 12a;7 + 19a:6 + 9x5 - ^a;4 - 9a:3 + 19a:2 + 12a: + 2 = 0.
8
5.3#14°. Let us consider a Newton’s method problem in two variables: As 
you will see in Chapter 8, a singularity of a system of differential equations 
occurs when x1 = 0 and yf = 0 simultaneously. If
x> = f(x,y) = к-I)2 -y,
x2 1
у1 = g(x,y) = у + — -

286
5. Iteration
and you try to find a singularity by Newton’s method, taking as an initial 
guess (tfo,2/o) = (0,0), find (a?i,2/i). Then find (ж2,г/2)-
5.3#15. Let f be a differentiable function near xq, with a degenerate root 
at жо, in the sense that at a?o, f has the asymptotic expansion (as explained 
in the Appendix) 
f(xo + £) = A£k + o($k)
for some к > 1.
(a) Show that Nf has an attracting fixed point at tq, and compute the 
multiplier of Nf at xq.
(b) If you try to solve xk = 0 by Newton’s method, starting at 
how many iterations will be required to find the root to within 10-6?
Exercises 5.4 Numerical Methods as Iterative 
Systems
5.4#1. Considering Euler’s method as an iterative system for an au­
tonomous differential equation x' — we derived equation (17) Fh(x) = 
x + hf(x).
(a) Derive our subsequent statement that the corresponding iterative sys­
tem for midpoint Euler is
Fh(x) = x + hf\x+^-f{x) 
\ 
и
(b) More of a challenge is to do the same for Runge-Kutta:
+ f (x + hf (ж + 
(x + 
) ) •
5.4#2°. Consider xf = — ax(a > 0), the equation of Examples 5.4.1 and
5.4.2.
(a) Analyze the iteration of the function resulting from midpoint Euler,
to show that all solutions tend monotonically to 0 as t ► oo if 0 < 
h < 2/a, to oo if h > 2/a.

Exercises
287
(b) Analyze the iteration of the function resulting from Runge-Kutta,
/ 
Jl2a2
Fh(x) = (1 - ah+ ——
h3a3 
Н*а4 \
6 
24 J
Find the range of values of h for which the orbits tend to 0 as t —> oo, 
and show that numerical instability sets in for h> k, where к > 2/а.
5.4#3. Consider x1 = x — ж2, the equation of Example 5.4.3.
(a) Show that Fh(x) = x — h(x — ж2) can be made equivalent to any 
quadratic polynomial by a proper choice of h and a change of variables 
и = px + q.
(b) Show that numerical solutions will never blow up by considering 
xf = x — x2 — (1 — x)x = ax if a = 1 — x,
thus treating the differential equation as in Example 5.4.1.
5.4#4. Consider x' = —tx, the equation of Example 5.4.4.
(a) Show that the condition for numerical stability of solutions is
2
/ь? Л A’
■ 2-'1
and that it occurs for tn < approximately 2/h.
(b) Explain the behavior of solutions to this differential equation by con­
sidering it as a case of x' = -ax, of Example 5.4.1.
5.4#5. Consider the equation v' = — 2y/tv, which occurs in the discussion 
of Example 5.4.5.
(a) As in the preceding exercises, analyze the behavior of its solutions.
(b) Apply implicit Euler to this equation, with tn = nh, and show that 
vn+i goes monotonically to zero, regardless of the stepsize h. Explain 
why this means that computer drawings of solutions to x1 = x2 — t 
would show no jaggies.
5.4#6°. For x' = x2 —t2, with stepsize h = 0.03, calculate at what value 
of t the Runge-Kutta solutions will break down into jagged junk. Refer 
back to the opening pictures of Section 5.4 and compare the results of your 
computer experiments on the second.
5.4#7. For each of the following differential equations, use stepsize h = 
0.03 and calculate where the solutions will lose their numerical stability. 
Make computer drawings to show these results.
(a) x' = y/1 — tx (b) x' = x — t2 (c) x' = x2 +12

288
5. Iteration
Exercises 5.5 Periodic Differential Equations
5.5#1.
(a) Prove direct from Definition 5.5.2 of the one period later mapping 
Pt0 that Pt0+T = Pt0, a useful exercise in clarifying the roles of the 
to’s and #o’s.
(b) Prove that for any s,t the period later mapping Pt is conjugate to 
Ps. Hint: Consider the map Q8,t(x) that associates to x the value of 
the solution to x' = f(t,x) through (s,x) at time t.
5.5#2. To provide some understanding of Figure 5.5.3 for the differential 
equation x' = sintcos(# +t),
(a) Using the computer program DiffEq, print out a large direction field 
(default window is fine) with a number of solutions.
(b) Mark this printout with vertical lines at t = — 2тг,0,2тг. For these 
three values of to, start solutions at the same #o’s and see that solu­
tions indeed follow the same shape for either of the three to’s. That 
is, for every initial condition (0, #o), try (—2тг,#0) and (2тг,ж0).
(c) Make an appropriate Pto drawing by hand, showing which groups of 
solutions in the direction field go to the nearly horizontal sections of 
Pt0, and which go to the nearly vertical sections, as in Figure 5.5.3.
(d) Show how the attracting fixed points of Pto correspond to attracting 
periodic solutions of the differential equation; look for and mark other 
corresponding features.
5.5#3. Find the one period later mapping Pto for the following periodic 
differential equations, with to = 0 and T = 2тг: (Remember that to keep the 
computing time finite, you must adjust your xo range to bracket just the 
values for which the solution makes it all the way across from to to to + T.) 
For equations (a) and (b) the direction fields are pictured in Figure 5.5.1. 
Make printouts of Pto and of the direction fields, and analyze, as in Exercise 
5.5#1.
(a) x1 — (2 + cost)# — 
- 1
(b) x' = (2 + cost)x - ^-x2 - 2
(c) x' — (ж2 + l)cost
(d) x' = (x2 — 1)cost.

Exercises
289
5.5#4. Give a discussion comparing the preceding Exercises 5.1#3a and 
#3b, using the following guides. You can then look ahead to Example 5.5.6 
to confirm and extend your comparison for x' = (2 + cos^)# — |x2 — a in 
general. Proceed as follows.
(a) Show that if x = u(t) is a solution, then x = u(t + 2тг) is also a 
solution.
(b) With the computer program DiffEqSys, make a computer drawing 
for a = —1 showing the following behavior: There are exactly two 
periodic solutions
Ui(t + 2тг) = Ui(t) i = 1,2.
Identify a funnel around one of the periodic solutions and an antifun­
nel around the other.
(c) There is no periodic solution for a = —2. By experimenting with the 
computer find the value to two significant digits of the constant “a” 
between —1 and —2, which separates the two behaviors: where there 
exists a periodic solution and where there exists no periodic solution.
(This is a repeat of Exercise 2.4-2.5#2, now that your perspective has a 
periodic point of view.)
5.5#5. Solve explicitly and give a discussion comparing Exercises 5.5#3c 
and #3d. These examples illustrate the theorem of the next exercise.
5.5#6°. For separable differential equations, sometimes the one period 
later mapping Pto will be the identity map, a segment of the diagonal, as 
in Exercise 5.5#3c, and sometimes not, as in Exercise 5.5#3d. The key is 
an additional “if and only if” hypothesis giving the following theorem:
Consider a separable and periodic differential equation x' = 
f(t)g(x) with f(t) = f(t + T) and g(x) differentiable. Suppose 
g(a) = g(b) = 0 with a < b. Then the one period later mapping 
PtQ is the identity on [a, b] if and only if
[ f(i)dt = o.
Jo
Prove this result. Hint: Show that if a and b are consecutive zeroes of g, 
and c G (a, 6), then
Гх du
Jc 9(u)
is a monotone function on (a, 6), tending to positive or negative oo as a; —> a 
and x b.

290
5. Iteration
5.5#7. Consider the differential equation
x' — (x2 + l)cost,
which is periodic with period 2тг.
(a) Solve the differential equation by separation of variables to show that 
the solution yx(t) with initial condition 7^(0) = x is
7z(t) = tan(sint + tan”1 (a;)).
(b) Show that for this differential equation, the one period later mapping 
is
Po(x) = 7x(2tt) = tan[sin(27r) + tan-1 ж] = x, 
which is the identity where Pq is defined.
(c) Show that 7x(t) only represents a solution of the differential equation 
over the entire interval 0 < t < 2тг if | sint + tan-1 < тг/2, which 
corresponds to | tan”1 ж| < (тг/2) — 1, so
P0(x) = x if И < tanf ~ — 1 j « .64209,
and otherwise Pq is undefined.
(d) Using the computer program DiffEq, draw some solutions to the dif­
ferential equation. Then using the program ID Periodic Equations, 
draw the graph of Po as discussed in part (c). Show how your two 
drawings relate to one another.
5.5#8°. For a linear differential equation x' = p(t)x + q(t) that is periodic 
in t with period T, prove that Pto is a linear function PtQ = ax + 5, for 
appropriate a and b.
5.5#9. Corroborate the statement of Example 5.5.3 for the differential 
equation
xf = (x2 — 1) (cost 4-1)
that if
x0> (e4’+ l)/(e4ir — 1),
then the formula
_ (x0 + 1) + (x0 - l)e2(*+™«)
" (x0 + 1) - (x0 - l)e2(‘+sint) ’ 
V >
does not correspond to a genuine solution of the differential equation, but 
has a pole at some t, 0 < t < 2тг.

Exercises
291
5.5#10. For Example 5.5.4, with the differential equation 
x1 — (sint + a)x — ^ecosfx2,
verify the three statements explaining how the behavior of P*o changes 
according to whether, for A = e“27ra, we have A > 1, A = 1, or A < 1, as 
shown in Figure 5.5.5.
5.5#11. For Example 5.5.4, show with graphs how the statements explain­
ing the behavior of Pto in terms of the values of the parameter A = e“27ra 
translate into statements about the solutions to the differential equation 
x' = (sint + a)x — (3ecostx2.
5.5#12. Prove that any continuous function R —> R which is onto and 
monotone increasing is the period-function g = Рп for some differential 
equation periodic of period 7Г.
Hint: the idea is to draw curves in R2 joining every point (0,x) to the 
point (тг,^(ж)), which are all disjoint, and fill up [0,7r] x R. In order to 
guarantee that these curves, continued periodically to [тг, 2тг] x R, and so 
on, form differentiable curves in R2, we will require that they be horizontal 
at the endpoints. One way to do this is to consider, for each ж, the graph 
of the function
M _ 9(.x) + x g(x) - x 
— 2 
2 COSW-
(a) Show that the graph of yx does join (0, x) to (тг,^(ж)).
(b) Show that every point (t, x) with 0 < t < 7Г is on the graph of yx for 
some unique x € R.
Consider the function f(t, #), defined on [0, тг] x R, which associates to 
(t, x) the slope of the curve through that point at that point.
(c) If f is extended to R2 so as to be periodic of period 7Г in t, show that 
it is a continuous function on R2.
(d) Show that the period map P^ for the differential equation x* = f(t, x) 
is exactly g.
5.5#13. The proof of Proposition 5.5.5 shows that the sequence Pf°n is 
monotone and bounded, so must tend to some limit. Show that the limit 
must be in the interval [x^ and must be a fixed point of Pto.
5.5#14. Predict and verify the implications of Figure 5.5.7 for the solutions 
to the differential equation x' = cos(x2 + sin27rt) — a from Example 5.5.7.
5.5#15. Show that x' = sin(l/x) + jsint has infinitely many periodic 
solutions.
5.5#16. Show that x' = sin(x2— t) has a unique periodic solution, unstable 
in the forward direction, stable in the backward direction.
5.5#17. Consider the differential equation xf = cosx -h cost.

292
5. Iteration
(a) Use DiffEq to make a computer picture of slopes and solutions.
(b) Show that the region t > 0, 0 < ж < ?r is a weak funnel, and that the 
region t > 0, 7Г < x < 2tt is a weak antifunnel.
(c) (harder) Show that the solution in the antifunnel is unique. Hint: You 
will need to show that such a solution satisfies гг 4- E < x(t) < 2тг — e 
for some e > 0, and use the theorem proved in Exercise 4.7#3 for the 
case of an antifunnel that does not narrow.
(d) Show that the solution in the antifunnel is periodic of period 2тг.
(e) Show that there is a unique periodic solution in the funnel.
Exercises 5.6 Iterating in c1
5.6# 1. Show that any quadratic polynomial can be conjugated to z2 4- c 
by an affine map az 4- b. (This is shown for real z in Exercise 5.1#19.)
5.6#2. Try iterating z2 — |, z2 — |, z2 + 1 and z2 — 3 for various complex 
starting points zq. Unless you have a program that will do complex arith­
metic, you must do these by hand. That means you can multiply or add 
digits by calculator, but you will have to combine the proper terms by the 
rules of complex arithmetic before proceeding to the next step. What can 
you tell or suspect about the boundedness of the orbits in each case? Your 
results should fit the information to be obtained from the Mandelbrot set.
5.6#3. In the manner of Example 5.1.9, let us look at the periodic orbits 
of period three for x2 4- c. The order 3 analog of equation (2) is
((O2 4- c)2 4- с)2 + c = 0,
which has two real solutions for c and two additional but complex solutions, 
c = -0.1226 ± 0.7499г. This means that quadratic polynomials of form 
x2 4- c for any of these four values of c, when iterated from xq = 0, produce 
a cycle of order three; one of these is actually a fixed point. We cannot 
directly graph this iteration as we did in Figure 5.1.10, since it involves 
complex numbers. But you can and should confirm by direct algebraic 
calculation that for с = -.1226±0.7499г iterating from z0 = 0 under z2 + c 
creates a cycle order three. Unless you have a program that will do complex 
arithmetic, you must do these by hand. That means you can multiply or 
add digits by calculator, but you will have to combine the proper terms by 
the rules of complex arithmetic before proceeding to the next step.
5.6#4°. Show that for z2 4- c the following is a practical test for knowing 
whether an orbit is unbounded: whenever an iterate exceeds |c| 4- 1 in 
absolute value, the successive iterates will go off toward infinity.

Exercises
293
5.6#5. Consider the polynomial г2 + 2. As stated in Example 5.6.8, show 
that all points z with |z| > 2 escape to oo.
5.6#6. You can learn a lot about the dynamics of complex iteration by ex­
amining the iteration of z2 — 1. Although the resulting picture is symmetric 
(because z gives the same result as —z), you will see that the dynamics are 
certainly not symmetric.
(a) Calculate the iteration for various complex starting points zq. For 
example, try zq = 0, 1, —1,1.5, г, —г, 1+г, 2г. (See the notes regarding 
calculation in Exercise 5.6#2 above.)
(b) You also have the option of performing these operations geometri­
cally: squaring means to square the absolute value and double the 
polar angle; adding or subtracting a real number amounts to a hori­
zontal translation.
(c) Calculate the fixed points.
(d) Show that your results fit with Figure 5.6.2, the Julia set for z2 - 1.
5.6#7. Confirm that there exist 2n 1 values of c for which z2 + c iterates 
from zq = 0 to a cycle of order n.
5.6#8. For f(z) = z2.
(a) Find the periodic points of periods 2 and 3.
(b) Show that the periodic points of period dividing p are e2fc7™/2P-1.
5.6#9.
(a) Prove that every complex quadratic polynomial has (2P — 2) complex 
periodic points of period exactly p, counting multiplicity, for every 
prime number p.
(b) How many periodic points of period 4 does a complex quadratic poly­
nomial have?
(c) How many periodic points of period 6 does a complex quadratic poly­
nomial have?
(d) In general, how many periodic points of period к does a complex 
quadratic polynomial have?
5.6#10. Consider the question of Example 5.6.1. For how many real values 
of c is 0 periodic of period p, for some prime number p? The answer is 
(2P-1 — l)/p for all primes p^2. That fact is too difficult to prove here, 
but you should verify it for p = 3, 5, 7, 11. That is, use the program 
Analyzer to graph the orbit of 0 under x2 + c and count the roots, using 
blowup when you need a clearer picture. (Two notes: (i) The function you

294
5. Iteration
wish to graph is a function of c; set Xq = 0 and then change c to x for 
entering in Analyzer, (ii) To avoid problems with the blowup, keep the 
lefthand endpoint less than —2, and the vertical heights on the order of 
±1.)
5.6#11°. Prove that K-2, the filled-in Julia set for z2 — 2, is simply the 
interval [—2,2]. For purely real z = a or for purely imaginary z = bi, it is 
straightforward to find when the orbits are unbounded. But for z = a + bi, 
you will find that a different approach is required. We suggest the following:
(a) Show that for every point z in the complex plane outside the interval 
[—2,2], there is a unique point £ with |£| < 1 such that £ + l/£ = z. 
Hint: Observe that £ + | = z is a quadratic equation for £ in terms 
of z, and that the two solutions are inverses of each other.
(b) Show that the map </?(£) = £ + l/£ conjugates P_2 to Pq, as shown 
in this diagram:
£-£ + 1/£
D-{0}
> * - [-2,2]
D- {0}
£-£+1/£
> C - [-2,2]
(c) Derive the result from the fact that if |£| < 1 then £2™ —> 0 as n —> oo.
5.6#12. The roots of the polynomial equation P°n(0) = 0 are those values 
of c for which 0 is m-periodic, where m divides n. In Exercise 5.6#7 you 
have confirmed that there are always k± = 2n~1 values of c that are roots 
of P°n(fi) = 0; this includes fixed points and cycles of lower order.
(a) For each of the first four possible values of n, find the c-values that are 
n-periodic and those that are exactly n-periodic. For each n locate 
these c-values on the Mandelbrot set and show that they confirm the 
following statement.
For any n, each value of c that is a root of P°n(fi) = 0 
lies within a unique “ball” (called a component) of the 
Mandelbrot set.

Exercises
295
(b) Verify by experiment with another point in the “2-ball,” such as 
(—1.1,0), and another point in the topmost “3-ball” that the orbit of 
0 confirms the following statement:
All other points of each component of the Mandelbrot set, 
have the property that 0 is not n-periodic, but is attracted 
to a cycle of period a.
If you are lucky enough to have access to a graphics program that 
follows orbits, fine; otherwise you can do this part and the next just 
with algebraic calculation.
(c) Pick a point in another ball or baby Mandelbrot set and determine 
what the periodicity of the orbit will be. This should enable you to 
appreciate the fact that there exists a whole combinatorial litany of 
what you will find where on the Mandelbrot set; that is, what kinds 
of cycles the orbit of 0 will be attracted to, for the different values 
of c at the different points within M. Now it is time to go looking at 
the references, listed at the end of this volume.
5.6#13. For the original Cantor set:
(a) Show that it satisfies the defining requirements: that it is closed and 
bounded, totally disconnected, and without isolated points.
(b) Show that the endpoints of the deleted intervals can be written two 
different ways in base 3 decimals, so that they can have the proper 
decimal representation to remain in the Cantor set.

Appendix. Asymptotic 
Development
Asymptotic development or asymptotic expansion is the technical way of 
saying that one function looks like another function, and since in this book 
we will be constantly concerned with making such statements, we save an 
appendix for precisely describing the rules of the game.
Al. Equivalence and Order
The idea we wish to pursue in general is
how to express any function in terms of powers ofx, or powers 
of (x — Xq).
The key property is the notion of equivalence of two functions near a 
point xq. Actually, we will frequently be concerned with functions defined 
only for x > 0 and the behavior near 0 or oo, so we will only require that 
functions be defined on a one-sided neighborhood of a point xG.
Most often, it will be behavior near infinity that matters. But the change 
of variables 
will transform a right ж-neighborhood of xq into a ^/-neighborhood of +oo; 
i.e., if we define
g(y) = f ( - + x0
\y
then the behavior of g near +oo will be the same as the behavior of f on 
a right-neighborhood of xq.
For the purposes of defining terms, we shall use ггдЫ-neighborhoods.
Definition Al.l. For two functions f and g defined to the right of жо, 
they are equivalent at жо if
lim ® = 1.
x|x0 g\x)
This equivalence is noted f g, or / 
g if ж —» oo.

298
Appendix. Asymptotic Development
This is not the only possible definition of equivalence, and it does not nec­
essarily correspond to functions behaving “like each other.” For instance, 
the functions
sin(a;) and sin(a; + е-ж)
behave very much like each other near +oo (it would be hard to distinguish 
them numerically for x > 20) but they are not equivalent in the sense 
above. This is because the values of x for which they vanish do not coincide 
(although they are very close), so the ratio ricochets near these values, being 
zero at some x and undefined at others between the zeroes.
Remark. Definition Al.l is only reasonable for functions which have con­
stant sign on the right side of xq‘, a different definition of asymptotic de­
velopment would be required for functions which oscillate.
Associated with equivalence, there is a natural notion of order on func­
tions defined on a right neighborhood of x$.
Definition Al.2. Near but to the right of xq, a function f is of higher 
order than a function g if
Ita ® = 0.
If this is true as я —> oo, a function f is said to be of lower order than a 
function g.
There is in general no reason to think, given two functions, that either 
is of higher order than the other; most often functions are not comparable.
We are now ready to proceed with the task of defining an asymptotic 
expansion.
A2. Technicalities of Defining Asymptotic 
Expansion
Setting Up a Scale
As we noted early in the last section, there is no loss of generality in de­
scribing only functions in a neighborhood of +oo, and we will do that in 
the rest of this appendix, except where noted otherwise.
The next task is to describe our scale functions, a set S of functions 
(defined near +oo) which can be considered to be known and which can be 
ranked in a linear order. The particular selection we will make is somewhat 
arbitrary, and the reader may at will discard some (fewer functions will 

Appendix. Asymptotic Development
299
then be describable) or add some (at the cost of being familiar with some 
rather elaborate functions).
Definition A2.1. Our set S of scale functions will be the functions 
a?a(lna?)^ep^
where
P(x) = ci a?71 + ... + cna?7n
with 7i > ... > 7n > 0, and with a, /3, and the Ci arbitrary real numbers.
These functions are the ones which we will consider known near infinity, 
and in case you don’t know them, the following may help:
Theorem A2.2. The family S of scale functions has the following proper­
ties:
(a) Every function in S is positive near infinity.
(b) 
The family S is closed under products, and under raising to any real 
power; in particular it is closed under quotients.
(c) Every element of S tends to 0 or oo as x —* oo except the constant 
function 1.
Proof. Statements (a) and (b) are clear upon reflection. Statement (c) 
is an elaborate way of saying that exponentials dominate powers, which 
themselves dominate logarithms.
Indeed, first suppose that P(x) = 0 and a = 0. Then the scale function 
is
(Ina?)^,
which clearly tends to 0, 1, or oo, if /3 < 0, /3 = 0 or (3 > 0.
Now suppose that P(x) = 0 and a / 0. Then the scale function is 
a?a(lna?)/3,
and it is easy to see that this rends to infinity if a > 0, and to 0 if a < 0. 
Finally if P(x) ф 0, say
P(a?) = ci a?71 + ... + cna?7n 
= a?71 (ci + C2a?72-71 + . • • + cna?7n-71),
we need to show that the term eC1a?71 dominates, so that the function tends 
to 0 or infinity when ci < 0 and when Ci > 0 respectively. This is left to 
the reader. □

300
Appendix. Asymptotic Development
Actually, these properties are the only properties of a scale we will re­
quire, and the reader may add or delete functions at will so long that they 
are preserved. Please note that they imply that any two functions in the 
scale are comparable: in fact, either two functions are equal, or one is of 
lower order than the other. In this way the scale guarantees a linear order­
ing of the known functions.
Principal Parts
This section includes two rather nasty examples of theoretical interest; the 
more practical examples will be given in Sections A3 and A4.
Definition A2.3. A function /(x) defined near -hoc has principal part 
cg(x) if g E S', c is some constant, and cg(x) is equivalent to f at -hoc. 
Then we can write f « g as
/(ж) = cg(x) 4- lower order terms. 
(1)
Other ways of expressing that f is asymptotic or equivalent to cg(x) are
/(a:) - cg(x) « g(x), 
(la)
and
f(x) = cg(x) + o(g(x)). 
(lb)
The “little o” notation o(#(x)) is in common usage, both to describe the 
set of all functions of lower order than g(x) and, as in this caSe, to refer to 
some particular member of that set.
Another notation, “big O”, which must be carefully distinguished from 
the “little o”, is in common usage, especially by computer scientists. Upper 
case O(<?(x)) means of order at most the same as g(x)—as opposed to lower 
case о(<?(ж)) meaning of order strictly less than that of g(x). In this text 
we shall stick with о(р(ж)).
A principal part already says a great deal about the function; and fre­
quently knowing just the principal part is a major theorem, as the following 
somewhat remarkable example illustrates:
Example A2.4. Consider the function
7г(ж) = number of prime numbers < x.
It is not at all clear that this function should even have a principal part, 
but Legendre and Gauss, on the basis of numerical evidence conjectured 
that it would be x/ In x. Indeed, the fact that
7г(ж) « Х/ In Ж,
was proved, about 1896, by the French mathematician Hadamard and the 
Belgian mathematician de la Vallee-Poussin. The result, called the Prime 
Number Theorem, is still considered a hard result, too hard for instance 
for a first graduate course in number theory. ▲

Appendix. Asymptotic Development 
301
Asymptotic Expansion
We are finally ready to finish the theoretical presentation. In many cases 
we will want more information about a function than just its principal part, 
and there is an obvious way to proceed, namely to look for a principal part 
for the difference, /(ж) — c<j(x), which is of lower order than g(x). That is, 
we now write
f (a;) - cg(x) = c2g2(x) + o(g2(x)).
This procedure can sometimes be repeated.
Definition A2.5. For a function f(x) an asymptotic expansion, or asymp­
totic development, occurs when we can write
/(ж) = С151(ж) + c2g2(x) + ... + cngn(x) + o(gn(x))
with the Qi(x) E S.
Definition A2.5 means that for each i = 1,..., n we have
/(®) - ^aj9j(x) e o(gi(x)).
j=i
Thus each term is a principal part of what is left after subtracting from f all 
the previous terms. Such an expansion is called “an asymptotic expansion 
to n terms.”
A3. First Examples; Taylor’s Theorem
We are now ready to state more precisely and prove Taylor’s Theorem:
Theorem A3.1. If f(x) is n times continuously differentiable in a neigh­
borhood of xq, then
f(x) = /(x0) + J'(xq)(x - x0) + ... + ^/(n)(xo)(a: - z0)n + о((ж - x0)n).
Proof. This theorem is proved by repeated applications of 1’Hopital’s rule:
1im f(x) - [/(a?o) + f'(xQ)(x - xp) + ■ ■ ■ + ^f(ra)(a?o)(a: - x0)w]
x^>x0 
(ж — Xo)n
/'(®) - [/'(zo) + f"(xo)(x -X0') + ... + 7^iyf(n)(x0)(x - Xo)n-1] 
= lim ----------------------------------------------- -——i---->-----------------------------
x^xq 
n{X — XQ)n~L
X—>Xq 
Til

302
Appendix. Asymptotic Development
At xq = 0, a few standard Taylor expansions are
ex = 1 + x + 
+ ... + —xn + о(жп)
2! 
n\
sina: = x - p + ... + 
+ o^1)
cos x = 1 - ^x2 + ... + ^^Tx2n + o(x2n+1)
ln(l + x) = x - ^-x2 + ... + 
-----xn + o{xn)
Z 
П
/i 
i 
a(a —1) о 
a(a — 1)... (a — n + 1) „
(1 + x)a = 1 + ax + 
—-ж2 + ... +
z! 
n!
+ o(zn).
Note carefully that these Taylor series are asymptotic developments in a 
neighborhood off). It is true that ex looks quite a bit like 1 and even more 
like 1 + x, and so on, near 0. But it does not look the least bit like 1 near 
infinity (or any other point except 0). In fact, the asymptotic development 
of ex near infinity is itself; it is an element of the scale and hence already 
fully developed.
Similarly, the function e1^ is an element of the scale at 0 and fully 
developed there, although at infinity we have
e1^ = 1 + 1/x + (l/2)l/a:2 + ... + (l/n!)l/xn + o(l/zn).
It is quite important to keep the notions of Taylor series and asymptotic 
development separate, and we will show this with the following example, 
actually studied by Euler:
Example A3.2. Consider the function
This function is well defined for 0 < x < oo. It has an asymptotic 
development at 0 which we shall obtain as follows:
First, observe that near 0, we have a geometric series (attainable by long 
division):
—- = 1 - xt + (xt)2 - ... + (-l)"(zt)” + 
(2)
1 + xt 
1 + xt
This is simply the formula for summing a finite geometric sequence.
Second, we will need to know that
[°° tne~tdt = n!. 
(3)
Jo

Appendix. Asymptotic Development
303
This is easy to show by induction on n and is left to the reader.
Third, multiply each term of (2) by e-f, then, remembering that x is 
independent of t and hence can be brought outside the integral, we use (3) 
to integrate each term with respect to t from 0 to oo, yielding
/ --------= 1 - x + 2! x2 - 3! x3 + ... + (-l)nn! xn
Jo 1 + xt
/•OO -in+1 p-t
= 1 - x + 2! ж2 - 3! ж3 + ... + (—1)пп!жп + o(xn).
Let us now examine the remainder, to show that it is in fact o(xn}. Since 
1 + xt > 1, we see that the remainder is bounded by
(n + l)!zn+1 €o(sn).
However, this power series
^(-l)"n!x"
does not converge for any x 0! 
▲
You should think of what Example A3.2 means. Here we have a series, 
and the more terms you take, the “better” you approximate a perfectly 
good function. On the other hand, the series diverges, the terms get larger 
and larger, and the more terms you take, the more the sum oscillates wildly. 
How can these two forms of behavior be reconciled? The key point is the 
word “better.” It is true that the nth partial sum is “better” than the 
n — 1th, in the sense that the error is less than xn near zero. But nobody 
says how close to zero you have to be for this to occur, and because of 
the (n + 1)! in front, it only happens for x's very close to zero, closer and 
closer as n becomes large. So we are really only getting better and better 
approximations to the function on smaller and smaller neighborhoods of 
zero, until at the end we get a perfect approximation, but only at a single 
point.
Also, Example A3.2 should show that even if a function has an asymp­
totic expansion with infinitely many terms, there is no reason to think that 
the series these terms form converges, or, if it converges, that it converges 
to the function. Somehow, these convergence questions are irrelevant to 
asymptotic expansions. In particular, the authors strongly object to the 
expression “asymptotic series,” which is common in the literature. It is 
almost always wrong to think of an asymptotic expansion as a series; you 
should think rather of the partial sums.

304
Appendix. Asymptotic Development
A4. Operations on Asymptotic Expansions
For nice functions, the previous section gave a way of computing asymp­
totic expansions. However, even if a function is perfectly differentiable, the 
computation of the successive derivatives that are needed in Taylor’s for­
mula is usually quite unpleasant, it is better to think of the function as 
made up from simpler functions by additions, multiplications, powers and 
compositions, and then to compute the first several terms of the asymptotic 
expansion the same way. We begin with several examples, before developing 
the theory which makes it work.
Example A4.1. Consider the function
f(x) — sin(:r + tan ж),
we will find an expansion near zero (where the function, on the right side 
of zero, is not oscillatory) with precision x4. First
sin x x — 
+ o(rr4)
cosrr 
+
X^ 
A
= x + — + o(x4').
+ 24х4) +°(a:4q
4\ 2
хй) +0(*4)
Now we can write 
/ хз 
sin(rr + tana;) = sin I 2x + — + o(x4)
= 2a; + — + o(a:4) - - (2x + — + o(a:4) ) + o(a:4) 
о 
О \ 
О 
J
— 2x — x3 + oQr4). a
Example A4.2. Consider the function f(x) = У(з: + 1) — y/x near oo. 
We find -h 1) = y/x у/(1 + l/ж), and since 1/x is small near oo, we can 
develop
A 1-1 11 _ _L1JL
у 
x 
2! x 2! 4 x2 ° \x2 )
leading to
/Г+1 - ^ = Ja:-1/2 - |aT3/2 + o(a:-3/2). 
2 
8

Appendix. Asymptotic Development
305
Example A4.3. Consider the behavior near x = 0 of the function
/(x) = x In |x| 
1 4-
We find
x In |x| 
x In |x|
1 + e* = 2(l + f+ ^+o(x2))
1 , | | f ( X X2 z 
. Qv 1
= 2 х ln И | (j ~ 2 “ ~4 + °(x ) J + °^x ) J
x In Ы x2 In Ы rr3ln|x| . 
. 14
= -4-1---4^---ln M)-
Z 
Ч: 
О
Example A4.4. Now for something nastier. How does
f(x) = (1 + x)1'*
behave near oo? Write
/(z) = (14- x)1/x = е(Ь(1+^))Л
Then we need to write
/ 
i \ 
i i 
/ i \
ln(l 4-z) = Inrr 4-In( 14--) = lna;4----------- x- 4-o| — )
\ 
xj 
x 
2x2 
\x2 J
so that
ln(l 4- x) _ Inrr 1 
1 
/ 1 \
x x x2 2rr3+
Since this function goes to 0 as x —> oo, we can apply the power series of 
the exponential at 0 to get
л/ x , Inx 1 
1 
/1
/ (ж) — 1 4- 
4- 
~ 2^3 + 0 (
JU 
JU 
^jJU 
\ JU
1 In# 
1 
1 
/ 1 V 2 
/ (lnx)3\
2 x 
x2 2x3 + \#3/. 
\ J
1 _|_ 
_|_ 0nx)2
In this case the result was not really obvious; it wasn’t even clear that 
f(x) tended to 1. 
▲

306
Appendix. Asymptotic Development
A5. Rules of Asymptotic Development
It should be clear from the examples of the last section that there are two 
aspects to asymptotically developing functions: one is combining the parts 
found so far, and the other is recognizing what can be neglected at each 
step. Here are the rules.
1. Addition. You can add two asymptotic developments terms by term, 
neglecting the terms of higher order than the lower of the two preci­
sions, and getting a development of that precision.
2. Multiplication. If
fi(^) = cigi(ar) + ... + cngn(x) + о(дп(ж))
f2(z) = b-ih^x) + ... + bmhm(x) + o(/iTO(x)),
then you get an asymptotic expansion of Д, /2 by multiplying to­
gether the expressions above, collecting terms of the same order, and 
neglecting all terms of higher order than
Р1(я:)^т(ж) or дп(ж)Л1(а:).
The development obtained has precision equal to the lower of these 
two precisions.
3. Powers (and in particular inverses). This is a bit more subtle. The 
idea is to factor out the principal part of the function to be raised to 
a power, i.e. to write
f(®) =С1Р1(х)
(4)
<P
so that we can apply the formula for (1 4- ip)a to the second factor. 
We get
(f (®))“ = Cl 9? (1 + «(<?) + 
1\(^)2 + •••)•
\ /
If the Taylor expansion of (1 4- ip)a is carried out to m terms, the 
precision of the development (4) is
Here we stop the list of rules. A good exercise for the reader is the 
problem, very analogous to the Taylor series, of developing
In/ and e$,
or rather of formulating conditions under which it can be done (this 
is not always possible without enlarging the scale).

References
For Differential Equations in General
Artigue, Michele and Gautheron, Veronique, Systemes Differentials: Etude 
Graphique (CEDIC, Paris, 1983).
Dieudonne, Jean, Calcul Infinitesimal (Hermann, Paris, 1968). This great 
book has deeply influenced the mathematical substance of our own vol­
ume; it is a valuable resource for anyone wanting a more mathematical 
treatment of differential equations. On page 362 is the Fundamental 
Inequality which we have expanded in Chapter 4.
Hirsch, Morris W. and Smale, Stephen, Differential Equations, Dynamical 
Systems, and Linear Algebra (Academic Press, 1974). This is the first 
book bringing modern developments in differential equations to a broad 
audience. Smale, a leading mathematician of the century and the only 
Fields medal winner who has worked in dynamical systems, also has 
profoundly influenced the authors.
Simmons, George F., Differential Equations, with Applications and Histor­
ical Notes (McGraw Hill, 1972). We have found this text particularly 
valuable for its historical notes.
Sanchez, David A., Allen, Richard C. Jr., and Kyner, Walter T., Differential 
Equations, An Introduction (Addison-Wesley, 1983). In many ways this 
text is close to our own treatment.
For Numerical Methods (Chapter 3)
Forsythe, George E. et al., Computer Methods for Mathematical Compu­
tations (Prentice-Hall, Inc., NJ, 1977). An interesting implementation 
of Runge-Kutta which has most of the benefits of the linear multistep 
methods can be found in Chapter 6.
Gear, C. William, Numerical Initial Value Problems in Ordinary Differ­
ential Equations (Prentice-Hall, Inc., NJ, 1971). A good summary of 
“what to use when” is Chapter 12.
Henrici, P., Discrete Variable Methods in Ordinary Differential Equations 
(John Wiley & Sons, 1962).

308
References
Moore, Ramon, Methods and Applications to Interval Analysis (SIAM Se­
ries in Applied Mathematics, 1979); helpful for error approximation. In 
particular, for appropriate cases this approach can even make Taylor 
series the method of choice.
Noye, J., Computational Techniques for Differential Equations (North- 
Holland, 1984), pp. 1-95; an excellent survey. An extensive bibliography 
of further references is provided therein.
For Iteration in the Complex Plane (Chapter 5.6)
Blanchard, Paul, “Complex analytic dynamics on the Riemann sphere,” 
Bull. Amer. Math. Soc. 11 (1984), pp. 85-141.
Devaney, Robert, An Introduction to Chaotic Dynamical Systems (Ben­
jamin Cummings, 1986); a text detailing the mathematics.
Dewdney, A.K., “A computer microscope zooms in for a look at the most 
complex object in mathematics” (Scientific American, August 1985); 
color illustrations.
Peitgen, H-0. & Richter, P.H., The Beauty of Fractals (Springer-Ver lag, 
1986); especially the chapter by Adrien Douady, “Julia Sets and the 
Mandelbrot Set”; color illustrations.
Peitgen, H-O. & Saupe, D., The Science of Fractals (Springer-Verlag, 1988); 
concentrating on computer algorithms for generating the beautiful pic­
tures; color illustrations.
For Computer Experimentation with Mandelbrot 
and Julia Sets (Chapter 5.6)
Munafo, Robert, SuperMandelZoom for the Macintosh; black and white, 
for all Macs; public domain; very useful documentation. Send disk to 8 
Manning Dr., Barrington, RI 02806.
Parmet, Marc, The Game of Fractals (Springer-Verlag, 1988); written for 
the MacII with optional color, but does not transmit to postscript for 
laser printing.
Write to Artmatrix, P.O. Box 880, Ithaca, NY 14851-0880 for listings of 
other software for the Mandelbrot set, including color programs, for Mac­
intosh, IBM, and other personal computers; Artmatrix also markets edu­
cational slide sets and zoom videos.
For Asymptotic Development (Appendix)
Erdelyi, Arthur, Asymptotic Expansions (Dover, 1956).
See also Dieudonne, op.cit.

Answers to Selected Problems
Since many of the problems in this book are unlike those in other differential 
equations texts, solutions to a selected subset of the problems are given here. 
It is hoped that this will provide the reader with useful insights, which will 
make all of the problems stimulating and tractable.
Solutions for Exercises 1.1
l.l#2.b. The isoclines for x' = x2 — 1 are of the form x = 
+ 1, with
slope m. A slope field is shown below, with dashed lines for isoclines. Note 
that — 1 < m < oo.
Solutions above x = 1 and below x = — 1 appear to become vertical. The 
functions x = (1—<7e2f)/(H-Ce2t) are solutions, with C = (1—жо)/(1+#о)- 
Therefore, for C > 0, the solutions lie between x = — 1 and x = 1; for 
—1 < C < 0, they are above x = 1; for C < —1, they lie below x = —1. The 
isocline x = —1 is a solution, but cannot be written as (1 —<7e2f)/(H-(7e2t) 
for any finite C.
l.l#4.iv. 1 - a, 2 - h, 3 - d, 4 - e, 5 - b, 6 - c, 7 - /, 8 - g.

310
Answers to Selected Problems
1.1#5. a — 4 (or 8), b — 6, c — 1, d — 2, e — 3, f — 5. Graph 7 could be 
x' = x + 1.
1.1#9. If t is replaced by — t in the equation xf = f(t,x), we get 
d(x)/d{—t) = —xf(—t) — /(—t,x(—£)), so symmetry about the т-axis will 
result from the transformed equation xf = — /(—t,x). Similarly, symmetry 
about the £-axis comes from replacing x by — x. This leads to the new equa­
tion x' — —f(t, —x). For symmetry about the origin, replace x by — x and 
t by —t, and the new equation will be x' = f(—t, —x). None of the other 
sign variants lead to symmetric graphs, in general.
1.1.#13. Implicit differentiation of x' = x2 - 1 gives x" = 2xxf = 2x(x2 — 
1), so that inflection points can only occur where x = 0,1, or — 1. Note in the 
graph for Exer. l.l#2.b the solutions do have inflection points at x = 0. 
No solutions pass through x = 1 or — 1 except the two constant solutions, 
which do satisfy x" = 0 at every value of t. It can also be inferred that 
all solutions above x = 1 are everywhere concave up, those below x = — 1 
are everywhere concave down, and those between x = — 1 and x = 1 are 
concave down where x > 0 and concave up where x < 0.
Solutions for Exercises 1.2-1.4
1.2—1.4#3.c. A funnel and antifunnel for x' = x2 — 1 are shown below.
1.2-1.4#4. The computer pictures are provided in the text, as Figure 
5.4.6. It shows three different sets of computer solutions for xf = x2 — t, 
with stepsize h = 0.4. The three pictures correspond to three different 
numerical methods for computing the solutions. You will learn all about 
these in Chapter 3. Your program DiffEq uses the third method.

Answers to Selected Problems
311
1.2—1.4#9. (a) In the equation x' = f(t, x) — x — x2, f does not depend 
explicitly on t. This means that the isoclines are horizontal lines x = con­
stant. Therefore, to check that |a?| < 1/2 is an antifunnel, we need only 
show that x' is (strictly) positive for x = 1/2 and (strictly) negative for 
x = —1/2. Similarly, 1/2 < x <3/2 is a funnel because x' < 0 on the 
isocline x = 3/2.
(b) A narrowing antifunnel containing x(t) = 0 can be obtained by using 
any monotonically decreasing curve 7(t), with lim7(t) = 1 as t => —oo 
and lim7(t) = 0 as t => oo, as the upper fence. The lower fence can be 
any monotonically increasing curve 6(t) with lim^(t) = 0 as t => oo. For 
example, let 6(t) = —е~г and 7(t) = (тг/2 — tan-1 t)/?r. See drawing a:
8(t) = - e-'
a) x1 = x — x2
b) x' = x — x2

312
Answers to Selected Problems
A narrowing funnel containing x(t) = 1 can be constructed by using 
/3(t) = 1 + a/t, a > 0 as an upper fence, and a(t) = 1 — a/t as a lower 
fence. To make \/3 — /32\ > |/3'|, we need |(1 + a/t) — (1 + a/t)2\ > | — a/t2\. 
If a > 1, this condition is satisfied for all t > 0. The curve a = 1 — a/t will 
be a lower fence if \a/t — a2/t2\ > \a/t2\. This holds whenever t > a + 1. 
Graph b shows a(t) = 1 — 2/t and (3(t) = 1 + 2/7 bounding a narrowing 
funnel about x = 1.
1.2— 1.4#15. For x' = —x/t, the isoclines are x = —mt. A slope field is 
shown in graph a.
(a) Use /3(t) = £-1/2 and a(t) = — t~xl2 as upper and lower fences, 
respectively. Any solution that starts between these two curves, for t > 0, 
will stay between them, and therefore approach 0 as t —> oo.
(b) Graph b shows that happens to solutions of the perturbed equation 
x' = —x/t + x2. This is a Bernoulli equation, and in Chapter 2 you will 
find that its solutions are x(t) = [Ct — Hn(tf)]-1. Therefore, any solution 
of this equation that starts near the ж-axis will move away from it for any 
C>0.
isoclines: x = mt *
/ x
а. X =

Answers to Selected Problems
313
Solutions for Exercises 1.5
1.5#l.h. The isoclines for xr = t(2 — x)/(t + 1) are of the form x = 
(2 — m) — m/t. Note that x = 2 is a solution. A narrowing funnel around the 
solution x = 2 can be constructed by using the upper fence /3(t) = 2+e-0,5* 
and lower fence a(t) = 2 — e-0’5t. Both of these satisfy the conditions for 
a strong fence, for all t > 1. A picture is shown below.
y(t) = 2 + e*2 x
I _ ^(2 x) 
t+1
1.5#10. (a) If x' = x2/(t2 +1) — 1, the isoclines of slope m satisfy x2/(t2 + 
1) = m + 1, hence are hyperbolas of the form x2 /{m + 1) — t2 — 1. The 

314
Answers to Selected Problems
sketch shows the isoclines of slope 0, -1. The slope is negative where |ж| < 
(t2 + l)1/2, and positive elsewhere.
(b) The region between the isoclines for m = 0 is a funnel for t > 
0; therefore, any solution satisfying |u(0)| < 1 stays inside that funnel 
for all t > 0. Every solution in this region is a monotonically decreasing 
function of t. These solutions cannot approach — oo faster than the lower 
fence — (t2 + l)1/2, hence are defined for all t > 0.
(c) For slope m = (1+д/5)/2, the isoclines satisfy a;2/[(3+v/5)/2]— t2 = 1. 
The asymptotes for these hyperbolas are x = ±[(3 + \/5)/2]1/2t. Check, by 
squaring both sides, that [(3 + v/5)/2]1/2 = (1 + л/5)/2. Therefore, the 
slope of these two asymptotes is ±(1 + \/5)/2. Since the hyperbola’s slope 
increases from 0 to (1 + \/5)/2 as t —> oo, it is always less than the slope xf 
along the hyperbola; therefore, the hyperbola is a lower fence. Along the 
line я = [(1 + \/5)/2]t, the value of x' is {[(l + \/5)/2]t2 - l}/(t2 + l), which 
is less than (1 + л/5)/2. Therefore the asymptote is an upper fence and the 
shaded region between the hyperbola and asymptote is an antifunnel. The 
dispersion df /дх = 2x/(t2 +1) > 0 for x > 0, and this implies that exactly 
one solution remains in the antifunnel as t —> oo.
(d) 
Solution curves are shown in both graphs below.

Answers to Selected Problems
315
Solutions for Exercises 1.6
1.6#3. (a) Use the isoclines /3(t) = (t2 — I)1/3 as an upper fence and 
a(t) = (t2 + I)1/3 as a lower fence. On x = /3(t\ x' — —1 and /?'(£) = 
2t/[3(£2 — I)2/3] > 0 for t > 1. On x = a(t), x1 = 1 and a'(£) = 2£/[3(£2 + 
I)2/3] < 1 for all t > 0. The shaded region in the graph below, between a 
and /3, is a narrowing antifunnel along x — t2^3. The dispersion df /дх = 
Зя2 > 0 implies that there exists a unique solution ж* which remains in the 
antifunnel.
xf = x3 - t2

316
Answers to Selected Problems
(b) To show that any solution in the antifunnel, above the exceptional 
solution a;*, has a vertical asymptote, let (ti, a?i) be any initial point in this 
region. Let u(t) be the solution through (ti,a?i). Then и must leave the 
antifunnel at a point (£2,^2)) on a(t), with t2 > Й, and u'(t2) = 1. In 
the shaded region where x3 — t2 > ж2, the solutions of xf = x2 will be lower 
fences for xf = x3 — t2, and they each have vertical asymptotes. Implicit 
differentiation of x3 — t2 — x2 = 0 gives x' = 2(x3 — x2)1/2/(3x2 — 2x) —► 0 
as x —* oo. Therefore, the solution u(f) will intersect this curve at some 
point (t3,u(t3)) with t3 > t2. The solution of xf = x2 through this point is 
a lower fence. Since it goes to infinity at finite t, so will u(t).
1.6#6. The solutions of x' = x2/2 are of the form x(f) = 2/((7 — t). In the 
region where x2 — t > x2/2, the solutions of x' = ж2/2 form a lower fence 
for xf = x2 — t. This region is shaded in the graph. Therefore, the solution 
of x' = x2 — t through ж(0) = 1 has the curve 2/(C — t) as a strong lower 
fence, where C = 2. This implies that the solution remains above the curve 
a(t) = 2/(2 — t) and must, therefore, have a vertical asymptote at some
x' = x2 — t

Answers to Selected Problems
317
Solutions for Exercises 2.1
2.1#2. The answers are:
(a) ln(ta:) + x — t = C
(b) (1+ ®)(1 — t) = C
(c) (t + x)/(tx) + ln(x/t) = C
(d) (ж-а) = Се1/*
(e) x2a = C(t - a)/(t + a)
(f) x = (t + C)/(l — Ct)
(g) tan(0) tan(c^) = C
(h) sin2(0) + sin2(<^) = C
(i) tan(a;) = C(1 — e*)3
(j) t2 + x2 — t2x2 + C
Solutions for Exercises 2.2-2.3
2.2-2.3#4. The answers are:
(a) x = t(t + l)2 + C(t + l)2 
(b) x = Cta + t/(l - a) — 1/a 
(c) x = at + Ct(l — t2)1/2 
(d) x = sin(t) — 1 + Ce~sin^)
(e) x = tn(<et +C)
(f) tnx = at + C
(g) e*x = t + C
(h) x = t2(l + Ce1^)
2.2—2.3#8. Assume a quadratic solution of the form x(t) = a + (3t + yt2. 
Then (t2 4-1)(27) + (27^ + (3)2 + fc(a + /3t + 7t2) = t2 implies that t2(27 + 
472 + ky) +1(47/3 4- k/3) 4- (27 + /32 + fca) = t2, which leads to the three 
equations
27 4- 472 4- &7 = 1
47/З 4- k(3 = 0
27 4- (32 4- ka = 0.
From the first of these equations the possible values for 7 are [—(2 + fc) ± 
{(2 + к)2 + 16}г/2]/8. If к = —2, then 7 = ±1/2. When 7 = 1/2, /3 is 
arbitrary, a = (1 + f32)/2, and there is a family of quadratic solutions of 
the form x(t) = t2/2 + /3t + (1+ /32)/2. When к = — 2 and 7 = —1/2, /3 is 0 
and a = —1/2, so there is one extra quadratic solution x(t) = — (t2 + l)/2. 
If к / —2, /3 = 0 and x(t) = 7(t2 — 2/fc), where 7 can have only the two 
distinct values shown in the quadratic formula above. This gives exactly 
two quadratic solutions, one opening upward and the other downward. Note 
that this method does not find any of the solutions that are not quadratics. 
But you can show that there are no other polynomial solutions of degree 
n > 2 because the middle term alone would then be of higher degree, and 
there would be nothing to match it up to.
2.2— 2.3#10. The answers are:
(a) 
x2(t2 + 1 + Ce*2) = 1 
(c) a2x3 = Ceat — a(t + 1) — 1
(b) 
[<7(1 — t2)1/2 — a]x = 1 (d) x = [tan(t) + sec(t)]/[sin(t) + C]

318
Answers to Selected Problems
Solutions for Exercises 2.4-2.5
2.4-2.5#2. (a) If u(t) satisfies du/dt = (2 + cos(t))u — u2/2 + q, then 
u(t + 2tt) = z(f) satisfies dz/dt = (2 + cos(t + 2tt)z - z2/2 + a, which is the 
same equation because of the periodicity of the cosine function.
(b) The graph below shows the two periodic solutions for a = — 1. The 
funnel around the upper solution and antifunnel around the lower one are 
shaded in.
#' = (2 + cost)# — x2/2 — 1
(c) The three graphs below show the slope fields for a = —1.37, —1.375, 
and -1.38. Notice that the two periodic solutions exist, but are very close 
together, for a = —1.37. By the time a reaches —1.38 it is clear that no 
periodic solution can exist, since it would have to cross the trajectory shown 
in the third figure. This in turn would violate the existence and uniqueness 
theorem.

Answers to Selected Problems
319
•8
a = -1.37
a = -1.375

320
Answers to Selected Problems
a = -1.38
For all three graphs — 2 < x < 8, —15 < t < 15.
2.4—2.5#6. The differential equation for N is obtained by setting the per 
capita growth rate (l/N)(dN/dt) = r± — 7*2 AT1/2. This is the net growth 
rate, or birth rate — death rate. The equation can then be written as 
N' = 7V(n — Г2ЛГ1/2). The slope field for this equation is graphed below. 
There are two equilibrium solutions, N = 0 and N = (тч/гг)2- With a 
change of dependent variable, N = Z2, the differential equation becomes 
2ZZ' = Nf = 7V(n - r2TV1/2) = Z2(ri - r2Z), which can be written 
Z' = Z(ri/2 — (r2/2)Z). This is the logistic equation in Z. Therefore, the 
solution TV = 0 is unstable and (ri/rz)2 is stable. This is indicated in the 
graph below.
-5
xf = ж(.5 — .3\/x)

Answers to Selected Problems
321
Solutions for Exercises 2.6
2.6#1. The answers are:
(a) t3/3 + xt - x2 = C
(b) 2x2 — tx + t3 = C
(c) x4 = 4tx + C
(d) 1п(жД) — tx/(t — x) — C
(e) not exact
(f) ln(t + x) - t/(t + x) — C
Solutions for Exercises 2.7
2.7#2.c. To solve (1 + t)x' — kx = 0, with a:(0) = 1, let u(t) = 1 + ait + 
a2t2 + - • • = 52 antn. Then u'(t) .= 52пап^”-1-If these sums are substituted 
into the differential equation, we have
(1 +1) nantn-1 - £ antn = 0. 
n=l 
n=0
Assuming the sum is convergent, we can write the first term as two sums:
У2 Tiant^1 + nantn — к У^ antn = 0. 
10 
0
A change of index on the first sum allows us to write
У7(п + l)an+i*n + У2 nantn - к У^ antn = 0, 
о 
oo
and this gives a recurrence relation for the coefficients, (n 4- l)an+i = 
(fc — n)an. Therefore, ao = 1, ai = kao = fc, U2 = (k — l)ai/2 = (k — l)k/21, 
andjn general an = k(k — l)... (k — n + l)/nl. Therefore, the series solution 
is u(t) = 1 4- kt 4- fc(fc — l)t2/2! + ... = (l + t)k (the Binomial Series), and 
converges for \t\ < 1.
2.7#7.c. If the series is substituted into the equation for x(t), you 
will obtain the recurrence relation an+2 = — (n — l)an/[(n4- l)(n4- 2)]. The 
initial conditions determine ao = 1 and ai = 0. Note that with cq = 0, the 
recurrence relation implies that all an, n odd, will be 0. The series solution 
is x(t) = 1 —12/2\ 4- £4/4! — 3tQ/6! 4- 5 • 3£8/8!----- with nth term given by
ant2n = (—l)n 1.3 • 5 • • • (2n - 3)t2n/(2n)!, for n > 2.
This is an alternating series, with terms monotonically decreasing to zero, 
which means that the error made by stopping with the nth term is less 
in absolute value than the (n 4- l)st term. Therefore x(0.5) = 7/8, with 
absolute error less than (l/2)4/4! = 0.0026.

322
Answers to Selected Problems
Solutions for Exercises 2. Miscellaneous Problems
2.Misc#2.(ii). The answers are:
(a) x2 + 2tx — t2 = C
(b) t2 + 2tx = C
(c) ln(t2 + ж2)1/2 - tan-1(a:/t) = C
(d) l + 2Cx-C2t2 = 0
(e) (t + x)2 (2t + x)3 = C
(f) texp^s/t)1/2] = C, 
or s = t[ln(C/t)]2
Solutions for Exercises 3.1-3.2
3.1—3.2#1. (a) The first two steps in the approximate solutions of x' = ж, 
#(0) = 1, with stepsize h = 1, are shown below:
X
Euler
Midpoint
R — K
Exact Sol.
0
1.0
1.0
1.0
1.0
1
2.0
2.5
2.7083333
2.71828183
2
4.0
6.25
7.33506945
7.38905610
(b) The analytical solution is x(t) = e*, with the values shown in the 
table above.
(c) Graphical solution:
THREE METHODS FOR NUMERICALLY APPROXIMATING SOLUTIONS TO X’ = X
STARTING AT (to, xo) = (0,1)
WITH STEPSIZE: h = 1
FOR TWO STEPS.

Answers to Selected Problems
323
3.1-3.2#7. (a) This method uses the formula xn+i = xn + hm, where m 
is the average of the slopes at the two ends of the interval. See the drawing 
below.
Midpoint Euler approximation for x’ = f(t,x)
xn + 1 =xn + hm
m = average of slope at two ends of the interval
(b) Let dx/dt — g(t). Then xn+i =xn + h[g(tn) + g(tn+i)]/2. This is the 
Trapezoidal Rule for integration, which approximates the area under the 
curve between t = tn and tn+i by the area of the trapezoid having sides of 
height g(tn) and 5(tn+i).
3.1-3.2#8. The Euler and Runge-Kutta approximations of #(1), where 
x' = x2 and x(fi) = 2, are shown below. The stepsize used is h = 0.1.
t
x (Euler)
x(R - K)
x (exact)
0
2.0
2.0
2.0
0.1
2.4
2.499940
2.5
0.2
2.976000
3.332946
3.333333
0.3
3.861658
4.996857
5.0
0.4
5.352898
9.930016
10.0
0.5
8.218249
82.032516
oo
0.6
14.972210
1.017404 x IO-12
-10.0
0.7
37.388917
overflow
-5.0
0.8
177.1820
-3.333333
0.9
3316.5292
-2.5
1.0
1103253.14
-2.0
(b) Looking at the results of the Euler approximation, you might think 
it was just increasing exponentially. After running the Runge-Kutta ap­
proximation, it appears that something is quite wrong.
(c) The analytic solution can be found by separation of variables, dx/x2 = 
dt —> —l/ж = t + C=^j: = 2/(l — 2t), where #(0) = 2. Note that this so­
lution is undefined at t = 0.5. It has a vertical asymptote there. This was 
not obvious from either approximation.

324
Answers to Selected Problems
(d) A smaller constant step h in the t-direction will not help, because 
eventually the distance from the approximate solution to the asymptote will 
be smaller than h, then the next step simply jumps across the asymptote 
without even knowing it is there.
(e) The only hope for not crossing the asymptote would be a step­
size that gets smaller in the t-direction as the approximate solution ap­
proaches the asymptote. The easiest experiments focus on Euler’s method 
for x' = /(£,#), (tn+i,a?n+i) = (tn,^n) + Л(1, 
and change the ex­
pression for the step. The term 7i(l, f(t, x)) represents a step length h in 
the t-direction. Changing it to fo(l//(t, ж), 1) maintains the proper slope, 
but forces each step to vary in the t-direction inversely as slope. An alter­
native interpretation for this second step expression is that the step has 
constant length h in the ж-direction. Using such a modified step greatly 
delays the approach to the asymptote, but the approximate solution nev­
ertheless eventually crosses it (e.g., with h = 0.1 at x « 78.7), because 
the approximate solution eventually gets closer to the asymptote than the 
horizontal component of the step.
(f) A possible refinement of the ideas in (e) is to force the Euler step to 
have length h in the direction of the slope, by using
This actually works better at first, but an approximate solution with this 
step also eventually crosses the asymptote, a little higher up (for h = 0.1 
at x « 79.7).
Our conclusion is that the step variations described in (e) and (f) def­
initely are much better than the original Euler method, but they fail to 
completely remove the difficulty. We hope you have seen enough to suggest 
directions for further exploration.
3.1-3.2#10. (a) x' = 4 — ж, x(0) = 1. The implicit Euler formula gives 
Xi+i = xi 4- fo(4 — Xz+i). This is easily solved for ж^+i to give x^i = 
(xi + 4/i)/(l + h). Therefore, with h = 0.1, we can find x± = 1.272727, 
X2 = 1.520661, and x% = 1.746056. This compares well with the exact 
solution x(t) = 4 — 3e-t, which gives ж(0.3) = 1.777545.
(b) Here, = Xi 4- Zi(4a;i+i — ж2+1), which can be written in the form 
/гж2+1 + (1 — 4/i)Xi+1 — Xi = 0. The quadratic formula can be used to obtain 
Xi+i = (4/i — l)/(2/i) ± [(1 — 4/i)2 4- Ahxi]1/2/(2h). This gives the values 
xi = 1.358899, X2 = 1.752788, and x$ = 2.150522. The analytic solution 
#(t) = 4e4t/[3(l + e4t/3)], so the exact value of ж(0.3) is 2.101301.
(c) The implicit Euler formula is Жг-ы = Xi + /i(4a?i_|_i — ^+1). In this 
case we must solve the cubic equation
hx?.^ 4- (1 - ^hjxi+t - Xi = 0
for at each step. The solution of 0.1a?i 4- (0.6)aq — 1 = 0 is x± = 
1.300271; and continuing, X2 = 1.548397, x% = 1.725068. In this case the 
analytic solution x(t) = [4/(3e~8t 4-1)] gives ж(0.3) = 1.773210.

Answers to Selected Problems
325
(d) For this last equation, the implicit Euler formula is Xi+i = Xi + 
ftsin(^+i^i-|-i). Here, the equation at each step must be solved by some­
thing like Newton’s method. For x± = 1 + 0.1 sin(O.lxi) we get x± = 
1.010084; and continuing, x2 = 1.030549, x3 = 1.061869.
Solutions for Exercises 3.3
3.3#2. (a) For the midpoint Euler method, Table 3.3.1 gives
N
h
E(h)/h2
512
1/256
2.4558
1024
1/512
2.4594
2048
1/1024
2.4612
4096
1/2048
2.4621
8192
1/4096
2.4626
16384
1/8192
2.4628
so that Cm appears to have a value of approximately 2.463.
(b) To find the asymptotic development of the error, use Uh(t) = (1 + 
h + h2/2y/h. Then uh(2) = eMun(2)) = e(2/h) ind+h+^/2), and using the 
Taylor series for ln(l + x), ln(l + h + h2/2) = (h + h2/2) - (Л + h2/2)2/2 + 
(h + h2/2)3/3------= h - h3/6 + O(h4). Therefore
e2 - ил(2) = e2 - e2-h2/3+o(ft3) = e2 _ e2[i + (-ft2/3 + O(h3))
+ [-h2/3 + O(h3))2/2! + •■■]= e2 - e2[l - ft2/3 + o(h3)] 
= e2h2/3 + O(h3) = CMh2 + O(h3).
The value CM = e2/3 
2.46302.
(c) The exact value of Cm compares very well with the experimental 
value found in part (a).
3.3#6. (a) The Taylor series can be computed from the successive deriva­
tives of /, as
x' = (x +t)-1, x" = —{x + 
+ 1) = —(x + t)“2[(a; +t)”1 + 1]
or x" — — (x + t)-3 — (x + t)-2. The successive derivatives can be written 
as polynomials in (x +t)”1. If we let z = (to + #(to))-15 the 6th degree 
Taylor polynomial is
x(tQ + h) = x(t0) + hz- h2(z3 + z2)2! + /i3(3z5 + 5z4 + 2z3)/3!
- h4(15z7 + 35z6 + 26z5 + 6z4)/4!
+ h5 (105z9 + 315z8 + 340z7 + 154z6 + 24z5)/5!
- h6(945г11 + 3465z10 + 4900z9 + 3304z8 + 1044z7 + 120z6)/6! 
+ О(Д7).

326
Answers to Selected Problems
To find ж(0.1), with z(0) = 1.0, for example, this polynomial must be 
evaluated with h = 0.1 and z = l/(0.0 + 1.0) = 1.
(b) The equation is a linear differential equation for t, as a function 
of x. That is dt/dx = x + t, or t'(x) — t(x) = x. This gives the result 
t(x) = —x - 1 + Cex. With ж(0) = 1, 0 = -l-l + Ce=>C = 2/e.
(c) The table below compares the Taylor series approximation and the 
Runge-Kutta approximation with the exact solution. Note that the Runge- 
Kutta solution is more accurate, and definitely easier to compute, especially 
if the program is already available on your computer.
t
exact sol.
Taylor series
R-K
0
1.0
1.0
1.0
0.1
1.09138791
1.09138403
1.09138895
0.2
1.16955499
1.16955068
1.16955629
(d) 
When ж(0) = 0, the solution is undefined, 
(e) The table for ж(0) = 2:
t
exact sol.
Taylor series
R-K
0
2.0
2.0
2.0
0.1
2.04822722
2.04822721
2.04822733
0.2
2.09326814
2.09326813
2.09326816
The Taylor series is more accurate when ж(0) = 2, because the value of z 
is smaller, so the series converges more quickly.
3.3#8.(a) The exact solution of x' = f(t) is of course
u(t) = [
Jt0
Euler’s method gives the lefthand Riemann sum approximation to the in­
tegral as
n—1
Uh(h) - 52 
“ s")/(sn),
2=0
where we have set Si = t0 + ih, and n is the number of whole increments 
of h which fit in [to,*i]« Then
fsi + l 
ft
= 
f(si))ds+ (/(s) -/(s„))ds.
**S1 
Jsn
Taylor’s theorem says that there exists a function сДз) with si < Ci(s) < 
s such that
l/W - 
- Si)| < 
- Si)2,

Answers to Selected Problems
327
— x a Riemann sum for
so that
fSi+i 
pSi+1
/ 
f'(8i)(8-8i)ds <sup|/"| —.
Jsi 
J Si 
u
Make a similar argument for the last term of the Riemann sum, and eval­
uate the second integral on the left, and sum, to get
b2 n 
h2
-у22/'(вг)| <suP|/"|(ti
2 6
The second term within the absolute value is
tl f{s)ds.
Э
An argument just like the one above gives the error:
h п 
Г*1 
h2
_ / /'(s)dsi supI/"k*i
i=0 
Jto
Putting the two estimates together, and calculating the integral of the 
derivative of /, this leads to
h 
h2
E(h) - ~№) - /(to)) < 8ир|/"|(й - to)^-.
(b) Part (a) was relatively straightforward: write everything down explic­
itly and apply Taylor’s theorem. This part is harder. To simplify notation, 
let us suppose that ti = t0 + (n + l)h. Then the Euler approximation Uh 
to the solution of xf = g(t)x with x(t0) = x0 is given at time by the 
product
logu^^i) = (1 + /ig(s0))(l + /ig(«i)) •••(! + hg(sn))x0.
Since xq is a factor of u(ti) and we can set xq = 1.
Use logarithms to study the product:
U2
logUft^i) = 521og(l + hg(si)) = 
- — (g(si))2 + O(/i3)).
г 
г
The first term in the sum is a Riemann sum for J^1 <j(s)ds, and we saw in 
part (a) that
$2Ы»»)=/ 
[ g'(s')ds + O(h2').
i 
J to 
Z Jto
The next term is a Riemann sum for (fo/2) 
(g(s))2ds.

328
Answers to Selected Problems
Putting this together, we find
[ g(s)ds — ( f g'(s)ds+ [ (g(s)2ds) + O(h2)Y
‘■'to 
Wto 
A) 
/
Now exponentiate, remembering that eah+O(h2) = i -j. 
+ O(h2)-.
uh(ti) = eY 9(s)ds Л - £ f f g’(s)ds+ f (g(s))2ds \ + O(h?)\ .
\ 
1 \Ji0 
Jto / 
/
Finally, we find that
E(h) = и^)-иМ = 9(s)ds [(j\g'(s))dS + (p(s)2ds) + O(h2)Y
(c) For the equation x' — x we have
logUh(t) = log(l + h)t,h = (h - у + О(Л3)^ = t - у + O{h2). 
Exponentiating leads to
e‘ - ufc(t) = ^(te‘) + O(h2);
this agrees with part (b), since 
I2ds = t.
(d) If g(t) = -t, then f*(g'(s) + g2(s))ds = 0 when t = \/3. The program 
numerical methods indicates that over 0 < t < >/3, Euler’s method for the 
differential equation x1 = —x converges with order 3. We don’t know why 
it doesn’t converge with order 2, as one would expect.
Solutions for Exercises 3.4
3.4# 1. The table below shows the results of the analysis of the errors when 
the equation x1 — x2 sin(t) is approximated by Euler’s method, with (a) 
18 bits rounded down, and (b) 18 bits rounded round. The solution was 
run from t — 0 to t — 6.28. The exact solution x(t) = [7/3 + cos(t)]-1, if 
x(0) = 0.3. The value used for x(6.28) was 0.3000004566.
(b) Rounded round (a) Rounded down
N
Error
order
Error
order
4
0.16463688
0.16463783
8
0.07148865
1.2035
0.07149055
1.2035
16
0.0401814
0.8312
0.0408524
0.8311
32
0.02201011
0.8684
0.02202919
0.8672
64
0.0116551
0.9172
0.01169327
0.9137
128
0.00601891
0.9534
0.00609329
0.9404
256
0.00306251
0.9748
0.00321319
0.9232
512
0.00154236
0.9896
0.00184372
0.8014

Answers to Selected Problems
329
(a) Rounded down
(b) Rounded round
N
Error
order
Error
order
1024
0.00080421
0.9395
0.00136497
0.4337
2048
0.00037125
1.1152
0.00160149
-0.2305
4096
0.00020340
0.8681
0.00262001
-0.7102
8192
0.00009849
1.0462
0.00488594
-8991
16384
0.000052718
0.9017
0.00965431
-0.9825
(c) Notice that the errors in the rounded round calculation are still de­
creasing, although the order is beginning to fluctuate further away from 
1.0000. For the rounded down case, the order of = 1 is seen for N between 
64 and 256. It goes to = —1.0 at N = 16384. The smallest error in the 2nd 
column occurs at N = 1024.
Solutions for Exercises 4.1-4.2
4.1-4.2#3. At any time t, r(t)/h(t) = tan 30° = l/\/3- The area of the 
cross-section A at height h is 7rr2 = 7r/i2(t)/3. From equation (3) in Section 
4.2, h'(t) = —y/2g(a/A)\/h = — 3ay/2gh~^2/к. We are given the constant 
area a = 5 x 10“5m2, and g = 9.8. The differential equation for h is 
h!(t) = —K/i~3/2, where К = x/19.6 x 15 x 10-5/тг = 2.114x 10-4. Solving, 
by separation of variables, gives h(t) = [—2.5Kt + C]2/5. At time t = 0, 
Д(0) = 0.1m. This makes С = (0.1)5/2 = 0.00316. To find the time T when 
A = (Tr/3)h2(t) = a, set h(T) = 6.91 x 10-3. Then T = 6 seconds.

330
Answers to Selected Problems
Solutions for Exercises 4.3
4.3#2. For the equation x' = cos(a;2 +t), df /дх = -2a;sin(a;2 + t). To 
have \df /дх\ > 5 requires |a;sin(a;2 +f)| > 2.5. This can only occur when 
|x| > 2.5. The narrow triangular regions where the inequalities df /дх > 5 
and df/дх < —5 are satisfied are shown in the graph below. These are 
found by graphing the functions apart or come together much more rapidly 
than in the part of the plane where |a;| < 2.5.
x1 = cos(a;2 +1)

Answers to Selected Problems
331
Solutions for Exercises 4.4
4.4#4. Let u(t) be the solution of x' = f(t,x), with u(0) = 0, and n(5) = 
17. Assume f has Lipschitz constant К = 2. If x(t) satisfies xf = f(t, x) + 
g(t, ж), with < 0.1, then
|u(t) - x(t)\ < 0.03e2|f-to1 + (0.1/2)(e2|t-to1 - 1).
Therefore, |w(5) — ж(5)| < 0.03e10 + O.O5(e10 — 1) < 1763. This says that 
ж(5) can lie between 17'± 1763 (not a very useful estimate).
Solutions for Exercises 4.5
4.5#1. (a) The differential equation xf = (|ж|1//2 + fc) can be solved explic­
itly by solving it in each of the regions x > 0 and x < 0, by separation of 
variables. For example, for x > 0, x' = >Jx + fc, or dx/{y/x + fc) = dt. This 
can be integrated to get 2y/x — 2k ln(^/x + fc) = t + C. A similar result 
for x < 0 leads to the solution 2|x|1/Z2 - 2k ln(|x|1/Z2 + fc) = t + C, which 
holds for all x. If к 0, there is a unique solution for any initial condition. 
Notice that x = 0 is not a solution of the equation if к ф 0.
(b) This differential equation does not satisfy a Lipschitz condition in 
any region including x = 0, because df jdx => oo as x => 0+ or x => 0". 
That implies that |/(£,0) — f(t,x)\ can be made greater than A"|x| for any 
constant K, This problem is meant to show you that a Lipschitz condition 
is sufficient, but not necessary, for uniqueness of solutions.
4.5#5. (a) If x(t) = Ct- C2/2, then tx' - (a/)2/2 = tC-C2/2 = x.
(b) If x(f) = t2/2, then txf — (a/)2/2 = t • t — t2/2 — t2/2 = x.

332
Answers to Selected Problems
(c) The line x = Ct — C2/2 intersects x = t2/2 where t2/2 = Ct — 
C2/2-, that is, where (t — C)2/2 = 0. Therefore the point of intersection 
is (C, C2/2). The slope of the parabola at this point is C, so the line is 
tangent there. This gives two solutions through every point outside the 
parabola; there are none inside. To see that this is true, let (f, x) be an 
arbitrary point in the plane. Set x = Ct — C2/2 and solve for the constant 
C. By the quadratic formula, there will be two solutions if x < t2/2, one 
solution if x = P/2, and none otherwise.
(d) In the text, a differential equation is defined to be an equation of 
the form xf = f(t,x), solved for xf in terms of x and t. If we try to put 
this equation in that form, we get x' = t ± (t2 — 2a;)1/2. In the region 
(t2 — 2ж) > 0, i.e., outside the parabola of equation x = t2/2, this gives 
two differential equations; both are easily seen to be Lipschitz, and the two 
straight lines tangent to the parabola from any point outside it are the 
solutions to those two equations through that point. Along the parabola, 
the equation is not defined in a neighborhood of any point of the parabola, 
so the existence and uniqueness theorem says nothing.
Solutions for Exercises 4.6
4.6#l.a. If x' = x2—t is solved by Euler’s Method, with ж(0) = 0, the value 
of the slope function /(t, x) = x2 — t > — t. This implies that x will remain 
above — t2/2. For small enough h, the solution will be strictly negative for 
t > 0, so we can take the region R to be 0 < t < 2, — 2 < x < 0. In this 
region M = sup|/| = 4, К = sup\df/dx\ = 4, and P = sup\df/dt\ = 1. 
This implies that £h < h(P+KM) < 17h. Now the Fundamental Inequality 
gives E(h) < (eh/K)(eK^~to^ — 1) = 12665h. To make ж(2) exact to 3 
significant digits, we must make 12665 h < 5 x 10“3, which requires that 
h < 4 x 10~7. This would take 5 million steps!!
4.6#3. (a) At each step, the initial slope is -Xi, and therefore with h < 1, 
you cannot reach 0 in one step. The Runge-Kutta slope (7711+2(7712+^3) + 
7714)/6 will be negative, but smaller in absolute value than because 77i2, 
7713, and 7714 are each less than 7711 in absolute value (i.e. 0 > ttii = —24, 
0 > 7712 = -(^i - hxi/2) > mr, etc.). With h < 1, you will never go down 
to zero, and the entire argument can be repeated for each subsequent step.
(b) Look at the Runge-Kutta approximation:
xi+1 = Xi + (/i/6)(mi + 2(t712 + m3) + m4), where
mi = f&i) = ~Xi
m2 = f(xi + /шц/2) = -Xi + (h/2)xi
m3 — f(xi + hm2/2) = —Xi + (h/2)xi — (h2/4)Xi
= f(xi + /ш13) = —Xi + hxi — (h2/2)xi + (h3 /4)^.

Answers to Selected Problems
333
Combining these gives
xi+1 = ж41 - h + h2/2 - h3/6 + Л4/24].
If we let
Uh(t) = «41 - (i - ti) + (t - ti)2/2 - (t - fi)3/6 + (f - ti)4/24],
in the interval between ti and ti+1, the slope error will be
£h = \u'h(t) - f(t,uh(t))\
= |о;4-1 + (t - tf) - (t - ti)2/2 + (t - ii)3/6] + uh(t)\
= \Xi(t - tj4/24| < (t - ti)4/24 < h4/24,
since 0 < Xi < 1, and t — ti < h.
The Fundamental Inequality now gives
\uh(t) - e-f I < (eh/K) (ек^ - 1) < £(е4 - 1),
since К = sup \df/dx\ = 1 and — 0- With e = h4/24, we get
\uh(t) - e-t| < (e* - 1)Д4/24 = C(t)h\
(c) To make |и^(1) — e-1| < 5 x 10-6, it is sufficient to take
h < [(24 x 5 x 10~6)/(e - l)]1/4 
0.092.
A numerical computation with h = 0.10 gives иЦ1) = 0.3678798, which is 
exact to 6 significant digits, and with h = 0.25 you get u^(l) = 0.367894, 
a drop to 4 significant digits (e-1 = 0.36787944 to 8 decimal places).
Solutions for Exercises 4.7
4.7#1. In the graph below, a funnel between x + t = Зтг/4 and x+t = 7r is 
shaded in on the slope field for xf = cos(t + x). The antifunnel is between 
x 4-1 = —it and x + t = —Зтг/4.

334
Answers to Selected Problems
x' = cos(£ + x)
(b) Both the funnel and antifunnel are weak, since the fences x +1 = ±7r 
are isoclines of slope —1. The function f(x) = cos(t + rr) satisfies \df /дх\ < 
1 for all x and t, so f satisfies a Lipschitz condition everywhere on R2. By 
Theorem 4.7.1 and Corollary 4.7.2 this implies that both fences x = — fl­
are nonporous, and that any solution that enters the funnel stays in the 
funnel. Furthermore, Theorem 4.7.3 implies that there exists a solution 
that remains in the antifunnel for all t.
(c) 
One solution that stays in the antifunnel is x — —tv — t.
(d) Notice that the funnel can be defined by x = tv — t and x = 7Г — t — e 
for any e > 0. Therefore, again by Corollary 4.7.2, any solution between 
x = —tv — t and x = tv — t will approach x = tv — t as closely as desired 
as t => oo. It is also true that any line x = —tv — t + s can be used to 
form the lower fence of the funnel; therefore all solutions to the right of 
x = —tv — t leave the antifunnel, so that the only solution that stays in it 
for all t is x = — tv — t. A nice finale to these arguments is suggested by 
John Hubbard: the curve a(t) = tv - t - t-1//3 can be shown to form the 
lower fence for a narrowing funnel about the trajectory x = tv — t. The 
slope af(t) = — 1 + t“4/3/3. The slope of the vector field on a is given by
x' = cos(t + x) = cos(t + tv — t — £1/Z3) = cos(fl- — t 1/Z3) ~ —1 +1 2/Z3/2 
for t large, and this is therefore greater than af.

Answers to Selected Problems
335
Solutions for Exercises 5.1
5.1#4. (a) The required Taylor Series expansion is
f(x0 + h) = /(so) + f'(x0)h + f"(x0)h2/y. + O(h3).
Given ftxo) = Xq, /'(^o) = 1, and f"(x0) = a2 > 0, this becomes
f(x0 + Л) = x0 + h + (a2/2)/i2 + O(h3).
If x0 + h is any nearby point (i.e., with a2h2/2 small relative to h), it will 
map into a point further away from жо if h is positive, but closer to xq if h 
is negative. See graph below for /(ж) = ж3/3 + 2/3.
ж3 4~ 2
(a) f(x) = —— 
(b) /(x) = ln(l + x)
(b) If f''(xo) = —a2 < 0, then f(xo + h) = xo + h — a2h2/2 + C?(h3), and 
the iterates will be attracted to жо on the right and repelled on the left. 
See right-hand graph above for /(ж) = ln(l 4- ж).
(c) If /"(жо) = 0, behavior at жо depends on the first nonzero derivative. 
For example, if /(жо 4- h) = жо 4- h 4- f^k\xo)hk/k\ 4- (9(^fc+1), then there 
are 4 cases, depending on whether к is even or odd, and whether У^(жо) 
is positive or negative.
(d) If /'(жо) = — 1, the series is
f(x0 + Л) = x0 - h + /"(a;o)h2/2! + f"(x0)h3/S\ + O(h4).
The iterates oscillate about жо, so convergence depends on whether /(/(жо4- 
Д)) is closer to, or further from жо than жо 4- h. Applying the Taylor Series 
to /(/(^o 4- h\) gives

336
Answers to Selected Problems
/(/(x0 + Л)) = *0 - [~h + /"ft2/2 + /'"ft3/6 + •]
+ (/"/2)[—ft + /"ft2/2 + • • -]2 + (/"'/6)[-ft + /"ft2 + • • •] 
+ • ■ • = x0 + ft{l - [(/")2/2 + /'"/3]ft2} + O(ft4).
Therefore, convergence depends on the sign of [(/"(^o)2/2+/"'(a:o)/3]. Two 
examples are shown below. For f(x) = — 1п(1 + ж), (/"(0))2/2 + /"'(0)/3 = 
—1/6 < 0, and a?o can be seen to be repelling. In the case of f(x) = — sin(a;), 
the sum of the derivatives is 1/3 > 0 and the point x = 0 is attracting.
(d) f(x) = — sin x
5.1#7.a. Setting x4 — 3x2 + 3x = x, we get solutions x = 0, 1, and —2 as 
fixed points. The derivatives are /'(0) = 3, /'(—2) = —17, and /'(1) = 1. 
Therefore 0 and —2 are repelling fixed points, and at x = 1 we need to 
calculate Since /"(1) = 6 > 0, this implies that x = 1 is repelling on 
the right and attracting on the left. For verification, see the graph below.

Answers to Selected Problems
337
5.1#10. (a) The fixed points of /(ж) = ж2 4- c must satisfy ж2 4- с = ж, or 
ж = 1/2 ± (1 — 4c)1/2/2. Therefore, there are 2 real fixed points if c < 1/4, 
one fixed point if c = 1/4, and none if c > 1/4.
(b) The graphs below show how the parabola intersects the line у = ж 
in each of the three different cases in (a).
c = 1/4
(c) A fixed point жо is attracting if |/'(жо)| = 11 ± (1 — 4c)1/21 < 1. If 
the plus sign is used, this inequality is never true. With the minus sign, 
|1 — (1 — 4c)1/21 < 1 implies —3/4 < c < 1/4.
(d) Let a denote [1/2 4- (1 — 4c)1/2/2]. Since Рс(ж) = ж2 + c has no local 
maxima, the maximum of ж2 4- c on the interval Ic = [—a, a] is realized at 
an end point; since Pc(—a) = Pc(a) = a, the maximum is in the interval

338
Answers to Selected Problems
Ic. The minimum is c = Pc(0); so PC(IC) C Ic precisely if c < 1/4 (for Ic 
to exist at all) and if c > —(1/2)[1 + (1 - 4c)1/2] or 1 + 2c > -(1 - 4c)1/2, 
which occurs precisely if c > —2 (and, of course, c < 1/4).
(e) If c e [-2,1/4], the result is already proved in (d), since 0 e Ic. For 
c > 1/4, the orbit of 0 is strictly increasing since Pc(x) > x for x > 0 and 
c > 1/4, so it must increase to oo or a fixed point; by (a) there is no fixed 
point in this case. If c < -2, с2 + c > (1/2) [1 + (1 - 4c)1/2], and on the 
interval (a, oo), we have Рс(ж) > ж, hence the sequence P°2(0), P°3(0),... 
is increasing, tending to +oo as above.
f{x) = x2 + c, 2 < c < 1/4
5.1#14. The answers are 1 — c, 2 — g, 3 — /, 4 — e, 5 — d, 6 — Д, 7 — a, 8 — b.
5.1#18. (a) For p(x) = ж3, 
1(x) = ж1/3, and
р(ж) = y>-1(/(y,(a;))) = 
= 21/3я;-
(b) The functions / and g are both linear, so that their multipliers are 2 
and 21/3 respectively.
(c) The fact that the multipliers are not equal does not contradict the 
statement proved in Ex. 5.1#16, because the hypothesis that “c^-1 is 
differentiable” is not satisfied at x = 0.
Solutions for Exercises 5.2
5.2#3. For the map q(n+ 1) = (1 + a)q(n) — a(g(n))2, let xn denote q(n). 
Then the equation becomes xn+i = (1 + a)xn - ax2 = f(xn). The fixed 
points of this map are points xs such that f(xs) = xs, or (1 + q)xs — ax2 = 
xs. This gives xs = 0 or 1. The derivative is /'(ж) = (1 + Q) — so 
/'(0) = 1 + a and /'(1) = 1 - «• Therefore, for 0 < a < 1, qs = 1 will be

Answers to Selected Problems
339
an attracting fixed point of the map, with positive derivative. Moreover, 
for x e [0,1], we have x < f(x) < 1, so any sequence xq, Xi = f(xo), 
x2 = f(xi),... with x e [0,1] is strictly increasing and bounded by 1. 
Hence the sequence must tend to a fixed point, which must be 1 (rather 
than 0).
f(x) = (1 + a}x — ax2
5.2#5. Let f(z) = z2, for z on the unit circle X, g(y) = 2y2 — 1 for 
у e [—1,1] = У, and = cos(0) be the map from X to Y.
(a) To show that tpo / = go (^, we simply calculate:
p о /(ег0) = р(е2гв) = cos(20),
and
g о р(егв) = <?(cos(0)) = 2cos2(0) — 1 = cos(20).
(b) The function p is not 1-1 since р(ег0) = <р(е^27Г”^).
(c) For егв to be a fixed point of /, we need е2гв = ег0, or егв = 1. 
Therefore 1 is the only fixed point in X, under f. For fixed points of g, it 
is necessary that 2y2 — 1 = 3/, so that у = 1 or —1/2. We see that the fixed 
points x = еОг and у = 1 correspond under the map <p, but the point —1/2 
does not correspond to a fixed point of f. Note that —1/2 is cos(2?r/3).
(d) The period 2 points of f must satisfy /o2(z) =z/(г2) = z4 = z. 
That is, z(zz — 1) = 0. Therefore, the period 2 points of / are e27™/3 and 
е4тгг/з 
z — 1 is a fixed point. Period 2 points of g must satisfy
^(0(?/)) — Уч or 2(2j/2 — I)2 — 1 — у = 0. This polynomial is divisible by 
2j/2 — у — x, so the period 2 points are roots of 4y2 + 2y — 1 = 0; that is, 
у = -1/4 ± л/5/4. Note that these can also be written as cos(4?r/3) and 
cos(4tp/5). Once again, one of the period 2 points of f corresponds under 
p to a period 2 point of g. but the other does not.

340
Answers to Selected Problems
(e and f) At this point it becomes imperative to look at the map 7? 
and see what it is telling us. For any n, a period n point of / is a point 
егв such that /оп(егв) = егв, that is, е2П0г = ee\ or (2n — 1)0 must be 
an integral multiple of 2%. Therefore the period n points of f are all егв 
such that 0 = [2тг/(2п — l)]fc for positive integers к such that the points 
are distinct and are not period m points for some m < n. The period n 
points of g are all у in [—1,1] for which gon(y) = у and gorn(y) у for 
m < n. Since \y\ < 1, we can write у = cos(0) for some 0 € [0,7г]. Then 
0(?/) — 0(cos(0)) — 2cos2(0) — 1 = cos(20), and (/On(cos(0)) = cos(2n0). So 
we are looking for values of 0, 0 < 0 < 7Г, such that cos(2n0) = cos(0). Two 
real numbers a and b have the same cosine if either a = b + 2Ajtt (k any 
integer), or a = (2tt — b) + 2ттт. Thus for any n we need to find all values 
of 0 in [0,7г] such that 
.
2n0 = 0 + 2Aj7t or 2n0 = 2(m + 1)тг - 0,
where к and m are non-negative integers, such that cos(0) is not a fixed 
point of g of order < n. Now we can use this information to find the fixed 
points of period 3. For f they are егв with 0 = 2/стг/7, к = 1,2,3,4,5, and 
6. For g they are cos(0) with 0 = 2/стг/7, к = 1,2, and 3; and cos(0) with 
0 = 2ттг/9, m = 1,2, and 4. Note that cos(6tt/9) has already been shown 
to be a fixed point of g. Note also that 3 of these points correspond under 
the map 7?, but the other 3 do not. This will be the general pattern for all 
n.
Solutions for Exercises 5.3
5.3#3. If Pb is the polynomial z3 — z + b, then the corresponding Newton’s 
method formula is
Nb(z) = z - (z3 - z + b)/(3z2 - 1) = (2z3 - b)/(3z2 - 1).
Therefore, Nb(Nb(0)) = Nb(b) = (2b3 - b)/(3b2 - 1). This will be 0 if, and 
only if, b(2b2 — 1) = 0, or b = 0, ±2-1/2. If b = 0, 0 is also a fixed point 
of the map Nb but for b = l/\/2, for example, 0 is a point on a period 
2 cycle. Furthermore, (d/dz)(^(^b(z)) = N^N^z^N^z), and N^z) = 
(6z4 — 6z2 + 6bz)/(3z2 — 1) = 0 at z = 0. This implies that the period 2 
cycle will be superattracting. If you start the Newton’s method for a root 
of z3 — z + (l/\/2) near 0 it will rapidly begin to cycle, rather than converge 
to a root.
5.3#9. The graph of Ta is shown below. To show that there are no fixed 
points, set Ta(x) = (x - afz)l2 — x. This gives x2 = -a, and with a > 0, 
there are no real solutions. A period 2 point must satisfy Та(Та(ж)) = x. 
This leads to the equation [(x/2 — a/{2x)) — a/(x/2 — a/(2x)) = x, or

Answers to Selected Problems
341
Зж4 + 2ax2 — a2 = 0. This can be factored to give (x2 + а)(3ж2 — a) = 0, and 
since a > 0, the only real roots are x = ±^/a/3. To show that the 2-cycle is 
repelling, look at (d/dx)(Г о Г)(ж) = (d/dz)(T(T(z)) = T'(T(z))T'(z) = 
т'(лД/з)Г(-^^/з) = 4> i.
(b) By the Chain Rule, the derivative of (T о T о • • • о T) = (Ton) is 
T,(Ton“1(x)) (d/dx)[Ton-1 (x)] and by induction on n, this is Т,,(Гоп“1(ж)) 
Т'(Топ“2(ж)) • • • T\T(x))T'(x). If x belongs to an n-cycle, this is just the 
product of the derivative of T at each point in the cycle, and is the same 
no matter which point of the cycle you start at.
5.3#11. Assume /(r) = 0 and x = r + e.
(a) Then f(x — e) = f(r) = 0, but by Taylor’s Theorem with remainder, 
f(x — e) = /(ж) — б/'(ж) + 
where c lies between x and x — e.
Therefore, 0 = /(ж) — е/'(ж) + ^2/,,(c)/2. If /'(ж) 0 we can divide by /' 
and get f(x)/ff (ж) = e - б2/"(с)/(2/'(ж)).
(b) 
Applying (a) to the function Nf(xi), gives
Nf(Xi) - r = Xi- f(Xi)/f'(Xi) - r
= Xi - [e - е2/"(с)/(2/'(^))] - r
= (xi - e - r) + e2/"(c)/(2/'(xj))
= £2/"(с)/(2/'(х0).
Setting e = Xi — r gives the desired result.
(c) Part (b) implies that if we take an initial guess Xi = r + 6, then the 
next iteration Nf(xi) will satisfy the inequality
|^(х<)-г|<е2|Г(С)/(2/'(^))|.

342
Answers to Selected Problems
If Xi is close enough to the root r, f(r) 0 => f'(xi) 0, and 
|/"(c)/(2/'(rcJ)| will be less than some positive constant B. Therefore, if 
e = (9(10“n), then the error at the next step will be (9(10-2n) = O(e2).
(d) The table below shows successive iterates of N3(x) = (x + 3/ж)/2, 
with = 1. Any initial value ж0 > 0 will converge to л/З because the map 
7V3 has a superattracting fixed point at x — л/З, and it is the only fixed 
point in the right-half plane.
step
tf3(s)
abs. error
1
2.0
2.68 x 10"1
2
1.75
1.79 x IO"2
3
1.732142857
9.20 x IO-5
4
1.732050810
2.43 x 10~9
5
1.732050808
exact to 9 decimal places
(e) The same type of table is shown below for the solution of the equation 
x — e~x = 0. In this case, the Newton function is Ne(x) = x — (x — e~x)/(l + 
e~x) = е-ж(1 + ж)/(1 + e~x).
step
Ne(x)
abs. error
1
1.0 = Xq
4.33 x 10"1
2
0.5378828427
2.93 x IO-2
3
0.5669869914
1.56 x 10~4
4
0.5671432860
4.41 x 10~9
5
0.5671432904
exact to 9 decimal places
5.3#14. Let xf = f(x, у) = (x — l)2 — у and yf = g(x, y) = y + x2/2 — 1/2. 
The zeros of xf and yf can be seen to lie on two parabolas which intersect at 
the points (1,0) and (1/3,4/9). To solve the nonlinear system by Newton’s 
method, we must solve the linear system:
/(^o + h,y0 + fc) = /(^o,2/o) + fx(xQ,yo)h + /у(хо,уо)к = 0 
g(x0 + Ь,у0 + к)= g(xQ,yQ) + gx(xQ,y0)h + ду(хц, yG)k = 0
for the increments h and k. The matrix of partial derivatives is
df/dx 
dg/dx
M =
df/dy] Г2(Ж-1) -1 
дд/ду] [ x 1
which gives
Using the initial guess (жо,3/о) = (0,0), Newton’s method leads to the
system
M|(0,0)
h 
к
= ■-/(0,0)'
.-9(0,0).
5
-1 
1/2

Answers to Selected Problems
343
Therefore, (aq, yj = (x0+h, y0+k) = (1/4,1/2) is the next approximation. 
The second approximation is found by repeating the process with (а:о,Уо) 
replaced by (a;i, yi). This leads to the system 
-3/2 -11 \h 
1/4 1J [k
-1/16
-1/32
having solution (h, k) = (3/40, —1/20). Therefore (x2, У2) is (13/40,9/20) — 
(0.325,0.45) which is quite close to the solution (1/3,4/9).
Solutions for Exercises 5.4
5.4#2. (a) Let w = ah. Then Fh(x) = [1 - w + w2/2]x, and we need to 
see where |1 — w + w2/2| < 1. This is a parabola with minimum 1/2 at 
w = 1, and remains below 1 for 0 < w < 2. Therefore, the iterates tend to 
0 if 0 < ah < 2, i.e., for h < 2/a. If h > 2/a, x will be multiplied for a 
value > 1 at each step, and the iterates will tend to oo.
(b) For Runge-Kutta Fh(x) = [1—w+w2/2—w3/6+w424]j:, where again 
we have let w = ah. It can be shown that the fourth degree polynomial 
p(w) = 1 — w + w2/2 — w3/6 + w4/24 has a unique minimum greater than 
0 at w = 1.596, and ranges between 0 and 1 for 0 < w < 2.78529356. 
Therefore, for h < 2.78529356/a, the iterates will tend to 0, and for h 
larger than that, they will tend to oo. This shows that the Runge-Kutta 
method has a slightly larger interval of stability than Euler’s method.
5.4#6. Theoretically, we know that the solutions of xf = x2 — t2 must 
enter the funnel and approach — t as t => oo. If a solution starts at a point 
near the funnel, so that x(t) = —t + w(t), with w(t) small, then
w'(t) = x'(t) + 1 = [—t + w(£)]2 — t2 + 1 = — 2tw(t) + [w(£)]2 + 1;
and for large t the approximation w'(t) = —2tw(t) is reasonable. The 
Runge-Kutta step for the differential equation x' = ax was shown in prob­
lem 5.4$2(b) to be xn+i = жп(1 - ah + a2h2/2 - a3h3/6 + а4Л4/24). It 
was shown there that the polynomial 1 — z + z2/2 — z3/6 + z4/24 has 
values between 0 and 1 for 0 < z < 2.7853, and is greater than 1 for 
z > 2.7853. Therefore, with h = 0.03, this multiplier will be < 1 for 
a < 2.7853/0.03 = 92.8. For the linearized equation w' = —2tw, this means 
that the solution will be stable as long as 2t < 92.8, or t < 46.4. A nu­
merical Runge-Kutta solution programmed on an IBM PC appeared to 
converge to x(t) = —t until t reached about 49. At that point it was clear 
that there was some kind of instability in the method. Results using the 
graphics program DiffEqSys are shown below. The trajectory starting at 
the dot appears to stay in the funnel for t < 46. At that point it seems to 
head off on a spurious trajectory, and finally breaks down completely into

344
Answers to Selected Problems
jagged junk. The solution between the dashed lines is correct; that in the 
shaded regions is not.
X
x' = X2 — t2
Solutions for Exercises 5.5
5.5#6. Choose c e [a, b], and consider
x Г du Г г и
G(a?) = 
—tor х е а, о .
Jc <?(«)
Then G(x) is monotone (increasing if g(x) > 0 for x G [a, b], and decreasing
if g(x) < 0), and tends to ±oo as x a or x —> b. In particular, the 
expression
ГХЮ du f*
СШ = 
f^ds
Jc 9\U) J to
defines x(t) implicitly as a function of and this function with x(t$) = c 
is the solution of our differential equation. If
rT 
fto+T
f(s)ds = f(s)ds = 0
0 
Jt0
this means that G(x(to + T)) = 0 = G(c), so x(t0 + T) = c, and the 
one-period later mapping is the identity on [a, b\.
5.5#8. Let xf = p(t)x + q(t) be a periodic linear differential equation. 
Then we can solve the equation by variation of parameters and get
x(t) = A(t)x(to) + where A(f) = exp
( р(т)с/т
<0

Answers to Selected Problems
345
and 
t
B(t) = A(t) f [1/A(s)]q(s)c!s.
Jto
Then x(to + T) = A(to+T)x(to) + B(to + T) which is of the form ax(t0) + b, 
with constants a and b.
Solutions for Exercises 5.6
5.6#4. If z satisfies |z| > |c| + 1, then |/(z)| = |z2 + c| > |z2| - |c| > 
(|c| + I)2 — |c| = 1 + |c| + |c|2. On the next iteration,
|Г2(г)|>(1 + |с| + |с|2)2-|с|>1 + |с|+3|с|2,
and by induction on n, |/on(z)| can be seen to be > 1 + |c| + where 
Kn = "IKn-x + 1, K\ = 1. Since the sequence Kn => oc with n, the orbit 
for z will be unbounded.
(a) Clearly, if £ is a solution of £ +1/£ = z, then by the quadratic 
formula, £ is either £i = [z + (z2 — 4)x/2]/2 or £2 = [z - (z2 - 4)x/2]/2. 
Furthermore, these two values of £ are reciprocals; so exactly one of |£x | or 
|£2| is < 1, or else |£x | = 1. In this latter case, z = £i + l/£i = e20 + e-20 = 
2cos# for some #, and z lies in the closed interval [—2,2].
(b) 
To show that cp = £ + l/£ conjugates P_2 to Po, we need only check 
that P_2(/?(£) = (рР0(£) for any £ in the unit disk. Simply compute:
+i/s) = (e+i/c)2 - 2=e + i/o
ап<ЫРоО = ^2)=£2 + 1/0
(c) 
For any £ € D (the unit disk), the orbit of £ under Pq converges to 
the fixed point 0, since |£| < 1. This implies that the corresponding point 
z = Ф(£) converges to Ф(0) under iteration by P_2. But Ф(0) is the point 
at oo. Therefore the orbit of z is unbounded.

Index
Accuracy, 129
Analytic functions, complex, 94
Analytic solutions, 67
Antifunnel, 31, 33, 34, 36, 44, 187
Antifunnel theorem
existence, 31, 185
uniqueness, 33
Approximate, 143-144
Approximation, 112, 157
Associated homogeneous equation,
71
Astronomy, 5, 6
Asymptotes, vertical, 42
Attractive cycle, 221
Autonomous, 67, 237
Babylon, 5, 114, 116
cuneiform tablets, 3
Bank accounts, 1, 79
Bernoulli equations, 101, 250
Bifurcates, 219
Bifurcation, 254
Bifurcations, of cascade, 231
Bisection method, 224
Bits, 133
Bounds, 133, 171, 180, 183
Cantor dusts, 262
Cantor, Georg, 260
Cantor set, 256, 261
Carbon 14, 3
Carbon dating, 85, 161
Cascade, 219, 222, 269
of bifurcations, 219
Cauchy, 114, 165
Census, U.S., 82, 102
Change of variables, 212, 216, 218
Chaotic, 202, 229
Coefficients, undetermined, 72, 76, 
92, 101, 106
Competition, 82, 83
Complementary equation, 71
Complex quadratic polynomial, 
255
Complex variables, 255
Compounded, 102
Computer, 1
Conditional stability, 238
Conjugation, 212
Conjugation diagram, 213
Connected, 269
Convergence, 144, 177, 230
Cosmic ray, 85
Crowding, 82
Cubic polynomial, 232
Curve-fitting, 145, 148
Cycle, 201
Degree, polynomial of, 199
Deposits, 79
Diagonal, 203
Difference equation, 198
Differential equation,
exact, 86
linear, 79
nonlinear, 4, 109
Disintegration, 85, 103
Discrete, 197
Dispersion, 32, 40
Distribution, 141
Dynamical plane, 257
Dynamical systems, 197, 227
347

348
Index
Electrical circuit, 1
Epicycles, 5
Equilibrium, 85, 104, 203
stable, 203
unstable, 203
Equivalence, 297
Error, 125, 171
Error bounds, 144
Error estimate, 157
Estimates, 129
Euler, 180
Euler approximate solution, 112
Euler approximations, 177
Euler’s method, 111, 175, 200, 237, 
238, 239, 241
Exactness, condition for, 86
Exceptional solution, 34
Exchange, 85
Existence, 13, 157, 171, 174, 178, 
185
Family, 13
Fatou, Pierre, 257
Feigenbaum point, 220
Fence theorem, 27, 183
Fences, 23, 34, 44
nonporous, 27, 30
porous, 27
strong, 27
weak, 27
Fertility coefficient, 82, 83
Fertility rate, 215
Field of slopes, 11
Filaments, 269
Fixed points, 203, 216, 223
attracting, 203, 206, 240
indifferent, 204
repelling, 203, 206
superattracting, 204, 223
Forces, 6
Fractal geometry, 261
Fundamental inequality, 169,171, 
174
Funnel, 31, 34
Funnel theorem, 31, 185
General solution, 101
Glucose, 104
Graphic iteration, 202
Gravitation, law of, 4
Grid method, of solution sketch­
ing, 11
Gronwall’s inequality, 169, 171,
174
Half-life, 103
Harmonic oscillators, 3
Harvesting, 84
Homogeneous, 109
Hunting licenses, 83
Implicit Euler method, 152,
239
Implicit methods, 124, 244
Implicit solutions, 71
Improved Euler method, 151
Infusion, 104
Initial condition, 13, 111, 171
Interest, 79, 102, 116
Intravenous, 104
Inverse image, 263
Islands, 269
Isoclines, 14, 34
Iterates, 199
Iteration, 197
circular representation, 217
Iteration picture, 210, 222
Iterative algorithms, 198
Jacobi’s method, 198
Julia, Gaston, 257
Julia sets, 257
Koch snowflake, 268
Kutta, W., 119
Leaking bucket, 158, 162, 168
Least squares, method of, 148
Leibniz’ notation, 68
Level curves, 85
Linear approximation, 224
Linear equations, 3, 91

Index
349
Linear first order equation, 71
homogeneous, 71 
nonhomogeneous, 71
Lipschitz, 165
Lipschitz constant, 165, 169
Lipschitz condition, 165, 183
Locally Lipschitz, 169, 174, 245
Logistic equation, 82
Logistic model, 214, 218
Lottery, 81
Malthusian growth rule, 103
Mandelbrot, Benoit, 261
Mandelbrot set, 264
Mapping, 212, 227, 247
one period later, 246
Midpoint Euler, 118,180, 237, 238, 
242
Midpoint Riemann sum, 117
Modified Euler, 118
Monotonicity, 255
Multiplier, 204, 214, 216
Neanderthal, 85
Newton, Sir Isaac, 4, 6, 114
Newton’s laws, 4
of cooling, 104
Newton’s method, 125, 198, 223, 
224, 232 
generalization of, 234
Nonautonomous, 241
Nonhomogeneous linear equation, 
76
Nonlinear, 7
Nonlinear equations, 1
Numerical approximation, order 
of, 129
Numerical methods, 172, 235
Orbit, 229, 232, 256
bounded, 256 
unbounded, 256
Order, 11, 206, 297
Parameter plane, 257
Parameter space, 264
Particular solution, 76, 101
Period, 206
Periodic cycles, 206
Periodic differential equations, 
198, 245
Periodic points, 206
Periodic solutions, 84
Picard iteration, 94
Piecewise differentiable, 29, 170
Planets, 5, 6
Planets program, 8
Poincare, 8, 9, 198
Poincare Lemma, 91
Poincare sections, 245
Pollutant, 104
Pony tail, 98
Population, 82
Population growth, 214
Power series, 92, 106
Predator, 7
Prey, 7
Principal part, 300
Probabilistically, 140
Pseudo-random number genera­
tor, 217
Ptolemy, 6
QR method, 198
Quadratic, 218
Quadratic convergence, 226
Quadratic equations, 227
Quadratic polynomials, 214, 240
Qualitative, 7
Quantitative, 7
Radioactive decay, 2, 158, 161, 
164, 167
Radioactive isotope C14, 85, 103
Radiocarbon, 85
Radius of convergence, 93
Raindrop, 105
Random walk, 140
Range of steps, 148
Riemann sum, 119, 142

350
Index
Rounding error, 125
Round-off, 133
Round-off error, 144
Runge, C., 119
Runge-Kutta, 119, 236, 237, 239,
242
Runge-Kutta methods, 180
Sardines, 7
Savings, 115
Scale functions, 299
Seed, 197, 217
Series solutions, 92
Separation of variables, 68
Sets,
not self-similar, 268
self-similar, 268
Sharks, 7
Simpson’s Rule, 117
Skeletal, human, 85, 103
Slope, 111
Slope error, 174, 180
Solution, 7, 11
Solution, approximate, 15
Solutions, sketching of, 11
Square roots, 227
Stability, 32, 84
islands of, 223
Stable, 215
Standard deviation, 140, 141
Step, 111
Stepsize, 235, 238
Stiffness, 238
Stochastic, 141, 171
Stocking, 83
Symmetry, 12, 53, 54
System, 5
Taylor polynomial, 153, 204
Taylor series, 107, 302
Taylor series method, 124
Taylor’s theorem, 301
Time series, 200, 210, 222, 233
Torricelli’s Law, 160
Trapezoidal rule, 117
Translates, 43
Truncation error, 125
Uniqueness, 13,158,163,165,171, 
174, 187
Universal, 220, 269
Universality, 232
Variation of parameters, 72, 76, 
78
Vector spaces, 73, 74
Volterra, 7
Windows of order, 220
Withdrawals, 79


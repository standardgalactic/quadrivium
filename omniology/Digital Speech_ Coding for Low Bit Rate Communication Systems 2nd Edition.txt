Digital Speech
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)
www.allitebooks.com

Digital Speech
Coding for Low Bit Rate Communication Systems
Second Edition
A. M. Kondoz
University of Surrey, UK.
www.allitebooks.com

Copyright 2004
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester,
West Sussex PO19 8SQ, England
Telephone (+44) 1243 779777
Email (for orders and customer service enquiries): cs-books@wiley.co.uk
Visit our Home Page on www.wileyeurope.com or www.wiley.com
All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval
system or transmitted in any form or by any means, electronic, mechanical, photocopying,
recording, scanning or otherwise, except under the terms of the Copyright, Designs and
Patents Act 1988 or under the terms of a licence issued by the Copyright Licensing Agency
Ltd, 90 Tottenham Court Road, London W1T 4LP, UK, without the permission in writing of
the Publisher. Requests to the Publisher should be addressed to the Permissions Department,
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ,
England, or emailed to permreq@wiley.co.uk, or faxed to (+44) 1243 770620.
This publication is designed to provide accurate and authoritative information in regard to
the subject matter covered. It is sold on the understanding that the Publisher is not engaged in
rendering professional services. If professional advice or other expert assistance is required,
the services of a competent professional should be sought.
Other Wiley Editorial Ofﬁces
John Wiley & Sons Inc., 111 River Street, Hoboken, NJ 07030, USA
Jossey-Bass, 989 Market Street, San Francisco, CA 94103-1741, USA
Wiley-VCH Verlag GmbH, Boschstr. 12, D-69469 Weinheim, Germany
John Wiley & Sons Australia Ltd, 33 Park Road, Milton, Queensland 4064, Australia
John Wiley & Sons (Asia) Pte Ltd, 2 Clementi Loop #02-01, Jin Xing Distripark, Singapore
129809
John Wiley & Sons Canada Ltd, 22 Worcester Road, Etobicoke, Ontario, Canada M9W 1L1
Wiley also publishes its books in a variety of electronic formats. Some content that appears
in print may not be available in electronic books.
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN 0-470-87007-9 (HB)
Typeset in 11/13pt Palatino by Laserwords Private Limited, Chennai, India
Printed and bound in Great Britain by Antony Rowe Ltd, Chippenham, Wiltshire
This book is printed on acid-free paper responsibly manufactured from sustainable forestry
in which at least two trees are planted for each one used for paper production.
www.allitebooks.com

To my mother Fatma,
my wife M¨unise, and our children Mustafa and Fatma
www.allitebooks.com

Contents
Preface
xiii
Acknowledgements
xv
1
Introduction
1
2
Coding Strategies and Standards
5
2.1 Introduction
5
2.2 Speech Coding Techniques
6
2.2.1
Parametric Coders
7
2.2.2
Waveform-approximating Coders
8
2.2.3
Hybrid Coding of Speech
8
2.3 Algorithm Objectives and Requirements
9
2.3.1
Quality and Capacity
9
2.3.2
Coding Delay
10
2.3.3
Channel and Background Noise Robustness
10
2.3.4
Complexity and Cost
11
2.3.5
Tandem Connection and Transcoding
11
2.3.6
Voiceband Data Handling
11
2.4 Standard Speech Coders
12
2.4.1
ITU-T Speech Coding Standard
12
2.4.2
European Digital Cellular Telephony Standards
13
2.4.3
North American Digital Cellular Telephony Standards
14
2.4.4
Secure Communication Telephony
14
2.4.5
Satellite Telephony
15
2.4.6
Selection of a Speech Coder
15
2.5 Summary
18
Bibliography
18
3
Sampling and Quantization
23
3.1 Introduction
23
www.allitebooks.com

viii
Contents
3.2 Sampling
23
3.3 Scalar Quantization
26
3.3.1
Quantization Error
27
3.3.2
Uniform Quantizer
28
3.3.3
Optimum Quantizer
29
3.3.4
Logarithmic Quantizer
32
3.3.5
Adaptive Quantizer
33
3.3.6
Differential Quantizer
36
3.4 Vector Quantization
39
3.4.1
Distortion Measures
42
3.4.2
Codebook Design
43
3.4.3
Codebook Types
44
3.4.4
Training, Testing and Codebook Robustness
52
3.5 Summary
54
Bibliography
54
4 Speech Signal Analysis and Modelling
57
4.1 Introduction
57
4.2 Short-Time Spectral Analysis
57
4.2.1
Role of Windows
58
4.3 Linear Predictive Modelling of Speech Signals
65
4.3.1
Source Filter Model of Speech Production
65
4.3.2
Solutions to LPC Analysis
67
4.3.3
Practical Implementation of the LPC Analysis
74
4.4 Pitch Prediction
77
4.4.1
Periodicity in Speech Signals
77
4.4.2
Pitch Predictor (Filter) Formulation
78
4.5 Summary
84
Bibliography
84
5 Efﬁcient LPC Quantization Methods
87
5.1 Introduction
87
5.2 Alternative Representation of LPC
87
5.3 LPC to LSF Transformation
90
5.3.1
Complex Root Method
95
5.3.2
Real Root Method
95
5.3.3
Ratio Filter Method
98
5.3.4
Chebyshev Series Method
100
5.3.5
Adaptive Sequential LMS Method
100
5.4 LSF to LPC Transformation
101
www.allitebooks.com

Contents
ix
5.4.1
Direct Expansion Method
101
5.4.2
LPC Synthesis Filter Method
102
5.5 Properties of LSFs
103
5.6 LSF Quantization
105
5.6.1
Distortion Measures
106
5.6.2
Spectral Distortion
106
5.6.3
Average Spectral Distortion and Outliers
107
5.6.4
MSE Weighting Techniques
107
5.7 Codebook Structures
110
5.7.1
Split Vector Quantization
111
5.7.2
Multi-Stage Vector Quantization
113
5.7.3
Search strategies for MSVQ
114
5.7.4
MSVQ Codebook Training
116
5.8 MSVQ Performance Analysis
117
5.8.1
Codebook Structures
117
5.8.2
Search Techniques
117
5.8.3
Perceptual Weighting Techniques
119
5.9 Inter-frame Correlation
121
5.9.1
LSF Prediction
122
5.9.2
Prediction Order
124
5.9.3
Prediction Factor Estimation
125
5.9.4
Performance Evaluation of MA Prediction
126
5.9.5
Joint Quantization of LSFs
128
5.9.6
Use of MA Prediction in Joint Quantization
129
5.10 Improved LSF Estimation Through Anti-Aliasing Filtering
130
5.10.1 LSF Extraction
131
5.10.2 Advantages of Low-pass Filtering in Moving Average
Prediction
135
5.11 Summary
146
Bibliography
146
6
Pitch Estimation and Voiced–Unvoiced Classiﬁcation of Speech
149
6.1 Introduction
149
6.2 Pitch Estimation Methods
150
6.2.1
Time-Domain PDAs
151
6.2.2
Frequency-Domain PDAs
155
6.2.3
Time- and Frequency-Domain PDAs
158
6.2.4
Pre- and Post-processing Techniques
166
6.3 Voiced–Unvoiced Classiﬁcation
178
6.3.1
Hard-Decision Voicing
178
6.3.2
Soft-Decision Voicing
189
www.allitebooks.com

x
Contents
6.4 Summary
196
Bibliography
197
7 Analysis by Synthesis LPC Coding
199
7.1 Introduction
199
7.2 Generalized AbS Coding
200
7.2.1
Time-Varying Filters
202
7.2.2
Perceptually-based Minimization Procedure
203
7.2.3
Excitation Signal
206
7.2.4
Determination of Optimum Excitation Sequence
208
7.2.5
Characteristics of AbS-LPC Schemes
212
7.3 Code-Excited Linear Predictive Coding
219
7.3.1
LPC Prediction
221
7.3.2
Pitch Prediction
222
7.3.3
Multi-Pulse Excitation
230
7.3.4
Codebook Excitation
238
7.3.5
Joint LTP and Codebook Excitation Computation
252
7.3.6
CELP with Post-Filtering
255
7.4 Summary
258
Bibliography
258
8 Harmonic Speech Coding
261
8.1 Introduction
261
8.2 Sinusoidal Analysis and Synthesis
262
8.3 Parameter Estimation
263
8.3.1
Voicing Determination
264
8.3.2
Harmonic Amplitude Estimation
266
8.4 Common Harmonic Coders
268
8.4.1
Sinusoidal Transform Coding
268
8.4.2
Improved Multi-Band Excitation, INMARSAT-M Version
270
8.4.3
Split-Band Linear Predictive Coding
271
8.5 Summary
275
Bibliography
275
9 Multimode Speech Coding
277
9.1 Introduction
277
9.2 Design Challenges of a Hybrid Coder
280
9.2.1
Reliable Speech Classiﬁcation
281
9.2.2
Phase Synchronization
281
9.3 Summary of Hybrid Coders
281
9.3.1
Prototype Waveform Interpolation Coder
282
www.allitebooks.com

Contents
xi
9.3.2
Combined Harmonic and Waveform Coding at Low Bit-Rates 282
9.3.3
A 4 kb/s Hybrid MELP/CELP Coder
283
9.3.4
Limitations of Existing Hybrid Coders
284
9.4 Synchronized Waveform-Matched Phase Model
285
9.4.1
Extraction of the Pitch Pulse Location
286
9.4.2
Estimation of the Pitch Pulse Shape
292
9.4.3
Synthesis using Generalized Cubic Phase Interpolation
297
9.5 Hybrid Encoder
298
9.5.1
Synchronized Harmonic Excitation
299
9.5.2
Advantages and Disadvantages of SWPM
301
9.5.3
Offset Target Modiﬁcation
304
9.5.4
Onset Harmonic Memory Initialization
308
9.5.5
White Noise Excitation
309
9.6 Speech Classiﬁcation
311
9.6.1
Open-Loop Initial Classiﬁcation
312
9.6.2
Closed-Loop Transition Detection
315
9.6.3
Plosive Detection
318
9.7 Hybrid Decoder
319
9.8 Performance Evaluation
320
9.9 Quantization Issues of Hybrid Coder Parameters
322
9.9.1
Introduction
322
9.9.2
Unvoiced Excitation Quantization
323
9.9.3
Harmonic Excitation Quantization
323
9.9.4
Quantization of ACELP Excitation at Transitions
331
9.10 Variable Bit Rate Coding
331
9.10.1 Transition Quantization with 4 kb/s ACELP
332
9.10.2 Transition Quantization with 6 kb/s ACELP
332
9.10.3 Transition Quantization with 8 kb/s ACELP
333
9.10.4 Comparison
334
9.11 Acoustic Noise and Channel Error Performance
336
9.11.1 Performance under Acoustic Noise
337
9.11.2 Performance under Channel Errors
345
9.11.3 Performance Improvement under Channel Errors
349
9.12 Summary
350
Bibliography
351
10 Voice Activity Detection
357
10.1 Introduction
357
10.2 Standard VAD Methods
360
10.2.1 ITU-T G.729B/G.723.1A VAD
361
www.allitebooks.com

xii
Contents
10.2.2 ETSI GSM-FR/HR/EFR VAD
361
10.2.3 ETSI AMR VAD
362
10.2.4 TIA/EIA IS-127/733 VAD
363
10.2.5 Performance Comparison of VADs
364
10.3 Likelihood-Ratio-Based VAD
368
10.3.1 Analysis and Improvement of the Likelihood Ratio Method
370
10.3.2 Noise Estimation Based on SLR
373
10.3.3 Comparison
373
10.4 Summary
375
Bibliography
375
11 Speech Enhancement
379
11.1 Introduction
379
11.2 Review of STSA-based Speech Enhancement
381
11.2.1 Spectral Subtraction
382
11.2.2 Maximum-likelihood Spectral Amplitude Estimation
384
11.2.3 Wiener Filtering
385
11.2.4 MMSE Spectral Amplitude Estimation
386
11.2.5 Spectral Estimation Based on the Uncertainty of Speech
Presence
387
11.2.6 Comparisons
389
11.2.7 Discussion
392
11.3 Noise Adaptation
402
11.3.1 Hard Decision-based Noise Adaptation
402
11.3.2 Soft Decision-based Noise Adaptation
403
11.3.3 Mixed Decision-based Noise Adaptation
403
11.3.4 Comparisons
404
11.4 Echo Cancellation
406
11.4.1 Digital Echo Canceller Set-up
411
11.4.2 Echo Cancellation Formulation
413
11.4.3 Improved Performance Echo Cancellation
415
11.5 Summary
423
Bibliography
426
Index
429
www.allitebooks.com

Preface
Speech has remained the most desirable medium of communication between
humans. Nevertheless, analogue telecommunication of speech is a cumber-
some and inﬂexible process when transmission power and spectral utilization,
the foremost resources in any communication system, are considered. Dig-
ital transmission of speech is more versatile, providing the opportunity of
achieving lower costs, consistent quality, security and spectral efﬁciency in
the systems that exploit it. The ﬁrst stage in the digitization of speech involves
sampling and quantizations. While the minimum sampling frequency is lim-
ited by the Nyquist criterion, the number of quantiﬁer levels is generally
determined by the degree of faithful reconstruction (quality) of the signal
required at the receiver. For speech transmission systems, these two limita-
tions lead to an initial bit rate of 64 kb/s – the PCM system. Such a high bit
rate restricts the much desired spectral efﬁciency.
The last decade has witnessed the emergence of new ﬁxed and mobile
telecommunication systems for which spectral efﬁciency is a prime mover.
This has fuelled the need to reduce the PCM bit rate of speech signals. Digital
coding of speech and the bit rate reduction process has thus emerged as
an important area of research. This research largely addresses the following
problems:
• Although it is very attractive to reduce the PCM bit rate as much as
possible, it becomes increasingly difﬁcult to maintain acceptable speech
quality as the bit rate falls.
• As the bit rate falls, acceptable speech quality can only be maintained by
employing very complex algorithms, which are difﬁcult to implement in
real-time even with new fast processors with their associated high cost and
power consumption, or by incurring excessive delay, which may create
echo control problems elsewhere in the system.
• In order to achieve low bit rates, parameters of a speech production and/or
perception model are encoded and transmitted. These parameters are
however extremely sensitive to channel corruption. On the other hand,
the systems in which these speech coders are needed typically operate

xiv
Preface
on highly degraded channels, raising the acute problem of maintaining
acceptable speech quality from sensitive speech parameters even in bad
channel conditions. Moreover, when estimating these parameters from
the input, speech contaminated by the environmental noise typical of
mobile/wireless communication systems can cause signiﬁcant degradation
of speech quality.
These problems are by no means insurmountable. The advent of faster and
more reliable Digital Signal Processor (DSP) chips has made possible the easy
real-time implementation of highly complex algorithms. Their sophistication
is also exploited in the implementation of more effective echo control, back-
ground noise suppression, equalization and forward error control systems.
The design of an optimum system is thus mainly a trading-off process of many
factors which affect the overall quality of service provided at a reasonable
cost.
This book presents some existing chapters from the ﬁrst edition, as well
as chapters on new speech processing and coding techniques. In order
to lay the foundation of speech coding technology, it reviews sampling,
quantizations and then the basic nature of speech signals, and the theory and
tools applied in speech coding. The rest of the material presented has been
drawn from recent postgraduate research and graduate teaching activities
within the Multimedia Communications Research Group of the Centre for
Communication Systems Research (CCSR), a teaching and research centre at
the University of Surrey. Most of the material thus represents state-of-the-art
thinking in this technology. It is suitable for both graduate and postgraduate
teaching. It is hoped that the book will also be useful to research and
development engineers for whom the hands-on approach to the base band
design of low bit-rate ﬁxed and mobile communication systems will prove
attractive.
Ahmet Kondoz

Acknowledgements
I would like to thank Doctors Y. D. Cho, S. Villette, N. Katugampala and
K. Al-Naimi for making available work in their PhDs during the preparation
of this manuscript.

1
Introduction
Although data links are increasing in bandwidth and are becoming faster,
speech communication is still the most dominant and common service in
telecommunication networks. The fact that commercial and private usage of
telephony in its various forms (especially wireless) continues to grow even
a century after its ﬁrst inception is obvious proof of its popularity as a form
of communication. This popularity is expected to remain steady for the fore-
seeable future. The traditional plain analogue system has served telephony
systems remarkably well considering its technological simplicity. However,
modern information technology requirements have introduced the need for
a more robust and ﬂexible alternative to the analogue systems. Although the
encoding of speech other than straight conversion to an analogue signal has
been studied and employed for decades, it is only in the last 20 to 30 years
that it has really taken on signiﬁcant prominence. This is a direct result of
many factors, including the introduction of many new application areas.
The attractions of digitally-encoded speech are obvious. As speech is con-
densed to a binary sequence, all of the advantages offered by digital systems
are available for exploitation. These include the ease of regeneration and
signalling, ﬂexibility, security, and integration into the evolving new wire-
less systems. Although digitally-encoded speech possesses many advantages
over its analogue counterpart, it nevertheless requires extra bandwidth for
transmission if it is directly applied (without compression). The 64 kb/s
Log-PCM and 32 kb/s ADPCM systems which have served the many early
generations of digital systems well over the years have therefore been found
to be inadequate in terms of spectrum efﬁciency when applied to the new,
bandwidth limited, communication systems, e.g. satellite communications,
digital mobile radio systems, and private networks. In these and other sys-
tems, the bandwidth and power available is severely restricted, hence signal
compression is vital. For digitized speech, the signal compression is achieved
via elaborate digital signal p rocessing techniques that are f acilitated by the
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

2
Introduction
rapid improvement in digital hardware which has enabled the use of sophis-
ticated digital signal processing techniques that were not feasible before. In
response to the requirement for speech compression, feverish research activ-
ity has been pursued in all of the main research centres and, as a result, many
different strategies have been developed for suitably compressing speech for
bandwidth-restricted applications. During the last two decades, these efforts
have begun to bear fruit. The use of low bit-rate speech coders has been
standardized in many international, continental and national communication
systems. In addition, there are a number of private network operators who
use low bit-rate speech coders for speciﬁc applications.
The speech coding technology has gone through a number of phases starting
with the development and deployment of PCM and ADPCM systems. This
was followed by the development of good quality medium to low bit-rate
coders covering the range from 16 kb/s to 8 kb/s. At the same time, very
low bit-rate coders operating at around 2.4 kb/s produced better quality
synthetic speech at the expense of higher complexity. The latest trend in
speech coding is targeting the range from about 6 kb/s down to 2 kb/s by
using speech-speciﬁc coders, which rely heavily on the extraction of speech-
speciﬁc information from the input source. However, as the main applications
of the low to very low bit-rate coders are in the area of mobile communication
systems, where there may be signiﬁcant levels of background noise, the
accurate determination of the speech parameters becomes more difﬁcult.
Therefore the use of active noise suppression as a preprocessor to low bit-rate
speech coding is becoming popular.
In addition to the required low bit-rate for spectral efﬁciency, the cost
and power requirements of speech encoder/decoder hardware are very
important. In wireless personal communication systems, where hand-held
telephones are used, the battery consumption, cost and size of the portable
equipment have to be reasonable in order to make the product widely
acceptable.
In this book an attempt is made to cover many important aspects of low bit-
rate speech coding. In Chapter 2, the background to speech coding, including
the existing standards, is discussed. In Chapter 3, after brieﬂy reviewing the
sampling theorem, scalar and vector quantization schemes are discussed and
formulated. In addition, various quantization types which are used in the
remainder of this book are described.
In Chapter 4, speech analysis and modelling tools are described. After
discussing the effects of windowing on the short-time Fourier transform
of speech, extensive treatment of short-term linear prediction of speech is
given. This is then followed by long-term prediction of speech. Finally,
pitch detection methods, which are very important in speech vocoders, are
discussed.

Introduction
3
It is very important that the quantization of the linear prediction coefﬁcients
(LPC) of low bit-rate speech coders is performed efﬁciently both in terms of
bit rate and sensitivity to channel errors. Hence, in Chapter 5, efﬁcient quan-
tization schemes of LPC parameters in the form of Line Spectral Frequencies
are formulated, tested and compared.
In Chapter 6, more detailed modelling/classiﬁcation of speech is studied.
Various pitch estimation and voiced – unvoiced classiﬁcation techniques are
discussed.
In Chapter 7, after a general discussion of analysis by synthesis LPC coding
schemes, code-excited linear prediction (CELP) is discussed in detail.
In Chapter 8, a brief review harmonic coding techniques is given.
In Chapter 9, a novel hybrid coding method, the integration of CELP and
harmonic coding to form a multi-modal coder, is described.
Chapters 10 and 11 cover the topics of voice activity detection and speech
enhancements methods, respectively.

2
Coding Strategies
and Standards
2.1 Introduction
The invention of Pulse Code Modulation (PCM) in 1938 by Alec H. Reeves
was the beginning of digital speech communications. Unlike the analogue
systems, PCM systems allow perfect signal reconstruction at the repeaters of
the communication systems, which compensate for the attenuation provided
that the channel noise level is insufﬁcient to corrupt the transmitted bit
stream. In the early 1960s, as digital system components became widely
available, PCM was implemented in private and public switched telephone
networks. Today, nearly all of the public switched telephone networks
(PSTN) are based upon PCM, much of it using ﬁbre optic technology which
is particularly suited to the transmission of digital data. The additional
advantages of PCM over analogue transmission include the availability of
sophisticated digital hardware for various other processing, error correction,
encryption, multiplexing, switching, and compression.
The main disadvantage of PCM is that the transmission bandwidth is
greater than that required by the original analogue signal. This is not desirable
when using expensive and bandwidth-restricted channels such as satellite
and cellular mobile radio systems. This has prompted extensive research into
the area of speech coding during the last two decades and as a result of this
intense activity many strategies and approaches have been developed for
speech coding. As these strategies and techniques matured, standardization
followed with speciﬁc application targets. This chapter presents a brief review
of speech coding techniques. Also, the requirements of the current generation
of speech coding standards are discussed. The motivation behind the review
is to highlight the advantages and disadvantages of various techniques. The
success of the different coding techniques is revealed in the description of the
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

6
Coding Strategies and Standards
many coding standards currently in active operation, ranging from 64 kb/s
down to 2.4 kb/s.
2.2 Speech Coding Techniques
Major speech coders have been separated into two classes: waveform approx-
imating coders and parametric coders. Kleijn [1] deﬁnes them as follows:
• Waveform approximating coders: Speech coders producing a recon-
structed signal which converges towards the original signal with decreasing
quantization error.
• Parametric coders: Speech coders producing a reconstructed signal which
does not converge to the original signal with decreasing quantization error.
Typical performance curves for waveform approximating and parametric
speech coders are shown in Figure 2.1. It is worth noting that, in the past,
speech coders were grouped into three classes: waveform coders, vocoders
and hybrid coders. Waveform coders included speech coders, such as PCM
and ADPCM, and vocoders included very low bit-rate synthetic speech
coders. Finally hybrid coders were those speech coders which used both of
these methods, such as CELP, MBE etc. However currently all speech coders
use some form of speech modelling whether their output converges to the
Poor
Good
Fair
Excellent
Quality
Bit rate (kb/s)
4
8
16
32
64
1
2
Waveform approximating coders
Parametric coders
Figure 2.1
Quality vs bit rate for different speech coding techniques

Speech Coding Techniques
7
original (with increasing bit rate) or not. It is therefore more appropriate to
group speech coders into the above two groups as the old waveform coding
terminology is no longer applicable. If required we can associate the name
hybrid coding with coding types that may use more than one speech coding
principle, which is switched in and out according to the input speech signal
characteristics. For example, a waveform approximating coder, such as CELP,
may combine in an advantageous way with a harmonic coder, which uses a
parametric coding method, to form such a hybrid coder.
2.2.1 Parametric Coders
Parametric coders model the speech signal using a set of model parameters.
The extracted parameters at the encoder are quantized and transmitted to the
decoder. The decoder synthesizes speech according to the speciﬁed model.
The speech production model does not account for the quantization noise
or try to preserve the waveform similarity between the synthesized and the
original speech signals. The model parameter estimation may be an open loop
process with no feedback from the quantization or the speech synthesis. These
coders only preserve the features included in the speech production model,
e.g. spectral envelope, pitch and energy contour, etc. The speech quality of
parametric coders do not converge towards the transparent quality of the
original speech with better quantization of model parameters, see Figure 2.1.
This is due to limitations of the speech production model used. Furthermore,
they do not preserve the waveform similarity and the measurement of signal
to noise ratio (SNR) is meaningless, as often the SNR becomes negative when
expressed in dB (as the input and output waveforms may not have phase
alignment). The SNR has no correlation with the synthesized speech quality
and the quality should be assessed subjectively (or perceptually).
Linear Prediction Based Vocoders
Linear Prediction (LP) based vocoders are designed to emulate the human
speech production mechanism [2]. The vocal tract is modelled by a linear
prediction ﬁlter. The glottal pulses and turbulent air ﬂow at the glottis are
modelled by periodic pulses and Gaussian noise respectively, which form
the excitation signal of the linear prediction ﬁlter. The LP ﬁlter coefﬁcients,
signal power, binary voicing decision (i.e. periodic pulses or noise excitation),
and pitch period of the voiced segments are estimated for transmission
to the decoder. The main weakness of LP based vocoders is the binary
voicing decision of the excitation, which fails to model mixed signal types
with both periodic and noisy components. By employing frequency domain
voicing decision techniques, the performance of LP based vocoders can be
improved [3].

8
Coding Strategies and Standards
Harmonic Coders
Harmonic or sinusoidal coding represents the speech signal as a sum of sinu-
soidal components. The model parameters, i.e. the amplitudes, frequencies
and phases of sinusoids, are estimated at regular intervals from the speech
spectrum. The frequency tracks are extracted from the peaks of the speech
spectra, and the amplitudes and frequencies are interpolated in the synthesis
process for smooth evolution [4]. The general sinusoidal model does not
restrict the frequency tracks to be harmonics of the fundamental frequency.
Increasing the parameter extraction rate converges the synthesized speech
waveform towards the original, if the parameters are unquantized. However
at low bit rates the phases are not transmitted and estimated at the decoder,
and the frequency tracks are conﬁned to be harmonics. Therefore point to
point waveform similarity is not preserved.
2.2.2 Waveform-approximating Coders
Waveform coders minimize the error between the synthesized and the origi-
nal speech waveforms. The early waveform coders such as companded Pulse
Code Modulation (PCM) [5] and Adaptive Differential Pulse Code Mod-
ulation (ADPCM) [6] transmit a quantized value for each speech sample.
However ADPCM employs an adaptive pole zero predictor and quantizes
the error signal, with an adaptive quantizer step size. ADPCM predictor
coefﬁcients and the quantizer step size are backward adaptive and updated
at the sampling rate.
The recent waveform-approximating coders based on time domain analysis
by synthesis such as Code Excited Linear Prediction (CELP) [7], explicitly
make use of the vocal tract model and the long term prediction to model
the correlations present in the speech signal. CELP coders buffer the speech
signal and perform block based analysis and transmit the prediction ﬁlter
coefﬁcients along with an index for the excitation vector. They also employ
perceptual weighting so that the quantization noise spectrum is masked by
the signal level.
2.2.3 Hybrid Coding of Speech
Almost all of the existing speech coders apply the same coding principle,
regardless of the widely varying character of the speech signal, i.e. voiced,
unvoiced, mixed, transitions etc. Examples include Adaptive Differential
Pulse Code Modulation (ADPCM) [6], Code Excited Linear Prediction (CELP)
[7, 8], and Improved Multi Band Excitation (IMBE) [9, 10]. When the bit rate
is reduced, the perceived quality of these coders tends to degrade more
for some speech segments while remaining adequate for others. This shows
that the assumed coding principle is not adequate for all speech types.
In order to circumvent this problem, hybrid coders that combine different
www.allitebooks.com

Algorithm Objectives and Requirements
9
coding principles to encode different types of speech segments have been
introduced [11, 12, 13].
A hybrid coder can switch between a set of predeﬁned coding modes.
Hence they are also referred to as multimode coders. A hybrid coder is an
adaptive coder, which can change the coding technique or mode according
to the source, selecting the best mode for the local character of the speech
signal. Network or channel dependent mode decision [14] allows a coder to
adapt to the network load or the channel error performance, by varying the
modes and the bit rate, and changing the relative bit allocation of the source
and channel coding [15].
In source dependent mode decision, the speech classiﬁcation can be based
on ﬁxed or variable length frames. The number of bits allocated for frames of
different modes can be the same or different. The overall bit rate of a hybrid
coder can be ﬁxed or variable. In fact variable rate coding can be seen as an
extension of hybrid coding.
2.3 Algorithm Objectives and Requirements
The design of a particular algorithm is often dictated by the target application.
Therefore, during the design of an algorithm the relative weighting of
the inﬂuencing factors requires careful consideration in order to obtain a
balanced compromise between the often conﬂicting objectives. Some of the
factors which inﬂuence the choice of algorithm for the foreseeable network
applications are listed below.
2.3.1 Quality and Capacity
Speech quality and bit rate are two factors that directly conﬂict with each
other. Lowering the bit rate of the speech coder, i.e. using higher signal
compression, causes degradation of quality to a certain extent (simple para-
metric vocoders). For systems that connect to the Public Switched Telephone
Network (PSTN) and associated systems, the quality requirements are strict
and must conform to constraints and guidelines imposed by the relevant
regulatory bodies, e.g. ITU (previously CCITT). Such systems demand high
quality (toll quality) coding. However, closed systems such as private com-
mercial networks and military systems may compromise the quality to lower
the capacity requirements. Although absolute quality is often speciﬁed, it is
often compromised if other factors are allocated a higher overall rating. For
instance, in a mobile radio system it is the overall average quality that is often
the deciding factor. This average quality takes into account both good and
bad transmission conditions.

10
Coding Strategies and Standards
2.3.2 Coding Delay
The coding delay of a speech transmission system is a factor closely related
to the quality requirements. Coding delay may be algorithmic (the buffering
of speech for analysis), computational (the time taken to process the stored
speech samples) or due to transmission. Only the ﬁrst two concern the speech
coding subsystem, although very often the coding scheme is tailored such that
transmission can be initiated even before the algorithm has completed pro-
cessing all of the information in the analysis frame, e.g. in the pan-European
digital mobile radio system (better known as GSM) [16] the encoder starts
transmission of the spectral parameters as soon as they are available. Again,
for PSTN applications, low delay is essential if the major problem of echo is to
be minimized. For mobile system applications and satellite communication
systems, echo cancellation is employed as substantial propagation delays
already exist. However, in the case of the PSTN where there is very little
delay, extra echo cancellers will be required if coders with long delays are
introduced. The other problem of encoder/decoder delay is the purely sub-
jective annoyance factor. Most low-rate algorithms introduce a substantial
coding delay compared with the standard 64 kb/s PCM system. For instance,
the GSM system’s initial upper limit was 65 ms for a back-to-back conﬁgura-
tion, whereas for the 16 kb/s G.728 speciﬁcation [17], it was a maximum of
5 ms with an objective of 2 ms.
2.3.3 Channel and Background Noise Robustness
For many applications, the speech source coding rate typically occupies only
a fraction of the total channel capacity, the rest being used for forward error
correction (FEC) and signalling. For mobile connections, which suffer greatly
from both random and burst errors, a coding scheme’s built-in tolerance to
channel errors is vital for an acceptable average overall performance, i.e. com-
munication quality. By employing built-in robustness, less FEC can be used
and higher source coding capacity is available to give better speech quality.
This trade-off between speech quality and robustness is often a very difﬁcult
balance to obtain and is a requirement that necessitates consideration from
the beginning of the speech coding algorithm design. For other applications
employing less severe channels, e.g. ﬁbre-optic links, the problems due to
channel errors are reduced signiﬁcantly and robustness can be ignored for
higher clean channel speech quality. This is a major difference between the
wireless mobile systems and those of the ﬁxed link systems.
In addition to the channel noise, coders may need to operate in noisy back-
ground environments. As background noise can degrade the performance of
speech parameter extraction, it is crucial that the coder is designed in such a
way that it can maintain good performance at all times. As well as maintaining
good speech quality under noisy conditions, good quality background noise

Algorithm Objectives and Requirements
11
regeneration by the coder is also an important requirement (unless adaptive
noise cancellation is used before speech coding).
2.3.4 Complexity and Cost
As ever more sophisticated algorithms are devised, the computational com-
plexity is increased. The advent of Digital Signal Processor (DSP) chips [18]
and custom Application Speciﬁc Integrated Circuit (ASIC) chips has enabled
the cost of processing power to be considerably lowered. However, complex-
ity/power consumption, and hence cost, is still a major problem especially in
applications where hardware portability is a prime factor. One technique for
overcoming power consumption whilst also improving channel efﬁciency is
digital speech interpolation (DSI) [16]. DSI exploits the fact that only around
half of speech conversation is actually active speech thus, during inactive
periods, the channel can be used for other purposes, including limiting the
transmitter activity, hence saving power. An important subsystem of DSI is
the voice activity detector (VAD) which must operate efﬁciently and reliably
to ensure that real speech is not mistaken for silence and vice versa. Obvi-
ously, a voice for silence mistake is tolerable, but the opposite can be very
annoying.
2.3.5 Tandem Connection and Transcoding
As it is the end to end speech quality which is important to the end user,
the ability of an algorithm to cope with tandeming with itself or with
another coding system is important. Degradations introduced by tandeming
are usually cumulative, and if an algorithm is heavily dependent on certain
characteristics then severe degradations may result. This is a particularly
urgent unresolved problem with current schemes which employ post-ﬁltering
in the output speech signal [17]. Transcoding into another format, usually
PCM, also degrades the quality slightly and may introduce extra cost.
2.3.6 Voiceband Data Handling
As voice connections are regularly used for transmission of digital data, e.g.
modem, facsimile, and other machine data, an important requirement is an
algorithm’s ability to transmit voiceband data. The waveform statistics and
frequency spectrum of voiceband data signals are quite different from those
of speech, therefore the algorithm must be capable of handling both types.
The consideration of voiceband data handling is often left until the ﬁnal
stages of the algorithm development, which may be a mistake as end users
expect nonvoice information to be adequately transported if the system is
employed in the public network. Most of the latest low bit-rate speech coders
are unable to pass voiceband data due to the fact they are too speech speciﬁc.

12
Coding Strategies and Standards
Other solutions are often used. A very common one is to detect the voiceband
data and use an interface which bypasses the speech encoder/decoder.
2.4 Standard Speech Coders
Standardization is essential in removing the compatibility and conforma-
bility problems of implementations by various manufacturers. It allows for
one manufacturer’s speech coding equipment to work with that of others.
In the following, standard speech coders, mostly developed for speciﬁc
communication systems, are listed and brieﬂy reviewed.
2.4.1 ITU-T Speech Coding Standard
Traditionally the International Telecommunication Union Telecommunica-
tion Standardization Sector (ITU-T, formerly CCITT) has standardized speech
coding methods mainly for PSTN telephony with 3.4 kHz input speech band-
width and 8 kHz sampling frequency, aiming to improve telecommunication
network capacity by means of digital circuit multiplexing. Additionally,
ITU-T has been conducting standardization for wideband speech coders to
support 7 kHz input speech bandwidth with 16 kHz sampling frequency,
mainly for ISDN applications.
In 1972, ITU-T released G.711 [19], an A/µ-Law PCM standard for 64 kb/s
speech coding, which is designed on the basis of logarithmic scaling of
each sampled pulse amplitude before digitization into eight bits. As the
ﬁrst digital telephony system, G.711 has been deployed in various PSTNs
throughout the world. Since then, ITU-T has been actively involved in
standardizing more complex speech coders, referenced as the G.72x series.
ITU-T released G.721, the 32 kb/s adaptive differential pulse code modulation
(ADPCM) coder, followed by the extended version (40/32/24/16 kb/s),
G.726 [20]. The latest ADPCM version, G.726, superseded the former one.
Each ITU-T speech coder except G.723.1 [21] was developed with a view
to halving the bit rate of its predecessor. For example, the G.728 [22] and
G.729 [23] speech coders, ﬁnalized in 1992 and 1996, were recommended at
the rates of 16 kb/s and 8 kb/s, respectively. Additionally, ITU-T released
G.723.1 [21], the 5.3/6.3 kb/s dual-rate speech coder, for video telephony
systems. G.728, G.729, and G.723.1 principles are based on code excited linear
prediction (CELP) technologies. For discontinuous transmission (DTX), ITU-T
released the extended versions of G.729 and G.723.1, called G.729B [24] and
G.723.1A [25], respectively. They are widely used in packet-based voice
communications [26] due to their silence compression schemes. In the past
few years there has been standardization activities at 4 kb/s. Currently there
two coders competing for this standard but the process has been put on
hold at the moment. One coder is based on the CELP model and the other

Standard Speech Coders
13
Table 2.1
ITU-T narrowband speech coding standards
Bit rate
Noise
Delay
Speech coder
(kb/s)
VAD
reduction
(ms)
Quality
Year
G.711 (A/µ-Law PCM)
64
No
No
0
Toll
1972
G.726 (ADPCM)
40/32/24/16
No
No
0.25
Toll
1990
G.728 (LD-CELP)
16
No
No
1.25
Toll
1992
G.729 (CSA-CELP)
8
Yes
No
25
Toll
1996
G.723.1
6.3/5.3
Yes
No
67.5
Toll/
1995
(MP-MLQ/ACELP)
Near-toll
G.4k (to be determined)
4
–
Yes
∼55
Toll
2001
is a hybrid model of CELP and sinusoidal speech coding principles [27, 28].
A summary of the narrowband speech coding standards recommended by
ITU-T is given in Table 2.1.
In addition to the narrowband standards, ITU-T has released two wideband
speech coders, G.722 [29] and G.722.1 [30], targeting mainly multimedia
communications with higher voice quality. G.722 [29] supports three bit rates,
64, 56, and 48 kb/s based on subband ADPCM (SB-ADPCM). It decomposes
the input signals into low and high subbands using the quadrature mirror
ﬁlters, and then quantizes the band-pass ﬁltered signals using ADPCM with
variable step sizes depending on the subband. G.722.1 [30] operates at the
rates of 32 and 24 kb/s and is based on the transform coding technique.
Currently, a new wideband speech coder operating at 13/16/20/24 kb/s is
undergoing standardization.
2.4.2 European Digital Cellular Telephony Standards
With the advent of digital cellular telephony there have been many speech
coding standardization activities by the European Telecommunications Stan-
dards Institute (ETSI). The ﬁrst release by ETSI was the GSM full rate (FR)
speech coder operating at 13 kb/s [31]. Since then, ETSI has standardized
5.6 kb/s GSM half rate (HR) and 12.2 kb/s GSM enhanced full rate (EFR)
speech coders [32, 33]. Following these, another ETSI standardization activity
resulted in a new speech coder, called the adaptive multi-rate (AMR) coder
[34], operating at eight bit rates from 12.2 to 4.75 kb/s (four rates for the
full-rate and four for the half-rate channels). The AMR coder aims to provide
enhanced speech quality based on optimal selection between the source and
channel coding schemes (and rates). Under high radio interference, AMR is
capable of allocating more bits for channel coding at the expense of reduced
source coding rate and vice versa.
The ETSI speech coder standards are also capable of silence compres-
sion by way of voice activity detection [35–38], which facilitates channel

14
Coding Strategies and Standards
Table 2.2
ETSI speech coding standards for GSM mobile communications
Bit rate
Noise
Delay
Speech coder
(kb/s)
VAD
reduction
(ms)
Quality
Year
FR (RPE-LTP)
13
Yes
No
40
Near-toll
1987
HR (VSELP)
5.6
Yes
No
45
Near-toll
1994
EFR (ACELP)
12.2
Yes
No
40
Toll
1998
AMR (ACELP)
12.2/10.2/7.95/
Yes
No
40/45
Toll
1999
7.4/6.7/5.9/
∼Communi-
5.15/4.75
cation
interference reduction as well as battery life time extension for mobile com-
munications. Standard speech coders for European mobile communications
are summarized in Table 2.2.
2.4.3 North American Digital Cellular Telephony Standards
In North America, the Telecommunication Industries Association (TIA) of
the Electronic Industries Association (EIA) has been standardizing mobile
communication based on Code Division Multiple Access (CDMA) and Time
Division Multiple Access (TDMA) technologies used in the USA. TIA/EIA
adopted Qualcomm CELP (QCELP) [39] for Interim Standard-96-A (IS-96-A),
operating at variable bit rates between 8 kb/s and 0.8 kb/s controlled by a
rate determination algorithm. Subsequently, TIA/EIA released IS-127 [40],
the enhanced variable rate coder, which features a novel function for noise
reduction as a preprocessor to the speech compression module. Under noisy
background conditions, noise reduction provides a more comfortable speech
quality by enhancing noisy speech signals. For personal communication
systems, TIA/EIA released IS-733 [41], which operates at variable bit rates
between 14.4 and 1.8 kb/s. For North American TDMA standards, TIA/EIA
released IS-54 and IS-641-A for full rate and enhanced full rate speech coding,
respectively [42, 43]. Standard speech coders for North American mobile
communications are summarized in Table 2.3.
2.4.4 Secure Communication Telephony
Speech coding is a crucial part of a secure communication system, where
voice intelligibility is a major concern in order to deliver the exact voice
commands in an emergency.
Standardization has mainly been organized by the Department of Defense
(DoD) in the USA. The DoD released Federal Standard-1015 (FS-1015) and FS-
1016, called 2.4 kb/s LPC-10e and 4.8 kb/s CELP coders, respectively [44–46].
The DoD also standardized a more recent 2.4 kb/s speech coder [47], based

Standard Speech Coders
15
Table 2.3
TIA/EIA speech coding standards for North American CDMA/TDMA
mobile communications
Bit rate
Noise
Delay
Speech coder
(kb/s)
VAD
reduction
(ms)
Quality
Year
IS-96-A (QCELP)
8.5/4/2/0.8
Yes
No
45
Near-toll
1993
IS-127 (EVRC)
8.5/4/2/0.8
Yes
Yes
45
Toll
1995
IS-733 (QCELP)
14.4/7.2/3.6/1.8
Yes
No
45
Toll
1998
IS-54 (VSELP)
7.95
Yes
No
45
Near-toll
1989
IS-641-A (ACELP)
7.4
Yes
No
45
Toll
1996
Table 2.4
DoD speech coding standards
Bit rate
Noise
Delay
Speech coder
(kb/s)
VAD
reduction
(ms)
Quality
Year
FS-1015 (LPC-10e)
2.4
No
No
115
Intelligible
1984
FS-1016 (CELP)
4.8
No
No
67.5
Communication
1991
DoD 2.4 (MELP)
2.4
No
No
67.5
Communication
1996
STANAG (NATO)
2.4/1.2
No
Yes
>67.5
Communication
2001
2.4/1.2 (MELP)
on the mixed excitation linear prediction (MELP) vocoder [48] which is based
on the sinusoidal speech coding model. The 2.4 kb/s DoD MELP speech
coder gives better speech quality than the 4.8 kb/s FS-1016 coder at half the
capacity. A modiﬁed and improved version of this coder, operating at dual
rates of 2.4/1.2 kb/s and employing a noise preprocessor, has been selected
as the new NATO standard. Parametric coders, such as MELP, have been
widely used in secure communications due to their intelligible speech quality
at very low bit rates. The DoD standard speech coders are summarized in
Table 2.4.
2.4.5 Satellite Telephony
The international maritime satellite corporation (INMARSAT) has adopted
two speech coders for satellite communications. INMARSAT has selected
4.15 kb/s improved multiband excitation (IMBE) [9] for INMARSAT M sys-
tems and 3.6 kb/s advanced multiband excitation (AMBE) vocoders for
INMARSAT Mini-M systems (see Table 2.5).
2.4.6 Selection of a Speech Coder
Selecting the best speech coder for a given application may involve extensive
testing under conditions representative of the target application. In general,
lowering the bit rate results in a reduction in the quality of coded speech.

16
Coding Strategies and Standards
Table 2.5
INMARSAT speech coding standards
Bit rate
Noise
Delay
Speech coder
(kb/s)
VAD
reduction
(ms)
Quality
Year
IMBE
4.15
No
No
120
Communication
1990
AMBE
3.6
No
No
–
–
–
Quality measurements based on SNR can be used to evaluate coders that
preserve the waveform similarity, usually coders operating at bit rates above
16 kb/s. Low bit-rate parametric coders do not preserve the waveform simi-
larity and SNR-based quality measures become meaningless. For parametric
coders, perception-based subjective measures are more reliable. The Mean
Opinion Score (MOS) [49] scale shown in Table 2.6 is a widely-used subjective
quality measure.
Table 2.7 compares some of the most well-known speech coding standards
in terms of their bit rate, algorithmic delay and Mean Opinion Scores and
Figure 2.2 illustrates the performance of those standards in terms of speech
quality against bit rate [50, 51].
Linear PCM at 128 kb/s offers transparent speech quality and its A-law
companded 8 bits/sample (64 kb/s) version (which provides the standard
for the best (narrowband) quality) has a MOS score higher than 4, which
is described as Toll quality. In order to ﬁnd the MOS score for a given
FS1015
G.728
G.711
G.726
Linear PCM
G.729
G.723.1
ITU 4
GSM FR
FS1016
Poor
2
4
8
16
32
64
128
Good
Fair
Excellent
Quality
Bit rate (kb/s)
In-M
New FS 2.4
JDC
JDC/2
IS96
IS54
GSM/2
GSM EFR
Figure 2.2
Performance of telephone band speech coding standards (only the top
four points of the MOS scale have been used)

Standard Speech Coders
17
Table 2.6
Mean Opinion Score (MOS) scale
Grade (MOS)
Subjective opinion
Quality
5 Excellent
Imperceptible
Transparent
4 Good
Perceptible, but not annoying
Toll
3 Fair
Slightly annoying
Communication
2 Poor
Annoying
Synthetic
1 Bad
Very annoying
Bad
Table 2.7
Comparison of telephone band speech coding standards
Standard
Year
Algorithm
Bit rate (kb/s)
MOS∗
Delay+
G.711
1972
Companded PCM
64
4.3
0.125
G.726
1991
VBR-ADPCM
16/24/32/40
toll
0.125
G.728
1994
LD-CELP
16
4
0.625
G.729
1995
CS-ACELP
8
4
15
G.723.1
1995
A/MP-MLQ CELP
5.3/6.3
toll
37.5
ITU 4
–
–
4
toll
25
GSM FR
1989
RPE-LTP
13
3.7
20
GSM EFR
1995
ACELP
12.2
4
20
GSM/2
1994
VSELP
5.6
3.5
24.375
IS54
1989
VSELP
7.95
3.6
20
IS96
1993
Q-CELP
0.8/2/4/8.5
3.5
20
JDC
1990
VSELP
6.7
commun.
20
JDC/2
1993
PSI-CELP
3.45
commun.
40
Inmarsat-M
1990
IMBE
4.15
3.4
78.75
FS1015
1984
LPC-10
2.4
synthetic
112.5
FS1016
1991
CELP
4.8
3
37.5
New FS 2.4
1997
MELP
2.4
3
45.5
∗The MOS ﬁgures are obtained from formal subjective tests using varied test material (from the literature).
These ﬁgures are therefore useful as a guide, but should not be taken as a deﬁnitive indication of codec
performance.
+ Delay is the total algorithmic delay, i.e. the frame length and look ahead, and is given in milliseconds.
coder, extensive listening tests must be conducted. In these tests, as well
as the 64 kb/s PCM reference, other representative coders are also used for
calibration purposes. The cost of extensive listening tests is high and efforts
have been made to produce simpler, less time-consuming, and hence cheaper,
alternatives. These alternatives are based on objective measures with some
subjective meanings. Objective measurements usually involve point to point
comparison of systems under test. In some cases weighting may be used to

18
Coding Strategies and Standards
give priority to some system parameters over others. In early speech coders,
which aimed at reproducing the input speech waveform as output, objective
measurement in the form of signal to quantization noise ratio was used.
Since the bit rate of early speech coders was 16 kb/s or greater (i.e. they
incurred only a small amount of quantization noise) and they did not involve
complicated signal processing algorithms which could change the shape of
the speech waveform, the SNR measures were reasonably accurate. However
at lower bit rates where the noise (the objective difference between the original
input and the synthetic output) increases, the use of signal to quantization
noise ratio may be misleading. Hence there is a need for a better objective
measurement which has a good correlation with the perceptual quality of the
synthetic speech. The ITU standardized a number of these methods, the most
recent of which is P.862 (or Perceptual Evaluation of Speech Quality). In this
standard, various alignments and perceptual measures are used to match the
objective results to fairly accurate subjective MOS scores.
2.5 Summary
Existing speech coders can be divided into three groups: parametric coders,
waveform approximating coders, and hybrid coders. Parametric coders are
not expected to reproduce the original waveform; they reproduce the per-
ception of the original. Waveform approximating coders, on the other hand,
are expected to replicate the input speech waveform as the bit rate increases.
Hybrid coding is a combination of two or more coders of any type for the
best subjective (and perhaps objective) performance at a given bit rate.
The design process of a speech coder involves several trade-offs between
conﬂicting requirements. These requirements include the target bit rate, qual-
ity, delay, complexity, channel error sensitivity, and sending of nonspeech
signals. Various standardization bodies have been involved in speech coder
standardization activities and as a result there have been many standard
speech coders in the last decade. The bit rate of these coders ranges from
16 kb/s down to around 4 kb/s with target applications mainly in cellular
mobile radio. The selection of a speech coder involves expensive testing under
the expected typical operating conditions. The most popular testing method is
subjective listening tests. However, as this is expensive and time-consuming,
there has been some effort to produce simpler yet reliable objective measures.
ITU P.862 is the latest effort in this direction.
Bibliography
[1] W. B. Kleijn and K. K. Paliwal (1995) ‘An introduction to speech coding’,
in Speech coding and synthesis by W. B. Kleijn and K. K. Paliwal (Eds),
pp. 1–47. Amsterdam: Elsevier Science
www.allitebooks.com

Bibliography
19
[2] D. O’Shaughnessy (1987) Speech communication: human and machine. Addi-
son Wesley
[3] I. Atkinson, S. Yeldener, and A. Kondoz (1997) ‘High quality split-band
LPC vocoder operating at low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 1559–62. May 1997. Munich
[4] R. J. McAulay and T. F. Quatieri (1986) ‘Speech analysis/synthesis based
on a sinusoidal representation’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 34(4):744–54.
[5] ITU-T (1972) CCITT Recommendation G.711: Pulse Code Modulation (PCM)
of Voice Frequencies. International Telecommunication Union.
[6] N. S. Jayant and P. Noll (1984) Digital Coding of Waveforms: Principles and
applications to speech and video. New Jersey: Prentice-Hall
[7] B. S. Atal and M. R. Schroeder (1984) ‘Stochastic coding of speech at very
low bit rates’, in Proc. Int. Conf. Comm, pp. 1610–13. Amsterdam
[8] M. Schroeder and B. Atal (1985) ‘Code excited linear prediction (CELP):
high quality speech at very low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 937–40. Tampa, FL
[9] DVSI (1991) INMARSAT-M Voice Codec, Version 1.7. September 1991.
Digital Voice Systems Inc.
[10] J. C. Hardwick and J. S. Lim (1991) ‘The application of the IMBE speech
coder to mobile communications’, in Proc. of Int. Conf. on Acoust., Speech
and Signal Processing, pp. 249–52.
[11] W. B. Kleijn (1993) ‘Encoding speech using prototype waveforms’, in
IEEE Trans. Speech and Audio Processing, 1:386–99.
[12] E. Shlomot, V. Cuperman, and A. Gersho (1998) ‘Combined harmonic
and waveform coding of speech at low bit rates’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing.
[13] J. Stachurski and A. McCree (2000) ‘Combining parametric and
waveform-matching coders for low bit-rate speech coding’, in X European
Signal Processing Conf.
[14] T. Kawashima, V. Sharama, and A. Gersho (1994) ‘Network control of
speech bit rate for enhanced cellular CDMA performance’, in Proc. IEE
Int. Conf. on Commun., 3:1276.
[15] P. Ho, E. Yuen, and V. Cuperman (1994) ‘Variable rate speech and channel
coding for mobile communications’, in Proc. of Vehicular Technology Conf.
[16] J. E. Natvig, S. Hansen, and J. de Brito (1989) ‘Speech processing in the
pan-European digital mobile radio system (GSM): System overview’, in
Proc. of Globecom, Section 29B.
[17] J. H. Chen (1990) ‘High quality 16 kbit/s speech coding with a one-way
delay less than 2 ms’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 453–6.

20
Coding Strategies and Standards
[18] E. Lee (1988) ‘Programmable DSP architectures’, in IEEE ASSP Magazine,
October 1988 and January 1989.
[19] ITU-T (1988) Pulse code modulation (PCM) of voice frequencies, ITU-T Rec.
G.711.
[20] ITU-T (1990) 40, 32, 24, 16 kbit/s adaptive differential pulse code modulation
(ADPCM), ITU-T Rec. G.726.
[21] ITU-T (1996) Dual rate speech coder for multimedia communications trans-
mitting at 5.3 and 6.3 kbit/s, ITU-T Rec. G.723.1.
[22] ITU-T (1992) Coding of speech at 16 kbit/s using low-delay code excited linear
prediction, ITU-T Rec. G.728.
[23] ITU-T (1996) Coding of speech at 8 kbit/s using conjugate-structure algebraic-
code-excited linear prediction (CS-ACELP), ITU-T Rec. G.729.
[24] ITU-T (1996) A silence compression scheme for G.729 optimised for terminals
conforming to ITU-T V.70, ITU-T Rec. G.729 Annex B.
[25] ITU-T (1996) Dual rate speech coder for multimedia communications transmit-
ting at 5.3 and 6.3 kbit/s. Annex A: Silence compression scheme, ITU-T Rec.
G.723.1 Annex A.
[26] O. Hersent, D. Gurle, and J. Petit (2000) IP Telephony: Packet-based multi-
media communications systems. Addison Wesley
[27] J. Thyssen, Y. Gao, A. Benyassine, E. Shylomot, H. -Y. Su, K. Mano, Y.
Hiwasaki, H. Ehara, K. Yasunaga, C. Lamblin, B. Kovest, J. Stegmann,
and H. -G. Kang (2001) ‘A candidate for the ITU-T 4 kbit/s speech coding
standard’, in Proc. of Int. Conf. on Acoust., Speech and Signal Processing.
May 2001. Salt Lake City, UT
[28] J. Stachurski and A. McCree (2000) ‘A 4 kb/s hybrid MELP/CELP coder
with alignment phase encoding and zero phase equalization’, in Proc. of
Int. Conf. on Acoust., Speech and Signal Processing, pp. 1379–82. May 2000.
Istanbul
[29] ITU-T (1988) 7 khz audio-coding within 64 kbit/s, ITU-T Rec. G.722.
[30] ITU-T (1999) Coding at 24 and 32 kbit/s for hands-free operation in systems
with low frame loss, ITU-T Rec. G.722.1
[31] ETSI (1994) Digital cellular telecommunications system (phase 2+); Full rate
speech transcoding, GSM 06.10 (ETS 300 580-2).
[32] ETSI (1997) Digital cellular telecommunications system (phase 2+); Half rate
speech; Half rate speech transcoding, GSM 06.20 v5.1.0 (draft ETSI ETS 300
969).
[33] ETSI (1998) Digital cellular telecommunications system (phase 2); Enhanced
full rate (EFR) speech transcoding, GSM 06.60 v4.1.0 (ETS 301 245), June.
[34] ETSI (1998) Digital cellular telecommunications system (phase 2+); Adaptive
multi-rate (AMR) speech transcoding, GSM 06.90 v7.2.0 (draft ETSI EN 301
704).

Bibliography
21
[35] ETSI (1998) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for full rate speech trafﬁc channels, GSM 06.32 (ETSI
EN 300 965 v7.0.1).
[36] ETSI (1999) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for full rate speech trafﬁc channels, GSM 06.42 (draft
ETSI EN 300 973 v8.0.0).
[37] ETSI (1997) Digital cellular telecommunications system; Voice activity detector
(VAD) for enhanced full rate (EFR) speech trafﬁc channels, GSM 06.82 (ETS
300 730), March.
[38] ETSI (1998) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for adaptive multi-rate (AMR) speech trafﬁc channels,
GSM 06.94 v7.1.1 (ETSI EN 301 708).
[39] P. DeJaco, W. Gardner, and C. Lee (1993) ‘QCELP: The North American
CDMA digital cellular variable rate speech coding standard’, in IEEE
Workshop on Speech Coding for Telecom, pp. 5–6.
[40] TIA/EIA (1997) Enhanced variable rate codec, speech service option 3 for
wideband spread spectrum digital systems, IS-127.
[41] TIA/EIA (1998) High rate speech service option 17 for wideband spread
spectrum communication systems, IS-733.
[42] I. A. Gerson and M. A. Jasiuk (1990) ‘Vector sum excited linear prediction
(VSELP) speech coding at 8 kb/s’, in Proc. of Int. Conf. on Acoust., Speech
and Signal Processing, pp. 461–4. April 1990. Albuquerque, NM, USA
[43] T. Honkanen, J. Vainio, K. Jarvinen, and P. Haavisto (1997) ‘Enhanced full
rate speech coder for IS-136 digital cellular system’, in Proc. of Int. Conf.
on Acoust., Speech and Signal Processing, pp. 731–4. May 1997. Munich
[44] T. E. Tremain (1982) ‘The government standard linear predictive coding
algorithm: LPC-10’, in Speech Technology, 1:40–9.
[45] J. P. Campbell Jr and T. E. Tremain (1986) ‘Voiced/unvoiced classiﬁcation
of speech with applications to the US government LPC-10e algorithm’,
in Proc. of Int. Conf. on Acoust., Speech and Signal Processing, pp. 473–6.
[46] J. P. Campbell, V. C. Welch, and T. E. Tremain (1991) ‘The DoD 4.8 kbps
standard (proposed Federal Standard 1016)’, in Advances in Speech Coding
by B. Atal, V. Cuperman, and A. Gersho (Eds), pp. 121–33. Dordrecht,
Holland: Kluwer Academic
[47] FIPS (1997) Analog to digital conversion of voice by 2,400 bit/second mixed
excitation linear prediction (MELP), Draft. Federal Information Processing
Standards
[48] A. V. McCree and T. P. Barnwell (1995) ‘A mixed excitation LPC vocoder
model for low bit rate speech coding’, in IEEE Trans. Speech and Audio
Processing, 3(4):242–50.
[49] W. Daumer (1982) ‘Subjective evaluation of several efﬁcient speech
coders’, in IEEE Trans. on Communications, 30(4):655–62.

22
Coding Strategies and Standards
[50] R. V. Cox (1995) ‘Speech coding standards’, in Speech coding and synthesis
by W. B. Kleijn and K. K. Paliwal (Eds), pp. 49–78. Amsterdam: Elsevier
Science
[51] W. Wong, R. Mack, B. Cheetham, and X. Sun (1996) ‘Low rate speech
coding for telecommunications’, in BT Technol. J., 14(1):28–43.

3
Sampling and Quantization
3.1 Introduction
In digital communication systems, signal processing tools require the input
source to be digitized before being processed through various stages of the
network. The digitization process consists of two main stages: sampling the
signal and converting the sampled amplitudes into binary (digital) code-
words. The difference between the original analogue amplitudes and the
digitized ones depend on the number of bits used in the conversion. A 16 bit
analogue to digital converter is usually used to sample and digitize the input
analogue speech signal. Having digitized the input speech, the speech coding
algorithms are used to compress the resultant bit rate where various quan-
tizers are used. In this chapter, after a brief review of the sampling process,
quantizers which are used in speech coders are discussed.
3.2 Sampling
As stated above, the digital conversion process can be split into sampling,
which discretizes the continuous time, and quantization, which reduces the
inﬁnite range of the sampled amplitudes to a ﬁnite set of possibilities. The
sampled waveform can be represented by,
s(n) = sa(nT)
−∞< n < ∞
(3.1)
where sa is the analogue waveform, n is the integer sample number and T is the
sampling time (the time difference between any two adjacent samples, which
is determined by the bandwidth or the highest frequency in the input signal).
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

24
Sampling and Quantization
The sampling theorem states that if a signal sa(t) has a band-limited Fourier
transform Sa(jω) given by,
Sa(jω) =
 ∞
−∞
sa(t)e−jωtdt
(3.2)
such that Sa(jω) = 0 for |ω| ≥2πW then the analogue signal can be recon-
structed from its sampled version if T ≤1/2W. W is called the Nyquist
frequency.
The effect of sampling is shown in Figure 3.1. As can be seen from
Figures 3.1b and 3.1c, the band-limited Fourier transform of the analogue
signal which is shown in Figure 3.1a is duplicated at every multiple of the
sampling frequency.
This is because the Fourier transform of the sampled signal is evaluated at
multiples of the sampling frequency which forms the relationship,
S(ejωT) = 1
T
∞

n=−∞
Sa(jω + j2πn/T)
(3.3)
This can also be interpreted by looking into the time domain sampling process
where the input signal is regularly (at every sampling interval) multiplied
Frequency
Frequency
Frequency
2fs
fs
2fs
fs
w
w
w
−w
−w
−w
(c)
(b)
(a)
Under Sampled Signal
Over Sampled Signal
Analog Signal
Magnitude
Magnitude
Magnitude
Figure 3.1
Effects of sampling: (a) original signal spectrum, (b) over sampled signal
spectrum and (c) under sampled signal spectrum

Sampling
25
with a delta function. When converted to the frequency domain, the multi-
plication becomes convolution and the message spectrum is reproduced at
multiples of the sampling frequency.
We can clearly see that if the sampling frequency is less than twice the
Nyquist frequency, the spectra of two adjacent multiples of the sampling
frequencies will overlap. For example, if 1
T = fs < 2W the analogue signal
image centred at 2π/T overlaps into the base band image. The distortion
caused by high frequencies overlapping low frequencies is called aliasing. In
order to avoid aliasing distortion, either the input analogue signal has to be
band-limited to a maximum of half the sampling frequency or the sampling
frequency has to be increased to at least twice the highest frequency in the
analogue signal.
Given the condition 1/T > 2W, the Fourier transform of the sampled
sequence is proportional to the Fourier transform of the analogue signal in
the base band as follows:
S(ejωT) = 1
TSa(jω)
|ω| < π
T
(3.4)
Using the above relationship, the original analogue signal can be obtained
from the sampled sequence using interpolation given by [1],
sa(t) =
∞

n=−∞
sa(nT)sin[π(t −nT)/T]
π(t −nT)/T
(3.5)
which can be written as,
sa(t) =
∞

n=−∞
sa(nT)sinc(φ)
(3.6)
where φ = π(t −nT)/T.
Therefore, if the sampling frequency is at least twice the Nyquist frequency,
the analogue signal can be recovered completely from its sampled version
by adding together sinc functions centred on each sampling point and scaled
by the sampled value of the analogue signal. The sinc(φ) function in the
above equation represents an ideal low pass ﬁlter. In practice, the front
end band limitation before sampling is usually achieved by a low pass
ﬁlter which is less than ideal and may cause aliasing distortion due to its
roll-off characteristics. In order to avoid aliasing distortion, the sampling
frequency is usually chosen to be higher than twice the Nyquist frequency.
In telecommunication networks the analogue speech signal is band-limited
to 300 to 3400 Hz and sampled at 8000 Hz. This same band limitation and
sampling is used throughout this book unless otherwise speciﬁed.

26
Sampling and Quantization
3.3 Scalar Quantization
Quantization converts a continuous-amplitude signal (usually 16 bit, rep-
resented by the digitization process) to a discrete-amplitude signal that is
different from the continuous-amplitude signal by the quantization error
or noise. When each of a set of discrete values is quantized separately the
process is known as scalar quantization. The input–output characteristics of
a uniform scalar quantizer are shown in Figure 3.2.
Each sampled value of the input analogue signal, which has an inﬁnite
range (16 bit digitized), is compared against a ﬁnite set of amplitude values
and the closest value from the ﬁnite set is chosen to represent the amplitude.
The distance between the ﬁnite set of amplitude levels is called the quantizer
step size and is usually represented by . Each discrete amplitude level xi
is represented by a codeword c(n) for transmission purposes. The codeword
c(n) indicates to the de-quantizer, which is usually at the receiver, which
discrete amplitude is to be used.
Assuming all of the discrete amplitude values in the quantizer are repre-
sented by the same number of bits B and the sampling frequency is fs, the
Input
Output
x4
x3
x2
x1
x5
x6
x7
y8
y7
y6
y5
y4
y3
y2
y1
101
000
001
010
100
110
111
011
Figure 3.2
The input–output characteristics of a uniform quantizer

Scalar Quantization
27
channel transmission bit rate is given by,
Tc = Bfs
bits/second
(3.7)
Given a ﬁxed sampling frequency, the only way to reduce the channel bit
rate Tc is by reducing the length of the codeword c(n). However, a reduced
length c(n) means a smaller set of discrete amplitudes separated by larger 
and, hence, larger differences between the analogue and discrete amplitudes
after quantization, which reduces the quality of reconstructed signal. In order
to reduce the bit rate while maintaining good speech quality, various types
of scalar quantizer have been designed and used in practice. The main aim of
a speciﬁc quantizer is to match the input signal characteristics both in terms
of its dynamic range and probability density function.
3.3.1 Quantization Error
When estimating the quantization error, we cannot assume that i = i+n if
the quantizer is not uniform [2]. Therefore, the signal lying in the ith interval,
xi −i
2 ≤s(n) < xi + i
2
(3.8)
is represented by the quantized amplitude xi and the difference between the
input and quantized values is a function of i. The instantaneous squared
error, for the signal lying in the ith interval is (s(n) −xi)2. The mean squared
error of the signal can then be written by including the likelihood of the signal
being in the ith interval as,
E2
i =
 xi+ i
2
xi−i
2
(x −xi)2p(x)dx
(3.9)
where s(n) has been replaced by x for ease of notation and p(x) represents the
probability density function of x. Assuming the step size i is small, enabling
very ﬁne quantization, we can assume that p(x) is ﬂat within the interval
xi −
2 to xi + 
2 . Representing the ﬂat region of p(x) by its value at the centre,
p(xi), the above equation can be written as,
E2
i = p(xi)

i
2
−i
2
y2dy = 3
i
12 p(xi)
(3.10)
The probability of the signal falling in the ith interval is,
i =
 xi+ i
2
xi−i
2
p(x)dx = p(xi)i
(3.11)

28
Sampling and Quantization
The above is true only if the quantization levels are very small and, hence,
p(x) in each interval can be assumed to be uniform. Substituting (3.11) into
(3.10) for p(xi) we get,
E2
i = 2
i
12 i
(3.12)
The total mean squared error is therefore given by,
E2 = 1
12
N

i=1
i2
i
(3.13)
where N is the total number of levels in the quantizer. In the case of a uniform
quantizer where each step size is the same, , the total mean squared error
becomes,
E2 = 2
12
N

i=1
i = 2
12
(3.14)
where we assume that the signal amplitude is always in the quantizer range
and, hence, N
i=1 i = 1.
3.3.2 Uniform Quantizer
The input–output characteristics of a uniform quantizer are shown in
Figure 3.2. As can be seen from its input–output characteristics, all of the
quantizer intervals (steps) are the same width. A uniform quantizer can be
deﬁned by two parameters: the number of quantizer levels and the quantizer
step size . The number of levels is generally chosen to be of the form 2B,
to make the most efﬁcient use of B bit binary codewords.  and B must be
chosen together to cover the range of input samples. Assuming |x| ≤Xmax
and that the probability density function of x is symmetrical, then,
2Xmax = 2B
(3.15)
From the above equation it is easily seen that once the number of bits to be
used, B, is known, then the step size, , can be calculated by,
 = 2Xmax
2B
(3.16)
The quantization error eq(n) is bounded by,
−
2 ≤eq(n) ≤
2
(3.17)
www.allitebooks.com

Scalar Quantization
29
In a uniform quantizer, the only way to reduce the quantization error is
by increasing the number of bits. When a uniform quantizer is used, it is
assumed that the input signal has a uniform probability density function
varying between ±Xmax with a constant height of
1
2Xmax . From this, the power
of the input signal can be written as,
Px =
 Xmax
−Xmax
x2p(x)dx = X2
max
3
(3.18)
Using the result of (3.14), the signal to noise ratio can be written as,
SNR = Px
Pn
= X2
max/3
2/12
(3.19)
Substituting (3.16) for  we get,
SNR = Px
Pn
= 22B
(3.20)
Taking the log,
SNR(dB) = 10 log10(22B) = 20B log10(2) = 6.02B dB
(3.21)
The above result is useful both in determining the number of bits needed in
the quantizer for certain signal to quantization noise ratio and in estimating
the performance of a uniform quantizer for a given bit rate.
3.3.3 Optimum Quantizer
When choosing the levels of a quantizer, positioning of these levels has to be
selected so that the quantization error is minimized. In order to maximize the
ratio of signal to quantization noise for a given number of bits per sample,
levels of the quantizer must be selected to match the probability density
function of the signal to be quantized. This is because speech-like signals
do not have a uniform probability density function, and the probability of
smaller amplitudes occurring is much higher than that of large amplitudes.
Consequently, to cover the signal dynamic range as accurately as possible,
the optimum quantizer should have quantization levels with nonuniform
spacing. The input–output characteristics of a typical nonuniform quantizer
where the step size of the quantizer intervals is increasing for higher input
signal values is shown in Figure 3.3. The noise contribution of each interval
depends on the probability of the signal falling into a certain quantization
interval. The nonuniform spacing of the quantization levels is equivalent to
a nonlinear compressor C(x) followed by a uniform quantizer. The nonlinear

30
Sampling and Quantization
Input
Output
x4
x5
y5
y4
x1
x2
x3
x6
x7
y3
y2
y8
y7
y6
y1
100
110
111
101
001
000
010
011
Figure 3.3
The input–output characteristics of a nonuniform quantizer
compressor, C(x), compresses the input samples depending on their statistical
properties. In other words, the less likely higher sample values are compressed
more than the more likely low amplitude samples. The compressed samples
are then quantized using a uniform quantizer. The effect of compression
is reversed at the receiver by applying the inverse C−1(x) expansion to the
de-quantized samples. The compression and expansion processes do not
introduce any signal distortions.
It is quite important to select the best compression–expansion combination
for a given input signal probability density function. Panter and Dite [3]
used analysis based on the assumption that the quantization is sufﬁciently
ﬁne and that the amplitude probability density function of the input samples
is constant within the quantization intervals. Their results show signiﬁcant
improvement in the signal to noise ratio over uniform quantization if the
input samples have a peak to root mean squared (rms) ratio greater than 4.
In designing an optimum quantizer, Max [4] discovered how to optimally
choose the output levels for nonuniform input quantizer levels. His analysis
required prior knowledge of the probability density function together with the

Scalar Quantization
31
Table 3.1
Max quantizer input and output levels for 1, 2, 3, 4, and 5 bit quantizers
Max quantizer thresholds
1 bit
2 bit
3 bit
4 bit
5 bit
i/p
o/p
i/p
o/p
i/p
o/p
i/p
o/p
i/p
o/p
0.0000
0.7980
0.0000
0.4528
0.0000
0.2451
0.0000
0.1284
0.0000
0.0659
0.9816
1.5100
0.5006
0.7560
0.2582
0.3881
0.1320
0.1981
1.0500
1.3440
0.5224
0.6568
0.2648
0.3314
1.7480
2.1520
0.7996
0.9424
0.3991
0.4668
1.0990
1.2560
0.5359
0.6050
1.4370
1.6180
0.6761
0.7473
1.8440
2.0690
0.8210
0.8947
2.4010
2.7330
0.9718
1.0490
1.1300
1.2120
1.2990
1.3870
1.4820
1.5770
1.6820
1.7880
1.9080
2.0290
2.1740
2.3190
2.5050
2.6920
2.9770
3.2630
variance, σ 2
x , of the input signal but made no assumption of ﬁne quantization.
The quantizer input–output threshold values for 1–5 bit Max quantizers are
tabulated in Table 3.1 [4]. The quantizers in Table 3.1 are for a unit variance
signal with a normal probability density function. Each quantizer has the
same threshold values in the corresponding negative side of the quantizer.
Nonuniform quantization is advantageous in speech coding, both in coarse
and ﬁne quantization cases, for two reasons. Firstly, a nonuniform quantizer
matches the speech probability density function better and hence produces
higher signal to noise ratio than a uniform quantizer. Secondly, lower ampli-
tudes, which contribute more to the intelligibility of speech, are quantized
more accurately in a nonuniform quantizer.
In speech coding, Max’s quantizer [4] is widely used to normalize the
input samples to unit variance, which guarantees the input dynamic range. In
many other cases, speciﬁc nonuniform quantizers are designed by optimizing
the quantizer intervals using a large number of samples of the signal to be
quantized. Although, these speciﬁc quantizers are not generally applicable,
they give the best performance for a given signal with a given probability
density function and variance. In cases where the variance of the signal has
a large dynamic range, the variance of the signal is transmitted separately at

32
Sampling and Quantization
known time intervals enabling a unit variance nonuniform quantizer to be
used. These quantizers are called forward adaptive nonuniform quantizers.
3.3.4 Logarithmic Quantizer
As was discussed above, an optimum quantizer is advantageous if the
dynamic range (or variance) of the input signal is ﬁxed to a small known
range. However, the performance of such a quantizer deteriorates rapidly
as the power of the signal moves away from the value that the quantizer
is designed for. Although, this can be controlled by normalizing the input
signal to unit variance, this process requires the transmission of the signal
variance at known time intervals for correct scaling of the de-quantized signal
amplitudes.
In order to cater for the wide dynamic range of the input speech signal,
Cattermole [2] suggested two companding laws called A-Law and µ-Law
Pulse Code Modulation (PCM). In both schemes, the signal to quantization
noise performance can be very close to that of a uniform quantizer, but their
performances do not change signiﬁcantly with changing signal variance and
remain relatively constant over a wide range of input speech levels. When
compared with uniform quantizers, companded quantizers require fewer
bits per input sample for a speciﬁed signal dynamic range and signal to
quantization noise ratio. In a companding quantizer, quantizer levels are
closely spaced for small amplitudes which progressively increase as the
input signal range increases. This ensures that, when quantizing speech
signals where the probability density function is zero mean and maximum
at the origin, the frequently occurring small amplitudes are more accurately
quantized than the less frequent large amplitudes, achieving a signiﬁcantly
better performance than a uniform quantizer.
The A-Law compression is deﬁned by:
ALaw(x) =
Ax
1 + log10(A)
for 0 ≤x ≤1
A
(3.22)
ALaw(x) = 1 + log10(Ax)
1 + log10(A)
for 1
A ≤x ≤1
(3.23)
where A is the compression parameter with typical values of 86 for 7 bit (North
American) PCM and 87.56 for 8 bit (European) PCM speech quantizers.
The µ-Law compression on the other hand is deﬁned by:
µLaw(x) = sign(x)
Vo log10

1 + µ|x|
Vo

log10[1 + µ]
(3.24)

Scalar Quantization
33
where Vo is given by Vo = Lσx in which L is the loading factor and σx is the
rms value of the input speech signal.
A typical value of the compression factor µ is 255. The above expressions
show that the A-Law is a combination of a logarithmic curve for large ampli-
tudes and a linear curve for small amplitudes. The µ-Law on the other hand is
not exactly linear or logarithmic in any range but it is approximately linear for
small amplitudes and logarithmic for large amplitudes. A comparison made
in [5] between a µ-Law quantizer and an optimum quantizer showed that
the optimum quantizer can be as much as 4 dB better. However, an optimum
quantizer may have more background noise when the channel is idle and
its dynamic range is limited to a smaller input signal range. For these two
reasons, logarithmic quantizers are usually preferred.
3.3.5 Adaptive Quantizer
As we have seen from the already discussed quantization schemes, the
dynamic range of the input signal plays a crucial role in determining the
performance of a quantizer. Although, the probability density function of
speech can easily be estimated and used in a quantizer design process, the
variations in its dynamic range, which can be as much as 30 dB, reduces
the performance of any quantizer. This can be overcome by controlling the
dynamic range of the input signal. As was brieﬂy mentioned earlier, one way
of achieving this is by estimating the variance of the speech segment prior
to quantization and hence, adjusting the quantizer levels accordingly. The
adjustment of the quantizer levels is equivalent to designing the quantizer
for unit variance and normalizing the input signal before quantization. This
is called forward adaptation. A forward adaptive quantizer block diagram
is shown in Figure 3.4. Assuming the speech is stationary during K samples,
the rms is given by:
σx =



 1
K
K

n=1
x(n)2
(3.25)
where the speech samples in the block are represented by x(n) and mean is
assumed to be zero. However, the choice of block length K is very important
because the probability density function of the normalized input signal can
be affected by K. As K increases the probability density of the normalized
speech signal changes from Gaussian (K ≤128) to Laplacian (K > 512 ) [6].
This method requires the transmission of the speech block variances to the
de-quantizer for correct signal amplitude adjustment. In order to make the
normalization and de-normalization compatible, a quantized version of the
speech rms, σx, is used at both the quantizer and the de-quantizer.

34
Sampling and Quantization
.
.
^
σx
^
^
σx
^
Estimate
x(n)
^
xn(n)
xn(n)
x(n)
De-quantizer
De-quantizer
Quantizer
Quantize
Quantizer
σx
Buffer
σx
Figure 3.4
Block diagram of a forward adaptive quantizer
Another adaptation scheme which does not require transmission of the
speech variance to the de-quantizer is called backward adaptation. Here,
before quantizing each sample, the rms of the input signal is estimated from
N previously quantized samples. Thus, the normalizing factor for the nth
sample is:
σx(n) =



a1
N
N

i=1
ˆx2(n −i)
(3.26)
where ˆx represents the quantized values of the past samples and a1 is a tuning
factor [6].
It has been shown [7] that for a band-limited stationary zero-mean Gaussian
input, as the period N increases, the obtained signal to noise ratio tends to an
asymptotic maximum. However, N must be such that the power of the signal
is fairly constant during the samples of estimation. On average, the backward
adaptive quantizer has 3–5 dB more signal to noise ratio compared with a
logarithmic quantizer. A block diagram of a backward adaptive quantizer is
shown in Figure 3.5.
An adaptation scheme called one word memory [8] has also been suggested.
It looks at only one previously quantized sample and either expands or
compresses the quantizer intervals as shown in Figure 3.6. Thus at the
(n + 1)th sample the value of the quantizer step size  is:
n+1 = nMi(|ˆx(n)|)
(3.27)
where, Mi is one of i ﬁxed coefﬁcients corresponding to quantizer levels
which control the expansion–compression processes.

Scalar Quantization
35
.
.
σx
x(n)
^
σx
^
^
Estimate
xn(n)
x(n)
xn(n)
Estimate
Quantizer
Quantizer
De-quantizer
De-quantizer
Figure 3.5
Block diagram of a backward adaptive quantizer
^
^
Mi
Fixed
Select
.
.
Mi
Select
Fixed
x(n)
^xn(n)
x(n)
xn(n)
Quantizer
Quantizer
De-quantizer
De-quantizer
Figure 3.6
Block diagram of one word memory (Jayant) quantizer
For large quantized previous samples, multiplier values are greater than
one and for small previously quantized samples multiplier values are less
than one. A typical set of step size multiplier values for 2, 3 and 4 bit quantizers
are shown in Table 3.2.

36
Sampling and Quantization
Table 3.2
Step size multiplier values for 2,
3, and 4 bit quantizers [9]
Adaptation multiplier values
Previous
o/p levels
2 bit
3 bit
4 bit
L1
0.60
0.85
0.80
L2
2.20
1.00
0.80
L3
1.00
0.80
L4
1.50
0.80
L5
1.20
L6
1.60
L7
2.00
L8
2.40
The recommended step size multiplier values [9] do not, in general, consti-
tute critical target values. As can be seen from Table 3.2 [9], the middle values
are fairly constant. What is critical, however, is that the step size increase
should be more rapid than its decrease. This is very important for preventing
quantizer overload.
3.3.6 Differential Quantizer
In a differential quantizer, the ﬁnal quantized signal, r(n) is the difference
between the input samples x(n) and their estimates xp(n).
r(n) = x(n) −xp(n)
(3.28)
and
xp(n) =
p

k=1
ˆx(n −k)ak
(3.29)
where ak is the weighting used for the previously quantized (n −k)th sample
and p is the number of previously quantized samples considered in the
estimation process.
The reason for this preprocessing stage to form the prediction residual
(prediction error signal) before quantization is that, in speech signals, there is
a strong correlation between adjacent samples and, hence, by removing some
of the redundancies that speech signals possess, the signal variance is reduced
before quantization. This reduces the quantization noise by employing a
smaller quantizer step size . Block diagrams of typical adaptive differential
quantizers are shown in Figures 3.7 and 3.8.

Scalar Quantization
37
+
Predict
r(n)
^r(n)
Predict
x(n)
x(n)
x(n)
r(n)
xp(n)
x(n)
xp(n)
+
+
+
−
Quantizer
Quantizer
De-quantizer
De-quantizer
^
^
Figure 3.7
Block diagram of a backward adaptive differential quantizer
+
r(n)
^
^
r(n)
x(n)
Estimate
Predictor
Parameters
Predictor
^ak
ak
Parameters
Buffer
Linear
Predictor
r(n)
ak
De-quantize
Predictor
Parameters
De-quantizer
De-quantizer
Quantize
Quantizer
Quantizer
−
+
+
xp(n)
x(n)
+
Linear
Predictor
^
^
xp(n)
Figure 3.8
Block diagram of a forward adaptive differential quantizer

38
Sampling and Quantization
In order to show the advantage of a differential quantizer over a nondif-
ferential quantizer, consider the following example: Assume that K input
samples are to be quantized with a nondifferential quantizer with a total of
K.B1 bits. Consider also the same K samples are to be differentially quantized,
in which case K error samples ei are quantized to B2 bits/sample accuracy.
In a differential quantizer, the weighting coefﬁcients ak can be calculated
using backward or forward techniques as shown in Figures 3.7 and 3.8. When
backward estimation of the ak parameters is used, the quantizer does not
need to send extra information to the de-quantizer. However, in the case of
forward estimation of the ak parameters, the differential quantizer would also
require K.B3 bits to transmit the ak parameters to the de-quantizer for correct
recovery of the quantized signal. As the correlation between the input speech
samples is usually high, the variance of the error signal to be quantized by
the differential quantizer is much smaller than that of the original speech
samples. Therefore, for the same accuracy of quantization, B2 < B1 and in
general B3 ≪B2 which means K.B1 > K(B2 + B3). This shows that the main
advantage of a differential over a nondifferential quantizer is due to the
reduction in the speech dynamic range to be quantized.
The performance of a differential quantizer can be approximately deﬁned
by its prediction gain (the amount of signal reduction before quantization)
and the performance of the residual error quantizer. Assuming that the
same type of quantizer is used for both the differential and nondifferential
quantization schemes, the difference in performance will depend on the
accuracy of the predictor. For simplicity, if we assume a predictor depth of 1,
and ˆx(n −1) ≃x(n −1) the residual error signal is obtained as,
r(n) = x(n) −ax(n −1)
(3.30)
where a is the weighting coefﬁcient used on the previous sample to predict
the current sample. The squared error is then given by,
r2(n) = [x(n) −ax(n −1)]2
(3.31)
or,
r2(n) = x2(n) + a2x2(n −1) −2ax(n)x(n −1)
(3.32)
Assuming, a is updated every N samples,
N

n=1
r2(n) =
N

n=1
x2(n) +
N

n=1
a2x2(n −1) −2a
N

n=1
x(n)x(n −1)
(3.33)
which can simply be written as,
σ 2
r = σ 2
x + a2σ 2
x −2a
N

n=1
x(n)x(n −1)
(3.34)
www.allitebooks.com

Vector Quantization
39
Substituting ρ =
N
n=1 x(n)x(n−1)
N
n=1 x2(n)
(ﬁrst order normalized autocorrelation coefﬁ-
cient) in (3.34) gives,
σ 2
r = σ 2
x + a2σ 2
x −2aσ 2
x ρ
(3.35)
The prediction gain Gp is then found as,
Gp = σ 2
x
σ 2r
=
1
1 + a2 −2aρ
(3.36)
To maximize the prediction gain, the denominator of equation (3.36) should
be minimized with respect to a, hence,
∂(1 + a2 −2aρ)
∂a
= 0 = (0 + 2a −2ρ)
(3.37)
which gives,
a = ρ
(3.38)
Substituting a = ρ in (3.36)
Gp =
1
1 + ρ2 −2ρρ =
1
1 −ρ2
(3.39)
The above result shows that if the correlation between the adjacent samples
is high, then a differential quantizer will perform signiﬁcantly better than a
nondifferential quantizer. In fact, if the signal to be quantized is a nonvarying
DC signal, where ρ = 1, the gain of the prediction process will be inﬁnite, i.e.
no residual error will be left and, hence, no residual information will need to
be transmitted. A typical ρ for speech is between 0.8 and 0.9 which may result
in 4–7 dB signal reduction before quantization, hence achieving signiﬁcant
increase in quantization performance.
3.4 Vector Quantization
When a set of discrete-time amplitude values is quantized jointly as a single
vector, the process is known as vector quantization (VQ), also known as block
quantization or pattern-matching quantization. A block diagram of a simple
vector quantizer is shown in Figure 3.9.
If we assume x = [x1, x2, . . . ., xN]T is an N dimensional vector with real-
valued, continuous-amplitude (short or ﬂoat representation is assumed to
be continuous amplitude) randomly varying components xk, 1 ≤k ≤N (the

40
Sampling and Quantization
Vector
Matching
index i
Input Vector
Buffer
Codebook
Y
x(n)
x
yi
Figure 3.9
Block diagram of a simple vector quantizer
superscript T denotes transpose in vector quantization), this vector is matched
with another real-valued, discrete-amplitude, N dimensional vector y. Hence,
x is quantized as y, and y is used to represent x. Usually, y is chosen from
a ﬁnite set of values Y = yi, 1 ≤i ≤L, where yi = [yi1, yi2, . . . ., yiN]T. The set
Y is called the codebook or reference templates where L is the size of the
codebook, and yi are the codebook vectors. The size of the codebook may be
considered to be equivalent to the number of levels in a scalar quantizer. In
order to design such a codebook, N dimensional space is partitioned into L
regions or cells Ci, 1 ≤i ≤L and a vector yi is associated with each cell Ci.
The quantizer then assigns the codebook vector yi if x is in Ci,
q(x) = yi
if x ϵ Ci
(3.40)
The codebook design process is also known as training or populating
the codebook. Figure 3.10 shows an example of the partitioning of a two-
dimensional space (N = 2 ) for the purpose of vector quantization. The ﬁlled
region enclosed by the bold lines is the cell Ci. During vector quantization,
any input vector x that lies in the cell Ci is quantized as yi. The other codebook
vectors corresponding to the other cells are shown by dots.
If the vector dimension, N, equals one vector quantization reduces to scalar
quantization. Scalar quantization has the special property that whilst cells
may have different sizes (step sizes) they all have the same shape. In vector
quantization, however, cells may have different shapes which gives vector
quantization an advantage over scalar quantization.
When x is quantized as y, a quantization error results and, to measure
the performance of a speciﬁc codebook, an overall distortion measure D is
deﬁned as,
D = 1
M
M

i=1
di[x, y]
(3.41)

Vector Quantization
41
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Cell Ci
.
X1
X2
.
.
Figure 3.10
Partitioning of a two-dimensional space into 18 cells
where di[x, y] is the distortion due to the ith vector in the database given by,
di[x, y] = 1
N
N

k=1
d[xik, ymk]
(3.42)
where M is the number of vectors in the database and ym is the quantized
version of xi. For transmission purposes, each vector yi is encoded using
a codeword of binary digits of length Bi bits. The transmission rate T is
given by,
T = BFc
bits/second
(3.43)
where,
B = 1
M
M

i=1
Bi
bits/vector
(3.44)
is the average codeword length (usually B = Bi), Bi is the number of bits
used to encode vector yi and Fc is the number of codewords transmitted per

42
Sampling and Quantization
second. The average number of bits per vector dimension (sample) is,
R = B
N
bits/sample
(3.45)
When designing a compression system, one tries to design a quantizer
in which the distortion between the original and the quantized vectors is
minimized for a given digital transmission rate. Therefore, during the design
of a quantizer it is important to decide which type of distortion measure is
likely to minimize the subjective distortion.
3.4.1 Distortion Measures
A distortion measure should be subjectively relevant, so that the differences in
distortion values can be used to indicate similar differences in speech quality.
However, a few dB decrease in the distortion may be quite perceptible by the
ear in one case but not in another. Whilst objective distortion measures are
necessary and useful tools in the design of speech coding systems, decisions
on the direction for improving coder performance should be made using
subjective quality testing.
Mean Squared Error
The most common distortion measure is the mean squared error (MSE)
deﬁned as,
d[x, y] = 1
N(x −y)(x −y)T = 1
N
N

k=1
[xk −yk]2
(3.46)
The popularity of the MSE is due to its simplicity.
Weighted Mean Squared Error
In the mean squared error method, it is assumed that the distortion con-
tributed by each element of the vector x is weighted equally. In general,
unequal weights can be introduced to render contributions of certain elements
to the distortion more important than others. Hence, a general weighted mean
squared error is deﬁned by,
dw[x, y] = (x −y)W(x −y)T
(3.47)
where W is a positive weighting matrix.
Perceptually Determined Distortion Measures
For high bit rates and hence small distortions, reasonable distortion measures,
including the two mentioned above, perform well with similar performances.

Vector Quantization
43
Furthermore, they correlate well with subjective judgements of speech quality.
However, as the bit rate decreases and distortion increases, simple distortion
measures may not be related to the subjective quality of speech. Since the
main application of vector quantization is expected to be at low bit-rates,
it is very important to develop and use distortion measures that are better
correlated with human auditory behaviour. A number of perceptually based
distortion measures have been developed [10, 11, 12]. Since the main aim is to
produce the highest speech quality possible at a given bit rate, it is essential
to use a distortion measure that correlates well with human perception.
3.4.2 Codebook Design
When designing an L level codebook, N dimensional space is partitioned into
L cells Ci, 1 ≤i ≤L, and each cell Ci is assigned a vector yi. The quantizer
chooses the codebook vector yi if x is in Ci. To optimize a quantizer, the
distortion in equation (3.41) is minimized over all L levels. There are two
necessary conditions for optimality. The ﬁrst condition is that the optimum
quantizer ﬁnds a matching vector for every input vector by minimizing the
distortion criterion. That is, the quantizer chooses the codebook vector that
results in the minimum distortion with respect to x [13].
q(x) = yi
if d[x, yi] ≤d[ x, yj],
j ̸= i,
1 ≤j ≤L.
(3.48)
The second necessary condition for optimality is that each codebook vector
yi is optimized to give the minimum average distortion in cell Ci.
Di = E{[d(x, yi)| xϵCi]} =

xϵCi
d[x, yi]p(x)dx
(3.49)
where p(x) is the probability density function of vectors that result in the
quantized vector yi in cell (cluster) Ci.
Vector yi is called the centroid of the cell Ci. Optimization of the centroid
of a particular cell depends on the deﬁnition of the distortion measure. For
either the mean squared error or the weighted mean squared error, distortion
in each cell is minimized by,
yin = 1
Mi
Mi

k=1
xkn
xϵCi
(3.50)
where yin {n = 1, 2, . . . , N} is the nth element of the centroid yi of the cluster Ci.
That is, yi is simply the sample mean of all the training vectors Mi contained
in cell Ci. One of the most popular methods for codebook design is an iterative
clustering algorithm known as the K-means algorithm [13] (also known as

44
Sampling and Quantization
Lloyd’s algorithm [14]). The algorithm divides the set of training vectors into
L clusters Ci in such a way that the two necessary conditions for optimality
are satisﬁed.
K-means Algorithm
Given that m is the iteration index and Cim is the ith cluster at iteration m with
yim its centroid:
1. Initialization: Set m = 0 and choose a set of initial codebook vectors yi0,
1 ≤i ≤L.
2. Classiﬁcation: Partition the set of training vectors xn, 1 ≤n ≤M, into the
clusters Ci by the nearest neighbour rule,
xϵCim
if
d[x, yim] ≤d[x, yjm]
for all
j ̸= i.
3. Codebook updating: m →m + 1. Update the codebook vector of every
cluster by computing the centroid of training vectors in each cluster.
4. Termination test: If the decrease in the overall distortion at iteration m
relative to m −1 is below a certain threshold, stop; otherwise go to step 2.
Any other reasonable termination test may be used for step 4.
The above algorithm converges to a local optimum [14, 15]. Furthermore,
any such solution is in general not unique [16]. Global optimality may
be achieved approximately by initializing the codebook vectors to different
values and repeating the above algorithm for several sets of initializations and
then choosing the codebook that results in the minimum overall distortion.
3.4.3 Codebook Types
Vector quantization can offer substantial performance over scalar quantiza-
tion at very low bit-rates. However, these advantages are obtained at con-
siderable computational and storage costs. In order to compromise between
the computation and storage costs, and quantizer performance, a number
of codebook types have been developed. Some codebooks are precomputed
and do not change while being used; others may be updated during quanti-
zation. Here, we will brieﬂy explain some of the widely-used codebooks in
speech coding.
Full Search Codebook
A full search codebook is one where during the quantization process each
input vector is compared against all of the candidate vectors in the codebook.
This process is called full search or exhaustive search. The computation and

Vector Quantization
45
storage requirements of a typical full search codebook can be calculated as
follows. If each vector in a full search codebook is represented by B = RN bits
for transmission, then the number of vectors in the codebook is given by,
L = 2B = 2RN
(3.51)
where N is the vector dimension in the codebook. In many applications,
computing the absolute value of the quantization error may not be necessary
as the main concern is to select the best performing vector. So a relative
performance rather than the absolute error is required. It is therefore possible
to compute the similarity rather than the difference between the input vector
and the codebook vectors. Therefore, assuming that the cross-correlation of
the input vector with each of the codebook candidates is computed and
the one resulting in the highest cross-correlation value is selected as the
quantized value of the input vector, the computation cost (assuming that
all the vectors are normalized, as differences in the energy levels will give
misleading cross-correlation values) is given by,
Comfs = N2RN
multiply −add per input vector
(3.52)
From this, we can also calculate the storage required for the codebook
vectors as,
Mfs = NL = N2B = N2RN
locations
(3.53)
It can be seen from the above expressions that the computation and storage
requirements of a full search codebook are exponentially related to the
number of bits in the codewords.
For a 16-bit ﬁxed point processor the storage Mfs in bytes is given by 2×Mfs
and for a 32-bit ﬂoating point implementation, storage is 4 × Mfs. In general,
the storage is deﬁned by the required number of words each corresponding to
a location. For example if N = 10 and R = 1 the number of codebook vectors
L will be 2NR = 1024. The number of multiply–add operations needed will be
N2RN = 10×1024 = 10 240 per input vector. Assuming a sampling frequency
of 8 kHz, the number of vectors per second will be 8000/10 = 800. Therefore,
the computation cost will be 800 × 10 240 = 8.192 × 106 multiply–add per
second. The storage requirement will be N2RN = 10 × 210 = 10 240 words
(locations).
Using the K-means algorithm, a full search codebook can be optimized
(trained) in two possible ways.
• Method 1: The process starts with two initial vectors which may be
chosen randomly or calculated as centroids of the two halves of the
large training database. The K-means algorithm is used to optimize the

46
Sampling and Quantization
initial vectors. After the optimization of each of the two initial vectors
v1 = [v11, v12, v13, . . . , v1N] and v2 = [v21, v22, v23, . . . , v2N] with dimensions
N, each is split into two further vectors as,
v3 = v1 −ε1,
v4 = v1 + ε1,
v5 = v2 −ε2,
v6 = v2 + ε2,
where ε1 = [e11, e12, e13, . . . , e1N] and ε2 = [e21, e22, e23, . . . , e2N]. In most
cases ε1 = ε2.
The vectors from the second stage are again optimized using the K-
means algorithm and split into further vectors and so on until the number
of optimized vectors is equal to the desired number. The optimization
process can also be terminated by comparing the overall quantization
noise performance of the codebook against a threshold.
During the optimization of a full search codebook using the above method,
it is important to check that all of the optimized vectors are in the densely-
populated areas and do not diverge into outer areas where their use will
be wasted. In such cases the perturbation vector ε is modiﬁed to change
the direction of the resultant vector.
• Method 2: The second method of optimization starts with randomly-
selected vectors from the training database. The number of initial vectors is
larger than the ﬁnal desired number of vectors in the codebook. Using the K-
means algorithm these vectors are optimized. After the ﬁrst optimization
process, the least used vectors are discarded from the codebook. The
remaining vectors are then optimized and the least used vectors are again
discarded from the optimized codebook. This process continues until the
ﬁnal size of the codebook is reached. Here, the number of vectors discarded
at each stage and the number of optimization iterations may vary with the
application but the initial size of the codebook should at least be 1.5 times
the ﬁnal size and the number of discarding stages should not be fewer
than ﬁve or six. The number of vectors discarded in each stage should be
reduced to increase the accuracy of optimization.
Binary Search Codebook
Binary search [17], known in the pattern recognition literature as hierarchical
clustering [14], is a method for partitioning space in such a way that the search
for the minimum distortion code-vector is proportional to log2 L rather than
L. In speech coding literature, binary search codebooks are also called tree
codebooks or tree search codebooks.
In a binary search codebook, N dimensional space is ﬁrst divided into two
regions (using the K-means algorithm with two initial vectors), then each of
the two regions is further divided into two subregions, and so on, until the
space is divided into L regions or cells. Here, L is restricted to be a power

Vector Quantization
47
v1
v3
v4
v5
v6
y1
y2
y3
y4
y5
y6
y7
y8
v2
Figure 3.11
Binary splitting into eight cells
of 2, L = 2B, where B is an integer number of bits. Each region is associated
with a centroid. Figure 3.11 shows the division of space into L = 8 cells. At
the ﬁrst binary division v1 and v2 are calculated as the centroids of the two
halves of the total space to be covered. At the second binary division four
centroids are calculated as v3 to v6. The centroids of the regions after the
third binary division are the actual codebook vectors yi. An input vector x is
quantized, searching the tree along a path that gives the minimum distortion
at each node in the path. Again assuming N multiply–adds for each distortion
computation, the computation cost will be,
Combs = 2N log2 L = 2NB
multiply −add per input vector
(3.54)
At each stage, the input vector is compared against only two candidates.
This makes the computation cost a linear function of the number of bits in the
codewords.
The total storage cost, on the other hand, has gone up signiﬁcantly,
Mbs = 2N(L −1)
locations
(3.55)
or,
Mbs = N
B

i=1
2i
locations
(3.56)
A tree search codebook need not be a binary search codebook. In other
words the number of splitting stages may be less than the number of bits, B, in
the codeword. In this case, each vector from the previous stage may point to
more than two vectors in the current stage. This can be seen as a compromise

48
Sampling and Quantization
between the extreme cases of low computation cost with high storage (binary
codebook) and high computation cost with low storage requirement (full
search codebook).
During the training of a binary codebook, at each stage of splitting using
the K-means algorithm and method 1, the resultant optimum codebooks are
stored. The database is also split into sections represented by each of the
resultant vectors. When the vectors are further split, each new pair of vectors
is optimized using the section of the database represented by their mother
vector. This process continues until the ﬁnal size codebook is reached and
optimized.
Cascaded Codebooks
The major advantage of a binary search codebook is the substantial decrease
in its computational cost, relative to a full search codebook, with a relatively
small decrease in performance. However, the storage required for a binary
searchcodebook relativetoafullsearchcodebook isnearly doubled. Cascaded
vector quantization is a method intended to reduce storage as well as
computational costs [18, 13]. A two-stage cascaded vector quantization is
shown in Figure 3.12. Cascaded vector quantization consists of a sequence of
vector quantization stages, each operating on the error signal of the previous
stage. The input vector x is ﬁrst quantized using a B1 bit L1 level vector
e(n)
index i
index k
x(n)
x
e
yi
Codebook  1
Codebook  2
Codebook  1
Codebook  2
index i
index k
x(n)
^
+
+
−
+
Vector Quantizer
Vector De-quantizer
Vector
Buffer
Vector
Buffer
Vector
Matching
Vector
Matching
Figure 3.12
A two-stage cascaded vector quantizer
www.allitebooks.com

Vector Quantization
49
quantizer and the resulting error signal is then used in the input to a B2 bit L2
level second vector quantizer. The sum of the two quantized vectors results
in the quantized value of the input vector x.
The computation and storage costs for a k -stage cascaded vector quantiza-
tion are respectively,
Comcc = N(L1 + L2 + . . . + Lk)
multiply −add per input vector
(3.57)
Mcc = N(L1 + L2 + . . . + Lk)
locations
(3.58)
Assuming L1 = 2B1, L2 = 2B2 and Lk = 2Bk and the total number of bits per
input vector B = B1 + B2 . . . + Bk, we can see that the number of candidate
vectors searched in a cascaded codebook for each input vector is less than in
a full search codebook,
k

n=1
2Bn < 2B
if
B =
k

n=1
Bn
and k > 1
(3.59)
We can also see that the storage of a cascaded codebook is less than that
required by a binary codebook,
N


k

n=1
2Bn

< N
 B

i=1
2i

for k > 1
(3.60)
Given the condition that the total number of bits used at various stages of a
cascaded codebook is B, both computation and storage requirements reduce
with an increase in the number of stages.
Split Codebooks
In all of the above codebook types an N dimensional input vector is directly
matched with N dimensional codebook entries. In a split vector quantization
scheme, an N dimensional input vector is ﬁrst split into P parts where P > 1.
For each part of the split vector a separate codebook is used and each part may
be vector quantized independently of the other parts using Bp bits. Assuming
a vector is split into P equal parts and vector quantized using Bp bits for each
part, the computation and storage requirements can be calculated as follows:
Comss = N
P (L1 + L2 + . . . + LP)
multiply −add per input vector
(3.61)
where Lp = 2Bp for p = 1, 2, . . . , P. Similarly, the storage is given by:
Mss = N
P (L1 + L2 + . . . + LP)
locations
(3.62)

50
Sampling and Quantization
The usefulness of a split vector quantization is in its ﬂexibility in choosing
the dimension of each split part and in the allocation of the overall bits per
input vector to these parts according to the perceptual importance of the
vector elements contained in each split part.
Gain Shape Codebooks
In the earlier discussion of scalar quantization, it was mentioned that the
variance of the input speech signal affected the performance of the quantizer.
This is also true in the case of a vector quantizer. For example, if the input
signal variance is ﬁxed at a certain value, all of the codebook entries will
have the same variance and differ only in the shape of vector elements.
In addition, if we assume that the same number of shape combinations is
repeated with another variance level at the input, the number of codebook
entries would have to be doubled to cover the vector shapes at two different
energy levels. Therefore, if the input vectors have a large dynamic range,
the required codebook size may be too large for practical implementation in
both computation and storage. This problem can be overcome by using the
same idea that is used in scalar quantization: each input vector is normalized
to a certain variance level (usually unity), and then its unit variance shape
is vector quantized using a shape codebook containing candidate vectors
with unity variance. The original variance of the input vector is separately
quantized and transmitted to the de-quantizer for correct scaling. This process
is called gain-shape vector quantization. A block diagram of a gain-shape
vector quantizer is shown in Figure 3.13. The gain of the input vector is
usually calculated and quantized using a scalar quantizer either before or
during the search of the shape codebook.
If the gain of the input vector is to be calculated and quantized before
ﬁnding its shape then the quantized gain is calculated as:
ˆσx = Q





 1
N
N

i=1
x2
i


(3.63)
index i
Input Vector
Buffer
Codebook
x(n)
x
Match
x and σi yi
yi
σi
Figure 3.13
Gain-shape vector quantizer

Vector Quantization
51
where
Q[.]
denotes quantization operation. The shape codebook is then searched and
the codebook vector which minimizes the expression,
Dk =
N

i=1
(xi −ˆσxyki)2
k = 1, 2, . . . , L
(3.64)
is chosen for transmission. This search scheme, called open loop, is not
optimum. Better performance can be achieved with a closed loop scheme
where the shape is ﬁrst found and then the corresponding gain is quantized
before computing the ﬁnal error. Here, we assume an optimum gain σk to be
used for each of the L shape codebook entries and compute the corresponding
distortion Dk as:
Dk =
N

i=1
(xi −σkyki)2
k = 1, 2, . . . , L
(3.65)
We wish to ﬁnd a vector yk from the shape codebook with a gain value of σk
such that the corresponding distortion Dk is minimized. However, we have
two unknowns, namely, yk and σk. To ﬁnd σk in terms of yk we differentiate
(3.65) with respect to σk and set it to zero for minimum error gain. This gives
the following σk for the codebook vector yk in relation to an input vector x,
σk =
N

i=1
(xiyki)
N

i=1
y2
ki
(3.66)
If we substitute (3.66) into (3.65) we can write the distortion Dk independently
of σk as,
Dk =
N

i=1
(xi)2 −
 N

i=1
xiyki
2
N

i=1
y2
ki
k = 1, 2, . . . , L
(3.67)
The ﬁrst term of Dk in equation (3.67) does not change with k, and
hence it is not computed during the search of the shape. The shape is
found by maximizing only the second term in (3.67). During the codebook
search process, the most likely shape values are found by maximizing the

52
Sampling and Quantization
second term in equation (3.67). Then, corresponding gain values given by
(3.66) are computed and quantized. Finally, each shape vector scaled by its
quantized gain is compared with the input vector. This whole process can be
simpliﬁed with only a small increase in the quantization error by computing
the second part of equation (3.67) for all k to select the best shape vector
without quantizing its gain (assuming that gain quantization noise will not,
in general, render other vectors more favourable). In this case only one shape
vector is considered which does not require further comparisons after the
gain quantization process.
Adaptive Codebooks
The above discussed codebooks do not vary with time. Therefore, it is
extremely important to train these codebooks for optimal performance with
varying time and hence varying input vector characteristics. One way of
making a codebook track the input vector characteristics with time is to
make the codebook adaptive. As in the case of an adaptive scalar quantizer,
the adaptation of a codebook can be achieved using either forward or
backward schemes.
In a forward adaptive vector quantizer, the codebook is updated with
respect to the input vectors before the quantization process, which requires
some side information to be transmitted to the de-quantizer for compatible
adaptation necessary for correct recovery of the signal.
In the case of a backward adaptive quantizer, the codebook is updated by
the appropriately transformed most recent quantizer output vectors. In this
case, no side information is needed since the same update process can be
performed at the de-quantizer using the previously recovered vectors.
An adaptive codebook is usually used in cascade with other (generally,
ﬁxed) codebooks, which provide the initial vectors to the adaptive codebook
as well as helping to speed up adaptation when signiﬁcant signal variations
occur. An adaptive codebook in a two-stage cascaded vector quantizer is
shown in Figure 3.14. The ﬁrst stage can be an adaptive codebook followed
by a ﬁxed second stage codebook. The adaptive codebooks used in these
conﬁgurations are called predictor codebooks and the whole process is called
predictive or differential vector quantization.
3.4.4 Training, Testing and Codebook Robustness
An important part of the codebook design is the training process used to
populate the codebook. The training process simply optimizes a codebook
for given training data by calculating the centroids of the cells. Because the
K-means algorithm is not guaranteed to result in a codebook that is globally
optimum, it is often suggested that one repeats the algorithm with a number
of different initial sets of codebook vectors [19].

Vector Quantization
53
Fixed
Codebook
yk
xp
Vector
Matching
Vector
Matching
Adaptive
Codebook
Index i
Index k
Fixed
Codebook
Adaptive
Codebook
Vector De-quantizer
Vector Quantizer
−
+
+
+
+
x(n)
^
^x
yk
xp
Index i
Index k
e
x
x(n)
Vector
Buffer
Figure 3.14
Adaptive vector quantizer in a cascaded setup
After designing a codebook to match a given set of training data, it is
important to test the performance of that codebook on data that was not
used in the training. Testing only on the training data will always give better
performance than the codebook will actually give in practice.
The robustness of a codebook can be measured by measuring its perfor-
mance on data whose distribution is different from that of the training data.
In practice, one cannot usually predict all of the situations under which a
quantizer will be used and so the distribution of the actual data may be
different from that of the training data. There are two major types of variation
that affect the design and operational performance of a codebook: input signal
variability and digital transmission channel errors.
Signal variability can be further classiﬁed as speaker variability and envi-
ronmental variability. Speaker variability covers the changes in the input
signal due to a change in the speaker’s voice and may, for example, be due
to multiple speakers or the health conditions of each speaker. Environmental
variability, on the other hand, refers to the background noise level and type.
For a given bit rate and speaker, a speaker-dependent codebook performs

54
Sampling and Quantization
better than a speaker-independent codebook. One method of maximizing the
performance of a codebook is to design a speaker-independent codebook
initially and then, as the system is used, have it adapt to the speech of new
speakers [20]. In such a system, automatic adaptation to the background noise
environment of the speaker is also possible.
As in the case of a scalar quantizer transmission channel errors affects
the performance of a vector quantizer. Channel errors translate directly into
distortion at the output, depending on the channel error rate. In general,
vector quantization systems tend to be less robust to random channel errors
than scalar quantizers, as a single bit error can cause all of the values
represented by that vector to be in error.
3.5 Summary
Many quantization schemes have been designed and deployed in practice.
With the advancement in the DSP technology which allowed more process-
ing power as well as storage, vector quantization techniques have become
widespread. Vector quantization schemes are very effective in reducing the
bit rate of the signal that is being quantized at the expense of increased imple-
mentation complexity. It is however crucial that the codebooks are trained
to match the incoming signal. As the training processes are usually applied
off-line they can be allowed to run for a long time so that the best codebooks
are obtained. In parallel with signiﬁcant advances in the DSP technology, the
implementation cost of various codebooks has been optimized by developing
intelligent search algorithms as well as different types of codebook.
Bibliography
[1] L. Rabiner and R. Schafer (1978) Digital Processing of Speech Signals.
Englewood Cliffs, NJ: Prentice-Hall
[2] K. W. Cattermole (1973) Principles of Pulse Code Modulation. London:
Illiffe
[3] P. F. Panter and W. Dite (1951) ‘Quantization distortion in PCM with
non-uniform spacing of levels’, in Proc. IRE, 39:44–8.
[4] J. Max (1960) ‘Quantising for minimum distortion’, in IRE Trans. on
Information Theory, 6:7–12.
[5] M. D. Paez and T. H. Glisson (1972) ‘Minimum mean-squared-error
quantization in speech PCM and DPCM systems’, in IEEE Trans. on
Communications, 20(4):225–30.
[6] P. Noll (1974) ‘Adaptive quantizing in speech coding systems’, in IEEE
Int. Zurich Seminar on Digital Comm., pp. B3.1–6, March.

Bibliography
55
[7] R. W. Stroh (1970) ‘Optimum and adaptive DPCM’, Ph.D. thesis, Poly-
technic Inst. of Brooklyn, USA.
[8] N. S. Jayant (1974) ‘Adaptive quantization with a one-bit memory’, in
Bell Sys. Technical Journal, 52.
[9] N. S. Jayant (1974) ‘Digital coding of speech waveforms: PCM, DPCM
and DM quantizers’, in IEEE Proc., 62(5):611–632.
[10] N. S. Jayant and P. Noll (1984) Digital Coding of Waveforms: Principles and
applications to speech and video. New Jersey: Prentice-Hall
[11] M. Schroeder and B. Atal (1979) ‘Predictive coding of speech signals
and subjective error criteria’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 27:247–54.
[12] V. Viswanathan et al. (1983) ‘Objective speech quality evaluation of
medium band and narrow band real-time speech coders’, in Proc. of Int.
Conf. on Acoust., Speech and Signal Processing, pp. 543–6.
[13] J. Makhoul, S. Roucos, and H. Gish (1985) ‘Vector quantisation in speech
coding’, in Proc. of IEEE, 23:1551–88.
[14] M. R. Anderberg (1973) Cluster Analysis for Applications, p. 22. Academic
Press
[15] Y. Linde, A. Buzo, and R. Gray (1980) ‘An algorithm for vector quantiser
design’, in IEEE Trans. on Communications, 28(1):84–95.
[16] R. Gray and E. Karnin (1982) ‘Multiple local in vector quantization’, in
IEEE Trans. on Information Theory, 28:256–61.
[17] A. Buzo, AH Gray Jr., R. M. Gray, and J. D. Markel (1980) ‘Speech coding
based upon vector quantisation’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 28(5):562–74.
[18] S. Roucos, R. Schwartz, and J. Makhoul (1982) ‘Vector quantization for
very low bit rate coding of speech’, in Proc. of Globecom, pp. 1074–8.
[19] R. Gray (1984) ‘Vector quantization’, in IEEE ASSP Magazine, 1:4–28.
[20] D. B. Paul (1983) ‘An 800 bps adaptive vector quantisation vocoder using
a perceptual distance measure’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 73–6.

4
Speech Signal Analysis
and Modelling
4.1 Introduction
The speech signal has been studied for various reasons and applications by
many researchers for many years. Some studies broke down the speech signal
into its smallest portions called phonemes. Here, we will describe the speech
signal in terms of its general characteristics. Speech signals can be classiﬁed
into voiced or unvoiced. A voiced speech segment is known by its relatively
high energy content but, more importantly, it contains periodicity which is
called the pitch of voiced speech. The unvoiced part of speech on the other
hand looks more like random noise with no periodicity. However, there are
some parts of speech that are neither voiced nor unvoiced, but a mixture of the
two. These are usually called the transition regions, where there is a change
either from voiced to unvoiced or unvoiced to voiced. The amplitude versus
time plots of typical voiced and unvoiced speech are shown in Figure 4.1
(Note: The unvoiced sound has been ampliﬁed ﬁve times).
In some speech coding schemes the frequency domain representation of the
speech signal is necessary. For this purpose, the short-time Fourier transform
is very useful. The short-time spectral transformation is also important to
look at a segment of the speech signal and determine features that are not
obvious from the time domain representation.
4.2 Short-Time Spectral Analysis
The short-time Fourier transform plays a fundamental role in frequency
domain analysis of the speech signal. It is used to represent the time-
varying properties of the speech waveform in the frequency domain. A
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

58
Speech Signal Analysis and Modelling
0.0
50.0
100.0
150.0
200.0
250.0
300.0
Time (Samples)
Amplitude
Voiced Speech
Unvoiced Speech
Figure 4.1
Voiced and unvoiced speech waveforms (unvoiced amplified by 5)
useful deﬁnition of the time-dependent Fourier transform is [1],
Sk(ejω) =
∞

n=−∞
w(k −n)s(n)e−jωn
(4.1)
where w(k −n) is a real window sequence used to isolate the portion of
the input signal that will be analysed at a particular time index, k. During
the analysis of speech signals, the shape and length of the window can
affect the frequency representation of speech (or any other signal). Various
types of window have been studied by researchers, producing window
shapes and characteristics suitable for various applications. In the following,
a brief description of windowing and its effects on the short-time Fourier
representation are given.
4.2.1 Role of Windows
The window, w(n), determines the portion of the speech signal that is to be
processed by zeroing out the signal outside the region of interest. The ideal
window frequency response has a very narrow main lobe which increases
the resolution and no side lobes (or frequency leakage). Since such a window
is not possible in practice, a compromise is usually selected for each speciﬁc
application. There are many possible windows (e.g. Rectangular, Bartlett,

Short-Time Spectral Analysis
59
Hamming, Hanning, Blackman, Kaiser, etc.), some of which are deﬁned as
follows:
Rectangular:
w(n) =
 1
;
0 ≤n ≤N −1
0
;
otherwise
(4.2)
Bartlett:
w(n) =



2n
N −1
;
0 ≤n ≤N −1
2
2 −
2n
N −1
;
N −1
2
≤n ≤N −1
0
;
otherwise
(4.3)
Hamming:
w(n) =



0.54 −0.46 cos

2π
n
N −1

;
0 ≤n ≤N −1
0
;
otherwise
(4.4)
Hanning:
w(n) =



0.5 −0.5 cos

2π
n
N −1

;
0 ≤n ≤N −1
0
;
otherwise
(4.5)
Blackman:
w(n)=



0.42 −0.5 cos

2π
n
N −1

+0.08 cos

2π
2n
N −1

;
0 ≤n ≤N −1
0
;
otherwise
(4.6)
Kaiser:
w(n) =



I0

β

1 −

2n
N −1 −1
2


I0(β)
;
0 ≤n ≤N −1
0
;
otherwise
(4.7)
www.allitebooks.com

60
Speech Signal Analysis and Modelling
0.0
0.2
0.4
0.6
0.8
1.0
w(n)
Hamming
Rectangular
Bartlett
Kaiser
Blackman
Hanning
(N −1)/2
N −1
0
n
Figure 4.2
Time plots of various window functions
where I0 is a zero order Bessel function given by,
I0(β) =
∞

k=0
β
2
2k
(k!)2
(4.8)
The time and frequency domain shapes of these window functions are illus-
trated in Figures 4.2 and 4.3 respectively. As can be seen in Figure 4.3, the rect-
angular window has the highest frequency resolution, as it has the narrowest
main lobe, but the largest frequency leakage. On the other hand, the Black-
man window has the lowest resolution and the smallest frequency leakage.
The effect of these windows on the time-dependent Fourier representation of
speech can be illustrated by discussing the properties of two representative
windows, e.g. the rectangular window and the Hamming window.
The effects of using the Hamming and rectangular windows for speech
spectral analysis are shown in Figures 4.4, 4.5 and 4.6. In each ﬁgure, plots
(a) and (b) show the windowed signal s(n)w(k −n) and log magnitude of the
Fourier transform, Sk(ω), respectively, of the rectangular window. Similarly,
plots (c) and (d) show the windowed signal and log magnitude spectrum
of the Hamming window. In Figure 4.4, the results for a window duration
of 220 samples (27.5 ms for 8 kHz sampling rate) for a section of voiced
speech is shown. When compared, the periodicity of the signal is clearly seen

Short-Time Spectral Analysis
61
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
20log|W(ω)|
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
20log|W(ω)|
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
20log|W(ω)|
20log|W(ω)|
20log|W(ω)|
−100.0
−80.0
−60.0
−40.0
−20.0
0.0
20log|W(ω)|
(a)
(b)
(c)
(d)
(e)
(f)
π
ω
π
ω
π
ω
π
ω
π
ω
π
ω
Figure 4.3
Frequency responses of various window functions: (a) Rectangular, (b)
Bartlet, (c) Hamming, (d) Hanning, (e) Kaiser β = 7.8, and (f) Blackman
in both Figures 4.4b and 4.4d. However the harmonics peaks at multiples
of the fundamental frequency are narrower and sharper in the rectangular
windowed speech. Also noticeable in Figures 4.4b and 4.4d is the formant
(speech sample) structure which consists of a strong ﬁrst formant peak at
about 500 Hz and three broader peaks at about 1350 Hz, 2300 Hz and 3400 Hz,
as well as a tendency to fall off at higher frequencies due to the low-pass
nature of the glottal pulse spectrum.
Although Figures 4.4b (rectangular window) and 4.4d (Hamming window)
show considerable overall similarity in terms of the pitch harmonics, formant
structure, and gross spectral shape, the pitch harmonics of Figure 4.4b are
sharper, due to the greater frequency resolution of the rectangular window

62
Speech Signal Analysis and Modelling
0.0
55.0
110.0
165.0
220.0
Time (samples)
(a)
(b)
(c)
(d)
0.0
55.0
110.0
165.0
220.0
0.0
1.0
2.0
3.0
Frequency (kHz)
Magnitude (dB)
0.0
1.0
2.0
3.0
Magnitude (dB)
Figure 4.4
Effects of window types on voiced speech with a 220 sample window
length: (a) and (b) show the time and frequency plots of speech using a rectangular
window, and (c) and (d) show the time and frequency plots of speech using a
Hamming window
relative to that produced by the Hamming window. However the high
frequency leakage produced by the larger side lobes makes rectangular
windowed speech look more noisy. This undesirable high frequency leakage
between adjacent harmonics tends to offset the beneﬁts of the ﬂat time domain
response (greater frequency resolution) of the rectangular window. As a
result, rectangular windows are not usually used in speech spectral analysis.
The effect of windowing unvoiced speech is shown in Figure 4.5. Again
the spectra are slowly varying with a series of sharp peaks and valleys.
The noisy appearance of the spectrum (for both windows) however, is due
to the random nature of unvoiced speech. Although the signal itself is
random, again the Hamming window produces a smoother spectrum than
the rectangular window.

Short-Time Spectral Analysis
63
0.0
55.0
110.0
165.0
220.0
Time (samples)
0.0
55.0
110.0
165.0
220.0
0.0
1.0
2.0
3.0
Frequency (kHz)
Magnitude (dB)
0.0
1.0
2.0
3.0
Magnitude (dB)
(a)
(b)
(c)
(d)
Figure 4.5
Effects of window types on unvoiced speech with a 220 sample window
length: (a) and (b) show the time and frequency plots of speech using a rectangular
window, and (c) and (d) show the time and frequency plots of speech using a
Hamming window
In order to see the effect of varying the window length, consider the example
in Figure 4.6 where a block of 40 sample (5 ms) long voiced speech is shown.
In this case, the time domain speech s(n) w(k −n) shown in Figures 4.6a and
4.6c do not show the signal periodicity accurately. This is also true for the
signal spectra shown in Figures 4.6b and 4.6d. When compared with Figure
4.4, the spectra of Figure 4.6 show only a few rather broad peaks at about
500, 1350, 2300, and 3400 Hz corresponding to the formants of the speech
contained within the window.
The effects of Hamming and rectangular windowing are still visible in
the spectra of Figures 4.6b and 4.6d. If windows of 5 ms duration were to
be positioned at the beginning and end of the 27.5 ms interval they would
show different spectral characteristics. Therefore, good temporal resolution
requires a short window while good frequency resolution of speech requires

64
Speech Signal Analysis and Modelling
0.0
20.0
40.0
Time (samples)
0.0
20.0
40.0
0.0
1.0
2.0
3.0
Frequency (kHz)
Magnitude (dB)
0.0
1.0
2.0
3.0
Magnitude (dB)
(a)
(b)
(c)
(d)
Figure 4.6
Effects of window types on voiced speech with a 40 sample window
length: (a) and (b) show the time and frequency plots of speech using a rectangular
window, and (c) and (d) show the time and frequency plots of speech using a
Hamming window
a longer window (with a narrower main lobe). Since the attenuation of
windows is essentially independent of the window duration, increasing the
length, N, simply decreases the bandwidth (main lobe). If N is small, e.g.
40 samples, the short-time energy will change very rapidly. If N is too large
on the other hand, e.g. on the order of several pitch periods, the short-time
energy will be averaged over a long time, and hence will not adequately
reﬂect the changing properties of the speech signal. This implies that there is
no satisfactory value N which can be chosen because the duration of a pitch
period varies from about 16 samples for a high pitched female or a child,
up to 160 samples for a very low pitched male voice. Therefore, in practice
a compromise is made by setting a suitable practical value for N between
120 and 240 samples (i.e. 15–30 ms duration). The size of the window is also
determined by practical factors. That is, when speech is analysed, some form

Linear Predictive Modelling of Speech Signals
65
of parametric information is extracted for transmission, which would require
a higher bit rate for a smaller window size (more frequent update rate). In
addition, during the analysis of speech, it is necessary to have a window
length which will represent the harmonic structure fairly accurately (i.e. to
have more than one or two pitch periods in each window).
4.3 Linear Predictive Modelling of Speech Signals
One of the most powerful speech analysis methods is the method of linear
prediction analysis [2, 3], or LPC analysis as it is commonly called. In LPC
analysis, the short-term correlations between speech samples (formants) are
modelled and removed by a very efﬁcient short order ﬁlter. As LPC is a
short term prediction process, the latest speech coders also call it a short
term predictor (STP). Another equally powerful and related method is pitch
prediction [4, 5]. In pitch prediction, the long-term correlation of speech
samples are modelled. In the following sections these linear prediction
techniques will be examined and discussed.
4.3.1 Source Filter Model of Speech Production
Before parameters can be extracted from a speech signal, it is necessary to
have a theoretical model for our analysis. In speech processing, the source-
ﬁlter model of speech production is generally used as a means of analysis.
A simpliﬁed block diagram of this model [1] is shown in Figure 4.7. In
this model the driving input, or excitation signal, is modelled as either an
impulse train (for voiced speech) or random noise (for unvoiced speech). The
combined spectral contributions of the glottal ﬂow, the vocal tract, and the
radiation of the lips are represented by a time-varying digital ﬁlter with a
LPC Coefficients
Output
Speech s(n)
x(n)
G
Random
Noise
Generator
Impulse
Train
Generator
Voiced/
Unvoiced
Switch
Pitch Period
r(n)
Time
Varying
Filter
Figure 4.7
Block diagram of the simplified source-filter model of speech production

66
Speech Signal Analysis and Modelling
steady state system function as given by,
H(z) = S(z)
X(z)
(4.9)
=
G

1 −
M

j=1
bjz−j


1 −
N

i=1
aiz−i
(4.10)
In (4.10), both poles and zeros exist in the transfer function. However, if the
order of the denominator is high enough, H(z) can be approximated by an
all-pole model as given by,
H(z) =
G
1 −
p

j=1
ajz−j
=
G
A(z)
(4.11)
where,
A(z) = 1 −
p

j=1
ajz−j
(4.12)
Transforming equation (4.11) into the sampled time domain we obtain,
s(n) = Gx(n) +
p

j=1
ajs(n −j)
(4.13)
Equation (4.13) is the well-known LPC difference equation which states
that the value of the present output, s(n), may be determined by summing the
present input, Gx(n), and a weighted sum of the past output samples. Hence,
in LPC analysis the problem can be stated as follows: given the measurements
of the signal, s(n), determine the parameters aj, j = 1, . . . , p which minimize
Gx(n). The resulting parameters are then assumed to be the parameters of
our model system transfer function H(z).
If αj represents the estimates of aj, the error or residual is given by,
e(n) = s(n) −
p

j=1
αjs(n −j)
(4.14)

Linear Predictive Modelling of Speech Signals
67
It is now possible to determine the estimates by minimizing the mean squared
error, i.e.
E{e2(n)} = E




s(n) −
p

j=1
αjs(n −j)


2


(4.15)
Setting the partial derivatives of the above with respect to αj to zero for
j = 1, . . . , p we get,
E




s(n) −
p

j=1
αjs(n −j)

s(n −i)


= 0,
for i = 1, . . . , p
(4.16)
That is, e(n) is orthogonal to s(n −i) for i = 1, . . . , p. Equation (4.16) can be
rearranged to give,
p

j=1
αjφn(i, j) = φn(i, 0),
for i = 1, . . . , p
(4.17)
where
φn(i, j) = E{s(n −i)s(n −j)}
(4.18)
In the derivation of equation (4.17), a major assumption is that the signal
of our model is stationary. For speech, this is obviously untrue over a long
duration. However, for short segments of speech the assumption that it is
stationary is reasonable. Consequently, our expectations in equation (4.18)
are replaced by ﬁnite summations over a short length of speech samples.
In this section the equation for LPC analysis was derived from the Least
Mean Square approach. An equally valid result can be obtained using the
Maximum Likelihood method and other formulations [6]. An interesting
aspect of LPC analysis is that it applies not only to speech processing, but also
to a wide range of other ﬁelds such as control and radar. However, it is in
speech processing that LPC analysis has been perhaps the most successful, as
it allows very accurate representation of speech with a small set of parameters.
4.3.2 Solutions to LPC Analysis
As mentioned above, in order to model the time-varying nature of the speech
signal whilst staying within the constraint of our LPC analysis, i.e. a stationary
signal, it is necessary to limit our analysis to short blocks of speech. This is

68
Speech Signal Analysis and Modelling
achieved by replacing the expectations of equation (4.17) by summations over
ﬁnite limits, i.e.
φn(i, j) = E{s(n −i)s(n −j)}
=

m
sn(m −i)sn(m −j),
for i = 1, . . . , p,
j = 0, . . . , p (4.19)
There are two approaches to interpret equation (4.19), and these lead to two
methods, namely the Autocorrelation and Covariance methods [1, 7].
The Autocorrelation Method
For the Autocorrelation Method (AM), the waveform segment, sn(m), is
assumed to be zero outside the interval 0 ≤m ≤N −1 where N is the length
of the sample sequence. Since for N ≤m ≤N + p we are trying to predict
zero sample values (which are not actually zero) the prediction error for these
samples will not be zero. Similarly, the beginning of the current frame will
be affected by the same inaccuracy incurred in the previous frame. The limits
for equation (4.19) can be expressed as,
φn(i, j) =
N−1−|(i−j)|

m=0
sn(m)sn(m + |i −j|),
1 ≤i ≤p,
0 ≤j ≤p
(4.20)
Equation (4.20) can be reduced to the short-time autocorrelation function
given by,
φn(i, j) = Rn(| i −j |),
for
i = 1, . . . , p
j = 0, . . . , p
(4.21)
where,
Rn(j) =
N−1−j

m=0
sn(m)sn(m + j)
(4.22)
Using the AM, equation (4.17) can be expressed as
p

j=1
αjRn(| i −j |) = Rn(i),
1 ≤i ≤p
(4.23)
or in matrix form by,


Rn(0)
Rn(1)
.
Rn(p −1)
Rn(1)
.
.
Rn(p −2)
...
...
...
...
Rn(p −1)
.
.
Rn(0)




α1
α2
...
αp

=


Rn(1)
Rn(2)
...
Rn(p)



Linear Predictive Modelling of Speech Signals
69
The above matrix has the property that it is symmetrical and all the
elements along a given diagonal are equal, i.e. it is a Toeplitz matrix.
Equation (4.23) can be solved by inversion of the p × p matrix, however
this is not usually performed as computational errors, such as ﬁnite pre-
cision, tend to accumulate. By exploiting the Toeplitz characteristic how-
ever, very efﬁcient recursive procedures have been devised. The most
widely used is perhaps Durbin’s algorithm, which is a recursive process
as follows:
E(0)
n
= Rn(0)
(4.24)
ki =

Rn(i) −
i−1

j=1
αi−1
j
Rn(i −j)

E(i−1)
n
1 ≤i ≤p
(4.25)
α(i)
i
= ki
(4.26)
α(i)
j
= α(i−1)
j
−kiα(i−1)
i−j
1 ≤j ≤i −1
(4.27)
E(i)
n = (1 −k2
i )E(i−1)
n
(4.28)
After solving equations (4.25) to (4.28) recursively for i = 1, 2, . . . , p, the αj
are given by,
αj = α(p)
j
1 ≤j ≤p
(4.29)
Consider an example where the order, p = 2,
 Rn(0)
Rn(1)
Rn(1)
Rn(0)
  α1
α2

=
 Rn(1)
Rn(2)

Then, for i = 1,
E(0)
n
= Rn(0)
k1 = Rn(1)
Rn(0)
α(1)
1
= Rn(1)
Rn(0)
E(1)
n
=

1 −R2
n(1)
R2n(0)

Rn(0) = R2
n(0) −R2
n(1)
Rn(0)

70
Speech Signal Analysis and Modelling
and for i = 2,
k2 = [Rn(2) −α(1)
1 Rn(1)]/E(1)
n
= Rn(2)Rn(0) −R2
n(1)
R2n(0) −R2n(1)
α(2)
2
= k2
α(2)
1
= α(1)
1
−k2α(1)
1
= Rn(1)Rn(0) −Rn(1)Rn(2)
R2n(0) −R2n(1)
and, from this,
α1 = α(2)
1 ,
and
α2 = α(2)
2 .
The Covariance Method
For the Covariance Method (CM), the opposite approach to the AM is taken.
Here the interval over which the mean squared error is computed is ﬁxed, i.e.
E =
N−1

m=0
e2
n(m)
(4.30)
Equation (4.19) can be written as,
φn(i, j) =
N−1

m=0
sn(m −i)sn(m −j),
1 ≤i ≤p, 0 ≤j ≤p
(4.31)
Changing the summation index,
φn(i, j) =
N−i−1

m=−i
sn(m)sn(m + i −j),
1 ≤i ≤p, 0 ≤j ≤p
(4.32)
The expression given by equation (4.32) is slightly different to equation (4.20)
used in the AM as it requires the use of samples in the interval −p ≤m ≤N−1.
In effect, equation (4.31) is not a true autocorrelation function, but rather the
cross-correlation between two very similar but not identical, ﬁnite-length
sampled sequences. Using equation (4.31), our original LPC equation (4.17)
can be expressed as,
p

j=1
αjφn(i, j) = φn(i, 0),
1 ≤i ≤p
(4.33)

Linear Predictive Modelling of Speech Signals
71
or in matrix form,


φn(1, 1)
φn(1, 2)
·
φn(1, p)
φn(2, 1)
·
·
φn(2, p)
...
...
...
...
φn(p, 1)
·
·
φn(p, p)




α1
α2
...
αp

=


φn(1, 0)
φn(2, 0)
...
φn(p, 0)


A solution to equation (4.33) is not as straightforward as for the equivalent
AM. This is because the covariance matrix, φn(i, j) = φn(j, i), but the p × p
matrix φ is not Toeplitz. However, efﬁcient matrix inversion solutions such
as Cholesky decomposition can be applied where φ is expressed as [1]:
φ = VDVT
(4.34)
V is a lower triangular matrix whose main diagonal elements are 1s and D
is a diagonal matrix. The elements of the V and D matrices are determined
from equation (4.34) as follows:
φn(i, j) =
j

m=1
VimdmVjm
1 ≤j ≤i −1
(4.35)
or equivalently,
Vijdj = φn(i, j) −
j−1

m=1
VimdmVjm
1 ≤j ≤i −1
(4.36)
and for the diagonal elements of D,
φn(i, i) =
i

m=1
VimdmVim
(4.37)
or,
di = φn(i, i) −
i−1

m=1
V2
imdm
for i ≥2
(4.38)
and,
d1 = φn(1, 1)
(4.39)

72
Speech Signal Analysis and Modelling
Lattice Methods
As shown in the previous sections, the solution to the LPC equation involves
two basic steps: (i) computation of a matrix of correlation values, φn(i, j),
and (ii) solution of a set of linear equations. Although the two steps are
already very efﬁcient, another class of autocorrelation based methods, called
Lattice Methods (LM), have been developed which combine the two steps
to compute the LPC parameters. The basic idea behind the LM is that
knowledge of the forward and backward prediction errors are incorporated
during the calculation of the intermediate stages of the predictor parameters.
A major incentive for using the LM is that the computed parameters are
guaranteed to form a stable ﬁlter, a feature which neither the AM nor the
CM possess.
Consider the ith stage of Durbin’s algorithm where the set of coefﬁcients
α(i)
j , j = 1, 2, . . . , i are the optimum linear prediction coefﬁcients of an ith order
ﬁlter. The inverse ﬁlter A(z) based on these i optimum coefﬁcients will be,
A(i)(z) = 1 −
i

j=1
α(i)
j z−j
(4.40)
and the prediction error e(i)
n (m) (or for simplicity e(i)(m)) will be,
e(i)(m) = s(m) −
i

j=1
α(i)
j s(m −j)
(4.41)
In a z-transform notation, the above equation becomes,
E(i)(z) = A(i)(z)S(z)
(4.42)
By combining (4.27) and (4.40) we have
A(i)(z) = 1 −
i−1

j=1
[α(i−1)
j
−kiα(i−1)
i−j
]z−j −α(i)
i z−i
(4.43)
but α(i)
i
= ki and hence,
A(i)(z) = 1 −
i−1

j=1
α(i−1)
j
z−j + ki
i−1

j=1
α(i−1)
i−j
z−j −kiz−i
(4.44)
A(i)(z) = A(i−1)(z) −ki

z−i −
i−1

j=1
α(i−1)
i−j
z−j


(4.45)

Linear Predictive Modelling of Speech Signals
73
Using (4.42) and (4.45),
E(i)(z) = A(i−1)(z)S(z) −ki

z−i −
i−1

j=1
α(i−1)
i−j
z−j

S(z)
(4.46)
The ﬁrst term in equation (4.46) represents the prediction error for an (i −1)th
order predictor employing the s(m −1), s(m −2), . . . , s(m −i + 1) samples to
predict s(m). Since the output of the ﬁlter i−1
j=1 α(i−1)
i−j
z−j operating on S(z)
is αi−1sm−1 + αi−2sm−2 + . . . + α1sm−i+1 the second term of equation (4.46)
represents the backward prediction error of the same predictor attempting to
predict s(m −i) from the i samples s(m −i + j), j = 1, 2, 3, . . . , i that follow
s(m−i). The prediction error sequence e(i)
m can therefore be expressed in terms
of the forward and backward error sequences as,
e(i)(m) = e(i−1)(m) −kib(i−1)(m −1)
(4.47)
It can also be shown that the ith stage backward prediction error b(i)(m) can
be expressed as,
b(i)(m) = b(i−1)(m −1) −kie(i−1)(m)
(4.48)
Equations (4.47) and (4.48) provide the forward and backward prediction
error sequences for an ith order ﬁlter, in terms of the corresponding errors of
a (i −1)th order ﬁlter. Note that
e(0)(m) = n(0)(m) = s(m)
(4.49)
i.e, the zero order ﬁlter error equals the original input. Furthermore, the
ki parameters can be directly computed from the forward and backward
prediction errors as [8],
ki =
N−1

m=0
e(i−1)(m)b(i−1)(m −1)




N−1

m=0
[e(i−1)(m)]2 ×
N−1

m=0
[b(i−1)(m −1)]2
(4.50)
without using the prediction coefﬁcients αj. From the above expression, it
is clear that the ki parameters represent the normalized cross-correlation
function between the forward and backward error sequences. It is for this
reason the ki parameters are known as the partial correlation (PARCOR)
coefﬁcients [8].

74
Speech Signal Analysis and Modelling
A popular lattice implementation of LPC analysis is that developed by
Burg [3]. Burg derived the ki parameters by minimizing the sum of the mean
squared forward and backward prediction errors, i.e.
ˆE(i) =
N−1

m=0
 
(e(i)(m))2 + (b(i)(m))2!
(4.51)
ˆE(i) is differentiated with respect to ki and then is set to zero to give,
ki =
2
N−1

m=0
e(i−1)(m)b(i−1)(m −1)
N−1

m=0
[e(i−1)(m)]2 +
N−1

m=0
[b(i−1)(m −1)]2
(4.52)
It can also be shown [8] that the above equation results in ki parameters,
−1 ≤ki ≤1. Burg’s algorithm operates as follows [3]:
1. Set e(0)(m) = s(m) = b(0)(m)
2. Compute k1 = α(1)
1
3. Determine e(1)(m) and b(1)(m) using (4.47) and (4.48)
4. Set i = 2
5. Find ki = α(i)
i
using (4.52)
6. Find α(i)
j
for j = 1, 2, . . . , i −1 using (4.27)
7. ﬁnd e(i)(m) and b(i)(m) using (4.47) and (4.48)
8. Set i = i + 1
9. If i ≤p go to step 5
10. End
4.3.3 Practical Implementation of the LPC Analysis
In the practical implementation of the LPC analysis, several important groups
of factors need to be addressed. The ﬁrst group comprises the performance,
efﬁciency and stability factors, which are not too dissimilar for all three
methods, although the LM is preferred in real-time systems where guaranteed
stability is very important. However, with careful choice of windowing and
ﬁne precision arithmetic, the AM and CM are equivalent to the LM for
stability. As quantization is usually applied to the coefﬁcients, stability can
always be maintained to some extent. The second group involves the choice of
the ﬁlter order, p, and the analysis frame size, N. Speech is usually sampled at
8 kHz, thus giving a 4 kHz spectrum for analysis. Within the 4 kHz spectrum,
the maximum number of formants displayed is usually four, thus indicating

Linear Predictive Modelling of Speech Signals
75
.
.
.
.
.
.
.
.
.
.
.
.
.
2
8
6
4
10
12
14
16
18
20
24
22
LPC Filter Order
2
6
4
8
10
Prediction Gain (dB)
Figure 4.8
LPC prediction gain versus LPC order
that the ﬁlter order needs to be at least eight. Usually, a 10-pole ﬁlter is used
so that formant resonances and general spectral shape is modelled accurately.
(However, much higher order ﬁlters have been used in elaborate schemes
such as the proposed ITU 16 kb/s G.728 standard [9], where a 50-order LPC
ﬁlter is used!). The LPC ﬁlter prediction gain versus the order of the ﬁlter is
shown in Figure 4.8 (note that the plot in this ﬁgure was obtained over a small
number of speech samples and it is only indicative, the LPC gain may vary
from sample to sample), and in Figure 4.9 the spectral envelope of various
ﬁlter orders is illustrated.
As for the frame size, the stationarity constraint applies, thus it is necessary
for us to choose a size which will conform to this. This usually implies
a frame size of 16–32 ms. Another related factor is the partitioning points
of the analysis frame. The position of the analysis window may affect
signiﬁcantly the performance of the LPC analysis. In order to reduce the
effect of window positioning some common preprocessing stages include the
use of pre-emphasis of the signal prior to the LPC analysis, and the use of
overlapping windowed frames. Overlapping frames try to overcome some
of the block-edge effects of the frame-based LPC analysis. The amount of
overlap is typically around 10–20 % of the frame size. Interpolation of the

76
Speech Signal Analysis and Modelling
0.0
1.0
2.0
3.0
4.0
Frequency (kHz)
Magnitude (dB)
p = 8
p = 10
p = 12
p = 16
p = 20
Original Spectrum
Figure 4.9
LPC magnitude responses for various LPC orders
0.0
50.0
100.0
150.0
200.0
Time (Samples)
Amplitude
Original Speech
LPC Residual
Figure 4.10
Waveform plots of original and LPC inverse-filtered speech

Pitch Prediction
77
0.0
1.0
2.0
3.0
4.0
Frequency (kHz)
Magnitude (dB)
(a)
(b)
(c)
Figure 4.11
(a) the original speech spectral envelope, (b) the original speech
spectrum and (c) the LPC residual spectrum
LPC coefﬁcients from one frame to the next is also commonly applied to
smooth out transitional effects.
After the LPC inverse ﬁltering, the resultant signal, e(n), should have a
much lower spectral variation than the original, s(n). This is illustrated in
Figures 4.10 and 4.11 where the time and frequency domain representation of
a typical frame of s(n) and e(n) are shown. Clearly, the error signal spectrum
is much ﬂatter. This result is not surprising since LPC can be viewed as a
method of short-time spectrum estimation.
Also illustrated in Figure 4.11 is the frequency response or spectral envelope
of the LPC ﬁlter. A feature that can be observed is that the LPC spectral
envelope matches the signal spectrum much better in the spectral peaks than
the spectral valleys. This can be expected as our model transfer function,
H(z), has poles only to model the formant peaks and no zeros to model the
spectral valleys.
4.4 Pitch Prediction
4.4.1 Periodicity in Speech Signals
In the previous section, the ability of LPC analysis to remove the adjacent
or neighbouring sample correlations present in speech was described. As
observed, this was equivalent to removing the spectral envelope in the signal

78
Speech Signal Analysis and Modelling
spectrum. However, as can be seen from Figure 4.11, after the LPC analysis
and inverse ﬁltering there are still considerable variations in the spectrum,
i.e. it is far from white. Looking at the residual signal in Figure 4.10, it is
clear that long-term correlations, especially during voiced regions, still exist
between samples. The most evident of these are the sharp periodic pulses of
the excitation signal, which is hardly surprising as our original source-ﬁlter
model assumes this type of input signal. This also explains why the LPC
analysis, which models our vocal tract, cannot adequately remove them.
To remove the periodic structure of the residual or excitation signal, a
second stage of prediction is required. The objective of this second stage is
again to spectrally ﬂatten our signal, i.e. to remove the periodic ﬁne structure.
But unlike LPC analysis, it exploits correlation between speech samples that
are one or more ‘pitch’ periods away. For this reason, the pitch prediction
(ﬁlter) is usually called the long-term prediction (LTP) and the ﬁlter delay is
called the lag.
4.4.2 Pitch Predictor (Filter) Formulation
Before discussing methods of pitch or long-term prediction, it is perhaps
worth considering what our objectives are. Our aim is to model the long-term
correlation left in the speech residual signal after LPC inverse ﬁltering (or in
the original speech signal) such that when the model parameters are used
in a ﬁlter, it will remove the long-term correlation as much as possible, or
spectrally ﬂatten our signal. There are no obvious reasons why we must
use the LPC residual and not the original signal to model the long-term
correlation in the speech signal, as long as the effects of the formants are
taken into account during the determination of the long-term delay (pitch) in
our model. Indeed, in Atal’s original formulations in APC [10] (and in other
APC-related schemes), the pitch predictor was applied before the LPC. The
order in which they are combined is not too critical if the combination is
carefully optimized, e.g. block edge effects must be carefully compensated to
avoid ‘clicking’ type distortions. It is worth noting that the prediction gain of
the combined system will always be less than the sum of the gains in systems
employing the pitch and LPC ﬁlters in isolation. This is because in reality
the vocal tract and excitation are not completely separable as assumed in our
model, but are interconnected. The pitch ﬁlter can be interpreted as
P(z) =
1
1 −
I

j=−I
bjz−(j+T)
(4.53)
where T is the ‘pitch period’, and bj are the ‘pitch gain’ coefﬁcients which
reﬂect the amount of correlation between the distant samples. Referring

Pitch Prediction
79
r(n)
LPC
Synthesis
Filter
s(n)
Pitch
Synthesis
Filter
Gx(n)
Figure 4.12
A typical pitch–LPC formulation model
to Figure 4.12, the combined analysis model can be represented by a time
domain difference equation,
s(n) = Gx(n) +
I

j=−I
bjr(n −T −j) +
p

j=1
ajs(n −j)
(4.54)
where r(n) is the past excitation (LPC residual) signal. Following a similar
procedure to that of LPC analysis, our goal is to determine estimates (βj, τ, αj)
of the model parameters (bj, T, aj). Then, the prediction error is given by
(e(n) = Gx(n)),
e(n) = s(n) −
I

j=−I
βjr(n −τ −j) −
p

j=1
αjs(n −j)
(4.55)
The mean squared error solution to equation (4.55) is not as straightforward
as for the LPC analysis due to the presence of the delay factor τ. In order to
overcome this hurdle two suboptimal approaches can be taken:
• One-Shot Optimization: If one assumes that the pitch spectrum infor-
mation of the residual r(n) is close to the pitch spectrum information of
the input speech s(n), then we can solve for αj as before and use the
residual from the LPC inverse ﬁlter to determine (βj, τ). Thus during the
ﬁrst iteration, the LPC coefﬁcients are estimated to minimize the interme-
diate residual energy. The pitch ﬁlter coefﬁcients are then found using this
intermediate residual signal. This procedure can be considered to be near
optimal provided the long-term lag, τ, is greater than the analysis frame
size, i.e. τ > N.
• IterativeSequentialApproach: AnanalysissimilartotheOne-Shot method
described above is ﬁrst performed. During subsequent iterations, the LPC
is re-optimized with the previously-determined pitch ﬁlter coefﬁcients
[11]. Also, the pitch ﬁlter parameters are recalculated based on the newly-
formed intermediate residual. This iteration process can be continued until
a certain threshold is reached or for a ﬁxed number of iterations.
For practical reasons, the one-shot method is usually preferred as it only
requires one iteration. In the iterative sequential method the main difﬁculty is

80
Speech Signal Analysis and Modelling
in setting a suitable threshold for the termination of the iteration run. Overall,
it is substantially more complicated. However, the iterative method has been
reported to give a better prediction gain and better perceptual performance
[5]. This is usually achieved with a shifting of the LPC prediction gain to
the pitch prediction gain. Here, only the one-shot method is considered
as follows:
By removing the LPC effect in equation (4.55), we obtain,
e(n) = r(n) −
I

j=−I
βjr(n −τ −j)
(4.56)
The estimates can now be determined by minimizing the mean squared
error, i.e.
E = E{e2(n)} = E




r(n) −
I

j=−I
βjr(n −τ −j)


2


(4.57)
Replacing the expectation with ﬁnite summations, we get
E =

m
e2
n(m) =

m

rn(m) −
I

j=−I
βjrn(m −τ −j)


2
(4.58)
By setting ∂E/∂βj to zero, we obtain
I

j=−I
βjV(i, j) = R(τ + i)
−I ≤i ≤I
(4.59)
which can be written in matrix form as,


V(−I, −I)
· · ·
V(−I, I)
...
...
...
V(I, −I)
· · ·
V(I, I)




β−I
...
βI

=


R(τ −I)
...
R(τ + I)


where,
R(τ + i) =
N−1

m=0
r(m −τ −i)r(m)
(4.60)
V(i, j) =
N−1

m=0
r(m −τ −i)r(m −τ −j), −I ≤i ≤I, −I ≤j ≤I
(4.61)

Pitch Prediction
81
The βj coefﬁcients can now be solved by inverting V(i, j), e.g. using Cholesky
decomposition. In the above formulation, a ‘ﬁx-up’ may be used to ensure
that the ﬁlter so formed is stable, e.g. by adding a small noise source into
the formulation, the matrix inversion to obtain [V(i, j)]−1 can be made more
reliably. However, a stable pitch ﬁlter is not a pre-condition for the pitch
analysis as rapid transitions are sometimes desired.
In the above formulation it is assumed that the pitch lag, τ, has already been
found and that βj = βj,τ. In order to determine τ, various pitch measurement
algorithms can be used. These include the Autocorrelation [12], average
magnitude difference function (AMDF) [13], Cepstrum [14] and Maximum
Likelihood [15]. These methods exhibit different characteristics especially
with a noisy input signal.
As the preceding analysis to determine βj has shown, pitch analysis is
performed on a block containing N samples. However, the size of our
window in which the block is taken is required to be considerably longer
than the analysis frame length, N. This is because our pitch value, τ, can
vary between a minimum, τmin, of around 16 samples to a maximum, τmax,
of around 160 samples. Therefore, our ideal analysis window is signiﬁcantly
greater in length (N + τmax ≥200 samples) such that it contains more than
one complete pitch period.
For simplicity, consider a 1-tap pitch ﬁlter, i.e. (I = 0),
P1(z) =
1
1 −βz−τ
(4.62)
Thus,
β =
R(τ)
V(0, 0)
(4.63)
=
N−1

m=0
r(m)r(m −τ)
N−1

m=0
r2(m −τ)
τmin ≤τ ≤τmax
(4.64)
Substituting this into equation (4.58),
E =
N−1

m=0
r2(m) −
N−1

m=0
r(m)r(m −τ)
2
N−1

m=0
r2(m −τ)
(4.65)

82
Speech Signal Analysis and Modelling
0.0
100.0
200.0
300.0
400.0
500.0
Time (Samples)
Amplitude
Original
LPC Residual
Pitch Residual
Figure 4.13
Time domain plots of original, LPC and pitch residuals
In order to determine the optimum τ, values of the lags are tested between
τmin and τmax, and the lag which minimizes the error E is the optimal value.
Having found τ, the gain β can be found. A plot of the LPC residual and the
signal after pitch inverse ﬁltering is shown in Figure 4.13. It is clear that the
pitch residual (secondary excitation) no longer possesses the sharp pulse-like
characteristics of the residual, i.e. it looks much whiter than the LPC residual.
Similar formulations can also be given for multiple-tap pitch ﬁlters.
A typical plot of τ and β for a block of voiced, unvoiced and transitional
speech is shown in Figures 4.14 and 4.15. As can be observed, during voiced
regions (refer to the steady regions in Figure 4.14), β stays close to unity,
whereas during transitional regions β ﬂuctuates signiﬁcantly.
Aswellasasingle-tapﬁlter,thethree-tappitchﬁltergivenby equation (4.66)
is commonly used. Here I = 1 which forms the pitch prediction based on
three past samples at τ −1, τ, τ + 1.
P3(z) =
1
1 −
1

j=−1
βjz−(j+τ)
(4.66)
A multiple-tap pitch ﬁlter tends to provide better performance than the
single-tap, but with increased complexity and larger capacity requirement
for the extra two ﬁlter taps β−1 and β1.

Pitch Prediction
83
0.0
50.0
100.0
150.0
200.0
250.0
300.0
Time (Frames)
30.0
40.0
50.0
60.0
70.0
Pitch Period (Lag)
Figure 4.14
Pitch lag variation with time
0.0
50.0
100.0
150.0
200.0
250.0
300.0
Time (Frames)
0.0
1.0
2.0
3.0
4.0
Pitch Gain
Figure 4.15
Pitch gain variation with time

84
Speech Signal Analysis and Modelling
4.5 Summary
Speech signal is a highly-correlated signal which possesses both short- and
long-term similarities. These similarities or redundancies can easily be mod-
elled by very compact LPC and pitch ﬁlter formulations. The redundancies
are usually removed at the analysis stage so as to reduce the bit rate required
for transmitting the remaining residual signal. During the analysis of speech
to obtain the short- and long-term ﬁlter coefﬁcients, reasonable lengths of
samples are needed, which introduces some delay into the analysis process.
A typical block length of samples required for good analysis performance
is around 20–30 ms which corresponds to 160–240 samples at 8 kHz sam-
pling. The assumption is that the samples contained in the block do not vary
signiﬁcantly and hence can be analysed reasonable accurately. A 10th-order
short-term LPC ﬁlter updated every 20 ms and a single-order long-term pitch
ﬁlter updated every 5 ms give good performance.
Bibliography
[1] L. Rabiner and R. Schafer (1978) Digital Processing of Speech Signals.
Englewood Cliffs, NJ: Prentice-Hall
[2] B. Atal and M. Schroeder (1970) ‘Adaptive predictive coding of speech
signals’, in Bell Sys. Technical Journal, pp. 1973–87. October 1970.
[3] J. Makhoul (1975) ‘Linear prediction: A tutorial review’, in Proc. of IEEE,
63:561–80.
[4] C. McGonegal, L. R. Rabiner and A. E. Rosenberg (1977) ‘A subjective
evaluation of pitch detection methods using LPC synthesised speech’, in
IEEE Trans. on Acoust., Speech and Signal Processing, 25(3):221–9.
[5] R. P. Ramachandran and P. Kabal (1989) ‘Pitch prediction ﬁlters in
speech coding’, in IEEE Trans. On Acoust., Speech and Signal Processing,
37:467–78.
[6] M. Srinath and P. Rajasekaran (1979) An Introduction to Statistical Signal
Processing with Applications. John Wiley & Sons Ltd
[7] S. Saito and K. Nakata (1985) Fundamentals of Speech Signal Processing,
Chapter 9. Academic Press
[8] J. Makhoul (1977) ‘Stable and efﬁcient lattice methods for linear predic-
tion’, in IEEE Trans. on Acoust., Speech and Signal Processing, 25:423–8.
[9] J. H. Chen (1990) ‘High quality 16 kbit/s speech coding with a one-way
delay less than 2 ms’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 453–6.
[10] M. Schroeder and B. Atal (1979) ‘Predictive coding of speech signals
and subjective error criteria’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 27:247–54.

Bibliography
85
[11] P. Kabal and R. P. Ramachandran (1988) ‘Joint solutions for formant and
pitch predictors in speech processing’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 315–18.
[12] L. Rabiner (1977) ‘On the use of autocorrelation analysis for pitch detec-
tion’, in IEEE Trans. on Acoust., Speech and Signal Processing, 25(1):24–33.
[13] E. Chilton (1990) ‘Factors affecting the quality of linear predictive coding
of speech at low bit-rates’, Ph.D. thesis, CCSR, University of Surrey, UK.
[14] A. M. Noll (1967) ‘Cepstrum pitch determination’, in Journal of the Acoustic
Soc. of America, 41:293–309.
[15] J. Wise, J. R. Caprio, and T. W. Parks (1976) ‘Maximum likelihood
pitch estimation’, in IEEE Trans. on Acoust., Speech and Signal Processing,
24:418–23.

5
Efficient LPC Quantization
Methods
5.1 Introduction
Linear predictive coding is a very powerful analysis technique and is used
in many speech processing systems. In speech coding and synthesis systems,
the analysis techniques for obtaining the LP coefﬁcients (LPC), e.g. autocorre-
lation, covariance, lattice, and the quantization of the LPC are very important
aspects of LPC analysis as minimization of coding capacity is the ultimate
aim in these applications. The main objective of the quantization procedure
is to code the LPC with as few bits as possible without introducing audible
spectral distortion. Whilst perfect reconstruction is not possible, subjective
transparency is achievable. Quantization of the LPC is usually performed by
transforming the LPC to other forms which enables predictive coding and
allows an easy ﬁlter stability check. The most popular LPC transformation is
the use of Line Spectrum Pairs (LSP), related to the Line Spectral Frequency
(LSF) representation of the LPC [1, 2]. In this chapter, the LSF representation
of the LPC will be described, followed by various LPC quantization schemes
using LSF transformation.
5.2 Alternative Representation of LPC
As was shown in Chapter 4, the LPC ﬁlter is given by
H(z) =
1
1 +
p

i=1
αiz−i
(5.1)
where p is the order of LPC ﬁlter.
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

88
Efficient LPC Quantization Methods
The αi coefﬁcients are the direct form of LPC. The ﬁlter H(z) is stable if it
is minimum phase, i.e. all the roots of the equation (5.1) are within the unit
circle. If αi were quantized directly, the stability of the ﬁlter H(z) is not easily
guaranteed as the roots of equation (5.1) are not usually computed to check
for stability. Thus a more useful parameter, the PARCOR (partial correlation)
coefﬁcients, ki, are usually used for quantization. The distribution plots of
PARCOR parameters for a 10th-order LPC ﬁlter are shown in Figure 5.1. The
forward and backward transformation are given below [3].
LPC to PARCOR:
ap
j = αj
1 ≤j ≤p
For i = p, p −1, . . . , 1
(5.2)
ai−1
j
= (ai
j + ai
iai
i−j)/(1 −k2
i ),
1 ≤j ≤i −1
ki−1 = ai−1
i−1
PARCOR to LPC:
For i = 1, 2, . . . , p
ai
i = ki
(5.3)
ai
j = ai−1
j
−kiai−1
i−j ,
1 ≤j ≤i −1
αj = ap
j ,
1 ≤j ≤p
The LPC ﬁlter is stable if |ki| ≤1.0. Although ki can easily be checked for
stability, they are not suitable for quantization because they possess a nonﬂat
spectral sensitivity, i.e. values of ki near unity require more quantization
accuracy than those away from unity. Thus, nonlinear functions of ki are
required, with the Log-Area Ratio (LAR) and inverse sine (IS) functions
being the most widely used [4]. For LAR and IS, the forward and backward
transformation are given below:
PARCOR to LAR:
gi = log
1 −ki
1 + ki

,
1 ≤i ≤p
(5.4)
LAR to PARCOR:
ki =
1 −10gi
1 + 10gi

,
1 ≤i ≤p
(5.5)

Alternative Representation of LPC
89
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
−1.0 −0.5
0.0
0.5
1.0
0.0
5.0
10.0
15.0
% of occurrence
% of occurrence
% of occurrence
0.0
1.0
2.0
3.0
0.0
1.0
2.0
3.0
4.0
0.0
1.0
2.0
3.0
% of occurrence
% of occurrence
% of occurrence
0.0
1.0
2.0
3.0
4.0
0.0
1.0
2.0
3.0
4.0
0.0
1.0
2.0
3.0
4.0
5.0
% of occurrence
% of occurrence
% of occurrence
0.0
1.0
2.0
3.0
4.0
5.0
0.0
1.0
2.0
3.0
4.0
5.0
0.0
1.0
2.0
3.0
4.0
5.0
% of occurrence
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
Figure 5.1
The distribution plots of PARCOR LPC

90
Efficient LPC Quantization Methods
PARCOR to IS:
si = sin−1(ki),
1 ≤i ≤p
(5.6)
IS to PARCOR:
ki = sin(si),
1 ≤i ≤p
(5.7)
The distribution plots of LAR and IS parameters for a 10th order LPC ﬁlter
are shown in Figures 5.2 and 5.3 respectively.
Although it is possible to design good performance quantizers using the
LAR and IS representations, the frame-to-frame correlation of LPC (which
evidently exists for slowly-varying parts of speech) is not highlighted in
either LAR or IS representations, i.e. it is difﬁcult to predict frame-to-frame
parameter values. Thus, not all the redundancies are fully exploitable.
In view of the shortcomings of LAR and IS representation, the line spectral
pairs (LSP) or frequencies (LSF) representations of LPC have been investigated
[2]. The concept of LSF was introduced by Itakura, but it remained almost
dormant until its usefulness was re-examined in the latest speech coding
standards. LSFs encode speech spectral information in the frequency domain
and have been found to be capable of improving the coding efﬁciency by
more than other transformation techniques, especially when incorporated into
predictive quantization schemes. For use in conventional scalar quantization,
it has been shown by Cox [4] and others that LSF is not signiﬁcantly better
than LAR or IS, but it does have other properties which are desirable, as
will be discussed in later sections. The fact that LSF representation is in the
frequency domain means that quantization can easily incorporate spectral
features known to be important in perceiving speech signals. In addition,
LSFs lend themselves to frame-to-frame interpolation with smooth spectral
changes because of their intimate relationship with format frequencies.
5.3 LPC to LSF Transformation
An all-pole digital ﬁlter for speech synthesis, H(z), can be derived from linear
predictive analysis and is given by
H(z) = 1/Ap(z)
(5.8)
where,
Ap(z) = 1 +
p

k=1
αkz−k
(5.9)

LPC to LSF Transformation
91
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
−4.0 −2.0
0.0
2.0
4.0
0.0
1.0
2.0
3.0
4.0
% of occurrence
% of occurrence
% of occurrence
0.0
2.0
4.0
6.0
8.0
0.0
2.0
4.0
6.0
8.0
10.0
0.0
2.0
4.0
6.0
8.0
10.0
% of occurrence
% of occurrence
% of occurrence
0.0
5.0
10.0
15.0
0.0
5.0
10.0
15.0
0.0
5.0
10.0
15.0
% of occurrence
% of occurrence
% of occurrence
0.0
5.0
10.0
15.0
0.0
5.0
10.0
15.0
0.0
5.0
10.0
15.0
20.0
% of occurrence
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
Figure 5.2
The distribution plots of LAR parameters

92
Efficient LPC Quantization Methods
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
−2.0 −1.0
0.0
1.0
2.0
0.0
1.0
2.0
3.0
4.0
5.0
% of occurrence
0.0
1.0
2.0
3.0
4.0
% of occurrence
0.0
1.0
2.0
3.0
4.0
5.0
% of occurrence
0.0
1.0
2.0
3.0
4.0
5.0
% of occurrence
% of occurrence
% of occurrence
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
% of occurrence
% of occurrence
% of occurrence
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
% of occurrence
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
Figure 5.3
The distribution plots of inverse sine parameters (horizontal axis is
in radians)

LPC to LSF Transformation
93
Z−1
Bp−1
Bp
Z−1
Ap−1
Ap
+1/−1
Z
With loss
−
−
kp
Lossless
X
Input
Terminal
B0
A0
Y
Output
Terminal
(k0 = −1)
Z−1
kp−1
Figure 5.4
PARCOR structure of LPC synthesis
The PARCOR representation is an equivalent version and its digital form is
as shown in Figure 5.4, where,
Ap−1(z) = Ap(z) + kpBp−1(z)
(5.10)
Bp(z) = z−1[Bp−1(z) −kpAp−1(z)]
(5.11)
where A0(z) = 1 and B0(z) = z−1, and
Bp(z) = z−(p+1)Ap(z−1)
(5.12)
The PARCOR representation as shown in Figure 5.4 is stable for |ki| < 1 for
all i. In Figure 5.4, the transfer function, TF, from X to Y is Hp(z), and from Y
to Z is Bp(z), therefore the TF from X to Z is given by equation (5.13) where
Rp(z) is the ratio ﬁlter,
Rp = Bp(z)/Ap(z)
(5.13)
The PARCOR synthesis process can be viewed as sound wave propagation
through a lossless acoustic tube, consisting of p sections of equal length but
nonuniform cross sections. The acoustic tube is open at the terminal corre-
sponding to the lips and each section is numbered from the lips. Mismatching
between the adjacent sections p and (p + 1) causes wave propagation reﬂec-
tion. The reﬂection coefﬁcients are equal to the pth PARCOR coefﬁcient kp.
Section p + 1, which corresponds to the glottis, is terminated by a matched
impedance. The excitation signal applied to the glottis drives the acoustic
tube.
In PARCOR analysis, the boundary condition at the glottis is impedance-
matched. Now consider a pair of artiﬁcial boundary conditions where the
acoustic tube is completely closed or open at the glottis. These conditions
correspond to kp+1 = 1 and kp+1 = −1, a pair of extreme values for the

94
Efficient LPC Quantization Methods
artiﬁcially-extended PARCOR coefﬁcients which correspond to perfectly loss-
less tubes. The value Q of each resonance becomes inﬁnite and the spectrum
of distributed energy is concentrated in several line spectra. The feedback
conditions for kp+1 = −1 correspond to a perfect closure at the input (glottis)
and for kp+1 = 1 correspond to an opening to inﬁnite free space. To derive
the line spectra or line spectrum frequencies (LSF), we proceed as follows
(it is assumed that the PARCOR ﬁlter is stable and the order is even). Ap(z)
may be decomposed to a set of two transfer functions, one having an even
symmetry and the other having an odd symmetry. This can be accomplished
by taking a difference and sum between Ap(z) and its conjugate functions.
Hence the transfer functions with kp+1 = ±1 are denoted by Pp+1(z) and
Qp+1(z).
For kp+1 = 1,
Pp+1(z) = Ap(z) −Bp(z) (Difference ﬁlter)
For kp+1 = −1,
Qp+1(z) = Ap(z) + Bp(z) (Sum ﬁlter)
(5.14)
⇒Ap(z) = 1
2[Pp+1(z) + Qp+1(z)]
(5.15)
Substituting equation (5.12) into (5.14),
Pp+1(z) = Ap(z) −z−(p+1)Ap(z−1)
(5.16)
= 1 + (α1 −αp)z−1 + . . . + (αp −α1)z−p −z−(p+1)
= z−(p+1)
p+1

i=0
(z + ai)
where ai is generally complex. Similarly,
Qp+1(z) = z−(p+1)
p+1

i=0
(z + bi)
(5.17)
As we know that two roots exist (kp+1 = ±1), the order of Pp+1(z) and Qp+1(z)
can be reduced, i.e.
P′(z) = Pp+1(z)
(1 −z)
(5.18)
= A0zp + A1z(p−1) + . . . + Ap

LPC to LSF Transformation
95
and,
Q′(z) = Qp+1(z)
(1 + z)
(5.19)
= B0zp + B1z(p−1) + . . . + Bp
where,
A0 = 1
(5.20)
B0 = 1
(5.21)
Ak = (αk −αp+1−k) + Ak−1
(5.22)
Bk = (αk + αp+1−k) −Bk−1
(5.23)
for k = 1, . . . , p
The LSFs are the angular positions of the roots of P′(z) and Q′(z) with
0 ≤ωi ≤π. The roots occur in complex conjugate pairs and have the
following properties:
1. All roots of P′(z) and Q′(z) lie on the unit circle.
2. The roots of Q′(z) and P′(z) alternate with each other on the unit circle, i.e.
the following is always satisﬁed, 0 ≤ωq,0 < ωp,0 < ωq,1 < ωp,1 . . . , ≤π.
5.3.1 Complex Root Method
The roots of equation (5.18) can be solved using complex arithmetic. This
will give complex conjugate roots on the unit circle and the frequencies are
then given by the inverse tangent of the roots. This method is obviously very
complex as it involves solving two polynomials of pth order using complex
arithmetic. Also, as it uses an iteration procedure for determining the roots,
the time required for this method is not deterministic which is undesirable
for real-time implementations.
5.3.2 Real Root Method
As the coefﬁcients of P′(z) and Q′(z) are symmetrical the order of equation
(5.18) can be reduced to p/2.
P′(z) = A0zp + Ap−1
1
+ . . . + A1z1 + A0
(5.24)
= zp/2[A0(zp/2 + z−p/2) + A1(z(p/2−1) + z−(p/2−1)) + . . . + Ap/2]

96
Efficient LPC Quantization Methods
Similarly,
Q′(z) = B0zp + Bp−1
1
+ . . . + B1z1 + B0
(5.25)
= zp/2[B0(zp/2 + z−p/2) + B1(z(p/2−1) + z−(p/2−1)) + . . . + Bp/2]
As all roots are on the unit circle, we can evaluate equation (5.24) on the unit
circle only.
Let z = ejω then z1 + z−1 = 2 cos(ω)
(5.26)
P′(z) = 2e jpω/2

A0 cos
p
2ω

+ A1 cos
p −2
2
ω

+ . . . + 1
2Ap/2

(5.27)
Q′(z) = 2e jpω/2

B0 cos
p
2ω

+ B1 cos
p −2
2
ω

+ . . . + 1
2Bp/2

(5.28)
By making the substitution x = cos(ω), equations (5.27) and (5.28) can be
solved for x. For example, with p = 10, the following is obtained:
P′
10(x) = 16A0x5 + 8A1x4 + (4A2 −20A0)x3 + (2A3 −8A1)x2
+(5A0 −3A2 + A4)x + (A1 −A3 + 0.5A5)
(5.29)
and similarly,
Q′
10(x) = 16B0x5 + 8B1x4 + (4B2 −20B0)x3 + (2B3 −8B1)x2
+(5B0 −3B2 + B4)x + (B1 −B3 + 0.5B5)
(5.30)
The LSFs are then given by:
LSF(i) = cos−1(xi)
2πT
,
for 1 ≤i ≤p
(5.31)
The distribution plots of LSFs for a 10th order LPC ﬁlter are shown in
Figure 5.5 and a typical LSF plot is shown in Figure 5.6, where the ﬁrst half
is active speech and the second half is silence. Notice that during silent
regions the frequencies are evenly spread between 0 and fs/2 where fs is the
sampling frequency. This method is obviously considerably simpler than the
complex root method, but it still suffers from indeterministic computation
time. However, a faster root search can be accomplished by noting that the
change from one LSF vector to the next is not too drastic in most cases. Thus
by using the previous values as the starting estimates of the roots, the number
of iterations required per root is considerably reduced, e.g. typically from 5
to 10 iterations.

LPC to LSF Transformation
97
0.0
1000.0
0.0
1.0
2.0
3.0
% of occurrence
0.0
1000.0
2000.0
0.0
0.5
1.0
1.5
% of occurrence
0.0
1000.0
2000.0
0.0
0.2
0.4
0.6
0.8
1.0
% of occurrence
0.0
1000.0
2000.0
0.0
0.5
1.0
1.5
% of occurrence
0.0
0.5
1.0
1.5
% of occurrence
0.0
0.5
1.0
1.5
% of occurrence
500.0
1500.0
2500.0
1000.0
2000.0
3000.0
1500.0
2500.0
3500.0
0.0
0.5
1.0
1.5
2.0
% of occurrence
2000.0
3000.0
4000.0
0.0
0.5
1.0
1.5
% of occurrence
2000.0
3000.0
4000.0
0.0
0.5
1.0
1.5
% of occurrence
2500.0
3500.0
0.0
0.5
1.0
1.5
2.0
% of occurrence
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
Figure 5.5
The distribution plots of LSF parameters (horizontal axis is in Hz)

98
Efficient LPC Quantization Methods
0.0
20.0
40.0
Time/160 sample frames
Frequency/Hz
60.0
80.0
100.0
4000.0
3000.0
2000.0
1000.0
0.0
Figure 5.6
Typical LSF trajectories for voiced and unvoiced speech
5.3.3 Ratio Filter Method
The expression for the ratio ﬁlter is given by equation (5.32). The phase
response, φ(kfs), of the ratio ﬁlter is given by equation (5.34). The frequency
corresponding to a multiple of −π and −2π radians are the lower and upper
line spectra of the LSF [5].
Rp(z) = z−(n+1)Ap(z−1)
Ap(z)
(5.32)
where,
Ap(z) = 1 −
n

i=1
βiz−i
(5.33)

LPC to LSF Transformation
99
and βi = −αi where αi are the LPC.
φ(kfs) = −(n + 1)(2πTkfs)
−2 tan−1



n

i=1
βi sin(2πiTkfs)
1 −
n

i=1
βi cos(2πiTkfs)



(5.34)
where T is the sampling period, fs is the frequency step, and k = 1, 2, 3, . . . ,
Kmax.
By performing a Discrete Fourier Transform (DFT) on the coefﬁcient
sequence, Ak and Bk, ωi can be solved as the zero-valued frequencies of a
power spectrum. A typical plot, showing the partial minima of the spectrum,
is shown in Figure 5.7.
If the spectrum were to be obtained directly, it would involve an enormous
number of computations. Fortunately, a number of computation reductions
can be made. The aim is to ﬁnd the partial minima of the response, thus
0.0
100.0
200.0
5.0
4.0
3.0
2.0
1.0
0.0
Q’(z)
P’(z)
Frequency/4000/512Hz per div
Relative amplitude
Figure 5.7
Zero frequency plot for one frame of the DFT–LSF method

100
Efficient LPC Quantization Methods
the absolute values of the response are not critical; only the locations of the
minima are vital. The spectrum is given by equation (5.35) where P is the
spectrum, W is the L × L DFT kernel, and S is the input sequence. L is the size
of the transform.


−
P
−

=


−
−
−
−
W
−
−
−
−




−
S
−


(5.35)
As the input sequences Ak and Bk are real, we can move them from the start
to the middle of S with zeros elsewhere. This will produce an even spectrum
which means that only fs/2 terms need to be computed. Also, the spectrum
will be real, thus only the cosine-terms in the kernel require computing. Since
the sequences Ak and Bk are even, only half of the values need to be computed,
i.e. A0 to Ap/2−1 and 1/2Ap/2, and similarly for Bk. With these savings the
number of multiply–adds is reduced to p/2 + 1 per spectrum point. The
cosine terms are ﬁxed for a particular transform size, therefore they can be
pre-computed and stored in a lookup table.
Once the spectrum is found the partial minima need to be located and this
involves computationally expensive comparisons. As the LSF are naturally
ordered, i.e. the frequencies alternate between Q(z) and P(z), they can be
located in an efﬁcient manner. The ﬁrst Q(z) LSF starts at the origin, then the
ﬁrst P(z) LSF starts from the previous Q(z) LSF location. Once the ﬁrst P(z)
LSF is found the second Q(z) LSF is located, starting from the previous P(z)
location. This alternation is repeated until all LSFs are found. Thus in total
only one pass of the frequency range is made instead of two.
5.3.4 Chebyshev Series Method
Another step-wise method which requires no prior storage or calculation of
trigonometric functions is the Chebyshev Series Method [6]. By expanding
equation (5.24) with the Chebyshev polynomial set, the mapping x = cos(ω)
maps the upper semicircle in the z-plane to the real interval [+1, −1]. There-
fore, all the roots xi lie between −1 and +1, with the root corresponding to
the lowest frequency LSF being the one nearest to +1. Thus the basic task
is similar to the DFT method, i.e. we isolate the roots of P′(z) and Q′(z)
by searching incrementally for intervals in which the sign changes which is
reﬁned by successive bisections of the root interval.
5.3.5 Adaptive Sequential LMS Method
All of the previously described methods for deriving the LSF parameters
required the intermediate step of calculating the LPC before proceeding
to the computation of the LSF parameters. However, using a Least Mean

LSF to LPC Transformation
101
Squares adaptive method [7] the LSF parameters can be computed directly
from the speech samples themselves. The LMS algorithm aims to minimize
the mean-square value of the PARCOR lattice ﬁlter output, and thus ﬂatten
its frequency spectrum by a ‘noisy steepest-descent’ procedure which uses
the squared value of a single output sample to approximate the mean-square
value. Thus the algorithm begins the sequential estimation using evenly-
distributed estimated LSFs and, as each sample of speech is processed, a
new LSF vector estimate is obtained. Depending on the adaptation rate
required, the algorithm converges to the correct value after around 100
samples of input.
The LMS method is very attractive because it requires no LPC analysis.
However, as it is a ‘learning’ type algorithm, it is susceptible to ‘out-lier’
input samples, i.e. samples which are different in character to the majority of
speech samples. The effect of these unusual inputs is to throw the algorithm
off its convergence curve; if this occurs at the end of a frame there will be no
time for correction before the ﬁnal values are used.
5.4 LSF to LPC Transformation
There are two methods for the inverse transformation, neither of which
is as computationally intensive as the forward transformation. The two
methods are equivalent but the LPC synthesis method is perhaps more easily
visualized.
5.4.1 Direct Expansion Method
In all of the LPC to LSF methods above the aim is to ﬁnd the roots of
equation (5.16), i.e. ai and bi. Having found these roots using any of the
methods, the LPC, αi, can be simply found by multiplying out the product
terms of equation (5.16), i.e.
Pp+1(z) = z−(p+1)[P′(z)(1 −z)]
(5.36)
= z−(p+1)[(1 −z)(z −r0)(z −r∗
0) . . . (z −rp/2)(z −r∗
p/2]
= z−(p+1)[(1 −z)(z2 −2u0z + t0) . . . (z2 −2up/2z + tp/2)]
= S0 + S1z−1 + . . . + Spz−p + Sp+1z−(p+1)
(5.37)
Similarly,
Qp+1(z) = T0 + T1z−1 + . . . + Tpz−p + Tp+1z−(p+1)
(5.38)

102
Efficient LPC Quantization Methods
where,
ri = ui + jvi
and
r∗
i = ui −jvi
⇒
ri + r∗
i = 2ui
and
ri × r∗
i = u2
i + v2
i = ti
(5.39)
Equating the terms of equations (5.37) and (5.16),
S0 = 1
(5.40)
T0 = 1
(5.41)
Sp+1 = −1
(5.42)
Tp+1 = 1
(5.43)
αi = 1
2(Ti + Si)
(5.44)
αp+1−i = 1
2(Ti −Si)
(5.45)
for i = 1, . . . , P/2
5.4.2 LPC Synthesis Filter Method
An LPC synthesis can be constructed directly using the LSF coefﬁcients. The
ﬁlter is derived from the following,
H(z) = 1/Ap(z) = 1/[1 + (Ap(z) −1)]
(5.46)
=
1
1 + 1/2[(Pp+1(z) −1) + (Qp+1(z) −1)]
i.e.
Ap(z) −1 = 1/2[(Pp+1(z) −1) + (Qp+1(z) −1)]
(5.47)
= 1/2


(1 −z)
p/2

i=1
(1 −2 cos ωiz + z2) −1
+(1 + z)
p/2

i=1
(1 −2 cos θiz + z2) −1



(5.48)
Let ui = −2 cos ωi, vi = −2 cos θi

Properties of LSFs
103
where wi and θi are the even and odd number LSFs given by LSF(i)2πT.
Ap(z) −1 = 1/2



p/2

i=1
(1 + uiz + z2)
(5.49)
−z
p/2

i=1
(1 + uiz + z2) −1



+1/2



p/2

i=1
(1 + viz + z2)
−z
p/2

i=1
(1 + viz + z2) −1



(5.50)
= z/2


(u1 + z) −
p/2

j=1
(1 + ujz + z2)
+
p/2−1

i=1
(ui+1 + z)
i
j=1
(1 + ujz + z2)



+ z/2


(v1 + z) −
p/2

j=1
(1 + vjz + z2)
+
p/2−1

i=1
(vi+1 + z)
i
j=1
(1 + vjz + z2)



(5.51)
An 8th order inverse ﬁlter is shown in Figure 5.8. The LPC are simply the
impulse response of the ﬁlter.
5.5 Properties of LSFs
A very important LSF property, as mentioned earlier, is the natural ordering
of its parameters. This ordering property was already used to good effect
in speeding up the LPC to LSF transformation procedure. The ordering
property indicates that the LSFs within a frame, and from frame to frame, are
correlated. In order to illustrate the intra-frame correlation property of the

104
Efficient LPC Quantization Methods
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
−1
+
−
1/2z−1
c3
c5
c7
c1
c2
c4
c6
c8
Output
Input
Figure 5.8
Practical scheme of LSF inverse filter (ci = −2 cos ωi, for even i, and
ci = −2 cos θi, for odd i)
Table 5.1
Experimental conditions for
estimating  and 
Sampling Frequency
8 kHz
Frame Update
10 ms
Window
20 ms Hamming
Analysis order
10
Number of Frames
6000
LSF vector, ω, Table 5.2 presents the matrix  = {φi,j} where,
φi,j = ωn,i × ωn,j, i = 1, 2, . . . , p, j = 1, 2, . . . , p
(5.52)
for the experimental conditions according to Table 5.1. The relatively high
correlation between neighbouring LSFs is clear. Similarly, to illustrate the
inter-frame correlation of the LSF parameters, Table 5.3 presents the matrix
 = {φi,k} where,
φi,k = ωn,i × ωn−k,i, i = 1, 2, . . . , p, k = 1, 2, . . . , p
(5.53)
From Tables 5.2 and 5.3, it is clear that there is a strong correlation between
the LSFs of adjacent frames as well as neighbouring parameters in the same
frame. Therefore, any compression algorithm that effectively makes use of
these correlations can result in improved performance over those that do not
incorporate this correlation property.

LSF Quantization
105
Table 5.2
Intra-frame correlation coefficients 
j
i
1
2
3
4
5
6
7
8
9
10
1
1.00
0.65
−0.30
−0.35
−0.41
−0.49
−0.39
−0.40
−0.36
−0.20
2
0.65
1.00
0.28
0.11
−0.07
−0.13
−0.07
−0.05
−0.06
−0.07
3
−0.30
0.28
1.00
0.72
0.50
0.53
0.46
0.54
0.39
0.28
4
−0.35
0.11
0.72
1.00
0.72
0.62
0.46
0.42
0.45
0.21
5
−0.41
−0.07
0.50
0.72
1.00
0.79
0.52
0.47
0.34
0.26
6
−0.49
−0.13
0.53
0.62
0.79
1.00
0.71
0.61
0.49
0.28
7
−0.39
−0.07
0.46
0.46
0.52
0.71
1.00
0.73
0.58
0.41
8
−0.40
−0.05
0.54
0.42
0.47
0.61
0.73
1.00
0.58
0.46
9
−0.36
−0.06
0.39
0.45
0.34
0.49
0.58
0.58
1.00
0.41
10
−0.20
−0.07
0.28
0.21
0.26
0.28
0.41
0.46
0.41
1.00
Table 5.3
Inter-frame correlation coefficients 
k
i
1
2
3
4
5
6
7
8
9
10
1
0.93
0.84
0.76
0.68
0.61
0.55
0.50
0.45
0.41
0.36
2
0.89
0.75
0.63
0.54
0.46
0.38
0.32
0.27
0.22
0.18
3
0.92
0.80
0.70
0.60
0.51
0.43
0.36
0.30
0.24
0.20
4
0.92
0.82
0.73
0.64
0.56
0.49
0.43
0.37
0.32
0.27
5
0.95
0.88
0.81
0.74
0.67
0.61
0.54
0.48
0.43
0.37
6
0.94
0.85
0.77
0.69
0.62
0.56
0.49
0.44
0.38
0.33
7
0.93
0.83
0.75
0.66
0.58
0.50
0.43
0.37
0.31
0.26
8
0.91
0.81
0.72
0.64
0.56
0.49
0.43
0.37
0.32
0.28
9
0.87
0.73
0.64
0.55
0.48
0.42
0.37
0.33
0.29
0.25
10
0.82
0.66
0.57
0.50
0.44
0.38
0.34
0.30
0.27
0.24
5.6 LSF Quantization
Most modern speech coders make use of LPC modelling during speech
processing. Although some coders use a backward-adaptive LPC ﬁlter [8],
most speech coders extract the LPC parameters from the input speech at
regular intervals, transform them into the LSF domain, and quantize them
for transmission to the decoder.
Low distortion LSF quantization is essential for the overall quality of
decoded speech, and the number of bits allocated to LSFs usually takes a
signiﬁcant proportion of the overall bit rate, up to over 50 % for very low

106
Efficient LPC Quantization Methods
bit-rate speech coders. Therefore the overall success of a given speech coding
scheme depends greatly on the quality of the LSF quantizer used.
Scalar schemes can be used, as they present very low complexity and
storage requirements. However they cannot make use of the high intra-frame
correlation exhibited by LSF vectors and, hence, they are very rarely used due
to their poor performance. Vector quantization (VQ) schemes can be used
to exploit intra-frame correlations. VQ exploits the redundancies in the LSF
vector well and can provide high quality quantization for a relatively limited
number of bits per frame of speech. As a result, they are widely used in
modern speech coders. The following sections investigate the use of VQ for
LSF quantization and ways of maximizing the performance of such schemes
in several coder conﬁgurations.
5.6.1 Distortion Measures
In order to achieve good performance quantization of LSF parameters, it is
necessary to have a way of linking the quantization error to the distortion
in perceptual quality. Due to the complex relationship that exists between a
set of LSF coefﬁcients and the frequency response of the corresponding LPC
ﬁlter, using a Mean-Square Error (MSE) measurement may not lead to an
optimal performance of the quantizer.
A widely-used technique for computing the distortion that exists between
the original set of LSFs and their quantized version is the Log Spectral
Distortion measure. However a Weighted Mean-Square Error (WMSE) mea-
surement may also lead to good results if an appropriate weighting function
is used.
5.6.2 Spectral Distortion
The mean square log spectral distortion, which will be referred to simply as
spectral distortion (SD), is deﬁned as:
sd = 1
π
 π
0
[10 log10S(w) −10 log10S′(w)]2
(5.54)
where S(w) and S′(w) are the frequency responses of the LPC ﬁlter derived
from the original and quantized LSFs, respectively. S(w) can therefore be
deﬁned as:
S(w) = 1/ | A(w) |2
(5.55)
which leads to,
S(w) = 1/| 1 −
p

k=1
ake−jwk |2
(5.56)

LSF Quantization
107
where ak are the LPC coefﬁcients. This can be evaluated using an N-point
Fourier Transform, giving the following expression:
SD =
1
N/2
N/2−1

k=0

10 log10 | A′(k) |2 −10 log10 | A(k) |22
(5.57)
Moreover, it is common practice to restrict the computation of the distortion to
a limited portion of the spectrum, typically the 125–3100 Hz band. The reason
is that the portions of the spectrum below 125 Hz and above 3100 Hz usually
have perceptually little impact but may signiﬁcantly affect the computed
spectral distortion, due to the use of the log function.
5.6.3 Average Spectral Distortion and Outliers
The spectral distortion (SD) measure gives a good indication of the perceptual
difference between two sets of LSFs. The overall distortion caused by a
quantization scheme can be computed by simply averaging the SD obtained
over a large sequence of LSF vectors. It is commonly accepted that an average
SD below 1 dB is necessary for an LSF quantizer to be transparent, i.e. not
to add any audible distortion to synthesized speech. However, the average
SD (aveSD) is not sufﬁcient to determine the performance of a quantizer. The
human ear is very sensitive to occasional large quantization errors. Therefore
it is also important that the number of times the quantizer gives a large
distortion is kept to a minimum. It is customary to use the percentage of input
vectors giving spectral distortions above 2 dB and 4 dB as a quality measure.
These measures are referred to as outliers at 2 dB and 4 dB, respectively.
The set of requirements usually considered necessary to achieve good
quality speech is [9]:
• Average spectral distortion less than 1 dB
• Fewer than 2 % outliers at 2 dB
• No outliers at 4 dB
These three parameters need to be considered when evaluating the perfor-
mance of an LSF quantizer. However an optimization has to be carried out
to achieve the best overall performance for a given bit rate, i.e. accepting a
larger average spectral distortion in return for fewer outliers.
5.6.4 MSE Weighting Techniques
Although spectral distortion is a fairly accurate representation of how quan-
tization noise in the LSF is perceived, its high computational complexity
limits its use. In order to compare two sets of LSFs, two fairly large fast
Fourier Transforms (FFT) need to be computed and a logarithm must then be

108
Efficient LPC Quantization Methods
computed for every bin of each FFT output. This is, of course, not a problem
when estimating the performance of a quantizer off-line, but severely limits
its use in a real-time coder.
On the other hand, simple MSE techniques have much lower complexity
and can easily be implemented in real-time coders. However the basic MSE
methods do not take into account the different perceptual effect of each of the
LSFs, and this may lead to poor performance of the quantizer. One simple
way to reduce this problem is to introduce an appropriate weighting function
in the calculation of the MSE (WMSE). The WMSE between the LSF vector f
and the candidate vector ˆf (frequencies are in Hz) is given by:
d(f, ˆf) = (f −ˆf)TW(f −ˆf)
(5.58)
where W is a positive diagonal matrix. This is equivalent to:
d(f, ˆf) =
p

n=1
wn( fn −ˆfn)2
(5.59)
where w is a positive weighting vector.
The weighting vector renders contributions of certain elements more impor-
tant than others in the summation process. The weighting vector is usually a
function of the original LSF vector, and therefore needs to be computed only
once per quantization (i.e once for every frame). A correctly chosen weighting
function will improve the perceptual quality of the quantization but ﬁnding a
suitable weighting function is difﬁcult, as it needs to be related to perceptual
quality. Various weighting functions have been investigated in the literature
and the most popular ones are presented here.
Paliwal–Atal
This LSF weighting method is based on the frequency response of the original
LPC ﬁlter [9]. The weights are calculated as:
wn = cn [P( fn)]τ
(5.60)
where P( fn) is the LPC power spectrum associated with the original set of
LSFs, fn is the nth LSF. τ is a constant used to determine the relative importance
of the LSF and is experimentally set to 0.3. Finally, the fact that the human ear
cannot resolve high frequencies very well is used in introducing the factor cn,
which reduces the inﬂuence of the last two LSFs in the summation.
cn =



1.0
for 1 ≤n ≤8
0.8
for n = 9
0.4
for n = 10
(5.61)

LSF Quantization
109
EFR weighting
This weighting function is used in the GSM Enhanced Full Rate standard
(EFR) [10]. The weights are calculated as follows:
wn =



3.347 −1.547
450 dn
for dn ≤450
1.8 −0.8
1050 (dn −450)
otherwise
(5.62)
where
dn = fn+1 −fn−1
(5.63)
and fn is the nth LSF, f0 = 0 and f11 = 4000.
LSF inverse distance
This method is based on the principle that the peaks in the LPC ﬁlter are
located where two consecutive LSFs are close to each other. The weighting is
given by:
wn =
4000
( fn −fn−1) +
4000
( fn+1 −fn)
(5.64)
Group Delays
This weighting is based on the group delay of the LPC ﬁlter and is deﬁned as
[11]:
wn =



u( fn)
 Dn
Dmax
1.375 ≤Dn ≤Dmax
u( fn)
Dn
√1.375 Dmax
Dn < 1.375
(5.65)
where
u( fn) =
 1
fn < 1000
1 −0.5
3000(fn −1000)
1000 ≤fn ≤4000
(5.66)
Dn is the group delay of the LPC ﬁlter at the frequency fn in milliseconds
whilst Dmax is the maximum group delay, experimentally found to be around
20 ms.
The group delays of the ﬁlter are larger at the formant frequencies, therefore
the weighting will be higher for these frequencies. The factor u(fn) simply
reduces the weights for the higher frequencies to take into account the

110
Efficient LPC Quantization Methods
0
1000
2000
3000
4000
Frequency (Hz)
0
0.25
0.5
0.75
1.0
Normalised Weights
LP Spectrum
LSF Locations
EFR Weights
PaliwalAtal
Group Delay
LSF Inverse Distance
0
10
20
30
LP Filter Frequency Response (dB)
LP Spectrum
LSF Locations
EFR Weights
PaliwalAtal
Group Delay
LSF Inverse Distance
Figure 5.9
Example of various LSF weighting functions
lesser sensitivity of the ear to the spectral distortion above 1000 Hz. The
relationship between the position of the LSF and the peaks in the LPC
spectrum is illustrated in Figure 5.9. It can be seen that a peak in the LPC
spectrum usually corresponds to a pair of LSFs close to each other, which
justiﬁes in particular the LSF inverse distance weighting method. The result
of each of the weighting functions described above is also plotted at each
LSF location, after normalization. It can be seen that although all have
similar overall characteristics close to the peaks, they vary signiﬁcantly in
the importance they place on the LSFs situated in the valleys and the higher
frequencies.
5.7 Codebook Structures
In order to obtain good quality speech with a low bit-rate speech coder, it
is necessary for the LSF quantizer to fulﬁl the requirements on the spectral
distortion described in Section 5.6.3. This is usually achieved using a vec-
tor quantizer in order to maximize the quantization efﬁciency, and such a

Codebook Structures
111
system typically requires 20 to 25 bits to represent a set of 10 LSF parameters
with the required accuracy. Such a large number of bits precludes the use
of straightforward vector quantization of the LSF vector, as the complexity
and storage requirements of such a system would be far too great to be
implemented on any reasonably priced device. Therefore, alternative subop-
timal methods have to be used, which add structure to the codebook in order
to reduce implementation costs. The two most common methods are split
vector quantization (SVQ) and multi-stage vector quantization (MSVQ).
5.7.1 Split Vector Quantization
Direct quantization of a set of LSF parameters with a typical vector quantizer
of 25 bits, would require a codebook with 1025 entries, which is not practical
from both the search complexity and memory point of view. An alternative
method is to use SVQ, where the 10-element LSF vector is split into a number
of smaller subvectors, each quantized independently using a small number
of bits. Since the complexity and storage requirements of a full-search vector
quantizer are exponential functions of the number of bits used to represent
the input vector, SVQ requires only a fraction of the complexity required by
a full search VQ.
In an SVQ system, an input vector f is represented by a vector ˆf given by:
ˆf = {{yi0
0 (0), .., yi0
0 (N0)}, .., {yiK−1
K−1(0), .., yiK−1
K−1(NK−1)}}
(5.67)
where K is the number of subvectors, each of length Nk, yik
k (n) is the nth element
from the kth codebook, and ik is the codebook index for the kth subvector.
Obviously K and Nk are chosen so that the sum of Nk for k = 0, 1, . . . , K −1 is
equal to the length of the input LSF vector.
Splitting the 10-element LSF vector can be performed in various ways and
some classic conﬁgurations are illustrated in Table 5.4. The split usually takes
into account some of the perceptual properties of the LSF vector, such as the
fact that lower frequency LSFs are usually more sensitive to distortion than
higher frequency ones. Therefore, a {4, 6} split would be preferred to a {5, 5}
split for instance. The conﬁgurations shown here have been chosen so that
they all have the same bit-rate of 24 bits. Complexity (in multiply–adds) and
memory storage (in words) for the typical SVQ conﬁgurations are presented
in Table 5.5. It can be seen that although the direct VQ approach is extremely
complex, the SVQ conﬁgurations are all practical. Even the most complex one
requires only 40 960 multiply–adds per input vector, which translates to only
2 MIPS if performed at a 20 ms update rate.
However, there are several drawbacks which relate to the efﬁciency of SVQ
quantization:

112
Efficient LPC Quantization Methods
Table 5.4
Typical examples of SVQ LSF quantizers (24
bits/frame)
Sub-vectors
Elements per subvector
Bit allocation
2
5,5
12,12
3
3,3,4
8,8,8
4
3,2,2,3
6,6,6,6
5
2,2,2,2,2
5,5,5,5,4
Table 5.5
Complexity and memory requirements for various SVQ
schemes
Sub-vectors
Split
Bits
Complexity
Memory storage
1
10
24
1.67 × 108
1.67 × 108
2
5,5
12,12
40 960
40 960
3
3,3,4
8,8,8
2560
2560
4
3,2,2,3
6,6,6,6
640
640
5
2,2,2,2,2
5,5,5,5,4
288
288
• The correlations between subvectors are not exploited. Therefore only a
fraction of the intra-frame correlation is used. In particular, a pair of LSFs
close to a peak in the spectrum may be split into two different subvectors
and, although there is a correlation between them, they are quantized
independently. As a result the quantization efﬁciency decreases greatly as
the size of the subvectors reduces.
• Some combinations of subvectors do not respect the ordering of the LSF,
or lead to neighbouring LSFs being too close to each other. As there is
a minimum spacing limit that a pair of adjacent LSFs are allowed to
have, this means that certain SVQ vector combinations will never be used,
which is a waste of bandwidth. This can however be alleviated to some
extent. For example, once the ﬁrst subvector has been quantized, a simple
transformation such as an offset shift can be applied to the vectors that
violate the minimum distance in the second codebook, so as to make them
usable. However this is difﬁcult to include in the training process, and the
resulting quantizer may not be optimal.
• The number of bits allocated to each subvector is ﬁxed. The effect of the
weighting function will therefore be limited to within one subvector. If a
subvector contains only LSFs of relatively small importance, they will still
use all the bits allocated to this subvector, whereas a classic VQ would
effectively shift some of that bandwidth towards the more important LSFs,
through the weighting function. This effectively reduces the use of the
weighting function to the LSF within a given subvector and lowers the
overall quantization efﬁciency of an SVQ quantizer.

Codebook Structures
113
5.7.2 Multi-Stage Vector Quantization
In an Multi-Stage Vector Quantizer (MSVQ), the input vector is quantized
as a sum of vectors from a number of codebooks. Each of these codebooks
can therefore be of relatively small size, making the storage requirements
reasonable. That is, an input vector f is represented by a vector ˆf given by:
ˆf = yi0
0 + yi1
1 + . . . + yiK−1
K−1
(5.68)
where K is the number of stages and ik is the codebook index for the kth stage.
It can easily be seen that SVQ systems are a particular type of MSVQ system,
where the codebook vectors for a given stage contain nonzero elements only
in the locations corresponding to the SVQ subvectors. This is illustrated in
the following example, where it is easily seen that an SVQ codebook can be
mapped onto an MSVQ codebook.
yi0
0
=
{yi0
0 (0)
yi0
0 (1)
0
0
. . .
. . .
0
0}
yi1
1
=
{0
0
yi1
1 (2)
yi1
1 (3)
. . .
. . .
0
0}
...
=
{0
0
0
0
...
...
0
0}
yiK−1
K−1
=
{0
0
0
0
. . .
. . .
yiK−1
K−1(p −2)
yiK−1
K−1(p −1)}
This obviously implies that an MSVQ system will have a performance at least
equivalent to that of an SVQ system and probably much higher, as the SVQ
imposes a strong constraint on the structure of the codebook. On the other
hand, complexity and memory requirements for the MSVQ will be higher,
i.e. the sparse structure of the SVQ codebook signiﬁcantly reduces the storage
requirement and a sequential search for each subvector is equivalent to an
exhaustive search, which is not the case for MSVQ. Examples of typical bit
allocations for MSVQ codebooks are illustrated in Table 5.6, including the bit
allocation for the 2.4 kb/s MELP coder [12].
Table 5.6
Typical examples of MSVQ LSF
quantizers (24–25 bits/frame)
Stages
Bit allocation
Total number of bits
2
12,12
24
3
8,8,8
24
4
6,6,6,6
24
4
7,6,6,6
25
5
5,5,5,5,4
24

114
Efficient LPC Quantization Methods
5.7.3 Search strategies for MSVQ
The usual search strategy for an SVQ codebook is straightforward: a full
search (FS) for each of the subvectors is applied. The structure of an MSVQ
quantizer, however, allows different types of search strategy depending on
the desired complexity. The simplest of the searches is the sequential search
(SS). In this search, the input vector f is ﬁrst approximated by the ith
0 vector
from the ﬁrst codebook Y0 which minimizes:
d(f, ˆf) =
p

n=1
wn

fn −(yi0
0 )n
2
(5.69)
The index for the ﬁrst codebook i0 is then ﬁxed and the quantization error
f −yi0
0 is then quantized using the ith
1 vector from the second codebook Y1
which minimizes:
d(f, ˆf) =
p

n=1
wn

( fn −(yi0
0 )n) −(yi1
1 )n
2
(5.70)
This process is repeated for each stage in the codebook. The complexity of this
search is the sum of the complexity of a full search through each codebook,
given by,
C = N
K

k=1
2Bk
(5.71)
where K is the number of stages, each with Bk bits, and N is the length
of the input vector. This search is, however, nonoptimal as there is no
guarantee that the set of codebook vectors giving the lowest intermediate
distortion will also result in the best overall distortion. A better way to ensure
that the best performance is obtained is simply to perform a full search
on all codebooks jointly. That is, every combination of codebook vectors
ˆx = yi0
0 + yi1
1 + . . . + yiK−1
K−1 is tested against the original input vector. This
guarantees optimal quantization, but at the cost of a very high complexity,
given by,
C = N 2
K
k=1 Bk
(5.72)
This complexity is equal to that of a direct vector quantization of the LSF,
which is far too high for practical applications. The only advantage of
the full-search MSVQ over a standard full search vector quantizer is the
reduced storage requirement. However, it is possible to obtain most of the

Codebook Structures
115
Y1
Y2
f
Y0
Y0
Y1
Y2
f
Y0
Y1
Y2
f
Y0
Y1
Y2
f
(a) Step 1
(b) Step 2
(c) Step 3
(d) Step 4
Figure 5.10
Steps in an M-best search
advantages of the full search over the sequential search, while still maintaining
a reasonable level of complexity, by using a tree-search algorithm (TS), such
as an M-best tree search.
An M-best tree search operates by exploring a certain number, M, of paths
in the quantizer tree. Starting with the ﬁrst codebook, the M code-vectors
giving the lowest distortion when compared with the input are kept, as well
as the M quantization error vectors resulting from these vectors. The second
codebook is then searched M times, once for each of these error vectors,
and the M paths which achieve the lowest overall distortion are kept. This
procedure is performed for each stage of the codebook. Finally, for the last
stage, the path giving the lowest overall distortion is selected. This process
is illustrated in Figure 5.10. For this example, M has been set to 2 and the
codebook consists of three stages of 3 bits each. In Figure 5.10a, the ﬁrst
codebook Y0 is searched to ﬁnd the M vectors that best match the original
LSF vector. In Figure 5.10b, the second codebook is searched to best match
the difference between the original input LSFs and the selected vector from
the ﬁrst codebook. This is performed for each of the M selected vectors in the
ﬁrst codebook. The M best paths are selected for the next stage. Figure 5.10c
shows the same process repeated for the third and ﬁnal codebook. Finally,
Figure 5.10d shows the ﬁnal M best paths. Out of these, the path with the
lowest overall distortion is selected. Experiments show that such a tree search
can give performance close to that of a full search even with a small value of

116
Efficient LPC Quantization Methods
M (i.e. 8–16). The complexity of this search is given by:
C = N

2B0 + M
K

k=2
2Bk

(5.73)
Obviously for M = 1, this equates to the complexity of the sequential search.
It can be seen that the M factor does not apply to the complexity of the
ﬁrst codebook search. This can be exploited in designing the structure of
the codebook. For example, if we have three stages for a total of 25 bits, it is
signiﬁcantly less complex to have a {9, 8, 8} structure than a {8, 9, 8} structure,
whereas storage is the same and performance is expected to be similar. One
interesting improvement to the M-best search strategy is to use a complex
perceptual measure in the ﬁnal stage only, to select which of the M ﬁnal paths
are the best. Since this computation only needs to be performed M times, it
is possible to use much more complex distortion measures than the WMSE
normally used. It is also possible to compute this measure on only a subset
of the M best ﬁnal paths, i.e. the ones which give the lowest WMSE. This
procedure signiﬁcantly enhances the performance of the quantizer, partly
solving the problem that the WMSE is not such a good distortion measure
compared to the SD for example.
5.7.4 MSVQ Codebook Training
The basic codebook training algorithms usually cater for single-stage code-
books. It is however possible to adapt the algorithm for MSVQ codebook
training. The most basic technique is called sequential optimization. In this
method, the codebook for stage 1 of the MSVQ is ﬁrst designed. The quanti-
zation errors for the training database are then computed and the codebook
for stage 2 is trained over the error vectors. This is then repeated for each
stage, until reaching the ﬁnal codebook.
However sequential optimization does not provide the best performance, as
each codebook is optimized as if it was the last stage of the MSVQ quantizer. A
better alternative is iterative sequential optimization, where an initial codebook
is chosen for each stage. Each codebook is then optimized by assuming all the
other stages to be ﬁxed and known, i.e. the quantization error is computed
using all the other stages except the current one, and training is used to
obtain an updated version of the current codebook. This process can then be
repeated until all of the codebooks have converged.
It is also possible to jointly optimize all codebooks using simultaneous
joint codebook design. This method gives slightly better results than the
previous methods but has a high computational cost, which is described
in [13].

MSVQ Performance Analysis
117
5.8 MSVQ Performance Analysis
In order to compare the relative performance of various MSVQs, quantizers
have been trained using the same training database, which has the following
characteristics:
• MIRS and FLAT ﬁltered speech in various languages are included
• Only speech-active regions are included
• LSFs are extracted with an update rate of 20 ms, over a 200 sample Ham-
ming window
• A bandwidth expansion factor of 0.994 is applied to the LP coefﬁcients
prior to LSF conversion
• 50 000 sets of LSF coefﬁcients are included
The speech database used is rather small to produce quantizers with good
performance in real-life applications. Typically, a speech database of over
1 000 000 LSF vectors is used for training codebooks for actual applications.
However, for the purpose of comparing performances of various quantization
schemes, the smaller speech database is adequate in providing indicative
results. Additionally, it signiﬁcantly reduces the time required to train the
quantizers, which is prohibitive for the bigger database (several weeks of
computing are usually required for typical codebook training with the larger
database).
5.8.1 Codebook Structures
For a given bit rate, MSVQ and SVQ codebooks can differ in the number
of stages and in the vector splits. The actual structure of the quantizer
affects complexity and memory storage, as discussed earlier, but also affects
performance. Typically, the more structure imposed on the codebooks, the
lower the complexity and storage, but also the poorer the performance.
All of the SVQ and MSVQ quantizers have been trained using 24 bits, for
various numbers of stages, from 2 to 5. The conﬁgurations used are shown in
Table 5.7. The results are plotted in Figure 5.11. As expected, the performance
is directly linked to the amount of structure present in the codebook.
5.8.2 Search Techniques
In order to compare the performance of various types of searches available
for a given codebook, an MSVQ codebook of 21 bits, using three stages of
7 bits each, has been trained. It uses no prediction and the search algorithm
used during training was a sequential search (SS). The performance of the
codebook was then measured using SS, FS, and TS with values of M from
2 to 32. The WMSE, average SD, and number of outliers at 2 dB are plotted

118
Efficient LPC Quantization Methods
Table 5.7
MSVQ and SVQ structures for Figure 5.11
Stages
MSVQ
SVQ
bit allocation
Bit allocation
Vector split
2
12,12
12,12
5,5
3
8,8,8
8,8,8
3,3,4
4
6,6,6,6
6,6,6,6
3,2,2,3
5
5,5,5,5,4
5,5,5,5,4
2,2,2,2,2
2
3
4
5
Number of stages
0.0
5.0
10.0
15.0
Outliers at 2 dB (%)
SVQ codebook
MSVQ codebook
2
3
4
5
Number of stages
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Average SD (dB)
SVQ codebook
MSVQ codebook
2
3
4
5
Number of stages
5.0e05
1.0e04
1.5e04
2.0e04
2.5e04
WMSE
SVQ codebook
MSVQ codebook
Figure 5.11
Performance comparison of various SVQ and MSVQ codebook
structures
in Figure 5.12. Outliers at 4 dB have not been plotted, as they are zero for all
cases. The advantage of a TS over both SS and FS is evident in these graphs.
For M greater than or equal to eight, the performance of the TS is very close
to that of the FS, at a much reduced complexity. It is also signiﬁcantly better
than that of SS, for a relatively small increase in complexity. The complexity

MSVQ Performance Analysis
119
SS
TS2
TS4
TS8
TS16
TS32
FS
Search strategy
1.2e04
1.3e04
1.4e04
1.5e04
WMSE
SS
TS2
TS4
TS8 TS16 TS32
FS
Search strategy
1.10
1.12
1.14
1.16
1.18
1.20
1.22
1.24
Average SD (dB)
SS
TS2
TS4
TS8 TS16 TS32
FS
Search strategy
2.00
2.50
3.00
3.50
Outliers at 2 dB (%)
Figure 5.12
Performance comparison of various search techniques
in multiply–adds per input vector is given in Table 5.8. It is to be noted that,
in the test, codebooks have been trained using the SS algorithm. Therefore,
they are only optimal for an SS search. Better performance for the TS and FS
cases can be obtained by using the same search in the training as the one used
during the operation of the quantizer. This is illustrated in Figure 5.13, where
WMSE, average SD and outliers at 2 dB are plotted for the original codebook
and the retrained codebooks, for SS and TS with values of M ranging from 2
to 32. Due to the very high complexity of the FS, it was not possible to fully
retrain the codebook using FS, although the results are expected to be similar
to that of TS with M = 32.
5.8.3 Perceptual Weighting Techniques
Several weighting techniques were described in Section 5.6.4. A good weight-
ing technique should give a distortion measure which is well correlated with
the spectral distortion measure, which is our reference here. For testing, we

120
Efficient LPC Quantization Methods
Table 5.8
Complexity of various
search strategies for a {7, 7, 7}
MSVQ codebook
Search type
Complexity
SS
3840
TS: M = 2
6400
TS: M = 4
11 520
TS: M = 8
21 760
TS: M = 16
42 240
TS: M = 32
83 200
FS
20 971 520
SS
TS2
TS4
TS8
TS16
TS32
Search strategy
1.00
2.00
3.00
4.00
Outliers at 2 dB (%)
codebook trained with SS
codebook retrained with TS
SS
TS2
TS4
TS8
TS16
TS32
Search strategy
1.0e04
1.1e04
1.2e04
1.3e04
1.4e04
1.5e04
1.6e04
WMSE
codebook trained with SS
codebook retrained with TS
SS
TS2
TS4
TS8
TS16
TS32
Search strategy
1.05
1.10
1.15
1.20
1.25
Average SD (dB)
codebook trained with SS
codebook retrained with TS
Figure 5.13
Performance comparison with and without codebook reoptimization
use a 21-bit MSVQ codebook with three stages of 7 bits each which has been
trained using the weighting methods listed below:
• W1: no weighting (all weights are equal to 1.0)
• W2: EFR weighting method

Inter-frame Correlation
121
W1
W2
W3
W4
W5
1.15
1.20
1.25
1.30
1.35
Average SD (dB)
Average SD (dB)
Weighting Method
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
Outliers at 2 dB (%)
Average SD (dB)
Outliers at 2 dB
Figure 5.14
Performance comparison of various weighting functions
• W3: LSF inverse distance method
• W4: Paliwal–Atal method
• W5: Group delays method
The results are shown in Figure 5.14. Only average SD and the number of
outliers at 2 dB are shown, as there are no outliers at 4 dB in any of the cases.
Figure 5.14 clearly shows the performance gain given by the use of weighting
over the simple MSE method. As indicated, a well-chosen weighting method
can give a reduction of up to 0.15 dB in average SD and up to 5 % in the
number of outliers. The ﬁgure also shows that some weighting methods
clearly outperform others. The weighting technique used in EFR and the LSF
inverse distance method are not the best. Better results are obtained with the
Paliwal–Atal method, however the best of all is the group delays method.
The main advantage of the group delays method is in the reduction in the
number of outliers at 2 dB (from 3 % to 1.8 %), whereas its average SD is
virtually identical to that of the Paliwal–Atal method.
5.9 Inter-frame Correlation
When quantizing LSF parameters, or any other parameter for that matter,
a good quantization scheme must make use of all the redundancies in the

122
Efficient LPC Quantization Methods
parameters to be quantized, so as to maximize the efﬁciency of the quantizer.
The MSVQ was shown above to provide better performance than the SVQ,
mostly because it makes better use of the correlations between the elements
of an LSF vector, i.e. the intra-frame correlations as shown in Table 5.2.
However, LSF vectors are extracted at a typical update rate of 20 ms and
speech characteristics often remain similar for longer than 20 ms. Therefore,
successive LSF vectors are correlated (see Table 5.3) and a good quantizer
should make use of these similarities to improve the quantization accuracy.
The inter-frame correlation can be exploited in various ways, the most
popular ones being the use of a predictor and joint quantization of several
sets of LSFs.
5.9.1 LSF Prediction
A popular approach to exploiting the inter-frame correlations of LSF vectors
is the use of prediction. Instead of quantizing an LSF vector directly, the
difference between a predicted vector and the actual LSF vector is quantized
and transmitted. If the predictor is good, then the residual signal should
be easier to quantize than the original LSF vector. It is common practice
to remove the long-term mean of each LSF before applying prediction. The
residual LSF is given by:
rn = fn −˜fn
(5.74)
where ˜fn is the prediction vector. The decoded LSF vector is then given by,
ˆfn = ˆrn + ˜fn
(5.75)
where ˆrn is the quantized value of rn.
This obviously implies that the decoder should have knowledge of ˜fn.
Therefore, the prediction used should be a function of some parameters
available at both the decoder and the encoder. One of the simplest predictors
assumes that a set of LSFs can be predicted using the previous quantized set
of LSFs, scaled by a weighting factor,
˜f k
n = αn ˆf k−1
n
(5.76)
This will be referred to as an LSF differential quantizer (LSF-DQ). The
computation of the prediction gain is made difﬁcult by the fact that knowledge
of the quantizer (codebook) is necessary to compute the prediction. One way
around this problem is to assume that the ﬁnal quantizer will be quite good
and therefore ˆf k−1
n
can be approximated by f k−1
n
in the equation above. The
optimal factors αn can then be determined by maximizing the prediction gain

Inter-frame Correlation
123
over a speech database. In order to increase the prediction accuracy, higher
order predictors can also be used. The prediction then becomes a weighted
sum of the LSF vectors for a given number of past frames. This increases the
performance of the predictor, at the small expense of slightly higher memory
requirements for storing the past values. Unfortunately this scheme has a
major drawback: the decoder must have correct knowledge of the prediction
used at the encoder. If a channel error occurs and corrupts the bitstream
for one frame, then the decoded LSF will be corrupted. Since the decoded
erroneous LSFs will be used for prediction, the LSF for the next frame will
also be corrupted and the error will then propagate indeﬁnitely.
A better approach is to generate the prediction from the decoded codebook
entries, rather than the decoded LSFs which will limit error propagation.
Such predictors are called moving average (MA) predictors. A ﬁrst-order MA
predictor is given by,
˜f k
n = αn ˆrk−1
n
(5.77)
The decoded vector is then given by,
ˆf k
n = ˆrk
n + αn ˆrk−1
n
(5.78)
Therefore if an error occurs, the only frames affected will be the frame where
the error occurs and the N following frames, where N is the order of the
predictor. For a ﬁrst-order MA predictor, only one extra frame will be affected
compared with a quantizer not using prediction. Intuitively, an MA predictor
will not be as efﬁcient as a DQ predictor, but its error resilience capabilities
are very signiﬁcant. This makes the MA predictor a better choice for the
majority of applications.
Assuming all αn are chosen equal to a constant α, the prediction gains of
the DQ and MA predictors are plotted against α in Figure 5.15, for an update
rate of 20 ms. Experiments show that forcing all αn to be equal does not
signiﬁcantly reduce the prediction gain over the ideal case.
Figure 5.15 shows that the DQ predictor can achieve a gain of up to 5 dB
with an α value of 0.8, whereas the MA predictor can achieve 3 dB for α
around 0.65. The MA predictor is not as efﬁcient as the DQ predictor, but still
provides a useful prediction gain, which in turn can help improve the overall
performance of the quantizer. The prediction gains for both DQ and MA
predictors depend on the LSF update rate which directly affects correlation
between adjacent sets of parameters. A faster update rate will give a higher
prediction gain, as consecutive sets of LSFs will be more correlated, and it will
usually be achieved with a higher value of α. For example, an update rate of
10 ms gives an optimal α of around 0.8 for the MA predictor. In the following
sections, only the MA predictor will be considered as the DQ predictor is not

124
Efficient LPC Quantization Methods
0
0.2
0.4
0.6
0.8
1.0
Prediction Factor
5.0
0.0
5.0
Prediction Gain (dB)
DQ prediction
MA prediction
Figure 5.15
Prediction gain of first order MA vs DQ Predictors (20 ms update rate)
suitable for a general purpose coder (with possible channel errors). However,
a DQ predictor may be applicable in cases where virtually no channel errors
are encountered, such as voice storage applications.
5.9.2 Prediction Order
MA prediction has been presented above for the case of a ﬁrst-order predictor.
It is also possible to have a higher order MA predictor, where the prediction
is a weighted sum of the quantized residuals received in N previous frames.
An Nth-order predictor would exploit correlations between the current frame
n and the frames n −1, n −2, . . . , n −N. As a result, its performance is
expected to be greater than that of a ﬁrst-order MA predictor. However,
the drawback of the improved performance is greater sensitivity to channel
errors as an error on one set of parameters will corrupt N + 1 frames of
speech. The optimal order of an MA predictor requires a trade-off between
prediction gain and error resilience. In order to estimate the optimal order to
be used in most applications, where channel errors are expected, the optimal
prediction factors have been derived for various orders. This was achieved by
computing the prediction gain for all possible combinations of the prediction
parameters, in steps of 0.05, over a database of 30 000 LSF, extracted at 20 ms
intervals with all silences removed. The results are shown in Table 5.9. It can

Inter-frame Correlation
125
Table 5.9
Prediction gain vs MA predictor order
Order
Optimum prediction parameters
Prediction gain (dB)
1
0.65
2.97
2
0.85,0.43
4.13
3
0.85,0.60,0.35
4.61
4
0.9,0.7,0.45,0.2
4.84
be seen from Table 5.9 that the increase in prediction gain when increasing
the order of the MA predictor from 1 to 2 is 1.16 dB. An increase in prediction
gain of 1.87 dB can be achieved by increasing the order from 1 to 4, whereas
the increase from order 0 (no prediction) to 1 is nearly 3 dB. Although higher
order predictors help to increase the prediction, the degradation in speech
quality due to channel errors is expected. If the order is 1, 40 ms of speech are
corrupted. With proper error concealment techniques, it is usually possible to
limit the distortion caused by the loss of LPC for 40 ms to an acceptable level.
However, for higher prediction orders, 60 ms or more are lost and the speech
degradation caused by such a loss is usually difﬁcult to recover, affecting
the overall speech quality signiﬁcantly. Therefore, for most applications with
a 20 ms parameters update rate which involve a noisy channel, it is better
to use a ﬁrst-order MA prediction. In case of shorter update rates, or very
low bit error conditions, higher order prediction can be used to improve the
MA prediction performance. In the following discussion, only ﬁrst-order MA
prediction is considered.
5.9.3 Prediction Factor Estimation
Figure 5.15 shows that the best prediction gain for a ﬁrst order MA is achieved
with a value of 0.65. Therefore it would be reasonable to assume that a
prediction factor of 0.65 will give the best performance in a ﬁrst-order MA
quantizer. Indeed, such a value is used in some speech coders such as EFR [10].
However, this value has been derived using the assumption that the original
residual rk
n is close enough to the quantized residual ˆrk
n that it can be used
instead to obtain the curve shown in Figure 5.15. In a practical quantizer, there
is no guarantee that this assumption will be true. Therefore the only way to
determine the optimal prediction factor is by training quantizers with various
prediction factors and comparing their performances. Various ﬁrst-order MA
quantizers have been trained for values of α ranging from 0.3 to 0.7 in 0.05
steps, for a 20 ms update rate. An MSVQ quantizer comprising three stages
of 8 bits each has been selected to quantize the residual, as it provides good
performance. The performance of these quantizers is plotted in Figure 5.16,
together with the performance of the quantizer without prediction. It can be
seen that the best overall performance of the quantizer is achieved for a value

126
Efficient LPC Quantization Methods
0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7
Prediction factor
6.0e05
6.5e05
7.0e05
7.5e05
8.0e05
WMSE
0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7
Prediction factor
0.82
0.84
0.86
0.88
0.90
0.92
Average SD (dB)
0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7
Prediction factor
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Outliers at 2 dB (%)
Figure 5.16
Performance of a MA-MSVQ Quantizer vs Prediction Factor
of α around 0.4, and not 0.65 as would be expected from Figure 5.15. WMSE
and average SD are lowest for 0.4, although outliers at 2 dB are slightly lower
at 0.3 than at 0.4. This is to be expected if the predictor does not work well,
at speech transitions for example. However this is not a problem as it is
possible to bias the training process towards producing fewer outliers with
only a small increase in the average SD. Moreover, the performance obtained
with 0.4 is signiﬁcantly better than that of 0.65. This clearly shows that the
‘intuitive’ way of determining the prediction factor may not be correct.
5.9.4 Performance Evaluation of MA Prediction
In order to compare the performance of quantizers with and without MA
prediction, several codebooks have been trained with α = 0.4 for various
bit rates, using a 20 ms update rate. In order to make comparisons with the
previous graphs easier, the quantizers all have 3-stage MSVQ codebooks. The
bit rates range from 20 to 26 bits, and the codebook structures are detailed
in Table 5.10. The search algorithm is a tree search with a depth of 32. The
overall performances are shown in Figure 5.17. The gain provided by the MA

Inter-frame Correlation
127
Table 5.10
MSVQ
bit
allocation
for
Figure 5.17
Bits
Bit allocation
20
7,7,6
21
7,7,7
22
8,7,7
23
8,8,7
24
8,8,8
25
9,8,8
26
9,9,8
20
21
22
23
24
25
26
Number of bits
0.00
0.50
1.00
1.50
2.00
2.50
3.00
3.50
Outliers at 2 dB (%)
No Prediction
MA Prediction
20
21
22
23
24
25
26
Number of bits
0.0e+00
2.0e05
4.0e05
6.0e05
8.0e05
1.0e04
1.2e04
1.4e04
1.6e04
WMSE
No Prediction
MA Prediction
20
21
22
23
24
25
26
Number of bits
0.60
0.70
0.80
0.90
1.00
1.10
1.20
1.30
1.40
Average SD (dB)
No Prediction
MA Prediction
Figure 5.17
MA prediction vs no prediction at various bit rates

128
Efficient LPC Quantization Methods
prediction is evident from the graphs, where similar performance is obtained
for the MA-MSVQ with around 3 bits less than for the MSVQ without
prediction. This 3 bits advantage is present for all performance measures.
Therefore it is possible to achieve a saving of 10–15 % in bit rate by using
MA prediction with an MSVQ quantizer, on top of the bit reduction already
obtained by using MSVQ instead of SVQ. The only cost of the MA prediction
is a slightly increased sensitivity to channel errors. However, during testing
of coders using such schemes, this extra sensitivity did not turn out to be a
signiﬁcant problem as the prediction order is limited to one.
5.9.5 Joint Quantization of LSFs
Prediction is an efﬁcient way of removing correlation from two or more
neighbouring sets of parameters. However it is a one-way only process where
information from frame k −1 is used in the prediction and quantization of
frame k, but information from frame k is not used for the prediction and
quantization of frame k −1. Indeed it is assumed that frame k is not known
when quantizing frame k −1, in order to keep the delay to a minimum.
However, in some applications it is worth accepting a slight increase in delay
and using a quantization scheme which makes use of the extra redundancies.
A simple way of achieving this is to jointly quantize several sets of parameters.
For example, a 1.2 kb/s version of the SB-LPC coder jointly quantizes three
sets of parameters extracted at 20 ms intervals, giving a 60 ms frame size. This
enables the coder to quantize the three sets of parameters jointly, making
the best use of the redundancies existing between them. This quantizer will
be referred to as JQ-MSVQ, and the large frame composed of several speech
frames will be referred to as a meta-frame. JQ-MSVQ is also used in a 4 kb/s
version of the SB-LPC, where two sets of LSF extracted every 10 ms are
quantized jointly, forming a 20 ms meta-frame.
One signiﬁcant issue with a JQ quantizer is that of weighting. Various
weighting functions have been discussed above and they can be used to
provide weights for each individual set of LSFs. However, all sets of LSFs in
a meta-frame are not usually of equal importance. For example, at a speech
onset, the ﬁrst set can be in a nonspeech region, whereas the other sets can
be in a speech-active region. Therefore the weight vector should ideally take
this into consideration, so as to maximize the quantization efﬁciency for the
important sets and not waste bits on a set of LSFs which will have very
little inﬂuence on the speech quality. This can be achieved by including a
bias based on the relative energies of the speech for each set of LSFs and
multiplying the weights for the nonspeech LSFs by a factor smaller than one.
A value of 0.1 has been found to give good performance. It is risky to use a
smaller value, as problems can arise from interpolation at the decoder if the
‘not so important’ set of LSFs is too poorly quantized.

Inter-frame Correlation
129
Table 5.11
Comparative performance of JQ-MSVQ and
MA-MSVQ
JQ-MSVQ
MA-MSVQ
MA-MSVQ
Number of bits
44
15
18
Number of bits per 60 ms
44
45
54
Stages
8
3
3
Bit allocation
6,6,6,6,6,6,6,2
5,5,5
6,6,6
M
32
32
32
Complexity (per 60 ms)
374 400
62 400
124 800
Memory
13 560
960
1920
WMSE
1.541 e-04
2.574 e-04
1.594 e-04
Average SD (dB)
1.2576
1.6383
1.3053
Outliers at 2 dB (%)
4.6563
17.2014
4.3119
Outliers at 4 dB (%)
0.0
0.1159
0.0185
The performance of JQ-MSVQ is illustrated in Table 5.11. The LSF quantizer
used in the 1.2 kb/s coder, which quantizes three sets of LSFs jointly using
44 bits in an 8-stage JQ-MSVQ quantizer, is compared against a classic
MA-MSVQ quantizer of similar bit rate and one of similar performance.
Complexity and memory requirements are also indicated. In this example,
LSFs are extracted every 20 ms. The results clearly show the advantage of
JQ-MSVQ over MA-MSVQ in terms of performance. JQ-MSVQ has the same
performance at 44 bits as MA-MSVQ at 54 bits, and is far superior to the
MA-MSVQ at 45 bits. Complexity is higher for the JQ-MSVQ, but this may be
reduced by lowering the depth of the tree search M. Memory requirements are
also higher for the JQ-MSVQ, but again they can be reduced by adding more
structure to the codebook (more stages of smaller sizes) and accepting a slight
reduction in performance. Overall, JQ-MSVQ is very effective at providing
reasonable LSF quantization at very low bit-rates. At 1.2 kb/s, only 72 bits are
available every 60 ms for quantizing all speech parameters. Assuming that the
gain, pitch and voicing are quantized using 28 bits, only 44 bits are left for the
spectral parameters. As shown in Table 5.11, an MA-MSVQ quantizer would
not work well under those circumstances, giving signiﬁcantly degraded
speech quality with over 17 % outliers at 2 dB. However the use of JQ-MSVQ
quantization makes a 1.2 kb/s coder a practical possibility, with only 4.6 %
outliers at 2 dB.
5.9.6 Use of MA Prediction in Joint Quantization
When using JQ-MSVQ, the redundancies between the jointly quantized sets
of LSFs are exploited. Using MA prediction within the meta-frame will not
therefore achieve any more gain. Indeed, a JQ codebook using MA from

130
Efficient LPC Quantization Methods
one set to the next can be transformed to a JQ codebook using no MA, by
simply adding the prediction to the predicted set. Therefore, MA prediction
is only useful if it uses correlation with a previously transmitted set of LSFs,
i.e. from one meta-frame to another. This means that the distance between
the predicted sets and the sets used to compute the prediction is usually
larger than in a nonJQ case, thereby reducing the efﬁciency of the prediction.
Moreover, a channel error on JQ-MSVQ quantizers using a ﬁrst-order MA
will affect two meta-frames, instead of just two speech frames for a nonJQ
quantizer. For the 1.2 kb/s conﬁguration with a 60 ms meta-frame described
above, this means an error will affect 120 ms of speech instead of 40 ms for a
nonJQ quantizer at the same update rate. As a result errors will have a much
greater impact on speech quality, as it is usually possible to limit the effect
of the loss of 40 ms of speech, but not the loss of 120 ms of speech. Therefore
MA prediction for JQ quantizers is mostly useful when the meta-frames are
relatively small.
For example, two sets of LSFs obtained at a 10 ms update rate can be jointly
quantized in a 20 ms meta-frame. The 20 ms meta-frame is small enough that
the MA prediction will give good prediction gain, while keeping the error
propagation down to a manageable level. For the prediction to be optimal,
it is better to predict both sets of LSFs in the meta-frame with the last set of
LSFs of the previous meta-frame. This way the time difference between the
predicting and predicted frames is kept to a minimum. However the optimal
prediction factors for both sets will not be the same, as the ﬁrst set will be more
correlated with the predicting set than the second set. Experiments using a
10 ms update rate with two sets forming a meta-frame indicated that, for this
conﬁguration, prediction factors of {0.5, 0.4} are suitable, i.e. the ﬁrst set is
predicted with a factor of 0.5 and the second set with 0.4. The quantizer jointly
quantizes two sets of LSFs extracted every 10 ms using 36 bits. Codebooks
are organized in six stages of 6 bits each, using the group delays weighting
method and a tree search of depth 32. The results are shown in Table 5.12.
They show that the MA prediction gives a large performance gain over the
nonMA case, and that using both MA and JQ together allows two sets of
LSFs to be quantized accurately with only 36 bits, i.e. only 18 bits per set. The
performance gain given by the MA predictor in the JQ case is consistent with
that observed in the nonJQ case.
5.10 Improved LSF Estimation Through Anti-Aliasing Filtering
When estimating speech model parameters at about 50 Hz over a 20–30 ms
analysis window, speech is assumed to be locally stationary [14] within
this analysis window. When closely investigated however, one can see that
speech has considerable variation even within the analysis window. Speech
parameters in general, and LSFs in particular, may contain high frequency

Improved LSF Estimation Through Anti-Aliasing Filtering
131
Table 5.12
Performance comparison of JQ-MSVQ and
MA-JQ-MSVQ
JQ-MSVQ
MA-JQ-MSVQ
Number of bits per 20 ms
36
Number of bits per set
18
Stages
6
Bit allocation
6,6,6,6,6,6
M
32
WMSE
2.227 e-04
1.622 e-04
Average SD (dB)
1.0926
0.9335
Outliers at 2 dB (%)
1.8135
0.5627
Outliers at 4 dB (%)
0.0052
0.0052
variations which violate the Nyquist sampling criterion. Therefore the use
of an anti-aliasing ﬁlter with cut-off frequency adequate for the chosen LSF
sampling rate may be used to alleviate possible spectral overlapping of
the LSFs. It is conﬁrmed [15] that this method offers an advantage over
the classic LSF extraction methods; during quantization, bit-saving and
signiﬁcant reduction in the percentage of outliers have been possible.
5.10.1 LSF Extraction
Al-Naimi investigated the speech stationarity assumption over the analysis
window with regard to LSF vector extraction by calculating LSF vectors at
every sample [15]. The centre of the analysis window is shifted by one sample
at a time, leading to an LSF vector extraction rate of 8 kHz. Evolution of each
LSF parameter over time, also referred to as an LSF track, is then produced
from the over-sampled LSF vectors. Decimation without any ﬁltering of
the LSF tracks at a given LSF vector transmission rate (i.e. 20, 10 or 5 ms)
should produce exactly the same LSF vectors as the classic methods. It is
therefore clear that LSF track frequency variations greater than half of the
LSF computation rate (frequency) will cause problems during the decimation
process i.e. by introducing aliasing distortion. Note that the LSF computation
rate need not be same as the frame transmission rate.
In order to measure the amount of aliasing introduced, the following test
was used:
1. Ten LPC parameters were calculated for every sample using Hamming
windowing over 200 samples and bandwidth expanded by 15 Hz, then
converted to LSFs.
2. The evolution of each LSF track fi over time was taken and FFT transformed.
The logarithmic magnitude of the FFT spectrum is shown in Figure 5.18.

132
Efficient LPC Quantization Methods
0
1000
2000
3000
4000
Frequency (Hz)
4
6
8
10
Ampltitude (dB)
f1
f2
f3
f4
f5
f6
f7
f8
f9
f10
Figure 5.18
Spectral variations of LSF tracks calculated every sample
In Figure 5.18, we can see that most of the LSF tracks’ spectra have a
substantial amount of their energy in the low frequency band (below 100 Hz).
However, if a coder calculates its LSF vectors every 20 ms (i.e. 50 Hz sampling),
for example, then all the energy in the band greater than 25 Hz will be a source
of spectral overlapping, producing inaccurate LSF parameters. In order to
identify the source of these high frequencies, two further tests were carried
out.
• Window position test
A synthetic speech segment was prepared by repeating a whole pitch
cycle from a voiced speech segment. Using this as the input, LSF vectors
were calculated every sample, as before. The results showed that the LSF
tracks were not affected by the positioning of the window. Therefore the
conclusion was that as long as the window size is of sufﬁcient length and
the speech contained in the window is stationary, the window position will
not be the source of the high frequency components evident in the spectra
of the LSF tracks.

Improved LSF Estimation Through Anti-Aliasing Filtering
133
Frequency (Hz)
60
50
100
150
8
10
Amplitude (dB)
f1
f2
f3
f4
f5
f6
f7
f8
f9
f10
Figure 5.19
Low frequency region of the plots in Figure 5.18 expanded
• The wide-sense stationary assumption of speech
In general, a signal s(t) is said to be wide-sense stationary (WSS) if the
expectation, E {s(t)s(t + τ)}, is independent of time t and only dependent
on the time difference τ. In the window position test, the LSF tracks do not
contain high frequencies indicating that, for the synthetic speech ﬁle, the
WSS assumption is valid. In reality, speech is changing in characteristics
during the analysis frame. Therefore, the stationary assumption of our
speech segment within the analysis window is not strictly correct and
this is why high frequency variations are evident in the spectra of the
LSF tracks.
Table 5.13 shows the percentage of energies for three different bands com-
puted over four male and four female speakers each uttering eight seconds
from the NTT speech database. The band below 25 Hz corresponds to a 20 ms
LSF vector transmission rate whereas a band below 50 Hz corresponds to a
10 ms LSF vector transmission rate. Even though more than 92 % of the energy
is present in the band below 25 Hz, the remaining 8 % of the energy is enough
to produce higher LSF parameter variations in some speciﬁc speech sections
(Note that these ﬁgures are average over 32 seconds of speech and instanta-
neous variations are much larger.) Therefore, following from the discussion

134
Efficient LPC Quantization Methods
Table 5.13
Percentage of energy con-
tained in frequency bands: A<25 Hz,
25 Hz≤B<50 Hz,
50 Hz≤C<100 Hz
and
D≥100 Hz
Frequency band
LSF
parameters
A
B
C
D
f1
94.52
4.24
1.07
0.17
f2
95.44
3.61
0.83
0.12
f3
96.67
2.71
0.54
0.08
f4
96.81
2.56
0.54
0.09
f5
98.10
1.51
0.33
0.05
f6
97.46
1.99
0.45
0.10
f7
96.36
2.88
0.64
0.12
f8
95.54
3.28
0.71
0.47
f9
94.64
4.41
0.98
0.24
f10
92.72
3.97
1.13
2.18
above, a low pass ﬁltering as a preprocessing stage prior to decimation has
been proposed [15] to alleviate the possible spectral overlapping distortion.
Of course one may question the use of low-pass ﬁltering when the same
can be achieved by increasing the analysis window length with overlapping.
Increasing the analysis window length, i.e. to greater than two and a half
times the average pitch, would increase the frequency resolution, but in the
time domain, the speech signal would have evolved considerably during a
longer analysis window. Even though a large window may result in smoothed
spectra, important details within the frame will not be modelled accurately.
In addition, even if the window length was increased there would still be no
guarantee that the high frequency components of the LSF tracks would not
be present. Al-Naimi’s proposal of the use of a low-pass ﬁlter with a cut-off
frequency that is dependent on the LSF vector transmission rate, is therefore
justiﬁable [15].
The following set-up has been used to show the effect of low-pass ﬁltering
over 8 seconds of speech [15]. First the LSF vectors f were extracted every
frame from the tracks fi which are formed by calculating the LSFs every
sample. Next, ﬁltering was applied in the frequency domain separately for
each LSF track, fi, with a cut-off frequency that is dependent on the LSF
vector transmission rate and another set of LSFs g = g1, g2, g3, . . . , gp were
extracted. In order to avoid the rectangular windowing effect at the edges of
the blocks, one large FFT transformation was used for whole of the 8 seconds.
Figures 5.20–5.23 show a section of the variations of certain LSF tracks for
both classic fi and low-pass ﬁltered gi methods. It is evident in these ﬁgures
that signiﬁcant variations exist in the LSF tracks produced by the classic

Improved LSF Estimation Through Anti-Aliasing Filtering
135
0
500
1000
1500
2000
Time (samples)
0
200
400
600
800
1000
Hz
f1
g1
Figure 5.20
LSF tracks f1 and g1 variations over time
method due to the weak stationarity assumption within the analysis window,
especially at transitions from voiced speech segments to unvoiced (offsets)
and vice versa (onsets). The low-pass ﬁltered method, on the other hand,
produces smoother and more slowly evolving LSF tracks. The differences in
the LSF tracks are more evident in the higher LSF parameters (f7 and f10) as
shown in Figures 5.22 and 5.23.
Work in [16] showed that using a perceptually-smoothed power spectral
envelope leads to a signiﬁcant increase in subjective performance. Addition-
ally, [17] showed that low-rate quantization is possible through smoothing
the LSF parameter evolution. An informal listening test comparing both the
classic, f, and low-pass ﬁltered, g, LSF vectors used in a 4 kb/s SB-LPC coder
showed no difference in speech quality. An advantage during quantization is
therefore expected with regard to smoother evolution of the LSF tracks.
5.10.2 Advantages of Low-pass Filtering in Moving Average Prediction
Although using the unquantized LSF parameters for both the new and classi-
cal methods did not show any subjective quality difference, the new method
is expected to produce better performance under predictive quantization.

136
Efficient LPC Quantization Methods
0
500
1000
1500
2000
Time (samples)
1200
1400
1600
1800
2000
2200
Hz
f4
g4
Figure 5.21
LSF tracks f4 and g4 variations over time
0
500
1000
1500
2000
Time (samples)
1500
2000
2500
3000
Hz
f7
g7
Figure 5.22
LSF tracks f7 and g7 variations over time

Improved LSF Estimation Through Anti-Aliasing Filtering
137
0
500
1000
1500
2000
Time (samples)
3250
3350
3450
3550
3650
3750
3850
Hz
f10
g10
Figure 5.23
LSF tracks f10 and g10 variations over time
This advantage is shown in the following test. First, the classical method of
LSF extraction is applied at various update rates. Next, the low-pass ﬁltered
method is used where LSFs are calculated at every sample. Each LSF track
is then ﬁltered with a low-pass ﬁlter which had its cut off frequency suitably
selected to be half of the LSF transmission frequency. A subsampling is then
applied to get the required number of LSF vectors. Finally, the variance
for each set of LSF vectors is computed after a single-order MA prediction.
According to the earlier observations, the new method is expected to pro-
duce smaller prediction residual with a greater prediction coefﬁcient owing
to its smoother evolution and hence higher correlation between successive
sets. Figure 5.26 shows that for a 20 ms update rate, the variance of the LSF
prediction residual is lower for the new method and the minimum variance
(best prediction) occurs at a higher value of prediction coefﬁcient which
indicates that the new method produces LSF vectors that are more correlated.
Figures 5.24–5.28 show similar results for various other LSF vector transmis-
sion rates. It can be seen that the variance of the LSF prediction residual is
always less in the new method, regardless of the LSF vector rate. In order to
quantify the amount of prediction achieved, prediction gain, Pg, is computed
using,
Pg = x0 −xmin
x0
× 100
(5.79)

138
Efficient LPC Quantization Methods
0
0.2
0.4
0.6
0.8
1
Prediction Parameter
7
6
5
4
3
Variance of residual LSF (×103)
f
g
Figure 5.24
Moving
average
LSF
prediction
residual
variance
for
5 ms
LSF
vector rate
0
0.2
0.4
0.6
0.8
1
Prediction Parameter
7
6
5
4
3
Variance of residual LSF (×103)
f
g
Figure 5.25
Moving
average
LSF
prediction
residual
variance
for
10 ms
LSF
vector rate

Improved LSF Estimation Through Anti-Aliasing Filtering
139
0
0.2
0.4
0.6
0.8
1
Prediction Parameter
9
8
7
6
5
4
Variance of residual LSF (×103)
f
g
Figure 5.26
Moving
average
LSF
prediction
residual
variance
for
20 ms
LSF
vector rate
0
0.2
0.4
0.6
0.8
1
Prediction Parameter
9
7
5
3
f
g
Variance of residual LSF (×103)
Figure 5.27
Moving
average
LSF
prediction
residual
variance
for
30 ms
LSF
vector rate

140
Efficient LPC Quantization Methods
0
0.2
0.4
0.6
0.8
1
Prediction Parameter
12
10
8
6
4
2
f
g
Variance of residual LSF (×103)
Figure 5.28
Moving
average
LSF
prediction
residual
variance
for
40 ms
LSF
vector rate
Table 5.14
Prediction gain for low-pass filtered
and classic LSF extraction methods at various vec-
tor rates
LSF vector transmission rate
40 ms
30 ms
20 ms
10 ms
5 ms
Pg for g
29.55
33.82
36.53
43.34
49.57
Pg for f
12.50
16.60
29.60
37.60
42.60
where x0 is the variance of LSF prediction residual when the prediction
factor is zero (the original LSF variance) and xmin is the minimum variance
of LSF prediction residual computed by selecting the optimum prediction
coefﬁcient. Higher Pg is an indication of performance improvement that can
be achieved through MA prediction before quantization. Table 5.14 shows
the value of prediction gains at different LSF vector transmission rates. The

Improved LSF Estimation Through Anti-Aliasing Filtering
141
0.35
0.45
0.55
0.65
0.75
Prediction Parameter
6
8
10
12
14
WMSE (×10−5)
f
g
Figure 5.29
WMSE performance curves for a range of MA prediction parameters
new method always has a higher prediction gain compared to the classic
extraction and, as expected, the difference between them becomes smaller for
higher LSF update rates.
In the MA predictor used above, the prediction is a function of the unquan-
tized LSF prediction residual from the preceding set. As shown before,
when used in a quantizer such as MSVQ, the prediction will be a function
of the quantized LSF prediction residual and, hence, it is expected to be
different.
Figures 5.29–5.31 show the effect of quantizing the prediction residual on
the moving average prediction coefﬁcient. These results were obtained by
varying the prediction coefﬁcient in steps of 0.05 and training a multi-stage
VQ with three stages of 7 bits each with M-best factor of eight. Table 5.15 gives
a summary of the comparative performance results for the classic and new
methods of LSF extraction where the low-pass ﬁltered method signiﬁcantly
outperforms the classical method.

142
Efficient LPC Quantization Methods
0.35
0.45
0.55
0.65
0.75
Prediction Parameter
0.9
1
1.1
1.2
1.3
Average SD (dB)
f
g
Figure 5.30
Average SD performance curves for a range of MA prediction
parameters
Table 5.15
Performance comparison of the new and classical
LSF extraction with quantization
Prediction
Average
2 dB outlier
4 dB outlier
WMSE
parameter
SD (dB)
(%)
(%)
g
0.5
0.926
0.036
0
7.85E-05
f
0.4
1.031
0.23
0
9.66E-05
As the WMSE, average SD and percentage of 2 dB outliers is signiﬁcantly
lower for the new method, bit savings can be achieved whilst maintaining the
same performance as the classic LSF VQ. The percentage of 4 dB outliers is not
shown since it was zero in all cases. Other bit combinations in a three-stage
MSVQ are shown in Table 5.16 where the new method has a clear advantage.
Figures 5.32–5.35 present the results obtained for WMSE, average SD (ASD)

Improved LSF Estimation Through Anti-Aliasing Filtering
143
0.35
0.45
0.55
0.65
0.75
Prediction Parameter
0
0.5
1
1.5
2
2.5
Outliers at 2 dB (%)
f
g
Figure 5.31
2 dB outliers performance curves for a range of MA prediction
parameters
Table 5.16
Bit allocation for MA-MSVQ codebooks
Total bit allocation
Bits allocated per codebook stage
15
5,5,5
16
6,5,5
17
6,6,5
18
6,6,6
19
7,6,6
20
7,7,6
21
7,7,7
22
8,7,7
23
8,8,7
24
8,8,8

144
Efficient LPC Quantization Methods
15
16
17
18
19
20
21
22
23
24
Codebook bits
2.5
5
7.5
10
12.5
15
17.5
20
22.5
25
WMSE (×10−5)
f
g
Figure 5.32
WMSE performance curves for a range of codebook bit allocations
15
16
17
18
19
20
21
22
23
24
Codebook bits
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
Average SD (dB)
f
g
Figure 5.33
Average SD performance curves for a range of codebook bit
allocations

Improved LSF Estimation Through Anti-Aliasing Filtering
145
15
16
17
18
19
20
21
22
23
24
Codebook bits
0
2.5
5
7.5
10
12.5
15
17.5
Outliers at 2 dB (%)
f
g
Figure 5.34
2 dB outliers performance curves for a range of codebook bit alloca-
tions
20
21
22
23
24
Codebook bits
0
0.1
0.2
0.3
0.4
0.5
0.6
Outliers at 2 dB (%)
f
g
Figure 5.35
2 dB outliers performance curves for codebook bit allocations ranging
from 20 to 24 bits

146
Efficient LPC Quantization Methods
and percentage of 2 dB outliers respectively. The percentage of 4 dB outliers is
shown in Table 5.17. For VQ bit allocation greater than 18 bits, the percentage
of 4 dB outliers is zero.
Informal listening tests also showed the superiority of the new method at
the expense of increased complexity.
Table 5.17
Percentage of 4 dB outliers
VQ bit allocation
15 bits
16 bits
17 bits
18 bits
g
0.0059
0.0059
0.0
0.0
f
0.0415
0.0119
0.0059
0.0
5.11 Summary
This chapter has presented the fundamental aspects of speech spectral repre-
sentation via linear prediction. Accurate spectral representation of speech is
crucial to the performance of low bit-rate speech coders, especially in sinu-
soidal coders where the simpliﬁed excitation model cannot compensate for
shortcomings in the LPC modelling. Various quantization techniques have
been investigated and have lead to the design of LSF quantization schemes
optimized for speciﬁc conﬁgurations. These schemes offer solutions to LPC
parameter quantization in the form of LSFs for several applications, with
varying degrees of performance at a given bit rate and implementation com-
plexity. A more fundamental approach to improving quantization by way
of anti-aliasing ﬁltering has also been presented with increased quantization
performance.
Bibliography
[1] S. Saito and K. Nakata (1985) Fundamentals of Speech Signal Processing,
Chapter 9. Academic Press
[2] F. Soong and B. H. Juang (1984) ‘Line spectrum pairs and speech data
compression’, in Proc. of Int. Conf. on Acoust., Speech and Signal Processing,
pp. 1.10.1–4.
[3] L. Rabiner and R. Schafer (1978) Digital Processing of Speech Signals.
Englewood Cliffs, NJ: Prentice-Hall
[4] B. Atal, R. V. Cox, and P. Kroon (1989) ‘Spectral quantization and
interpolation for CELP coders’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 69–72.
[5] G. Kang and C. J. Francen (1984) ‘Low bit rate speech encoders based on
line spectrum frequencies (LSF)’, Tech. Rep. 8857, U.S. Naval Lab. Report.

Bibliography
147
[6] P. Kabal and R. P. Ramachandran (1986) ‘The computation of line spectral
frequencies using Chebyshev polynomials’, in IEEE Trans. on Acoust.,
Speech and Signal Processing, 34(6):1419–25.
[7] B. Cheetham (1987) ‘Adaptive LSP ﬁlter’, in IEE Electronics Letters,
23:89–90.
[8] ITU-T (1992) Coding of speech at 16 kbit/s using low-delay code excited linear
prediction, ITU-T Rec. G.728.
[9] K. K. Paliwal and B. Atal (1993) ‘Efﬁcient vector quantisation of LPC
parameters at 24 bits/frame’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 1(1):3–14.
[10] ETSI (1998) Digital cellular telecommunications system (phase 2); Enhanced
full rate (EFR) speech transcoding, GSM 06.60 v4.1.0 (ETS 301 245), June.
[11] F. Tzeng (1989) ‘Analysis by synthesis linear predictive speech coding at
2.4 kb/s’, in Proc. of Globecom, pp. 1253–7.
[12] A. McCree, K. Truong, E. B. George, T. P. Barnwell, and V. Viswanathan
(1996) ‘A 2.4 kbit/s MELP coder candidate for the new US Federal
Standard’, in Int. Conf. on Acoust., Speech and Signal Processing, 1:200–203.
[13] W. P. LeBlanc, B. Bhattacharya, S. A. Mahmoud, and V. Cuperman (1993)
‘Efﬁcient search and design procedures for robust multi-stage VQ of LPC
parameters for 4 kb/s speech coding’, in IEEE Trans. on Speech and Audio
Processing, 1(3):373–85.
[14] J. Makhoul (1975) ‘Linear prediction: A tutorial review’, in Proc. of IEEE,
63:561–80.
[15] K. T. Al-Naimi (2002) ‘Advanced speech processing and coding tech-
niques’, Ph.D. thesis, CCSR, University of Surrey, UK.
[16] W. B. Kleijn and J. Hagen (1995) ‘Waveform interpolation for coding and
synthesis’, in Speech coding and synthesis by W. B. Kleijn and K. K. Paliwal
(Eds), pp. 175–207. Amsterdam: Elsevier Science
[17] T. Eriksson, H-G Kang, and P. Hedelin (2000) ‘Low-rate quantization of
spectrum parameters’, in Int. Conf. on Acoust., Speech and Signal Processing,
pp. 1447–50.

6
Pitch Estimation and
Voiced–Unvoiced
Classification of Speech
6.1 Introduction
Low bit-rate speech coders, traditionally called vocoders, rely heavily on
extracting the correct speech parameters from a given speech segment. The
three main speech features are the spectral envelope, the pitch and the
voiced–unvoiced classiﬁcation. The spectral envelope is usually extracted by
a standard autocorrelation method which results in a linear predictive (LP)
parameters representation. However extracting the correct pitch and voicing
classiﬁcation is not as straightforward and may require a combination of
methods.
When measuring the pitch, it is assumed that the voiced signals are formed
by passing quasi-periodic excitation signals through the LPC ﬁlter. The
duration between the pulses in the excitation signal is called the pitch period
T0 or fundamental frequency f0. Correct estimation of the pitch is essential
for good quality speech-coding. Incorrect estimation of the pitch period
can seriously degrade the quality of synthesized speech. Pitch determination
algorithms (PDAs) have been studied in both the time and frequency domains,
and a comparison is discussed in [1, 2]. Traditionally, autocorrelation-based
methods [3] and their variants [4, 5] have been intensively investigated
and widely applied to various speech coders [6–11]. Frequency domain
approaches [12–14], on the other hand, have become popular recently due
to the growing interest in sinusoidal speech coders, such as the multi-band
excitation (MBE) [13] and the sinusoidal transform coder (STC) [14], which
conduct pitch determination based on a spectral synthesis (SS) method.
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

150
Pitch Estimation and Voiced–Unvoiced Classification of Speech
In addition to correct pitch estimation, correct voiced–unvoiced estima-
tion is also crucial for good quality speech synthesis. Traditional vocoders,
which have been in use for many years, classify the input speech signal
either as voiced or unvoiced. A voiced speech segment is known by its
relatively high energy content but, more importantly, it contains periodicity.
The unvoiced part of speech on the other hand looks more like random
noise with no periodicity. However, there are some parts of speech that are
neither voiced nor unvoiced, but a mixture of the two. These are usually
called the transition regions, where there is a change either from voiced
to unvoiced or unvoiced to voiced. In low bit-rate speech coding, correct
classiﬁcation of speech blocks (usually frames or subframes 20 ms long, or
shorter) is very critical for good quality speech synthesis. If voiced speech
is classiﬁed as unvoiced, the synthesized output will sound rough and less
intelligible. If, on the other hand, unvoiced speech is classiﬁed as voiced,
the synthesized speech will sound annoyingly metallic or robotic. In older
versions of vocoders, a hard decision voicing was used and the transitions
were classiﬁed into either fully voiced or fully unvoiced. In newer vocoders,
such as sinusoidal based coders (IMBE, MELP, etc.), soft decision voic-
ing is employed: a third class, in which both voiced and unvoiced exists
together, has been deﬁned. This mix of voiced and unvoiced decision is
usually carried out in the frequency domain where voiced and unvoiced
frequencies are appropriately selected to represent the mixed signal. As a
result, better quality synthesized speech is produced. In this chapter we
review some of the advanced techniques which are used in extracting the
correct pitch and subsequently estimating the correct voicing in each speech
segment.
6.2 Pitch Estimation Methods
The excitation model used in source-ﬁlter vocoders relies heavily on the
correct determination of the pitch parameter. Incorrect pitch estimation may
signiﬁcantly degrade the speech quality, and in particular its intelligibility,
by introducing artifacts into the synthetic speech. Moreover, other parameter
estimations such as voicing and spectral amplitudes in vocoders often assume
accurate pitch determination, and are severely affected by pitch errors.
Therefore, the reliability of the pitch determination algorithm (PDA) used
has a dramatic effect on the quality of the synthesized speech.
Pitch period is deﬁned as the time interval between two consecutive voiced
(periodic) excitation cycles. Although, this interval may vary from cycle to
cycle, it usually evolves slowly, and therefore it can be estimated. Estimating
the pitch period is generally easy for highly periodic sounds, but some speech

Pitch Estimation Methods
151
segments do not exhibit such characteristics. In some parts of speech as well
as having the pitch period varying the speech may contain a mixture of voiced
(periodic) and unvoiced (random) signals which may cause estimation errors.
Formant interaction can also be a problem as the speech may become highly
resonant and this may cause incorrect pitch estimation. Onsets and offsets
are also problem areas. Finally, large amounts of background noise present
in the signal can also complicate the task of the PDA.
PDAs are generally classiﬁed in two main categories: time or frequency
domain techniques. However in the last few years more complicated tech-
niques which use both time and frequency domain characteristics of speech
have been developed. These are summarized below.
6.2.1 Time-Domain PDAs
The most obvious feature of periodic signals is the similarity of the waveform
at different times. The main principle of pitch detection algorithms (PDAs)
which rely on time-domain waveform similarities is to ﬁnd the pitch period by
comparing the similarity between the original signal and its shifted version.
If the shifted distance is equal to the pitch period, the two signal waveforms
should have the greatest similarity. The majority of existing PDAs are based
on this concept. Among them, the average magnitude difference function
(AMDF) and the autocorrelation (AC) method are the two most widely
used.
Average Magnitude Difference Method
A simple way to compare the current speech with its time-delayed version is
to compute the average magnitude difference function (AMDF) [4] given by:
A(τ) =
N−1

n=0
|s(n) −s(n −τ)|
(6.1)
where τ is the lag. This function is computed over a given pre-determined
range for τ and the value of τ minimizing A(τ) is selected as the pitch
period. The value of N is typically 160 samples, corresponding to a 20 ms
speech frame. A plot of the AMDF function against the speech signal is
shown in Figure 6.1. The main advantage of the AMDF function is that
it only requires additions and subtractions, making it very suitable for
hardware implementation. However, current DSPs normally offer a one-cycle
multiply–add instruction, making this less signiﬁcant. The performance of
the AMDF function is relatively poor and, in particular, it does not cater for
variations in the energy of the speech.

152
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0
40
80
120
0
40
80
120
0
40
80
120
0
40
80
120
0
160
320
480
640
Samples
Speech
Signal
AMDF
Function
Lag Values
Figure 6.1
AMDF and speech signal: the minima of the AMDF corresponding to the
pitch values are indicated by circles
Autocorrelation Method
The direct distance measurement is the most popular criterion examining the
similarity between two waveforms; it can be expressed as,
E(τ) = 1
N
N−1

n=0
[s(n) −s(n + τ)]2
(6.2)
Equation (6.2) assumes that the average signal level is ﬁxed. However, at
speech onsets and offsets this is not true, hence, the use of the normalized
similarity criterion which considers the effect of nonstationarity of speech
signals,
E(τ) = 1
N
N−1

n=0
[s(n) −βs(n + τ)]2
(6.3)
where β is a scaling factor, or the pitch gain, controlling the changes in signal
level. Under the assumption that the signal is stationary (i.e. s(n) = s(n + τ)),

Pitch Estimation Methods
153
the error criterion of equation (6.2) can be written as,
E(τ) = [R(0) −R(τ)]
(6.4)
where
R(τ) =
N−1

n=0
s(n)s(n + τ)
(6.5)
The minimization of the estimation error, E(τ), in equation (6.2) is equivalent
to maximizing the autocorrelation (or cross-correlation) R(τ). The variable τ
is called lag, or delay, and the pitch is equal to the value of τ which results
in the maximum R(τ). Although the autocorrelation computation involves
a large number of multiplications, it is very easy to implement these in
real-time due to its regular form of computation, i.e. multiply–adds. With
today’s modern DSPs, multiply–add operations are very easily computed
in one instruction. Another advantage of autocorrelation PDA is that it is
phase-insensitive. Hence, it performs well in detecting the pitch of speech
which may suffer some degree of phase distortion.
Nguyen generalized the direct similarity measure [15] as,
E(τ) = 1
N
N−1

n=0
|s(n) −s(n + τ)|k
 1
k
(6.6)
where k is a constant. Although k can be arbitrary, Nguyen proved that k
values of 1, 2 and 3 are appropriate. In his experimental investigation, he
showed that 2 is the most appropriate value for speech signals, implying that
the autocorrelation method is superior to the AMDF. A typical autocorrelation
function is shown in Figure 6.2.
Speech in the long-term is a nonstationary signal and the direct similarity
criterion may exhibit large errors, implying fewer similarities in positions
where the shift is equal to the real pitch period. Figure 6.3b illustrates the
direct autocorrelation function which indicates more similarities in the triple
pitch period as the amplitude increases. The normalized similarity criterion
of equation (6.3) is derived under the consideration of such a nonstationary
process. Setting ∂E(τ, β)/∂β = 0 in equation (6.3) the optimum normalization
coefﬁcient (pitch gain) can be calculated using,
β =
N−1

n=0
s(n)s(n + τ)
N−1

n=0
s2(n + τ)
(6.7)

154
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0
40
80
120
0
40
80
120
0
40
80
120
0
40
80
120
0
160
320
480
640
Samples
Speech Signal
Normalised
Autocorrelation
Lag Values
Figure 6.2
Autocorrelation and speech signal. The maxima of the autocorrelation
function corresponding to the pitch values are indicated by circles
Lag
Lag
0
0
Window
(a)
(b)
(c)
Pitch
Figure 6.3
Autocorrelation of speech: (a) original speech, (b) direct autocorrelation
function and (c) normalized autocorrelation function

Pitch Estimation Methods
155
By substituting the optimum gain back into the error function of equation
(6.3), the pitch can be estimated by minimizing
E(τ, β) =
N−1

n=0
s2(n) −
N−1

n=0
s(n)s(n + τ)
2
N−1

n=0
s2(n + τ)
(6.8)
This is equivalent to maximizing the second term on the right hand side,
R2
n(τ) =
N−1

n=0
s(n)s(n + τ)
2
N−1

n=0
s2(n + τ)
(6.9)
Direct use of the above equation may result in some errors. This is because
the square of the autocorrelation may result in a maximum even if the
correlation is negative, forcing possible pitch-halving errors. In order to
eliminate this problem, the square root of equation (6.9) is taken to remove
the square from the correlation and, hence, eliminate the possibility of
lags with negative correlation from being selected as the pitch. The ﬁnal
normalized autocorrelation function is therefore given by,
Rn(τ) =
N−1

n=0
s(n)s(n + τ)




N−1

n=0
s2(n + τ)
(6.10)
The normalized autocorrelation function, shown in Figure 6.3c, shows much
better performance than the direct (un-normalized) autocorrelation method.
6.2.2 Frequency-Domain PDAs
Although most waveform similarity methods have their frequency domain
equivalents, the frequency domain PDAs directly operate on the speech
spectrum. The main frequency domain feature of a periodic signal is the har-
monic structure, with the distance between harmonics being the fundamental
frequency or the frequency equivalent of the pitch period. The main draw-
back of frequency-domain methods is their high computational complexity.

156
Pitch Estimation and Voiced–Unvoiced Classification of Speech
However, modern DSP techniques make the computational complexity of
frequency-domain PDAs insigniﬁcant, making them very popular in sinu-
soidal coders. In the following, we brieﬂy explain two frequency-domain
PDAs.
Harmonic Peak Detection
An obvious way of determining the pitch in the frequency domain would be to
extract the spectral peak at the fundamental frequency. This requires the ﬁrst
harmonic to be present, which cannot, in general, be expected because of the
front-end ﬁltering. A more practical method is to detect all of the harmonic
peaks and then measure the fundamental frequency (pitch frequency) as
either the common divisor of these harmonics or the spacing of the adjacent
harmonics. This can be done using a comb ﬁlter given by
C(ω, ω0) =
	 W(kω0)
;
ω = kω0, k = 1, 2, . . . m
ω0
0
;
otherwise
(6.11)
and correlating it with the speech spectrum. The output of the correlation,
Ac(ω0), is the summation of weighted comb peaks as,
Ac(ω0) = ω0
m
m/ω0

k=1
S(kω0)W(kω0)
2π
τmax
≤ω0 ≤2π
τmin
(6.12)
where m is the maximum frequency considered in the speech spectrum.
If ω0 is equal to the fundamental frequency, the comb response will match
the harmonic peaks, and the maximum output will be obtained as shown in
Figure 6.4. In order to obtain better subjective quality, a weighting coefﬁcient
can be applied to the individual teeth, normally decreasing weights with
increasing frequency [16].
Spectrum Similarity
This method assumes that the spectrum is fully voiced and is composed only
of a number of harmonics each located at multiples of the pitch frequency. A
synthetic spectrum is reconstructed using this assumption for each possible
pitch frequency candidate and is compared to the original spectrum. The
pitch frequency leading to the best matching reconstructed spectrum is then
selected [13] as the fundamental or pitch frequency. The speech spectrum is
assumed to be composed of voiced harmonics only, located at multiples of
the candidate pitch frequency ω0. Therefore the synthetic spectrum ˆS(m, ω0)
is an approximation of the convolution of pulses located at multiples of the
candidate pitch frequency ω0, by the spectrum W of the window used on

Pitch Estimation Methods
157
0.0
1000.0
2000.0
3000.0
4000.0
Frequency (Hz) 
0.0
50.0
100.0
|S(f)| & C(f, fo)  (dB)
Original Spectrum
Comb Response
fo
Figure 6.4
Harmonic peak matching method
the original speech prior to its Fourier transformation. The pulses are scaled
by a factor Al(ω0) so as to provide the best possible match with the original
spectrum. The synthetic spectrum ˆS(m, ω0) is deﬁned by:
ˆS(m, ω0) =



A0(ω0) W

2π
M m

A1(ω0) W

2π
M m −ω0

...
Al(ω0) W

2π
M m −l ω0

...
(6.13)
where M is the length of the DFT and Al(ω0) is deﬁned as:
Al(ω0) =
bl

m=al
S(m)W
2π
M m −l ω0

bl

m=al
W
2π
M m −l ω0

2
(6.14)
Al(ω0) is such that the scaled harmonic lobe spectrum Al(ω0)W(2π
M m −lω0)
is the best possible match for S(m), using an MSE criterion. The harmonic

158
Pitch Estimation and Voiced–Unvoiced Classification of Speech
boundaries al and bl are deﬁned as:
al =
 M
2π

l −1
2

ω0

(6.15)
bl =
 M
2π

l + 1
2

ω0

= al+1 −1
(6.16)
Finally, the synthetic spectrum ˆS(m, ω0) for candidate pitch frequency ω0 is
compared with the speech spectrum S(m) through an MSE measure, given
by:
E(ω0) =
M−1

m=0

S(m) −ˆS(m, ω0)
2
(6.17)
The value of ω0 minimizing E(ω0) is then selected as the pitch frequency. Typ-
ical original and synthetic spectra with correct pitch are shown in Figure 6.5.
6.2.3 Time- and Frequency-Domain PDAs
Pitch Estimation using Spectral Autocorrelation
The time domain autocorrelation (temporal autocorrelation, or TA) has been
used in various PDAs. Given a segment of speech signals s(n), 0 ≤n ≤N −1,
0.0
1000.0
2000.0
3000.0
4000.0
Frequency (Hz)
0.0
20.0
40.0
60.0
80.0
100.0
|Sw(f, fo)|  (dB)
Synthetic Spectrum
Original Spectrum
fo
Figure 6.5
Original and synthesized speech spectra used in the spectrum-similarity
PDA method

Pitch Estimation Methods
159
the normalized TA for a pitch candidate τ is given by
RT(τ) =
N−τ−1

n=0
s(n)s(n + τ)




N−τ−1

n=0
s2(n)
N−τ−1

n=0
s2(n + τ)
(6.18)
which differs from the autocorrelation method discussed earlier in the limits
of the summations (the earlier method was more like a cross-correlation). The
TA has been widely used for PDAs due to its relatively good performance
especially over noisy speech signals [2]. Autocorrelation can also be used in
the frequency domain to bring out spectral similarities which are mainly due
to the pitch frequency spacing of the harmonics. If the spectrum of windowed
speech is given by S(m) = A(m)ejθ(m) for 0 ≤m ≤M −1, where A(m) and
θ(m) are the magnitude and phase of the normalized spectral autocorrelation
(SA), RS(τ) can be deﬁned as
RS(τ) =
⌊M/2⌋−ωτ

m=0
Az(m)Az(m + ωτ)




⌊M/2⌋−ωτ

m=0
A2
z(m)
⌊M/2⌋−ωτ

m=0
A2
z(m + ωτ)
, for T(l)
0 ≤τ ≤T(u)
0
(6.19)
where ωτ = ⌊M/τ + 0.5⌋, and T(l)
0
and T(u)
0
are the lower and upper limits
for the pitch search. In equation (6.19), the zero-crossing spectrum Az(m) is
given by
Az(m) = A(m) −gA(m)
(6.20)
where A(m) is the spectral envelope of A(m). The envelope may be estimated
using the peak-picking method [17, 18]. The magnitude spectrum, A(m), is
converted into the zero-crossing spectrum Az(m) to make it feasible for the
autocorrelation deﬁned in equation (6.19). The gain, g, is calculated as:
g =
⌊M/2⌋

m=0
A(m)A(m)/
⌊M/2⌋

m=0
A(m)A(m)
(6.21)
In equation (6.20), the logarithmic spectrum can also be considered to obtain
a zero-crossing spectrum. However, the SA with the logarithmic spectrum
produces a high correlation ratio for large lags, τ, close to T(u)
0
(small

160
Pitch Estimation and Voiced–Unvoiced Classification of Speech
50
100
150
200
−1000
−500
0
500
1000
(a) Time (samples)
Amplitude
0
1000
2000
3000
4000
0
0.5
1
1.5
2
× 104
(b) Frequency (Hz)
Amplitude
0
1000
2000
3000
4000
−1
−0.5
0
0.5
1
× 104
(c) Frequency (Hz)
Amplitude
50
100
150
−1
−0.5
0
0.5
1
(d) Lag (samples)
Spectral autocorrelation
Figure 6.6
An example of (a) speech signal of T0 = 34-sample (Fs = 8 kHz), (b)
magnitude spectrum, (c) zero-crossing spectrum, and (d) spectral autocorrelation
overlapping area) corresponding to very small ωτ, i.e. ⌊M/T(u)
0
+ 0.5⌋≤
ωτ << ⌊M/(2T0) + 0.5⌋. Thus, the linear magnitude spectrum is used instead
of the logarithmic one.
Figure 6.6 shows an example illustrating the characteristics of SA. For a
speech segment in Figure 6.6a, the magnitude and its zero-crossing spectra
are shown in Figures 6.6b and 6.6c, respectively. Finally, the spectral autocor-
relation is shown in Figure 6.6d, indicating a prominent peak at the pitch lag.
The TA over a periodic signal produces high correlation for integer multiples
of the pitch period T0. This means that the spectral autocorrelation, RS(τ) in
equation (6.19), has peaks for the integer submultiples of T0, i.e. τ = T0/k,
for 1 ≤k ≤⌊T0/T(l)
0 ⌋. Figure 6.7 shows an example featuring high SAs for
pitch period submultiples. Thus, the TA-based PDA may result in detecting
an unwanted pitch period multiple, and the SA-based PDA may result in
pitch-halving. The pitch period multiple and submultiple problems can be
compensated for by combining the two autocorrelation methods, TA and SA,
in an advantages way. Hence, the spectro-temporal autocorrelation (STA) is
deﬁned as [19],
RST(τ) = αRT(τ) + (1 −α)RS(τ)
(6.22)

Pitch Estimation Methods
161
50
100
150
200
−2000
−1000
0
1000
2000
(a) Time (samples)
Amplitude
0
1000
2000
3000
4000
0
5000
10000
15000
(b) Frequency (Hz)
Amplitude
0
1000
2000
3000
4000
−6000
−4000
−2000
0
2000
4000
6000
(c) Frequency (Hz)
Amplitude
50
100
150
−1
−0.5
0
0.5
1
(d) Lag (samples)
Spectral autocorrelation
Figure 6.7
An example of (a) speech signal of T0 = 59-sample (Fs = 8 kHz), (b)
magnitude spectrum, (c) zero-crossing spectrum, and (d) spectral autocorrelation
where α is a weighting factor, 0 ≤α ≤1. The cases of α = 0 and α = 1 reduce
the STA to SA and TA, respectively. The estimated pitch period ˆT0 using the
STA is the argument maximizing (6.22) as:
ˆT0 = arg maxτ{RST(τ)}
(6.23)
Because of the dual relation between the temporal and the spectral autocorre-
lations, it is found that the STA has a useful property for pitch estimation. For
a segment of periodic signal with a pitch period T0, T(l)
0 ≤T0 ≤T(u)
0 , RST(τ)
in (6.22) has the strongest peak at τ = T0 compared with the integer multiple
and submultiple periods of T0, i.e. τ = pT0 and T0/p for 2 ≤p ≤⌊T(u)
0 /T0⌋and
2 ≤p ≤⌊T0/T(l)
0 ⌋. In (6.22), RS(τ) and RT(τ) terms suppress the undesirable
high peaks for the multiples and submultiples of T0 excluding τ = T0. Con-
sequently, the STA for τ = T0 remains relatively more prominent compared
with those for the rest.
The range of the pitch period can be split into three groups as high (short
pitch period), mid, and low (long pitch period), based on the expected number
of prominent peaks in TA and SA. The minimum pitch period producing a
pitch period submultiple in SA is 2T(l)
0 . The SA can only rarely produce pitch

162
Pitch Estimation and Voiced–Unvoiced Classification of Speech
50
100 150 200
−1000
−500
0
500
1000
(a) Time (samples)
50
100 150 200
−1000
0
1000
(b) Time (samples)
50
100 150 200
−1000
−500
0
500
1000
(c) Time (samples)
50
100
150
−1
−0.5
0
0.5
1
(d)
TA
50
100
150
−1
−0.5
0
0.5
1
(g)
SA
50
100
150
−1
−0.5
0
0.5
1
(j) Lag (samples)
STA
50
100
150
−1
−0.5
0
0.5
1
(e)
50
100
150
−1
−0.5
0
0.5
1
(h)
50
100
150
−1
−0.5
0
0.5
1
(k) Lag (samples)
50
100
150
−1
−0.5
0
0.5
1
(f)
50
100
150
−1
−0.5
0
0.5
1
(i)
50
100
150
−1
−0.5
0
0.5
1
(l) Lag (samples)
Figure 6.8
Comparison of TA, SA, and STA (α = 0.5) for 32-sample (left column),
59-sample (middle column), and 100-sample (right column) speech signals
period submultiples for short pitch period signals, i.e T(l)
0
≤T0 ≤2T(l)
0 −1.
On the other hand, in TA, the maximum pitch period generating the pitch
period multiple is T(u)
0 /2. Thus, for T(u)
0 /2+1 ≤T0 ≤T(u)
0 , TA can be relatively
robust against pitch multiple errors. However, STA gives robust results for
the whole pitch range by combining the two functions. Examples comparing
the characteristics of TA, SA, and STA for speech signals with various pitch
periods are shown in Figure 6.8. As can be seen from the ﬁgure, STA has a
prominent peak regardless of the pitch period.

Pitch Estimation Methods
163
Spectral Synthesis–Spectral Autocorrelation PDA
The frequency spectrum of windowed speech can be decomposed into its
spectral envelope and the ﬁne structure spectra. The spectral envelope is the
smoothed version of the speech spectrum. Spectral ﬁne details (the excitation
spectrum), on the other hand, exhibit harmonics for voiced components in
which each harmonic typically has the shape of a sinc function corresponding
to the applied window frequency response. Spectral synthesis (SS) methods
[13, 14] determine the pitch so as to minimize the distortion between the
original and synthesized spectra. The synthesized spectrum is generated by
shifting the centre frequency of the sync function spectrum to harmonic
frequencies.
In [14], McAulay’s SS-based PDA, the metric for pitch determination is
given by,
	(τ) =
H(τ)

h=1
A
hK
τ
 


K/2

k=1
A(k)D
 k
K −h
τ

−1
2A
hK
τ



(6.24)
where H(x) = ⌊x/2⌋and D(x) = sin(2πx)/(2πx). The performance of the
above PDA has been improved against the spectral formant effect by incor-
porating an energy-based metric ϕ(τ) [20, 17], given by,
ϕ(τ) =
N

k=0
|dτ(k)|
N

k=0
|eτ(k)|
(6.25)
where eτ(n) = τ
k=0 s2(n−⌊τ/2⌋+k) and dτ(n) = 0.95dτ(n−1)+eτ(n)−eτ(n−1)
with dτ(0) = 0. The improved SS-based metric is deﬁned as,
	ϕ(τ) = 	(τ)
ϕ(τ)
(6.26)
in which 	ϕ(τ), if not positive, is bounded to a small positive value.
The SS-based PDA can be further improved by incorporating the spectral
autocorrelation metric given in (6.19) to reduce pitch multiple effects which
may occur in 	ϕ(τ). Hence, SS incorporating SA (called SS–SA [19]), is deﬁned
as:
	SA(τ) = 	ϕ(τ)β
	RS(τ) + 1
2
1−β
(6.27)

164
Pitch Estimation and Voiced–Unvoiced Classification of Speech
50
100
150
0
0.2
0.4
0.6
0.8
1
(a)
SS
50
100
150
0
0.2
0.4
0.6
0.8
1
(d) Lag (samples)
SSSA
50
100
150
0
0.2
0.4
0.6
0.8
1
(b)
50
100
150
0
0.2
0.4
0.6
0.8
1
(e) Lag (samples)
50
100
150
0
0.2
0.4
0.6
0.8
1
(c)
50
100
150
0
0.2
0.4
0.6
0.8
1
(f) Lag (samples)
Figure 6.9
Comparison of SS and SS–SA (β = 0.25) for the high-, mid-, and low-
pitched signals shown in Figures 6.8a–c
where β is a weighting factor, 0 ≤β ≤1, controlling the effect of SA. The
SS–SA becomes SS when β = 1 and SA when β = 0. Examples examining
the characteristics of 	ϕ(τ) and 	ST(τ), are shown in Figure 6.9; the measured
value of each subﬁgure is normalized by each maximum value. The input
speech signals used in Figure 6.9 are the same as the ones used in the
analysis of the STA. For the high-pitched signals (short pitch periods) in
Figure 6.8a, the lag corresponding to the pitch period double has a strong
peak in Figure 6.9a, which seems to be even stronger than the peak at the
correct pitch. The SS–SA alleviates this problem as shown in Figure 6.9d
where the peak at the correct pitch lag becomes prominent in comparison
with other peaks. For the mid- and low-pitched (long pitch period) signals in
Figures 6.8b and 6.8c, the maximum peaks of 	ϕ(τ) and 	ST(τ) are relatively
obvious as illustrated in Figures 6.9b, 6.9c, 6.9e, and 6.9f.
Comparison
An objective test was conducted to determine various tuning factors. The
performance of the PDAs was measured in terms of pitch error rates (Ep).
The speech test material, sampled at 8 kHz and ﬁltered through the modiﬁed
intermediate response system (MIRS) [21], was composed of 56 seconds each
of male and female speech, each uttered by eight speakers. The reference
pitch periods were manually marked for each 10 ms frame.

Pitch Estimation Methods
165
The range of the pitch search was limited to between 15 and 150 samples.
Spectral analysis was conducted using a 240-sample Hamming window and a
256-point FFT with 16-sample zero padding. When computing the TA, a 240-
sample rectangular or Hamming window was applied to the input signals.
Pitch error decisions were checked in each frame by comparing the detected
pitch period with the reference. A frame was classiﬁed as erroneous if the
absolute difference between the reference and the detected pitch periods was
more than 1 ms (8-sample) as in [1]. Extra processing, such as pitch tracking
using the pitch history of the past frames, was not incorporated in order
to evaluate only the main algorithmic contributions. Although the unvoiced
speech regions were not taken into account, transitions were included in the
performance evaluations as these regions are perceptually very important.
• Analysis of the STA Weighting Factor
The effect of the STA rate α in terms of Ep is shown in Figure 6.10. The
results show that the STA gives improved performance compared with
the TA and the SA, corresponding to α = 1 and α = 0, respectively. The
lowest Ep was obtained when α = 0.5 for both the female and male speech
samples.
• Analysis of the SS–SA Weighting Factor
The weighting factor β of SS–SA in equation (6.27) was analysed by varying
β between 0 and 1 (see Figure 6.11). As in STA, the SS–SA also shows much
less Ep in comparison with those of the SS and the SA, corresponding to
β = 1 and β = 0, respectively. Additionally, the lowest Ep values were
0
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
STA weighting rate (a)
Pitch error rate (%)
Male/Rectangular
Male/Tapered
Female/Rectangular
Female/Tapered
Figure 6.10
Analysis of the effect of the STA weighting factor α in terms of the pitch
error rate; the formant weighting factor γ is 0.9

166
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Pitch error rate (%)
Male
Female
SS-SA weighting factor (  ) 
β
Figure 6.11
Analysis of the effect of the SS–SA weighting factor β in terms of the
pitch error rate. Here, the formant weighting factor γ is 0.9
obtained when β = 0.1 for the female speech and β = 0.3 for the male
speech, which means that the optimum β differs slightly depending on
the pitch period of the signal. Higher performance can be achieved by
weighting the SA more during shorter pitch period speech and less during
longer pitch period speech.
Examples of pitch contours of the various PDAs are illustrated in Figures
6.12 and 6.13 in which the rectangular window is applied to the TA and STA.
It shows that pitch errors in strongly-voiced regions are reduced considerably
by the combination of time and frequency domain PDAs. Most of the errors
were caused at speech onset and offset regions where irregular pitch pulse
sequences are present.
6.2.4 Pre- and Post-processing Techniques
In addition to the main pitch determination processes described previously,
there are several important pre- and post-processing techniques which can
signiﬁcantly improve the pitch determination performance. These techniques
supplement the PDAs and are used before or after the pitch determination
process. Hence, they are usually called preprocessing or post-processing
stages.
Spectrum Flattening
Although the pitch of a voiced speech segment can be directly estimated
from the original speech, the ﬁrst formant frequency may affect the accuracy

Pitch Estimation Methods
167
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
−2
0
2
× 104
Time (sec)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
0
100
200
Time (sec)
samples
(h)
Figure 6.12
Comparison of pitch contours of PDAs for (a) female speech. (b)
Reference; (c) TA, (d) WTA, (e) STA, (f) SS, (g) WSS, and (h) SS–SA-based PDAs
of the estimation. Several methods have been proposed to ﬂatten the speech
spectrum in order to avoid the formant interaction effect [20, 5, 22, 23]. The
speech spectrum is ﬁrst ﬂattened by removing the formants (by either linear
or nonlinear methods) before the pitch estimation process can begin.
The linear spectrum-ﬂattening method uses the LPC inverse ﬁlter to remove
the formants from the speech signal. The main drawback of this method is that
for high-pitched speech, like that of females and children, the ﬁrst complex

168
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−2
0
2
× 104
Time (sec)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
100
200
Time (sec)
samples
(h)
Figure 6.13
Comparison of pitch contours of PDAs for (a) male speech (b) Refer-
ence; (c) TA, (d) WTA, (e) STA, (f) SS, (g) WSS, and (h) SS–SA-based PDAs
zero of the inverse ﬁlter may be adjusted to become the ﬁrst harmonic,
and the second complex zero to the second harmonic. This may destroy
the entire periodicity information in the residual signals [16, 1]. Thus, a
formant weighting ﬁlter [24] is adopted as a preprocessor to control the de-
emphasizing factor of the formants while keeping the harmonics structure.
As described in the equation below, it is, effectively, a process of obtaining the

Pitch Estimation Methods
169
0
2
4
6
8
10
12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Formant weighting factor (  ) 
γ
Pitch error rate (%)
STA / Male
STA / Female
SS-SA / Male
SS-SA / Female
Figure 6.14
Analysis of the effect of the formant weighting factor γ in terms of the
pitch error rate; α and β defined in the STA and SS–SA functions are 0.5 and 0.25,
respectively, and the rectangular window is applied to TA calculation in the STA
intermediate signal between the original speech and its LPC residual signals.
Sf(z) =
A(z)
A(z/γ )S(z)
(6.28)
where S(z), Sf(z), and A(z) are the z-transform of the input speech signal
s(n), the formant-suppressed signal, and the inverse ﬁlter, respectively. The
parameter γ is the formant weighting factor, 0 ≤γ ≤1. For the case of γ = 1,
the ﬁltered signal is identical to the original speech signal. On the other hand,
γ = 0 makes the ﬁltered signal equal to the LPC residual of s(n). It can be
seen that Sf(z) is the intermediate spectrum between the original and residual
spectra for 0 < γ < 1. The effect of the formant weighing factor γ in equation
(6.28) was observed over the STA and SS–SA-based PDAs and the results
are shown in Figure 6.14. It can be seen that the value around 0.7 ∼0.9 gives
improved performance.
The effect of the ﬂattening ﬁlter is shown in Figure 6.15. The formant
inﬂuence has been greatly reduced but not completely eliminated, while the
harmonic structure is well-preserved. A better performance may be obtained
by making the spectral-ﬂattening factor a function of the average pitch
(tracked pitch) as shown in Figure 6.16.
Nonlinear spectrum-ﬂattening is usually achieved by centre-clipping the
speech signal. The ﬁrst centre-clipping PDA was proposed by Sondhi [22] in
1968 and various centre-clippers for autocorrelation PDAs were investigated
by Rabiner [3] in 1976. The characteristics of three types of centre-clipping

170
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0.0
1000.0
2000.0
3000.0
4000.0
Frequency (Hz)
−5.0
0.0
5.0
10.0
15.0
20.0
Magnitude (dB)
Original Speech Spectrum
Flattened Speech Spectrum (β = 0.8) 
Figure 6.15
Influence of the spectrum-flattening filter
0.8
1.0
β
30
60
Tracked Pitch (in samples)
Figure 6.16
Value of the flattening factor β against tracked pitch
functions used for ﬂattening the spectrum are shown in Figure 6.17 where,
y = clc(x) =



x + CL
;
x ≤−CL
x −CL
;
x ≥CL
0
;
−CL < x < CL
(6.29)

Pitch Estimation Methods
171
CL
−CL
−CL
CL
y = cpl(x)
x
y
CL
−CL
y = clc(x)
x
y
1
−1
−CL
CL
y = sgn(x)
x
y
Figure 6.17
Clipper functions
y = clp(x) =
	 x
;
−CL ≥x ≥CL
0
;
−CL > x < CL
(6.30)
y = sgn(x) =



1
;
x ≥CL
−1
;
−CL ≥x
0
;
−CL > x < CL
(6.31)
The centre clipped signal y(n) is generally deﬁned as:
y(n) = f[s(n)]
(6.32)
The clipper function f[.] can be any one of the functions in equations
(6.29)–(6.31). For the autocorrelation method, the clipped autocorrelation
function Rc(τ) is deﬁned as:
Rc(τ) =
N−1

n=0
y(n)y(n + τ) =
N−1

n=0
f[s(n)] f[s(n + τ)]
(6.33)
More generally, these two clipper functions can have any combination, e.g.
Rc(τ) =
N−1

n=0
f1[s(n)] f2[s(n + τ)]
(6.34)
A set of typical combinations of f1(n) and f2(n) are listed in Table 6.1. It has
been shown that[3, 16]:
• For high-pitch speakers, the differences in performance scores between the
various clipping combinations are small and probably insigniﬁcant.
• For low-pitch speakers, fairly signiﬁcant differences in performance scores
exist. Combination 1 in Table 6.1 gives the worst performance for all
utterances in this class. Combinations 4, 5, and 6 (those involving one
unprocessed component) are also poor in their overall performances.

172
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Table 6.1
Combinations of
clipper functions
Type
f1(n)
f2(n)
1
s(n)
s(n)
2
clc[s(n)]
clc[s(n)]
3
clp[s(n)]
clp[s(n)]
4
s(n)
clc[s(n)]
5
s(n)
clps(n)
6
s(n)
sgns(n)
7
clcs(n)
sgns(n)
8
clps(n)
sgns(n)
9
clps(n)
clcs(n)
10
sgns(n)
sgns(n)
• Differences in the performance of the remaining six combinations are not
consistent, thus any one of these correlators can be applied for ﬂattening
the spectrum.
• The performance is improved if the nonlinear processing is performed
before low-pass ﬁltering. This applies especially to band-limited signals,
where the weakly-attenuated waveform of the ﬁrst formant is often the
only information available for periodicity detection after low-pass ﬁltering.
The key problem in centre clipping is the choice of the clipping threshold.
Sondhi proposed a method based on a short time interval (5 ms), where the
threshold for clipping was set at 30 % of the maximum absolute signal value
within the block [22]. In Rabiner’s method, the threshold was set to be a ﬁxed
percentage (68 %) of the smaller of the maximum absolute signal value over
the ﬁrst and last 10 ms of the analysis frame, which is normally 30 ms [25].
Pitch Tracking
The principle of pitch tracking is based on the continuity characteristic of pitch,
i.e. once a voiced sound is established, its pitch varies within a limited narrow
range. The pitch tracking principle can be used in two ways, one operating
after the main pitch determination process as an error-checking function
and the another within the main pitch determination process ensuring the
estimation follows the correct route.
The ﬁrst method of pitch tracking is also called pitch smoothing because
it forces the pitch contour to be smooth. Pitch smoothing is a passive way
of utilizing the continuity characteristic. The risk in using this method is
that some abrupt changes in pitch are smoothed out, as there are occasional
instances of dramatic change.

Pitch Estimation Methods
173
τmin
τmin
(b)
(a)
frame 2
frame 1
frame 0
frame 2
frame 1
frame 0
τmax
τmax
0.8τ1
0.8τ0
1.2τ1
1.2τ0
τ0
τi
τj
τ2
τ1
τj2
τj1
τi2
τi1
Figure 6.18
Forward pitch tracking: (a) Setting search range limits and (b) Possible
tracked pitch candidates
The active way of using pitch tracking is to apply it at the beginning of
the main processing. Thus, the pitch is not estimated in isolation but by
considering the neighbouring frames. With this pitch-tracking method, the
pitch is estimated as a minimum path error overall. Path error refers to an
accumulated error for a number of adjacent frames, also called the path
penalty. For instance, if a pitch path consists of τ0, τ1, τ2 (see Figure 6.18), the
path penalty is the accumulated error on the path given by:
Epath = E0(τ0) + E1(τ1) + E2(τ2)
(6.35)
where Ei(τj) is the estimated error for candidate τj in the ith frame. Constraint
conditions must be applied to the possible pitch paths so that the continuity
characteristic can be maintained. Pitch-tracking constraints are as follows:
(1 −α)τ0 ≤τ1 ≤(1 + α)τ0
(1 −α)τ1 ≤τ2 ≤(1 + α)τ1
(6.36)

174
Pitch Estimation and Voiced–Unvoiced Classification of Speech
τmin
τmax
τ2
τ1
τ−1
τ−2
τ0
τ0
previous 2
previous 1
future 1
future 2
current
Frame
Pitch 
Figure 6.19
A two trace-tracking scheme
where α is chosen according to the short-time analysis (frame duration) or
path step. Since the frame duration is equivalent to the interval between
two consecutive pitch analysis blocks, as the frame gets larger, the next
pitch could be expected to have more deviation. According to the data by
Sundberg [26], the maximum rate of change of fundamental frequency is in
the order of 1 %/ms. For a 20 ms frame size, the maximum frequency change
would be 20 % which corresponds to a pitch range from 0.8ω0 to 1.2ω0. In the
time domain, the corresponding range is 0.8τ0 to 1.2τ0, approximately, with
α = 0.2.
In order to take into account the effects of both continuous pitch and
changing pitch, two pitch paths should be considered (see Figure 6.19). One
traces the pitch from previous frames to the current frame, and the other
traces the pitch from the current frame to incoming or future frames that
forecast a new pitch trace. If the future path penalty is less, it is assumed that
a new pitch trace is starting; if the path penalty with the previous frames is
smaller, then the existing pitch is assumed to continue into the current frame.
Since future pitch tracking requires the storage of the future frames, extra
delay is unavoidable. The pitch-tracking procedure using three frames can be
described as follows:
• Forward Tracker
1. For each candidate pitch (τ0) in the current frame, ﬁnd E0(τ0).
2. Find the joint minimum E1(τ1)+E2(τ2) under the constraints of equation
(6.36).
3. Add E0(τ0) to the corresponding minimum E1(τ1) + E2(τ2) to form each
candidate’s accumulated forward path error.
4. Search for the minimum accumulated error and ﬁnd the forward pitch.

Pitch Estimation Methods
175
• Backward Tracker
1. Add E−2(τ−2) and E−1(τ−1) corresponding to the pitch periods of the
two previous frames to ﬁnd the accumulated error up to the current
frame.
2. Find the minimum E0(τ0) under the constraint that 0.8τ−1 ≤τ0 ≤1.2τ−1.
3. Add E0(τ0) to the backward-accumulated error to ﬁnd the backward
pitch-tracking error.
Finally the forward and backward path errors are compared and the
optimum pitch τopt is selected.
It can be seen from the above description that the forward pitch-tracking
procedure is equivalent to a full search scheme for a given path boundary
which makes it very complex to implement in real-time. Although dynamic
programming techniques can reduce this procedure to a sequential search, it
is still very complex and thus not widely used in practice. However, using
the fact that the search ranges are heavily overlapped, a fast algorithm has
been developed that reduces the computations signiﬁcantly [27].
If we assume Ri is the search range in the next frame for the candidate pitch
τi in the current frame and similarly, Rj deﬁnes the search range of τj, then
according to the deﬁnition, we have
Ri = {τ i
b, τ i
b + p, τ i
b + 2p, τ i
b + 3p, . . . , τ i
e}
(6.37)
Rj = {τ j
b, τ j
b + p, τ j
b + 2p, τ j
b + 3p, . . . , τ j
e}
(6.38)
where τ i
b and τ i
e are the ﬁrst and last pitch values in the range Ri and similarly
τ j
b and τ j
e are the ﬁrst and last pitch values in the range Rj, p is the pitch
resolution or the step size and,
τ k
b =



τmin
if (1 −α)τk < τmin
(1 −α)τk
otherwise
τ k
e =



τmax
if (1 + α)τk > τmax
(1 + α)τk
otherwise
It can be shown that all possible search ranges can be divided into three
main groups. The ﬁrst and third cases, in Figures 6.20a and 6.20c respectively,
have the same characteristic in that one range is fully overlapped by the next,
i.e. Ri ∈Rj. In these cases, it is not necessary to search through all these ranges

176
Pitch Estimation and Voiced–Unvoiced Classification of Speech
0.8τi
0.8τj
1.2τi
1.2τj
τi
τj
(a) 
(b) 
(c) 
1.2τi
1.2τj
τi
τj
τmin
0.8τj 0.8τi
τj
τi
τmax
Figure 6.20
Three possible pitch-lag search ranges
separately. The smaller range is fully searched and then its minimum error
value is compared with the new candidates in the next bigger range.
The second case (Figure 6.20b) is not as easy as the other two, because
the ranges are only partially overlapping. Assuming the error function is as
sketched in Figure 6.21a, the two search ranges should be fully searched to
prevent the incorrect pitch being selected. However each partly-overlapped
range, Ri, can be split into two fully overlapped subranges, R′i and R′i, as
shown in Figure 6.21b.
R′4 ∈R′3 ∈R′2 ∈R′1
(6.39)
R′1 ∈R′2 ∈R′3 ∈R′4
(6.40)
τi
τ1
τ2
τ3
τ4
τj
E(P)
R’4
R’1
R’2
R’3
R"4
R"3
R"2
R"1
right to left search
left to right search
(a)
(b)
Figure 6.21
Partially-overlapped search schematics

Pitch Estimation Methods
177
The whole search, therefore, is divided into two procedures: subrange search
and subrange comparison. Since these subranges are all fully overlapped,
searching over the subranges only need be done twice, from left to right and
from right to left. We start with the range R′′1 and compare its minimum with
the nonoverlapped part of R′′2 and so on until all of the right hand side is
completed. The same procedure is applied to the left hand side starting with
R′4. Finally, the left hand and right hand side minima are compared and the
overall minimum is selected. We can also see that the number of comparisons
during the search is independent of the size of the pitch search ranges and is
equal to three times the number of pitch candidates.
Multiple Pitch and Half Pitch Errors
Almost all PDAs have a peak detector which decides the pitch by the peak
position. In time-domain methods for example, the peak to be detected is
not only positioned at the correct pitch lag, but also at its integer multiples.
Therefore it is possible that a multiple of the real pitch may be chosen. In
order to ﬁnd the desired peak among the peaks, a complicated procedure
is normally needed. The basic idea for solving this problem includes two
steps: picking the maximum peak; checking the submultiple positions to see
if there is a comparable peak. However, since there is no ﬁxed solution to this
problem, tuned comparison thresholds are generally used.
For example, in the case of the cross-correlation pitch estimation method,
the comparison is made by looking at the ratio R(τ0/i)/R(τ0) where i is
an integer, which produces pitch submultiples greater than or equal to the
minimum expected pitch. The smallest submultiple which may produce a
ratio greater than the set threshold is selected as the pitch.
In frequency-domain methods, such as the spectrum similarity method,
a similar procedure can be applied. In this case, the average sum of the
harmonics in the signal may be used in the comparison. At every submultiple,
the average sum of harmonic magnitudes are computed by
Av(ωk) = 1
Lk
Lk

i=1
A(iωk) ;
k = 1, 2, 3, . . . , n.
(6.41)
where Lk is the total number of harmonics in a 4 kHz speech bandwidth,
A(iωk) are harmonic magnitudes and ωk =
2π
τ0/k is the fundamental frequency
of the kth submultiple of the initial pitch. The ratio between the Av(ωk) of the
smallest submultiple and the initial pitch, τ0, is then computed and compared
with a threshold which may vary for each submultiple. If this ratio is bigger
than the corresponding threshold, then the smallest submultiple is selected as
the pitch estimate. Otherwise the next largest submultiple is checked against

178
Pitch Estimation and Voiced–Unvoiced Classification of Speech
the above procedure and it is selected as the pitch estimate if it satisﬁes
the condition. This process continues until all submultiples have been tested
against this condition. If none of the submultiples of the initial pitch satisfy
the condition, then the initial τ0 becomes the ﬁnal pitch estimate.
In some cases, where the decision threshold is wrongly exceeded, a multiple
or submultiple of the correct pitch may be selected. This may cause signiﬁcant
performance degradations. Therefore, when designing a vocoder that requires
accurate pitch estimation, other measures which can reduce the effects of these
occasional pitch errors should be considered.
6.3 Voiced–Unvoiced Classification
The voicing is another very important parameter which must be estimated
correctly for good quality speech reproduction. In the old vocoders, a single
(binary) voicing decision was made by classifying the frame (or half the
frame) as either voiced or unvoiced. However, it is well known that the
transitions are very important for good quality speech synthesis and most
of the time the transitions are a mixture of voiced and unvoiced signals.
Therefore a mixed decision of voicing has been developed and used in many
of the latest vocoders. In the following, we review and discuss both binary
(hard) and mix (soft) decision voicing.
6.3.1 Hard-Decision Voicing
Voiced and unvoiced sounds have very well-known characteristics which can
be used to classify them reasonably correctly. Some of the most distinctive of
these characteristics are discussed below.
Periodic Similarity
Themostprominentcharacteristicthatseparatesvoicedspeech fromunvoiced
speech is its regularity and fairly well-deﬁned pitch. During voiced speech,
samples in one pitch period look very similar to the samples in the adjacent
pitch period. Hence, measuring the similarity between samples in consecutive
pitch cycles can give a reasonably good idea if the speech is voiced or
unvoiced. The measurement of similarity, Ps, can be computed by
Ps =
 N

i=1
s(i)s(i −T)
2
N

i=1
s2(i)
N

i=1
s2(i −T)
(6.42)

Voiced–Unvoiced Classification
179
Figure 6.22
Original speech waveform and the corresponding pitch similarity plot
with a possible voicing threshold of 0.5 (shown by the dashed line)
which has a value between 0 and 1, indicating no similarity and 100 %
similarity, respectively. Time plots of typical voiced and unvoiced speech
against pitch similarity are shown in Figure 6.22. As can be seen from the
ﬁgure the voiced parts of speech clearly have higher pitch similarity than
the unvoiced parts. This is expected since two adjacent unvoiced speech
segments do not possess noticeable similarities.
Peakiness of Speech
Periodic or voiced speech contains regular pulses which do not appear in
unvoiced speech. This feature is described as peakiness of speech and it can
be used to identify voiced speech when it has a relatively high value. In order
to enhance the peakiness, the LPC residual can be used to compute its value.
Pk =



 1
N
N

i=1
r2(i)
1
N
N

i=1
|r(i)|
(6.43)

180
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Figure 6.23
LPC residual and corresponding peakiness plots with a possible voicing
threshold of 1.4 (shown by the dashed line)
where r(i) is the LPC residual signal. Plots of LPC residual and the corre-
sponding peakiness measure are shown in Figure 6.23. Although the voiced
speech is clearly peaky, there are some unvoiced parts which contain a major
spike. In these cases, the peakiness measure may incorrectly indicate the
frame as voiced instead of unvoiced. In order to avoid this problem, a second
peakiness measure can be computed by excluding the largest magnitude
sample and its immediate neighbours from the computation. If the two peak-
iness measures are signiﬁcantly different, then the frame is not really voiced
but contains a spike.
Zero Crossing
Unvoiced speech has random characteristics, which means that the number of
times the signal crosses the zero line (i.e. that the sign changes) is signiﬁcantly
higher than with the voiced part of speech, which has a much slower zero-

Voiced–Unvoiced Classification
181
Figure 6.24
Speech waveform and its zero-crossing rate with a possible voicing
threshold of 60 (shown by the dashed line)
crossing rate. The simple logic shown below can be used to compute the
zero-crossing rate:
count=0;
for(i=1;i<N;i++)
{
if((data[i] x data[i-1]) < 0.0)
count = count + 1
}
Zc=count
A speech waveform and its corresponding zero-crossing rate is shown in
Figure 6.24. The zero-crossing rate also depends on the pitch of the signal (if
voiced). For example, the zero-crossing rate of voiced female speech (with
a short pitch period) is higher than that of voiced male speech (with a long
pitch period). A small pitch weighting can be used to weight the decision
threshold.

182
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Spectrum Tilt
Voiced speech has higher energy in low frequencies and unvoiced speech
usually has higher energy in high frequencies resulting in opposite spec-
tral tilts. The spectral tilt can be represented by the ﬁrst-order normalized
autocorrelation or ﬁrst reﬂection coefﬁcient.
St =
N

i=1
s(i)s(i −1)
N

i=1
s2(i)
(6.44)
This is a very reliable parameter especially for plosive detection and to avoid
individual spikes in low-level signals. As can be seen from Figure 6.25,
its ability to indicate unvoiced and voiced sounds in general is also very
accurate.
Figure 6.25
Speech waveform and its spectral tilt with a possible voicing threshold
of 0.25 (shown by the dashed line)

Voiced–Unvoiced Classification
183
Pre-emphasized Energy Ratio
Voiced and unvoiced speech can be discriminated by normalized pre-
emphasized energy.
Pr =
N

i=1
|s(i) −s(i −1)|
N

i=1
|s(i)|
(6.45)
The variance of the difference between adjacent samples is usually much
lower in voiced regions than in unvoiced regions. The ﬁrst-order correlation
of voiced samples is around 0.85 but that of unvoiced samples is nearly
zero, which is a clear indication of the voiced–unvoiced discriminatory
characteristic of this parameter. A speech waveform and its corresponding
normalized pre-emphasized energy is shown in Figure 6.26.
Figure 6.26
Speech waveform and its normalized pre-emphasized energy with a
possible voicing threshold of 0.9 (shown by the dashed line)

184
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Low-Band to Full-Band Energy Ratio
Voiced speech usually has a higher low-frequency energy than unvoiced
speech. Therefore the energy ratio of the ﬁrst 1 kHz to the full-band energy
can give a good indication whether the speech is voiced. When voiced, the
energy ratio is close to one and when unvoiced, since the low-band energy is
signiﬁcantly smaller, the ratio will be less than one.
LF =
N

i=1
s2
lpf (i)
N

i=1
s2(i)
(6.46)
where slpf (i) is low-pass ﬁltered speech at 1 kHz. A speech waveform and its
corresponding low-band to full-band energy ratio is shown in Figure 6.27.
Figure 6.27
Speech waveform and its low-band to full-band energy ratio with a
possible voicing threshold of 0.4 (shown by the dashed line)

Voiced–Unvoiced Classification
185
Frame Energy
Voiced speech usually has a higher energy than unvoiced speech. However,
the actual value of the energy in each frame also depends on the dynamic
range of the signal. Therefore a more useful measure is to have a comparison
of current frame energy with the tracked maximum and minimum energies.
The voiced speech should ideally be closer to the maximum track energy and
unvoiced speech should be closer to the minimum track energy (excluding
silences). The maximum track energy must go up quickly and come down
slowly and the minimum tracked energy must come down quickly and go
up slowly.
Emax(n) =
	 αEmax(n −1) + (1 −α)E0
;
if E0 > Emax(n −1)
γ Emax(n −1) + (1 −γ )E0
;
otherwise
(6.47)
where E0 is the current frame energy and Emax(n−1) is the previously tracked
maximum energy. Typically α = 0.5 and γ = 0.98 enables the maximum
energy to go up fast and come down slowly.
Emin(n) =
	 ζEmin(n −1) + (1 −ζ)E0
;
if E0 < Emin(n −1)
βEmin(n −1) + (1 −β)E0
;
otherwise
(6.48)
where Emin(n) and Emin(n−1) are the current and previously tracked minimum
energies. Typical values of ζ = 0.55 and β = 0.98 are selected so that the
minimum energy can come down fast and go up slowly. In addition to the
above tracked maximum and minimum energies, the average energy of the
speech signal may also be tracked by,
Eav(n) = 0.75Eav(n −1) + 0.25E0
(6.49)
The current frame energy, tracked average energy and tracked minimum
energy will be low in the unvoiced regions. In the voiced regions, on the other
hand, current frame energy will be close to the tracked maximum. A speech
waveform with its corresponding maximum, minimum and average tracked
energies, and the frame energy are shown in Figure 6.28. The following piece
of logic can be used to indicate voiced or unvoiced,
if((E0 + th1 > Emax)||(E0 > Eave))
Fe = voiced
else if(E0 < th2 + Emin)
Fe = unvoiced
else
Fe = notsure
(6.50)
where th1 and th2 are tuning tolerance thresholds.

186
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Figure 6.28
Speech waveform and (top) maximum, (middle) average, and (bot-
tom) minimum tracked energies, and the frame energy (shown by the dotted line);
energies have been shifted up
Decision-Making
Having computed the most useful voicing indicators, a combined decision
has to be made. The simplest decision-making rule is to use a majority
vote. A better decision rule could be to use a weighted combination of the
voicing indicators. Two types of weighting can be applied to produce a
combined decision. Different parameters have different degrees of reliability
in indicating the correct voicing and the weighting could be used to reﬂect
these variations in reliability. The weighting of parameters such as periodic
similarity and spectral tilt can be higher to reﬂect their greater reliability.
In addition, a second set of weightings can be used to reﬂect the difference
of each parameter from the optimum decision threshold. For example, the
variations of each parameter can be normalized to be ±1 around the optimum
threshold and these values can be used in a summation with the appropriate
weights which reﬂect the importance of the corresponding parameter. The

Voiced–Unvoiced Classification
187
normalized parameters are given by,
Ps′ =
	 (Ps −Thps)/(Psmax −Thps)
;
if Ps > Thps
(Ps −Thps)/(Thps −Psmin)
;
if Ps < Thps
(6.51)
Pk′ =
	 (Pk −Thpk)/(Pkmax −Thpk)
;
if Pk > Thpk
(Pk −Thpk)/(Thpk −Pkmin)
;
if Pk < Thpk
(6.52)
Zc′ =
	 (Thzc −Zc)/(Thzc −Zcmin)
;
if Zc < Thzc
(Thzc −Zc)/(Zcmax −Thzc)
;
if Zc > Thzc
(6.53)
St′ =
	
(St −Thst)/(Stmax −Thst)
;
if St > Thst
(St −Thst)/(Thst −Stmin)
;
if St < Thst
(6.54)
LF′ =
	 (LF −Thlf )/(LFmax −Thlf)
;
if LF > Thlf
(LF −Thlf )/(Thlf −LFmin)
;
if LF < Thlf
(6.55)
Pr′ =
	 (Thpr −Pr)/(Thpr −Prmin)
;
if Pr < Thpr
(Thpr −Pr)/(Prmax −Thpr)
;
if Pr > Thpr
(6.56)
Fe′ =



(E0 −Thv)/(Emax −Thv)
;
if voiced
(E0 −Thuv)/(Thuv −Emin)
;
if unvoiced
0
;
if not sure
(6.57)
where Thps, Thpk, Thzc, Thst, Thlf and Thpr are ﬁxed voicing thresholds for the
pitch similarity, peakiness, zero crossing, spectral tilt, low-band to full-band
energy ratio, and pre-emphasized energy ratio respectively, and Thv and Thuv
are adaptive voiced and unvoiced thresholds used to compare the frame
energy. The overall voicing indicator V is then computed by combining the
contributions of all indicators.
V = w1Ps′ + w2Pk′ + w3Zc′ + w4St′ + w5LF′ + w6Pr′ + w7Fe′
(6.58)
The weights w1, . . . , w7 are chosen according to the reliability of each indica-
tor. The sign of the voicing V will indicate voiced when positive and unvoiced
when negative. If V is close to zero it will indicate an unsure case, and the
voicing of the previous frame could be used to increase reliability. Further-
more, in cases where V = ±δ where δ has a small value (indicating an unsure
case), individual voicing parameters can be checked to see if one or more of
them has a clear indication of voiced or unvoiced. This can be achieved by
selecting two further thresholds for each parameter, one indicating voiced
and the other unvoiced. These thresholds must be selected by carrying out
long simulations. Typically Ps can be above 0.7 for voiced and below 0.3 for

188
Pitch Estimation and Voiced–Unvoiced Classification of Speech
Figure 6.29
Clean (top) and 10 dB SNR noisy (bottom) speech waveforms
unvoiced. The St can have values above 0.6 for voiced and below 0.2 for
unvoiced. Similarly Zc can have values below 40 and above 90 out of 160 for
voiced and unvoiced respectively.
The above hard-decision voicing method works very well with clean
background speech signals. However when speech is mixed with background
noise, the set thresholds may not be valid anymore. Hence a more careful
decision-making logic needs to be employed. Waveforms of original speech
and 10 dB SNR heavy vehicle noise are shown in Figure 6.29. As can be seen
from the ﬁgure, most unvoiced and some voiced sounds have been submerged
in the noise, making it very difﬁcult to see them. Under noisy conditions,
voicing parameters are expected to differ considerably. The variations of
three voicing parameters (spectrum tilt, pre-emphasized energy ratio and
pitch similarity) are shown in Figure 6.30.
When there is a transition from voiced to unvoiced or unvoiced to voiced,
even during clean speech conditions, a frame can be mistakenly declared
as voiced or unvoiced since both voiced and unvoiced exist together in
that frame. It is therefore necessary to reﬁne the voicing decision further
by introducing a mixed frame type in addition to completely voiced and
unvoiced frames. The all-important question is what proportion of the frame

Voiced–Unvoiced Classification
189
Figure 6.30
Reading from the top: St, Pr, and Ps voicing parameters (dotted for
noisy speech), and the original and noisy speech waveforms
will be voiced and unvoiced? This leads to an adaptive mixed-voicing decision
process which has been used in MBE, MELP, etc.
6.3.2 Soft-Decision Voicing
Although fully voiced and fully unvoiced frames can be identiﬁed in the
time domain by using the voicing parameters discussed above, in the case
of noisy speech this becomes more difﬁcult and more mistakes are made.
In order to avoid this problem and to deal with the mixed frames in one
process, a frequency-domain voicing-decision process is more appropriate.
The mixed voicing-decision process usually makes use of the harmonic and
random structures of voiced and unvoiced sounds in the frequency domain.
For example, in MBE-based coders, a synthetic spectrum (constructed by
using the measured pitch of the frame) tests the degree of match with the
original spectrum. Better-matched frequencies are declared voiced and the
rest are classiﬁed as unvoiced. In the case of MELP, the input frame is ﬁrst
split into subbands and the long-term correlation in each band is measured to
classify the band as voiced (high correlation) or unvoiced (low correlation).

190
Pitch Estimation and Voiced–Unvoiced Classification of Speech
MBE Mixed Voicing
The voicing decision is made by examining the normalized distance Dk
between the original and estimated speech spectra in frequency bands,
Dk =
bk

m=ak
|S(m) −ˆS(m, ω0)|2
bk

m=ak
|S(m)|2
(6.59)
where ω0 is the reﬁned fundamental frequency, ak and bk are the ﬁrst and last
harmonic in the kth band, S(m) is the original speech spectrum, and ˆS(m, ω0)
is the reconstructed speech spectrum which is calculated using:
ˆS(m, ω0) = Al(ω0)W(m)
1 ≤l ≤L,
⌈al⌉≤m < ⌈bl⌉
(6.60)
where al = (l −0.5)ω0, bl = (l + 0.5)ω0, ⌈.⌉means the nearest integer greater
than or equal to, L is the number of harmonics within the 4 kHz speech
bandwidth, W(m) is the frequency response of a suitable window centred at
the lth harmonic of the fundamental frequency (see Figure 6.31) and Al(ω0) is
−4.0
−2.0
0.0
2.0
4.0
Frequency (kHz)
−60.0
−40.0
−20.0
0.0
20.0
40.0
Magnitude (dB)
Figure 6.31
Frequency response of the Hamming window

Voiced–Unvoiced Classification
191
the lth harmonic amplitude which is computed using:
Al(ω0) =
⌈bl⌉

m=⌈al⌉
S(m)W(m)
⌈bl⌉

m=⌈al⌉
|W(m)|2
(6.61)
When creating the synthetic spectrum, it is very important to adjust the
position of W(m) and the size of the transform used, to make sure that
the peak of the window is centred on the harmonic and dies down to a
very small value at ±0.5 ω0 around each harmonic. As can be seen from
the formulation above, the synthetic spectrum is assumed to be all voiced.
However, the speech spectrum is not all voiced and, although the synthetic
spectrum will be very similar to the original spectrum in the voiced regions, it
will have larger differences in the unvoiced regions. Therefore, this similarity
(or dissimilarity) measure can be used to make a reasonably correct voicing
decision by comparing it against a pre-determined threshold. The value of
the threshold is set to give the proper mix of voiced and unvoiced energy.
Listening tests can be used to set the adaptive threshold function to values
where the ratio of voiced and unvoiced energy is perceptually optimum. To
determine the voicing decisions, the normalized error, Dk, for each frequency
band is compared with this adaptive threshold, k(ω0) given by [28]
k(ω0) = (α + βω0) [1.0 −ε(k −1)ω0] M(E0, Eav, Emin, Emax)
(6.62)
where α = 0.35, β = 0.557, and ε = 0.4775 are the factors that give good
subjective quality and,
M(E0, Eav, Emin, Emax) =



0.5
; Eav < 200
(E0 + Emin)(2E0 + Emax)
(E0 + µEmax)(E0 + Emax)
; Eav ≥200 and
Emin < µEmax
1.0
; otherwise
(6.63)
is the adaptation factor that controls the decision threshold for voicing
decisions, and µ = 0.0075. The parameters Eav, Emax, and Emin roughly
correspond to the local average energy, the local maximum energy and the
local minimum energy respectively. These three parameters are updated

192
Pitch Estimation and Voiced–Unvoiced Classification of Speech
−10.0
20.0
50.0
80.0
Magnitude (dB)
Local Ave. Energy
Average Energy
0.0
40.0
80.0
120.0
No of Frames
−10.0
20.0
50.0
80.0
Magnitude (dB)
Local Max. Energy
Local Ave. Energy
Local Min. Energy
(a)
(b)
Figure 6.32
The relationship between the energy levels used in determining voiced
and unvoiced speech
every speech frame according to [28],
Eav(n) = 0.7Eav(n −1) + 0.3E0
(6.64)
Emax(n) =
	0.5Emax(n −1) + 0.5E0
;
if E0 > Emax(n −1)
0.99Emax(n −1) + 0.01E0 ;
otherwise
(6.65)
Emin(n) =



0.5Emin(n −1) + 0.5E0
;
if E0 ≤Emin(n −1)
0.975Emin(n −1) + 0.025E0 ;
if Emin(n −1) ≤E0 < 2Emin(n −1)
1.025Emin(n −1)
;
otherwise
(6.66)
Relative variations of these energy levels are illustrated in Figure 6.32. The
voicing decision for each band is made by comparing the normalized error
for the band with the value of the threshold function which is computed
using the above procedure. If the normalized error is less than the threshold
function, the corresponding frequency band is declared voiced; otherwise,
the frequency band is declared unvoiced. The variations of the threshold and
the corresponding error function are shown in Figure 6.33.

Voiced–Unvoiced Classification
193
0.0
2.0
4.0
6.0
8.0
10.0
Decision Bands
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Error
Error Function
Threshold Function
Figure 6.33
The error and threshold functions for one frame
Split-Band Mixed Voicing
Correct estimation of the threshold level for each band is the most important
stage in MBE mixed-voicing estimation. The other important factor is that
more than one bit will be needed during the coding of the mixed-voicing
decision estimate. Since each band will require one bit, more bands will mean
higher accuracy but an increased bit rate. When closely examined however,
we see that if a spectrum contains an unvoiced band between two voiced
bands, the unvoiced signal in the middle is usually relatively small and if
it is declared as voiced, subjectively it would not make much difference.
This is very important because it saves bits when coding the mixed voicing-
decision. In this case a single point in the frequency spectrum can be used to
identify the voiced (low frequency) and unvoiced (high frequency) regions.
There may be several ways to obtain the single frequency marker or cut-off
point which separates the voiced and unvoiced parts. For example, using
MBE mixed-voicing, above, one can assume that the spectrum is voiced up
to the highest frequency voiced band. Alternatively, the total number of
voiced bands obtained in MBE mixed-voicing can be counted and used to
set the same number of low frequency bands to voiced. In this case, some
high-frequency voiced bands will be swapped with low-frequency unvoiced
bands. Although these methods may give good quality in the majority of

194
Pitch Estimation and Voiced–Unvoiced Classification of Speech
mixed-signal frames, they still rely on hard-decision voicing in individual
bands. A more reliable approach is to consider the actual voicing value or
voicing likelihood in each band. This can be measured by the degree of
harmonic structure in each frequency band. If a harmonic band is voiced,
then its content will have a shape similar to the spectral shape of the window
used prior to the Fourier transform, while unvoiced bands will be random in
nature. Hence the level of voicing in a frequency band can be measured by
the normalized correlation between the content of a frequency band and the
spectral shape of the window positioned on each harmonic bin.
V(l) =
 ω0

m=l
S(m −lω0 + 0.5 ω0)W(m)
2
ω0

m=l
W2(m)
ω0

m=l
S2(m −lω0 + 0.5 ω0)
(6.67)
where S(m) is speech spectrum and W(m) is the Fourier transform of the
analysis window. W(m) is usually up-sampled by computing its Fourier
transform using a larger transform size (compared to S(m)) and then down-
sampled with respect to the fundamental frequency, so as to have the same
number of points within each harmonic region of S(m). The voicing V(l)
has a value between 0.0 and 1.0, which indicate fully unvoiced and voiced
respectively. Similar to MBE mixed-voicing, V(l) is compared against a
threshold in each band. Since the voicing probability varies in each band as
well as in each frame, the threshold value needs to be adaptive. This threshold
can be computed by combining the voicing indicators, such as pitch similarity,
zero crossing, peakiness, low-band to high-band energy ratio, E0/Emax, etc.
Having computed V(l) and the threshold T(l) for each band, we need to make
a decision to choose the best cut-off frequency. Since this cut-off point will be
quantized before transmission, it is more convenient to test each quantizer
level against a measure so that the selected cut-off frequency is also quantized.
For each quantizer value i, a matching measure M(i) can be computed as
given below:
M(i) =
L

l=1
(V(l) −T(l)vi(l)E(l)B(l)
(6.68)
This takes into account the energy of each harmonic, E(l), and a biasing,
B(l), which represents the perceptual weighting. For a given quantizer level
i, individual voicings vi(l) will have values of +1 up to the cut-off i and
−1 for the higher harmonics. The weighting, B(l), is usually set to 1.0 when
unvoiced (T(l) > V(l)) and higher for voiced. The above voicing process takes

Voiced–Unvoiced Classification
195
0
1000
2000
3000
4000
Frequency  (Hz)
−0.5
0.0
0.5
1.0
Voicing likelihood and threshold 
Voicing likelihood
Threshold function
Speech spectrum
Figure 6.34
Original speech spectrum with voicing likelihood and threshold func-
tion; the voicing cut-off frequency is indicated by the vertical dashed line
into account the difference between V(l) (the voicing likelihood) and the
threshold T(l), which replaces the hard decision used in MBE mixed-voicing
with a soft decision in each band. An example of a voicing likelihood and
threshold function is shown in Figure 6.34.
It is also possible to use this weighted-sum approach on the voicing measure
used in MBE. However, the MBE approach requires the computation and
generation of a synthetic spectrum, as described above. This is not required for
the voicing likelihood method discussed here. However, as for the MBE and
MELP voicing-decision algorithms, the most important stage during split-
band voicing estimation is the calculation of the threshold function. Using
a limited number of speech characteristics for the threshold computation
does not lead to good voicing determination. For example, the energy alone
is not a reliable enough voicing indicator, since there can be high-energy
unvoiced speech segments and low-level voiced speech. The peakiness factor
is not entirely reliable either: single spikes can lead to high peakiness, but
they should be declared as unvoiced for optimal speech quality. Likewise,
the periodic similarity measure has its limits: when the pitch varies, the

196
Pitch Estimation and Voiced–Unvoiced Classification of Speech
normalized autocorrelation may be quite low whereas the speech is clearly
voiced. It is therefore necessary to make full use of the speech characteristics
described above to generate a good threshold function. In split-band voicing,
the threshold function is generated as follows [29]:
1. An initial linear threshold function is generated which starts at 0.4 and
goes up to 0.55. The value of the threshold is increased for harmonics
which correspond to the unvoiced harmonics in the previous frame. If the
previous frame is completely unvoiced the threshold increases to 0.55–0.65
(increasing the chance of an unvoiced decision in the current frame).
2. The voicing-threshold function is biased using the following individual
parameters:
• Low- to full-band energy ratio
• Pre-emphasis energy ratio
• Zero-crossing rate
• Frame energy
These parameters have their high and low thresholds set and, if they are
triggered, the voicing threshold function is biased towards either voiced
or unvoiced.
3. The voicing-threshold function is biased using the pitch value. A high
number of harmonics present in the speech implies that the harmonic
bands are narrow and contain a small number of frequency bins. As
a result, the voicing likelihood tends to increase, as the matching is
performed on fewer points. The voicing threshold function needs to be
biased to compensate for this effect.
4. Finally, very speciﬁc cases detected in individual speech characteristics
are used to bias the threshold. For example, very high periodic similarity
is used to increase the voiced likelihood and very high zero-crossing rate
(in clean conditions) is used to increase the unvoiced likelihood.
This voicing determination method provides very robust detection accuracy,
even under signiﬁcant background noise conditions.
6.4 Summary
Developments in the ﬁeld of fast DSP technology have allowed the use of more
and more sophisticated algorithms required for accurate pitch estimation and
voiced–unvoiced classiﬁcation. With the new multi-domain (frequency and
time) pitch estimation, it is possible to get good performance even under noisy
conditions. However, even the latest and most complex pitch estimation algo-
rithms are not perfect. In some speech segments, the pitch is not well-deﬁned

Bibliography
197
and some errors are inevitable. Overall performance of the pitch-estimation
algorithms, however, can be considered to be pretty good. Voiced–unvoiced
classiﬁcation, on the other hand, has moved from a single (binary) indicator,
where each block of speech was classiﬁed either as voiced or unvoiced, to
more elaborate frequency-domain mixed decisions. This has increased the
quality of synthetic speech dramatically. The performance of voicing estima-
tion under noisy conditions has also been improved with developments in
mixed-voicing classiﬁcation.
Bibliography
[1] L. R. Rabiner, M. J. Cheng, A. E. Rosenberg, and C. A. McGonegal (1976)
‘A comparative performance study of several pitch detection algorithms’,
in IEEE Trans. on Acoust., Speech and Signal Processing, 24(5):399–418.
[2] W. J. Hess (1992) ‘Pitch and voicing determination’, in Advances in Speech
Signal Processing by S. Furui and M. M. Sondhi (Eds), pp. 3–48. New
York: Marcel Dekker Inc.
[3] L. Rabiner (1977) ‘On the use of autocorrelation analysis for pitch detec-
tion’, in IEEE Trans. on Acoust., Speech and Signal Processing, 25(1):24–33.
[4] M J. Ross, H. L. Shaffer, A. Cohen, R. Freudberg, and H. J. Manley (1974)
‘Average magnitude difference function pitch extractor’, in IEEE Trans.
on Acoust., Speech and Signal Processing, 22(5):353–62.
[5] C. K. Un and S.-H. Yang (1977) ‘A pitch extraction algorithm based on
LPC inverse ﬁltering and AMDF’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 25(6):565–72.
[6] ITU-T (1996) Dual rate speech coder for multimedia communications trans-
mitting at 5.3 and 6.3 kbit/s, ITU-T Rec. G.723.1.
[7] ITU-T (1996) Coding of speech at 8 kbit/s using conjugate-structure algebraic-
code-excited linear prediction (CS-ACELP), ITU-T Rec. G.729.
[8] ETSI (1997) Digital cellular telecommunications system (phase 2+); Half rate
speech; Half rate speech transcoding, GSM 06.20 v5.1.0 (draft ETSI ETS 300
969).
[9] ETSI (1998) Digital cellular telecommunications system (phase 2); Enhanced
full rate (EFR) speech transcoding, GSM 06.60 v4.1.0 (ETS 301 245), June.
[10] ETSI (1998) Digital cellular telecommunications system (phase 2+); Adaptive
multi-rate (AMR) speech transcoding, GSM 06.90 v7.2.0 (draft ETSI EN 301
704).
[11] FIPS (1997) Analog to digital conversion of voice by 2,400 bit/second mixed
excitation linear prediction (MELP), Draft. Federal Information Processing
Standards
[12] A. M. Noll (1967) ‘Cepstrum pitch determination’, in Journal of the Acoustic
Soc. of America, 41:293–309.

198
Pitch Estimation and Voiced–Unvoiced Classification of Speech
[13] D. Grifﬁn and J. S. Lim (1988) ‘Multiband excitation vocoder’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 36(8):1223–35.
[14] R. J. McAulay and T. F. Quatieri (1995) ‘Sinusoidal coding’, in Speech
coding and synthesis by W. B. Kleijn and K. K. Paliwal (Eds), pp. 121–74.
Amsterdam: Elsevier Science
[15] L. P. Nguyen and S. Imai (1977) ‘Vocal pitch detection using generalized
distance function associated with a voiced-unvoiced decision logic’, in
Bull, P.M.E. (T.I.T), 39:11–12.
[16] W. Hess (1983) Pitch Determination of Speech Signals: algorithms and devices.
Berlin: Springer-Verlag
[17] I. Atkinson (1997) ‘Advanced linear predictive speech compression at
3.0 kbit/s and below’, Ph.D. thesis, CCSR, University of Surrey, UK.
[18] D. B. Paul (1981) ‘The spectral envelope estimation vocoder’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 29(4):786–94.
[19] Y. D. Cho (2001) ‘Speech detection enhancement and compression for
voice communications’, Ph.D. thesis, CCSR, University of Surrey, UK.
[20] I. Atkinson, S. Yeldener, and A. Kondoz (1997) ‘High quality split-band
LPC vocoder operating at low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 1559–62. May 1997. Munich
[21] ITU-T (1996) Software tool library.
[22] M. M. Sondhi (1968) ‘New methods of pitch extraction’, in IEEE Trans.
on Audio and Electroacoustics, 16:262–6, June.
[23] J. D. Markel (1972) ‘The SIFT algorithm for fundamental frequency
estimation’, in IEEE Trans. Audio and Electroacoustics, 20:367–77.
[24] M. Schroeder and B. Atal (1979) ‘Predictive coding of speech signals
and subjective error criteria’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 27:247–54.
[25] J. J. Dubnowski, R. W. Schafer, and L. R. Rabiner (1976) ‘Real-time digital
hardware pitch detector’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 24:2–8.
[26] J. Sundberg (1979) ‘Maximum speed of pitch changes in singers and
untrained subjects’, in Journal of Phonetics, 7:71–9.
[27] W. Ma, A. M. Kondoz, and B. G. Evans (1992) The Real-Time Implementa-
tion of INMARSAT Standard-M Codec.
[28] DVSI (1991) INMARSAT M Voice Codec, Version 1.3. Digital Voice Sys-
tems Inc.
[29] S. Villette (2001) ‘Sinusoidal speech coding for low and very low bit rate
applications’, Ph.D. thesis, University of Surrey, UK.

7
Analysis by Synthesis
LPC Coding
7.1 Introduction
The broad classiﬁcation of speech coding techniques that attempt to reproduce
the original speech waveform as best as possible can be split into two
basic groups, namely analysis-and-synthesis (AaS) schemes and analysis-by-
synthesis (AbS) schemes. Although AaS schemes, such as APC [1, 2], ATC [3]
and SBC [4], have been successful at rates around 16 kb/s and above, below
16 kb/s they can no longer reproduce good quality speech. In addition, AaS
coders that have been used at bit rates of around 9.6–16 kb/s can not achieve
true toll quality performance (MOS≥4). There are two main reasons for their
shortcomings: ﬁrst, the coded speech is not analysed to see if the coding
procedure is operating efﬁciently, i.e. there is no check on or control over the
distortions of the reconstructed speech; and secondly, in adaptive schemes,
the errors accumulated from previous frames are not usually considered in
the current frame of analysis, hence the errors propagate into the following
frames without any form of resetting. In AbS schemes, particularly AbS-LPC
schemes [5, 6], these two factors are incorporated in the coding process.
In AbS-LPC coding systems, a closed-loop optimization procedure is used
to determine the excitation signal, which produces a perceptually optimum
synthesized speech signal when used to excite the model ﬁlter. It is this
closed-loop approach which enables AbS-LPC coding schemes to be far more
successful at 4.8 to 16 kb/s than conventional AaS schemes such as APC and
SBC.
The method of AbS is not unique to speech coding, but is a general
technique used in other areas of estimation and identiﬁcation. The basic idea
behind AbS is as follows. First it is assumed that the signal can be observed
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

200
Analysis by Synthesis LPC Coding
−
+
^
Error
Signal
e(n)
s(n)
s(n)
Observed
Signal
Improve Model
Parameters
+
System
Model
Figure 7.1
General block diagram of analysis-by-synthesis closed-loop analysis
and represented in some form, e.g. the time or frequency domain. Then a
theoretical form of the signal production model is assumed, as depicted in
Figure 7.1. The model has a number of parameters which can be varied to
produce different variations of the observable signal. In order to derive a
representation of the model that is of the same form as the true signal model,
a trial and error procedure can be applied. By varying the parameters of
the model in a systematic way, it is possible to ﬁnd a set of parameters that
can produce a synthetic signal which matches the real signal with minimum
possible error (assuming the model is valid to begin with). Therefore, when
such a match is calculated, the parameters of the model are assumed to be the
parameters of the true signal.
The AbS procedure outlined above was applied to speech processing in the
earlier days of formant estimation [7] but, because of its obvious complexity, it
was not re-applied until Atal outlined the basis of Multi-pulse LPC (MPLPC)
in [8] for low bit-rate coding. In Atal’s work, the time-domain representation
of speech was used and a model very similar to the conventional source-ﬁlter
model was selected. However, AbS with other domains and models are
equally applicable [9]. In the following sections a uniﬁed presentation of the
various AbS-LPC schemes using Atal’s modelling is described.
7.2 Generalized AbS Coding
The basic structure of an AbS-LPC coding system is illustrated in Figure 7.2.
There are basically three blocks in the model that can be varied to match our
true model and, hence, obtain a good synthesized speech signal: time-varying
ﬁlter, excitation signal and perceptually-based error minimization procedure. As our

Generalized AbS Coding
201
Encoder
Original
Speech
Pitch
Synthesis
Filter 
Optimum
Excitation
Output
Speech
Decoder
s(n)
LPC
Synthesis
Filter 
LPC
Synthesis
Filter 
s(n)
+
−
+
^
s(n)
^
Error
Minimization
Error
e(n)
Excitation
Generator
Pitch
Synthesis
Filter
Figure 7.2
Block diagram of AbS-LPC coding scheme
model requires frequent updating of the parameters to yield a good match
with the original signal, the analysis procedure of the system is carried out
in blocks, i.e. the input speech is partitioned into suitable blocks of samples.
The update rate of the analysis block or frame determines the bit rate or
capacity of the coding schemes. The basic operation of an AbS-LPC scheme
is as follows:
1. Initialize the contents of the time-varying ﬁlter (LPC and pitch) to pre-
determined values (usually zero or low level random noise).
2. A frame of speech samples is buffered and a set of LPC coefﬁcients are
computed, using LPC analysis on the frame.
3. As the LPC analysis frame is usually too large for efﬁcient analysis to
determine the excitation, the frame is subdivided into a subframes.
4. For each subframe:
(a) Using the computed LPC coefﬁcients (usually interpolated for each
subframe) in the LPC ﬁlter, its memory effect (ﬁlter-ringing response)
is computed and subtracted from the original signal, which is usually
perceptually weighted.
(b) The pitch ﬁlter delay (the pitch or its integer multiples) and its associ-
ated scaling factor (pitch gain) are then calculated. This calculation is
performed in such a way that the difference between the synthetically-
generated speech and the remaining original signal is minimized.
(c) Once the pitch ﬁlter parameters are found, the pitch and LPC synthesis
ﬁlter can be grouped together to form a cascaded ﬁlter. Using this
cascaded ﬁlter, the best secondary excitation is determined in such a

202
Analysis by Synthesis LPC Coding
way as to minimize the difference between the synthetically-generated
speech and the original speech.
5. The ﬁnal synthetic speech is generated by passing the optimum secondary
excitation through the cascaded ﬁlter with all the initial memory contents
of the ﬁlters (left over from the previous subframe synthesis) restored.
6. Repeat steps 2 to 5 for subsequent frames.
Note that the synthetic speech is generated at both the encoder and decoder.
This is necessary in order to update the memory contents of the time-varying
ﬁlters such that both encoder and decoder possess replica conditions in
their ﬁlter memories. In fact, a major concern of AbS-LPC schemes is how
to preserve this identical condition at both encoder and decoder when the
transmitting medium is imperfect, e.g. in mobile radio links where the error
rates can be very high.
It can be observed from the above descriptions that the AbS-LPC scheme
is not truly analysis-by-synthesis. This is because the procedure is actually
sequential in nature, i.e. the LPC ﬁlter parameters are calculated and ﬁxed,
then the pitch ﬁlter parameters are calculated, followed by the computation of
the secondary excitation. Consequently, although the secondary excitation is
obtained optimally with respect to the original reference signal, its optimality
is limited by the optimality of the ﬁlters it uses. The best combination of
the excitation and the ﬁlters is desired, which means optimizing all the
parameters in parallel. Obviously, this joint procedure is very complicated as
well as being very computationally intensive, thus it is split into the sequential
stages described above.
It is interesting to note that this model is very similar to that of the
classical source-ﬁlter vocoders [7]. However, there is one major difference
between basic vocoders and AbS-LPC coders. In classical vocoders, the source
excitation is classiﬁed into voiced (pulse excitation) and unvoiced (random
noise excitation), which is a major source of model inaccuracy. However, in
AbS-LPC, this categorization is not explicit and therefore the excitation signal
can be anything from pulse-like to noise-like in characteristic, thus enabling
much better quality speech to be synthesized.
7.2.1 Time-Varying Filters
The block representing the time-varying ﬁlter in our model is usually made of
two linear predictors, namely the LPC or short-term predictor (STP) and the
pitch or long-term predictor (LTP). The LPC models the short-term correlation
in the speech signal (the spectral envelope) and is given by,
1
A(z) =
1
1 −
p

i=1
aiz−i
(7.1)

Generalized AbS Coding
203
where ai are the LPC coefﬁcients and p is the ﬁlter order. It is made time-
varying to reﬂect the change in the speech spectrum with adaptation rates of
typically around 20–30 ms. The order of the ﬁlter, p, is usually chosen to be
around 8 to 12.
The pitch ﬁlter models the long-term correlation in speech (the ﬁne spectral
structure) and is given by,
1
P(z) =
1
1 −
I

i=−I
biz−(D+i)
(7.2)
where D is a pointer to long-term correlation which usually corresponds
to the pitch period or its multiples and bi are the pitch (or LTP) gain
coefﬁcients. Again, this is a time-varying ﬁlter but it usually has higher
adaptation rates than the LPC, e.g. 5–10 ms. The number of ﬁlter taps typ-
ically takes the form I = 0, i.e. 1 tap, and I = 1, i.e. 3 taps. Note that
because of the recursive nature of the two ﬁlters, both contain memory in
their working buffers carried over from the previous frame of analysis.
The preservation and inclusion of this ﬁlter memory in the AbS analysis is
very important as it reﬂects the past history of the analysis, and includes
any errors incurred in the previous frames. Also, it provides a smooth-
ing effect to the distortions caused by the block-oriented analysis, such as
edge effects.
7.2.2 Perceptually-based Minimization Procedure
The AbS-LPC coder of Figure 7.2 minimizes the error between the orig-
inal s(n) and the synthesized signal ˆs(n) according to a suitable error
criterion, by varying the excitation signal and the LPC and pitch ﬁl-
ters. As described earlier, this is achieved via a sequential procedure.
First the time-varying ﬁlter parameters are determined, then the excitation
is optimized.
The optimization criterion used for both procedures is the commonly
used mean squared error, which offers simplicity and adequate performance.
However, at low bit-rates there is one or fewer bit per sample coding capacity,
thus it is more difﬁcult to match the waveform closely than in, say, higher than
16 kb/s schemes, where more than 1 bit/sample is available. Consequently,
the mean squared error between the original and reconstructed signal is less
meaningful and less than adequate. What is required is an error criterion
which is more in sympathy with human perception. Although much work
on auditory perception is in progress, no satisfactory error criterion has yet
emerged. In the meantime, however, a popular but not totally satisfactory

204
Analysis by Synthesis LPC Coding
method is the use of a weighting ﬁlter in AbS-LPC schemes. This weighting
ﬁlter is given by,
W(z) =
A(z)
A(z/γ )
(7.3)
=
1 −
p

i=1
aiz−i
1 −
p

i=1
aiγ iz−i
,
0 ≤γ ≤1
This weighting ﬁlter is the same as that proposed by Atal [2] for APC
schemes and a typical plot of its frequency response is shown in Figure 7.3.
The effect of the factor γ does not alter the centre formant frequencies but
just broadens the bandwidth of the formants by f given by,
f = −fs
π ln γ
(Hz)
(7.4)
0.0
2.0
3.0
4.0
1.0
Frequency (kHz)
-45.0
-35.0
-25.0
-15.0
-5.0
5.0
15.0
Magnitude (dB)
γ = 0.98
γ = 0.95
γ = 0.90
γ = 0.80
γ = 0.70
γ = 0.50
Original Envelope
Figure 7.3
Typical plots of weighting filter spectra compared with the original
speech envelope

Generalized AbS Coding
205
sw(n)
^
−
+
+
s(n)
s(n)
W(z)
1/P(z)
e(n)
Excitation
Generator
Error
Minimization
1/Aw(z)
Figure 7.4
Modified AbS-LPC encoder with the weighting filter moved to the two
branches of the error minimization procedure
where fs is the sampling frequency in Hz. As can be observed from Figure 7.3,
the weighting ﬁlter de-emphasizes the frequency regions corresponding to the
formants as determined by the LPC analysis. By allocating larger distortion
in the formant regions, noise that is more subjectively disturbing in the
formant nulls can be reduced. The amount of de-emphasis is controlled by
γ which introduces a broadening effect and must lie between 0 and 1. The
most suitable value of γ is selected subjectively by listening tests; for 8 kHz
sampling, γ is usually around 0.8 to 0.9.
Although the weighting ﬁlter can be used as it is in its normal position (after
subtraction of ˆs(n) from s(n)), it can also be modiﬁed in a computationally-
advantageous way by moving it to the two branches contributing to the
subtraction operation, as illustrated in Figure 7.4. This results in a block of
the input samples being weighted only once prior to the AbS search. At the
same time W(z) is combined with the LPC ﬁlter to form a modiﬁed all-pole
synthesis ﬁlter.
1
Aw(z) =
1
A(z).W(z)
(7.5)
=
1
1 −
p

i=1
aiγ iz−i

206
Analysis by Synthesis LPC Coding
Note that, in the latest CELP coders, the above weighting ﬁlter has been
slightly changed by modifying its zeros (coefﬁcients in the numerator) as well
as its poles.
W(z) = A(z/β)
A(z/γ )
(7.6)
=
1 −
p

i=1
aiβiz−i
1 −
p

i=1
aiγ iz−i
0 ≤γ ≤β ≤1
If this structure is to be used then in Figure 7.4, 1/Aw(z) should contain both
the above weighting ﬁlter and the usual LPC synthesis ﬁlter.
7.2.3 Excitation Signal
The excitation signal represents the input to the AbS-LPC model and is
therefore an important block of the model shown in Figure 7.2. It provides
any residual structures that are not represented by the spectral model of
the time-varying ﬁlters, including pitch or long-term dependent structures
that exhibit signiﬁcant correlation which is not covered by the pitch ﬁlter
and random structures that cannot be modelled efﬁciently by deterministic
methods. A proper excitation model is vital to the pitch-ﬁltering efﬁciency
as the pitch ﬁlter memory is built up of its scaled versions. Therefore, the
excitation is usually represented by a shape vector with its associated gain or
scale factor. The various shapes that have been reported include multi-pulse,
regular-pulse, codebook, etc. Some mixtures of the above excitation schemes
have also been used.
Codebook Excitation
In codebook excitation (CELP) [10], the excitation vector is chosen from
a stored collection of C possible unity variance stochastic sequences with
an associated scaling or gain factor. In the AbS procedure, the C possible
sequences are systematically passed through the combined synthesis ﬁlter
(pitch, LPC and perceptual ﬁlters); the vector that produces the lowest error
is the desired sequence and is then scaled by its gain. Since the set of
sequences are present at both the encoder and the decoder, only an index to
the codebook and the gain level are required to be transmitted. Therefore,
less than 1 bit/sample coding is possible.
As the codebook is of ﬁnite dimension, it must be populated with represen-
tative vectors of the excitation. In Atal’s original proposal, unit-variance white

Generalized AbS Coding
207
Gaussian random numbers were used. This choice of population was reported
to give very good results, partly due to the fact that the probability-density
function of the prediction error samples, produced by inverse ﬁltering the
speech through both the LPC and pitch ﬁlters, is very close to having a Gaus-
sian shape. Another popular choice of codebook entries are centre-clipped
Gaussian vectors, which reduce complexity and improve performance. How-
ever later developments in the codebook design such as VSELP and, more
signiﬁcantly, ACELP structures reduce the codebook storage and search
complexity as well as improving the resultant speech quality.
Self-Excitation
Self-excitation (SELP) [11] can use the excitation signals derived from the past
history of the coded excitation function itself, using a structure similar to a
pitch ﬁlter in the form more than one long-term predictor (LTP). The self-
excitation LTP is ‘started’ by initially ﬁlling its memory with some random
contents. Then at each analysis-by-synthesis procedure, a sequence equal to
the block length L, indexed in time by k, is selected and passed through a
combined synthesis ﬁlter. The best vector with index kopt is the vector which
minimizes the difference error. When the best vector is found and used to
synthesize the current block, it is fed back into the LTP with the oldest L
samples discarded. The self-excitation LTP is effectively a CELP coder with
an adaptive codebook. However, in SELP, the C possible sequences are not
codebook entries but a windowed version of optimum excitations.
Multi-Pulse and Regular Pulse Excitation
Multi-pulse LPC (MPLPC) [8, 12] was the ﬁrst of the AbS-LPC coding schemes.
In MPLPC, the rigorous division of the excitation into voiced and unvoiced
classes is avoided by making no prior assumptions about the nature of the
excitation signal. In MPLPC, rather than selecting an optimum sequence from
a codebook as in CELP, the excitation is speciﬁed by a small set of pulses with
different amplitudes located at nonuniformly-spaced intervals. The encoding
involves the determination of the pulse positions and the amplitudes of the
excitation which produce the minimum error. The only prior information
required is the number of pulses per analysis block (or an error threshold). A
typical pulse rate for good quality speech synthesis is around one pulse per
4–8 samples.
The MPLPC can be viewed as a CELP system with a very large codebook,
the size of which is determined by the number of pulses and the number of
bits used to quantize the pulse amplitudes and locations.
The MPLPC makes no restriction on the spacing or spread of the pulses
except that the number of pulses must be ﬁxed in advance. This arrangement

208
Analysis by Synthesis LPC Coding
n = L − 1
n = 0
k = 4
k = 3
k = 2
k = 1
L = 40, N = 4, M = 10
Phase
(grid position)
Figure 7.5
Typical make-up of an RPELPC pulse-positioning structure
obviously requires a large number of bits to encode the pulse positions.
Therefore, a more structured allocation of the pulses would be more desirable
both in terms of bit savings and complexity in determining the optimum
positions. A sparse codebook CELP is effectively an MPLPC with severe
restrictions on both the positions and amplitudes of the pulses. This is
obviously a very drastic compromise, but if structure is only imposed on
the pulse-positioning then the amplitudes can vary. This is the structure
of the regular pulse excited LPC (RPELPC) [13] shown in Figure 7.5. In an
RPELPC excitation frame, the pulses are equally spaced, with spacing N, and
their positions are speciﬁed completely by the position of the ﬁrst pulse. In
Figure 7.5, the excitation frame size L is 40 and N is 4. The total number of
pulses per frame is M = L/N. Thus, in RPELPC, the N-sequences are passed
through combined synthesis ﬁlters and the sequence which minimizes the
error is chosen as the best sequence.
7.2.4 Determination of Optimum Excitation Sequence
In the previous section, the different forms of excitation signals were described
without any detailed description of their determination. While each of the
excitation techniques best models different types of structure that may exist
in the residual, the formulation for determining the optimum excitation
sequence for each is the same [6, 14]. The only difference is the function space
from which an optimum excitation can be chosen. In CELP, a sample space of

Generalized AbS Coding
209
random functions corresponding to the L-point Gaussian random sequences
contained in the codebook is searched. In MPLPC and RPELPC, the search
is in time, but through a set of delayed impulse response functions. For a
given technique, the criterion for ﬁnding the optimum excitation function is
the same. The objective is to determine the shape matrix X and the associated
gain g (assuming MPLPC and RPELPC have normalized shape vectors) so
that gX produces a synthetic signal that minimizes the weighted error e(n)
shown in Figure 7.4, i.e.
ek = sw −ˆsk
(7.7)
where sw is the weighted original reference signal, ˆsk is the synthesized signal
(with pitch, LPC and perceptual-weighting ﬁlter contributions), and k denotes
the particular excitation.
Let H be an L × L matrix whose jth row contains the (truncated) combined
impulse response h(n) of the pitch, LPC and perceptual weighting ﬁlters
caused by a unit impulse δ(n −j), i.e.
H =


h(0)
h(1)
· · ·
h(L −1)
0
h(0)
· · ·
h(L −2)
...
...
...
...
0
0
· · ·
h(0)


(7.8)
If sm denotes the output of the cascaded ﬁlters with zero input, i.e. the memory
hangover from previously synthesized frames, then the reference signal ˜s to
be matched can be described as,
˜s = sw −sm
(7.9)
⇒ek = ˜s −gkXkH
(7.10)
= ˜s −gkˆsk
(7.11)
where,
ˆsk = XkH
(7.12)
and Xk and gk are the kth excitation shape and gain vectors. The criterion is
minimum-squared error, thus our objective is to minimize Ek where,
Ek = ekeT
k
(7.13)
and T denotes transpose. The optimum amplitude vector gk for the kth
candidate excitation can be computed from equations (7.11) and (7.13) by
requiring the error ek to be orthogonal to our estimation ˆsk, i.e.
ekˆsT
k = 0
(7.14)

210
Analysis by Synthesis LPC Coding
Therefore,
(˜s −gkˆsk).ˆsT
k = 0
(7.15)
⇒gk = ˜sˆsT
k [ˆskˆsT
k ]−1
(7.16)
By substituting equation (7.16) into equation (7.11), equation (7.13) can be
rewritten as
Ek = ˜s[I −ˆsT
k [ˆskˆsT
k ]−1ˆsk]˜sT
(7.17)
where I is the identity matrix. The vector gk and matrix Xk that yield the
minimum value of Ek over all k are then selected as the optimum excitation.
The above expression for Ek is generalized for all the possible forms
of excitations and is, therefore, rather more complicated than required in
practical cases. The [ˆskˆsT
k ]−1 inversion, for instance, is unnecessary in most
cases, as illustrated below using codebook excitation.
ˆskˆsT
k = σ
(scalar)
(7.18)
gk = ˜sˆsT
k
σ
= gk
(scalar)
(7.19)
⇒Ek = ˜s˜sT −gkˆsk˜sT
(7.20)
= ˜s˜sT −Qk
(7.21)
Rewriting equations (7.19) and (7.20) in time-domain samples form,
gk =
L−1

i=0
˜s(i)ˆsk(i)
L−1

i=0
ˆs2
k(i)
(7.22)
Ek =
L−1

i=0
˜s2(i) −gk
L−1

i=0
ˆsk(i)˜s(i)
(7.23)
and, substituting gk into equation (7.23), we can rewrite equation (7.21) as,
Ek =
L−1

i=0
˜s2(i) −
L−1

i=0
ˆsk(i)˜s(i)
	2
L−1

i=0
ˆs2
k(i)
(7.24)

Generalized AbS Coding
211
and Qk is given by,
Qk =
L−1

i=0
ˆsk(i)˜s(i)
	2
L−1

i=0
ˆs2
k(i)
(7.25)
The scalar factor gk is simply the cross-correlation of the weighted speech
with the synthesized excitation response divided by the squared sum of
the synthesized excitation response. The squared error Ek is the difference
between the energy of the weighted speech and Qk. In practice, we ﬁnd the
maximum of Qk to select the best excitation.
In MPLPC, the above procedure to determine the excitation shape is not
practical as it would involve searching for all possible combinations of pulse
location, e.g. for L = 40 and M = 4, the number of pulse position vectors is
91 390. Therefore to simplify computation, suboptimal strategies are usually
used. A simple and popular method involves locating one pulse at a time. The
optimum location for any of these pulses is found by computing the error for
all possible pulse locations in a given interval L and locating the minimum
error location. Once that location is known, the contribution of the pulse at
position k is subtracted from the reference signal (similarly to equation (7.9)),
and the procedure is repeated for the next pulse until all pulses are found.
This is summarized below:
Let ˜s0 = sw −sm, where sm is the combined ﬁlter memory response, and Hk
be the combined impulse response scaled by the kth pulse.
For i = 1, . . . , M,
1. Find Ek
i = min{˜si ˜sT
i −gkHk˜sT
i }, for k = 1, . . . , L
2. ˜si+1 = ˜si −gi,optHi
Finally, ˆsmp =
M

i=1
gi,optHi
This pulse-at-a-time procedure is obviously suboptimal and procedures
which try to add more optimality have been extensively reported. For
example, a popular post-processing method is to re-optimize the pulse
amplitudes after the pulse positions are found by performing an M by M
matrix inversion. Most of these involve substantially more computations
with some reported improvements especially if M is large compared with
the size of the analysis block L. In RPELPC, the positions of the pulses are

212
Analysis by Synthesis LPC Coding
ﬁxed, therefore there is no requirement to locate one pulse at a time. Thus
in RPELPC, the amplitude vector is jointly optimized in one step, repeated
N times (for the number of possible amplitude sequences), and the vector
that minimizes the mean squared error is chosen, also identifying the grid
position (phase position 0 ≤k < N).
7.2.5 Characteristics of AbS-LPC Schemes
Before investigating AbS-LPC schemes in more detail in the form of code-
excited linear prediction (CELP), it is worthwhile to highlight some of
the similarities and differences in the way the different AbS-LPC schemes
operate. These mainly lie in the characteristics of the excitation behaviour as
described below.
CELP
In the CELP system, the objective is to select from the codebook the vector
which best matches the original reference vector. Typical plots of consecutive
vector searches are shown in Figures 7.6 and 7.7, where the output error (in
fact Qk for each codebook vector) is plotted. In Figure 7.6, the best matching
vector (v1) from the codebook is fairly distinct from the remaining vectors.
However, in Figure 7.7, we ﬁnd that the error is less distinctive, as illustrated
by the similarity in the ﬁrst, second, and third best matching vectors. The
mean squared error is clearly inadequate as a selection criterion in AbS-
LPC coding schemes (indeed, in speech coding in general): the selection
of an optimum candidate is by no means readily controllable. Although
the selection of v1 in Figure 7.6 is probably correct, even subjectively, the
selection of v1 in Figure 7.7 is not as clear-cut. What subjective difference
would result if, say, the second-best vector was selected instead? This test
was performed and, not surprisingly, the quality of the processed speech
was not noticeably degraded. This prompts the question as to whether
or not the codebook vectors can be better optimized such that there is a
clearer distinction, both objectively and subjectively, between the best and
the second-best vectors. Unfortunately, trained codebooks (whether multi-
pulse characteristic codebooks, glottal-pulse codebooks, or other types) have
been largely unsuccessful in this respect.
SELP
In SELP, the best excitation is generated from previous excitations. Here, we
assumed that the number of secondary long-term predictors was ﬁxed at
one. As the secondary long-term prediction (LTP2) tries to model long-term
correlations not modelled by the primary (and previously much shorter)
excitation memory pitch ﬁlter (LTP1), it can be expected that some form of

Generalized AbS Coding
213
Optimal vector
0.0
0.0
10.0
20.0
30.0
Samples
40.0
50.0
60.0
500.0
Codebook Index
(a)
(b)
1000.0
Reference vector
Amplitude
v1
v2
v3
v4
v5
Relative Error (maximum is optimal vector)
Figure 7.6
Typical example of a distinctive codebook vector selection for CELP
(a) Plot of error versus codebook index for CELP, and (b) Synthesized best codebook
entries compared with original
structure may exist in the selected time index for LTP2. Figure 7.8 shows a
plot of the distribution of the time-delay index for both LTP1 and LTP2 for
a female speaker. From the histogram of LTP1, it is clear that the speaker
used for the test had a pitch period peaking at around 42 samples. However,

214
Analysis by Synthesis LPC Coding
Optimal vector
0.0
500.0
(a)
1000.0
Relative Error (maximum is optimal vector)
0.0
10.0
20.0
30.0
Samples
40.0
50.0
60.0
(b)
Reference vector
Amplitude
v1
v2
v3
v4
v5
Codebook Index
Figure 7.7
Example of a less distinctive codebook vector selection for CELP (a) Plot
of error versus codebook index for CELP, and (b) Synthesized best codebook entries
compared with original
from the similar plot for LTP2 very little structure can be deduced. It would
appear that if there were any extra long-term structure in the test speech
then the SELP did not model it properly. Alternatively, the assumption of the
existence of extra long-term structures as used in SELP could be at fault.

Generalized AbS Coding
215
4.0
1.0
0.8
% occurrence
0.6
0.4
3.0
2.0
0.0
0
20.0
40.0
60.0
LTP1 delay/samples
(a)
80.0
100.0
1.0
% occurrence
0.0
0.2
1000
500
Time delay/samples
(b)
Figure 7.8
Distribution of delays for LTP1 and LTP2 in SELP (a) Plot of the distribution
of delays for LTP1 for SELP, and (b) Plot of the distribution of delays for LTP2 for SELP
MPLPC
In MPLPC the best set of randomly-placed pulses that minimizes the output
error is selected. In order to obtain the pulses, three search strategies are
generally applied:

216
Analysis by Synthesis LPC Coding
• Method 1: At stage j, all pulse amplitudes and locations up to stage j −1
are assumed to be known and only the pulse location nj and the pulse
amplitude gj are computed.
• Method 2: At stage j, only the pulse locations n1, n2, . . . , nj−1 remain
constant and the pulse amplitudes up to gj−1 are optimized. As all the pulse
amplitudes can be modiﬁed to compensate for inaccuracies of previous
pulses, they remain accurate even when they are closely spaced.
• Method 3: Only after the last stage are all the amplitudes g1, g2, . . . , gM
re-optimized and the pulse locations remain constant.
As expected, method 2 gives the best performance both objectively and
subjectively depending on the analysis block size and the number of
pulses. However, the gain of method 2 reduces when quantization is intro-
duced. Therefore, from a complexity point of view, method 3 is prefer-
able as it is very similar in performance to method 2, but has only one
re-optimization loop.
The variation in the pulse amplitudes for method 2, normalized to unit
variance in each frame, are shown in Figure 7.9. As can be observed from
the plot without the pitch prediction (LTP) the histogram is bimodal with
little content around zero, and with most amplitudes lying within ±3σ.
This is expected as small pulses contribute little energy to the error mini-
mization process. For the amplitude plot with the LTP, the range is even
shorter. This is also expected as the majority of the large energy pulses
would have been removed by the LTP. The more conﬁned spread of values
indicates that the quantization of the pulses in MPLPC with LTP can be
much more efﬁcient than without LTP. This efﬁciency in pulse quantization
is very noticeable because, for similar bit-rate MPLPC schemes with and
without LTP, the MPLPC with LTP is generally preferred both objectively
and subjectively as the output speech with the LTP becomes smoother and
more natural.
The histograms of the pulse locations are also interesting (see Figure 7.10).
Note that pulse locations at the beginning of the frame are favoured more
than the other locations since these locations allow large errors inside the
frame to be reduced. In order to compensate for this uneven spread of pulse
positions due to the autocorrelation type of analysis used in the derivations,
the covariance form of analysis has been suggested. The covariance method
attempts to account for the block edge effects of the autocorrelation analysis
by taking into account the part of the impulse response of the cascaded ﬁlter
that spills outside the analysis block. The positions of the pulses also have an
impact on the choice of subframe sizes. Ideally, large subframe sizes are better
suited to MPLPC because the limited number of pulses can be put in the most
useful locations, e.g. pitch pulse locations. With small subframes, pulses are
assigned even for relatively unimportant details of the speech signal which

Generalized AbS Coding
217
−4.0
0.8
0.6
0.4
% occurrence
% occurrence
0.2
−2.0
0.0
Normalized variance
(a)
2.0
4.0
−4.0
−2.0
0.0
Normalized variance
(b)
2.0
4.0
0.0
1.2
1.0
0.8
0.6
0.4
0.2
0.0
Figure 7.9
Distribution plot of MPLPC pulse amplitudes (a) The distribution of MPLPC
pulses with no LTP, and (b) The distribution of MPLPC pulses with LTP
lowers the coding efﬁciency. The disadvantage with large subframe sizes is
that the complexity is increased.
RPELPC
As described earlier, RPELPC is similar to MPLPC except that the pulse
locations are pre-structured. The amount of structuring obviously determines
the amount of freedom that the pulses have in estimating the reference signal.

218
Analysis by Synthesis LPC Coding
0.0
3.0
2.5
2.0
1.5
1.0
20.0
40.0
Pulse Locations
% occurrence
(a)
60.0
80.0
0.0
20.0
40.0
Pulse Locations
(b)
60.0
80.0
0.5
3.5
3.0
2.5
2.0
1.5
1.0
% occurrence
0.5
Figure 7.10
Distribution plots of MPLPC pulse locations with subframe size of 80
samples (a) The distribution of MPLPC pulse locations with no LTP, and (b) The
distribution of MPLPC pulse locations with LTP
The effect of the decimation ratio on the performance of RPELPC is shown in
Figure 7.11 for analysis frames of 40 and 80 samples with and without LTP.
The relative SNR differences are partly dependent on the speech material but,
generally, the inclusion of the LTP improves the performance of the RPELPC
especially at higher decimation ratios. As can be observed from the plot, the

Code-Excited Linear Predictive Coding
219
25.0
0.0
0.0
10.0
Decimation factor
SegSNR (dB)
5.0
10.0
15.0
20.0
5 ms, with LTP
10 ms, with LTP
5 ms, no LTP
10 ms, no LTP
20.0
Figure 7.11
SNR plot of RPELPC with different configurations
performance at a lower analysis frame size is better. This is because it has
more degrees of freedom to vary the excitation to match the reference vector.
7.3 Code-Excited Linear Predictive Coding
Amongst the variations of AbS-LPC schemes, the most widely-reported
scheme used at 8 kb/s and below is code-excited linear predictive coding
(CELP). As the name suggests, the excitation of the time-varying ﬁlters is
provided by a codebook. CELP operates as follows (see the simpliﬁed block
diagram in Figure 7.12):
1. The original speech, s(n), is partitioned into analysis frames of around
20–30 ms. LPC analysis is performed on the frame of s(n) to give a set of
LPC coefﬁcients which are used in the short-term LPC predictor to model
the spectral envelope of the speech.
2. Having computed the LPC parameter, the frame is usually split into a
number of subframes (usually 40 or 60 samples long). The following
processing is carried out for each subframe.

220
Analysis by Synthesis LPC Coding
W(z)
1/P(z)
+
+
Select D and β
for minimum error
1/Aw(z)
1/P(z)
+
+
Codebook
G
Zero
Excitation
Weighted
LPC coeffs
Select index and
G for minimum error
L
C
−
−
+
+
Zero
Excitation
−
1/Aw(z)
Weighted
LPC coeffs
1/Aw(z)
Weighted
LPC coeffs
Original Speech
Figure 7.12
Block diagram of the standard CELP algorithm
(a) The memory of the combined LPC and perceptual weighting ﬁlters
(the initial conditions) is removed from the reference (the perceptually-
weighted original speech) to give a memoryless ﬁlter for subsequent
analysis.
(b) Pitch prediction proceeds to deal with the long-term redundancies.
The pitch analysis is performed by testing all possible pitch lags and
selecting the lag D that minimizes the difference between the reference
(the original speech remaining after the above step) and the speech
produced by passing the pitch excitation at each possible pitch delay
through memoryless LPC and perceptual weighting ﬁlters. Having
selected the best delay, D, its associated gain β, is then computed.
This process is computationally very complex. In order to simplify it, an
open-loop pitch may be computed ﬁrst and only a limited range around
this open-loop pitch is searched. The open-loop pitch computation is

Code-Excited Linear Predictive Coding
221
usually carried out on the perceptually-weighted original speech to
obtain a good idea of the likely pitch period before a closed-loop search
is applied around this value. The pitch contribution is then subtracted
from the reference signal to update it for the next stage (the codebook
search). Since the pitch can change up to 1 %/ms, the pitch delay is
updated more frequently than the LPC for accurate voice periodicity
generation in the synthesized speech.
(c) Once the parameters of the two synthesis ﬁlters are found, the excitation
is determined. Each codebook vector is passed through the memoryless
LPC and perceptual weighting ﬁlters and the codebook vector which
gives the minimum squared difference between the output it produces
and the reference signal is selected and its corresponding scaling factor
is computed. Note that if the delay D in the pitch ﬁlter is greater
than the subframe size, it will not affect the synthesized codebook
vector. In addition, the pitch ﬁlter is usually implemented as an
adaptive codebook operating in parallel with the stochastic codebook
and, hence, its response is eliminated from the stochastic codebook
search loop.
(d) Finally the initial conditions (i.e. the memory) of the ﬁlters are restored,
and the synthetic speech is generated by ﬁltering the scaled optimum
codebook sequence through the ﬁlters so as to update the ﬁlters for
processing the next subframe.
3. In the synthesizer (decoder), the initial conditions (i.e. the memory) of
the ﬁlters are restored and the synthetic speech is generated by ﬁltering
the scaled optimum codebook sequence through the ﬁlters without any
perceptual weighting.
From the above description it is clear that the computation can be broken
down into three blocks: LPC analysis to compute the LPC parameters; pitch
analysis to compute the long-term predictor parameters; and a codebook
search to determine the shape and gain of the excitation vector.
7.3.1 LPC Prediction
The role of LPC prediction is to represent the general shape of the speech
spectrum. Therefore, in the CELP synthesizer, the (ideally ﬂat) excitation is
shaped by the spectral envelope of the LPC ﬁlter. The LPC parameters can
be computed by a number of methods as discussed in Chapter 4. However,
most CELP coders use a 10th-order LPC ﬁlter based on autocorrelation
estimation. The speech signal, which is usually 20 ms long, is passed through
a Hamming window which is usually placed half a frame ahead so as to enable
accurate parameter interpolation for each subframe. However, many delay-
sensitive applications and standards use an asymmetric window to give more
weighting to the latest samples contained in the analysis frame. The delay

222
Analysis by Synthesis LPC Coding
problem can also be solved by employing backward forms of LPC analysis, i.e.
using quantized (or past) samples only, to estimate the LPC coefﬁcients as in
the 16 kb/s LD-CELP (G.728) proposed for the ITU standard [15]. However,
such backward techniques can only operate successfully at around 16 kb/s,
because the prediction accuracy reduces rapidly with the increase in the
quantization noise of the encoded speech.
7.3.2 Pitch Prediction
Pitch prediction is an essential part of all CELP coders. Since the early versions
of CELP had Gaussian-noise-populated excitation codebooks, pitch ﬁltering
was required to introduce the necessary pitch of the voiced speech parts. The
order of the pitch ﬁlter is usually less than the order of the LPC ﬁlter and is
given in its general form as,
P(z) = 1 −
I

i=−I
βiz(−D−i)
(7.26)
The pitch predictor in CELP generates long-term correlation, either due to
the actual pitch excitation or other long-term similarities. Thus the term
’long-term predictor’ (LTP) is usually preferred to ‘pitch predictor’, which
is somewhat misleading in describing the action of this ﬁlter for unvoiced
speech and even, to some extent, for voiced speech when D is equal to
pitch multiples. In CELP and other AbS-LPC schemes, the LTP analysis
is usually performed in a closed loop [16] with single or multiple taps.
In CELP, one is interested in minimizing the error between the weighted
original and the synthesized output speech. By this deﬁnition, analysis of
the signal to derive the desired LTP parameters must minimize the error
between the weighted original and the synthesized speech, and not mini-
mize the LTP prediction error (or second residual) as is the case in older
analysis and synthesis systems. Assuming that the LPC parameters have
already been calculated, the remaining undetermined parameters are Gx(n),
D, and βk. Although these parameters can be obtained by exhaustively
searching for all Gx(n) as well as the LTP parameters, the procedure becomes
very computationally-intensive and thus suboptimal solutions have to be
used. One way of reducing the complexity is by obtaining the LTP and
Gx(n) in two sequential steps. First we assume Gx(n) is zero, and cal-
culate the LTP parameters such that e(n) is minimized. Next the LTP is
held constant and Gx(n) is computed. Thus, let the codebook excitation
be zero, i.e.
x(n) = 0,
0 ≤n ≤L −1
(7.27)

Code-Excited Linear Predictive Coding
223
The synthetic speech is produced only by the LTP excitation passing through
the combined LPC and perceptual weighting ﬁlters.
⇒ˆs(n) =
I

i=−I
βi
n

k=0
ˆr(n −k −D −i)h(k)
(7.28)
Then the weighted squared error E for the delay D is given by,
E(D) =
L−1

n=0
e2(n) =
L−1

n=0
(˜s(n) −ˆs(n))2
(7.29)
where,
˜s(n) = sw(n) −sm(n)
(7.30)
and sm(n) is the memory response contribution of the combined LPC and
perceptual weighting ﬁlters, and sw(n) is perceptually weighted original
speech. Therefore,
∂E
∂βi
= 2


L−1

n=0
˜s(n) −
I

i=−I
βi
n

k=0
ˆr(n −k −D −i)h(k)


(7.31)
×

−
n

k=0
ˆr(n −k −D −j)h(k)
	
= 0
(7.32)
Let Zi(n) =
n

k=0
ˆr(n −k −D −i)h(k)
(7.33)
⇒
L−1

n=0
˜s(n)Zj(n) −


I

i=−I
βi
L−1

n=0
Zi(n)Zj(n)

= 0,
−I ≤j ≤I (7.34)
Hence, in matrix form, assuming a 3-tap ﬁlter,


β−1
β0
β1

=


φ(−1, −1)
φ(0, −1)
φ(1, −1)
φ(−1, 0)
φ(0, 0)
φ(1, 0)
φ(−1, 1)
φ(0, 1)
φ(1, 1)


−1 

B(−1)
B(0)
B(1)


(7.35)
and for a single-tap ﬁlter where I = 0,
β0 =
B(0)
φ(0, 0)
(7.36)

224
Analysis by Synthesis LPC Coding
where,
φ(i, j) =
L−1

n=0
Zi(n)Zj(n)
(7.37)
B(i) =
L−1

n=0
˜s(n)Zi(n)
(7.38)
Once the LTP gain coefﬁcients are found, they are substituted back into
equation (7.29) and the delay D for which E(D) is a minimum gives the
optimum delay Dopt and the corresponding gains βi. The excitation Gx(n) can
then be found with Dopt and βopt ﬁxed. In practice however, the optimum
delay Dopt is usually found before computing the gain coefﬁcients.
There may be problems with the LTP when the delay D is less than the
subframe L, i.e. when the LTP recurses within the same analysis subframe
[16]. The basic problem in solving for the gain and delay coefﬁcients for lags
less than the subframe size is that the weighted mean squared error equation
becomes nonlinear in the coefﬁcients for D < L. Consider the case in which a
single LTP coefﬁcient is being determined and the LTP lag lies in the interval
L/2 ≤D ≤L −1. The signal takes one of two forms:
ˆr(n) =

 βˆr(n −D)
0 ≤n ≤D −1
β2ˆr(n −2D)
D ≤n ≤L −1
(7.39)
The weighted squared error, E, can then be expressed as,
E =
L−1

n=0
e2(n) =
L−1

n=0
[˜s(n) −ˆs(n)]2
(7.40)
Deﬁning,
ZD(n) =
n

k=0
ˆr(n −k −D)h(k)
(7.41)
we can expand the error equation as
E =
D−1

n=0

˜s2(n) + β2Z2
D(n) −2β˜s(n)ZD(n)

+
L−1

n=D

β4Z2
D(n) −2β2˜s(n)ZD(n)

(7.42)
where the ﬁrst part of the right hand side is for the ﬁrst D samples and the
second is for the recursive part (which may cause some difﬁculties). As can

Code-Excited Linear Predictive Coding
225
be seen from the above, when D = L the second term has no effect. To solve
the equation with respect to β we get:
∂E
∂β = 2β3
L−1

n=D
Z2
D(n) + β
D−1

n=0
Z2
D(n) −2
L−1

n=D
˜s(n)ZD(n)
	
(7.43)
−
D−1

n=0
˜s(n)ZD(n) = 0
From the above we can see that the solution to E involves solving a cubic in
β. This is obviously very costly as it is required for every value of D less than
L. One solution to the above is to adopt a trial-and-error method based on
quantized values for β. In this method, the sum terms are precomputed, and
each of the possible quantized values of β is substituted into the equation.
The value of β which gives the smallest squared error is thus the desired
value. Obviously the computation involved is still quite large, e.g. if L = 50,
Dmin = 20, and β = 3 bits, the number of searches is 23 × (50 −20) = 240,
with the addition for D > L.
A second method for D < L is to use an adaptive codebook formulation of
the LTP [17], to periodically repeat the past LTP output, i.e.
ˆr(n) =



βˆr(n −D)
0 ≤n ≤D −1
βˆr(n −2D)
D ≤n ≤2D −1
...
βˆr(n −aD)
aD ≤n ≤L −1
(7.44)
In other words, the previously undeﬁned part of the LTP excitation in a
subframe is constructed by repeating its deﬁned part with periodicity D.
Using this method only β terms needs to be solved. This scheme does not
allow for pitch pulses in a subframe to change amplitude from one period
to another. Using this adaptive method, the CELP synthesis procedure is as
shown in Figure 7.13, where the LPC and perceptual weighting ﬁlters have
been represented by a single short-term ﬁlter (STP).
Fractional-Delay LTP
In the above LTP computation, the matching of the reference signal with
the LTP contribution is achieved via a cross-correlation procedure. A major
restriction of this is the inherent sampling resolution of the signal, i.e. for
our cross-correlation to be most effective we would ideally like a continuous
signal such that the best instance of similarity between the reference and the
synthetic signal can be obtained. However, as our delay, D, is restricted to

226
Analysis by Synthesis LPC Coding
βi
^
^r(n−D)
STP buffer
s(n)
e(n)
s(n)
+
−
"0"
LTP buffer
Dmax
L
a
a
a
a
---
---
---
---
+
+
+
Dmin < L  
~
Figure 7.13
Block diagram illustrating pitch repetition for delays less than the opti-
mization interval
integer values of the sampling rate, the LTP is not able to cope with arbitrary
lag intervals without replacing, in some way, the optimum noninteger delay
by an integer value which may degrade the performance of the LTP in terms of
objective matching. As explained in the previous section, higher-order LTPs
can be used where its multiple coefﬁcients can provide interpolation between
the adjacent samples around D even if the lag value does not correspond to
an integer number of samples. However, the disadvantage of higher-order
LTPs is the increased coding capacity required to code the additional gains.
Based on this observation, in order to achieve a greater LTP delay resolution
but to minimize coding capacity, an up-sampling procedure [18] can be used.
Increased Resolution by Up-sampling
The LTP delay, D, is expressed as an integer number of samples at sampling
rate fs. When trying to replace D by a real number, Dr, it is necessary to convert
the discrete time signal s(n), to a continuous time signal as noninteger values
are not deﬁned by s(n). As our signal s(n) is sampled according to the
Nyquist rate, the continuous form, sc(t), can be recovered at any time instant
by ﬁltering through an ideal noncausal low-pass ﬁlter,
sc(t) =
∞

k=−∞
s(k) sin(π(t −k))
π(t −k)
(7.45)
As we are only interested in the submultiples of D, the sc(t) signal is not
required, but a higher sampled signal, sup(m), is required. The ideal low-pass
prototype ﬁlter is replaced by a ﬁnite length ﬁlter. The up-sampling of s(n)

Code-Excited Linear Predictive Coding
227
h(m)
U
p[0](n)
p[1](n)
p[U−1](n)
Polyphase
Filter
s(n)
s(n)
Sampling Rate
Expander
Low-pass
Filter
m = 0
Commutator
sup(m)
sup(m)
s0(n)
s1(n)
sU−1(n)
Figure 7.14
Polyphase structure for implementing interpolation
to sup(m) is illustrated in Figure 7.14 where U is the up-sampling factor and a
polyphase structure is used.
From the up-sampling procedure a noninteger lag value of (D + d/U) at
sampling rate fs now corresponds to an integer delay of (UD + d) where
d = 0, 1, . . . , U −1 at a rate of Ufs. Therefore, to implement a delay of
noninteger values, one simply takes the appropriate branch of the polyphase
structure of the interpolation ﬁlter (see Figure 7.14). An important aspect of
the interpolation process is the choice of the low-pass ﬁlter both in terms of
performance and complexity. As suggested by Kroon in [18], a simple but
effective ﬁlter design is to use a Hamming windowed sin(x)/x design, which
has three advantages:
1. The resultant FIR ﬁlter has exactly linear phase and a ﬁxed delay.
2. The characteristics of the ﬁlter are adequate with only a short ﬁlter length,
i.e. the aliasing components are small.
3. The original signal can be obtained without any phase-shift, i.e. the top
branch of the polyphase structure in Figure 7.14. This means that the
number of ﬁltering operations is decreased as the top branch is effectively
just a delay operation.
In order to obtain the above advantages, the ﬁlter length N must be chosen
such that the ﬁlter delay, (N −1)/2, at sampling rate Ufs is an integer multiple
of U, i.e.
N = 2IU + 1
(7.46)
where I is the delay of the low-pass ﬁlter at sampling rate fs.

228
Analysis by Synthesis LPC Coding
As for the integer delay LTP, there exists a problem when the candidate
delay is less than the optimization interval, as the ﬁlter then recurses. Again,
the technique of adaptive codebook structure, where the available part of the
LTP buffer can be repeated to form the missing part, is applicable.
Performance Comparison of LTP Methods
In order to assess the performance of the different LTP analysis methods, an
unquantized CELP has been used with the following LTPs:
• One-tap (CL1) and three-tap (CL3) using the closed-loop analysis method
with the adaptive codebook method when D < L.
• Modiﬁed CL3 (MCL3), where the delay is calculated by CL1 but the gain
coefﬁcients are calculated by CL3.
• Fractional delay closed-loop (basically CL1 with up-sampling of the LTP)
with U = 2 (F2CL1) and U = 4 (F4CL1).
The conﬁguration of the CELP coder is shown in Table 7.1. In order to
assess the performance of the LTP, the overall SNR of the CELP coder is
split into three parts (see Figure 7.15): (i) the LPC ﬁlter memory contribution,
shown as short-term prediction (STP), (ii) the LTP contribution, and (iii) the
codebook contribution. In this test no perceptual weighting ﬁlter is used.
Table 7.2 shows the result of the comparison between the different LTP
methods using the conﬁguration in Table 7.1. Note that the SNRs given in
this table have been computed using only a few short speech sentences and
are intended to give a quick comparison for the LTP methods. The SNRs
are dependent on the input signal and may vary signiﬁcantly for other
input samples, so we should only consider their relative variations. From
the comparison test and segmental SNR values shown in Table 7.2 several
interesting points can be gathered:
• As expected, the contribution from the memory of the LPC ﬁlter (the
short-term predictor) is more or less constant. A plot of the SNR values of
Table 7.1
CELP coder configuration
for the LTP comparison test
Sampling freq.
8 kHz
Parameters
Update Rate
LPC order = 10
160 samples
LTP, various
40 samples
Codebook, 10-bit
40 samples
Weighting
None

Code-Excited Linear Predictive Coding
229
−
+
−
+
−
+
+
+
+
with I.C.
with no I.C.
with no I.C.
with no I.C.
with I.C.
Reference
Codebook
SNR
LTP SNR
STP SNR
Codebook
Contribution
LTP
Contribution
STP
Memory
A(z)
1
A(z)
1
A(z)
1
P(z)
1
P(z)
1
Optimal
Excitation
"0"
"0"
LTP parameters
Figure 7.15
Breakdown of the CELP scheme subsystems which contribute to the
overall SNR (I.C. means ’initial conditions’)
Table 7.2
Breakdown of segmental SNR values for different LTPs
Scheme
Delay range
STP (dB)
LTP (dB)
Codebook (dB)
Overall (dB)
CL1
20–147
1.76
7.28
2.74
11.77
CL3
21–146
1.81
9.13
2.33
13.26
MCL3
21–146
1.77
8.98
2.42
13.16
F2CL1
20–147
1.79
7.37
2.79
11.95
F4CL1
20–147
1.80
7.68
2.76
12.24
the LPC ringing is shown in Figure 7.16. It can be seen from the ﬂuctuation
in the SNR that the LPC contribution does not always provide a positive
SNR to the overall total, i.e. the memory is actually making the rest of the
coding process work harder. This obviously reﬂects the past history of the
encoding process, i.e. if the previous subframe was poorly matched, then
the LPC memory will also be poor. This generally occurs during speech
transitions.
• The fractional resolution closed-loop LTPs objectively perform slightly
better than the integer resolution closed-loop LTPs. However, subjectively,
the improvements are more substantial than the SNR suggests. The speech
becomes cleaner, especially for female speakers. Only uniform spacing of
the delays have been investigated. The performance can be improved if
nonuniform spacing is used as reported in [18].
• From Table 7.2 we can see that the three-tap integer-delay closed-loop
LTPs provide signiﬁcantly higher SNR than the one-tap integer and frac-
tional delay LTPs. The LTP contributions are signiﬁcantly higher than the

230
Analysis by Synthesis LPC Coding
100.0
Signal
Power
Total
STP
LTP
EXC
SNR in dB (Total, STP and LTP are offset by 20 dB)
0.00.0
20.0
40.0
60.0
80.0
100.0
200.0
Time/40-sample subframe
300.0
Figure 7.16
Breakdown of SNR values for the CL3 LTP structure
codebook contribution. This is due to the fact that the codebook provides
contribution to match the remaining signal after the LPC and LTP contri-
butions have been subtracted from the original target. So the higher the
matching in LTP, the lower the contribution of the codebook will be. In
order to ﬁll in the remaining information, the 10-bit codebook can only
provide up to a certain threshold in the overall SNR. In order to provide
more contribution, the LTP and codebook can be jointly optimized [19], or
a better codebook excitation source can be used.
Limited informal subjective listening tests have also shown that three-tap
LTPs are generally better than one-tap LTPs, although the difference between
the three-tap integer delay and the one-tap fractional delay (up-sampling
four times) is not very noticeable.
7.3.3 Multi-Pulse Excitation
Early versions of CELP coding used multi-pulse excitation with and without
LTP to match the original input signal. As was brieﬂy discussed earlier,
a low-complexity MPLPC coder sequentially determines the locations and
amplitudes of the excitation pulses so as to minimize the error between the
original and the synthesized speech. The optimum pulse locations are found
by computing the error for all possible pulse locations with their optimum
amplitudes in a given analysis block and selecting the allowable number of
locations and their amplitudes that result in the minimum error. A block

Code-Excited Linear Predictive Coding
231
sw(n)
+
s(n)
e(n)
s(n)
^
+
−
Error
Minimization
Pulse
Generator
1/Aw(z)
W(z)
Figure 7.17
Block diagram of a simple MPLPC coder
diagram of a simple MPLPC is shown in Figure 7.17. Assuming that h(n) is
the impulse response of the combined LPC and perceptual weighting ﬁlters,
the squared error for a single pulse excitation at location mi with amplitude
gi can be written as,
E =
L−1

n=0
[˜s(n) −gih(n −mi)]2
(7.47)
where ˜s(n) is the perceptually-weighted target signal with the combined
LPC and perceptual-weighting ﬁlter memory effect subtracted. The optimum
pulse location is obtained by differentiating equation (7.47) with respect to gi
and setting the derivative to zero,
∂E
∂gi
= −2
L−1

n=0
[˜s(n) −gih(n −mi)] × h(n −mi) = 0
(7.48)
This yields,
gi =
L−1

n=0
˜s(n)h(n −mi)
L−1

n=0
h2(n −mi)
(7.49)

232
Analysis by Synthesis LPC Coding
which in general can be written as,
gi =
	(mi)
φ(mimi)
(7.50)
where 	(mi) is the cross-correlation between the perceptually-weighted
target speech and the combined LP and perceptual-weighting ﬁlter impulse
response; φ(mimi) is the autocorrelation of the combined LPC and perceptual-
weighting ﬁlter impulse response at positions mi; and 0 ≤i ≤M −1. Sub-
stituting equation (7.50) into (7.47) gives an expression for the perceptually-
weighted squared error in terms of the pulse locations,
E =
L−1

n=0

˜s2(n) −	2(mi)
φ(mimi)
	
(7.51)
To minimize the error in equation (7.51), it can be seen that the best position for
a single pulse is that value of mi which maximizes the term, 	2(mi)/φ(mimi).
Once the search for the optimum pulse has ﬁnalized, the effect of this newly-
found pulse is removed from the perceptually-weighted input speech to give
a new reference signal to be used in determining the next pulse location.
Hence the updated reference speech is,
˜si+1(n) = ˜si(n) −gih(n −mi)
(7.52)
The steps carried out from equations (7.49)–(7.52) are repeated to ﬁnd
the remaining pulse locations and amplitudes. Figure 7.18 shows a typi-
cal example of speech signal and the excitation signal produced in an AbS
manner, as discussed above. It can clearly be seen from Figure 7.18 that the
multi-pulse structure is very effective in producing a ﬂexible excitation signal
in modelling the glottal characteristics, especially the pitch pulses.
Optimum Amplitude Excitation MPLPC
The sequential AbS method described is simple and fast but it has several
shortcomings. Successive optimization of individual pulses becomes inaccu-
rate when the number of pulses per frame increases. In order to improve the
performance one needs to consider the interactions amongst all the pulses
during optimization. To consider the interaction between the pulses, let the
weighted mean squared error after having placed M pulses at positions mi,
be given by,
E =
L−1

n=0

˜s(n) −
M−1

i=0
gih(n −mi)
	2
(7.53)

Code-Excited Linear Predictive Coding
233
0
100
200
300
400
500
600
Time (Samples)
-4000.
0
1000.
0
6000.
0
11000.
0
16000.
0
Amplitude
(a)
(b)
(c)
(d)
Figure 7.18
Waveform illustrations of the MPLPC coder: (a) original speech, (b)
multi-pulse excitation, (c) synthesized speech, and (d) error signal
Differentiating the above equation with respect to the amplitudes gi gives a
solution for optimum amplitudes,
∂E
∂gi
= −2
L−1

n=0

˜s(n) −
M−1

i=0
gih(n −mi)
	
× h(n −mk) = 0
k = 0, 1, . . . , M −1
(7.54)
or,
L−1

n=0
˜s(n)h(n −mk) =
L−1

n=0
M−1

i=0
gih(n −mi)
	
× h(n −mk)
k = 0, 1, . . . , M −1
(7.55)
Rearranging the summation on the right hand side,
L−1

n=0
˜s(n)h(n −mk) =
M−1

i=0
gi
L−1

n=0
h(n −mi)h(n −mk)
	
k = 0, 1, . . . , M −1
(7.56)

234
Analysis by Synthesis LPC Coding
Now deﬁning,
φ(k, i) =
L−1

n=0
h(n −k)h(n −i)
(7.57)
and,
	(k) =
L−1

n=0
˜s(n)h(n −k)
(7.58)
simpliﬁes equation (7.56) to a form,
	(mk) =
M−1

i=0
giφ(mi, mk)
k = 0, 1, . . . , M −1
(7.59)
which can be written in the form of a correlation matrix as,


	(m0)
	(m1)
...
	(mM−1)

=


g0
g1
...
gM−1




φ(m0m0)
φ(m0m1)
. . .
φ(m0mM−1)
φ(m1m0)
φ(m1m1)
. . .
φ(m1mM−1)
...
...
...
...
φ(mM−1m0)
φ(mM−1m1)
. . .
φ(mM−1mM−1)


(7.60)
The optimum amplitudes gi can now be solved utilizing the Cholesky decom-
position of the correlation matrix.
Using the above analysis, two forms of pulse amplitude re-optimization
procedure can be used [12]. One can re-optimize the amplitudes after all
of the M pulses have been located within a subframe, or after each new
pulse is located. Of course the latter method has the greater computational
burden of the matrix inversion, but the overall quality compared to the
former method is superior. If amplitude re-optimization takes place once at
the end of each subframe, the required matrix inversion size is (M × M). If it
takes place after each new pulse is located, amplitude re-optimization occurs
M times with matrix sizes of (1 × 1) up to (M × M). Figure 7.19 shows the
variation of number of pulses versus segSNR for three different algorithms.
Curve (c) is for a basic sequential MPLPC coder. It shows increasing segSNR
as the number of pulses increases but, after 30 pulses per 160 samples, its
performance tends to saturate. Curves (b) and (a) are for the improved MPLPC
algorithms, i.e. amplitude re-optimization after all pulses have been located
and amplitude re-optimization after location of each new pulse respectively.
Objective results show curve (b) giving lower segSNR than (a), as expected.

Code-Excited Linear Predictive Coding
235
0
10
20
30
40
Number of pulses/160 samples
6.0
8.0
10.0
12.0
14.0
16.0
18.0
20.0
SegSNR (dB)
(a)
(b)
(c)
Figure 7.19
The number of pulses against the segmental SNR: (a) amplitude re-
optimization after each pulse, (b) amplitude re-optimization after all pulses, and (c)
sequential MPLPC with no amplitude re-optimization
Subjectively, curves (a) and (b) are superior only when a high number of
pulses (eight or more every 20 ms) are employed in the process of amplitude
re-optimization. This is expected since the re-optimization process improves
the performance of closely placed pulses. On average, ﬁve pulses per 4–5 ms
are adequate to achieve good speech quality. One major problem during
the search for the pulses is pulse-doubling. Pulse-doubling usually occurs in
voiced regions with greater than about eight pulses every 10 ms and involves
the re-selection of already-selected pulse positions. In order to avoid this
effect, the newly-found pulse amplitude is added to the existing amplitude
or the already-found pulse locations are excluded from further pulse position
selection. If joint pulse amplitude re-optimization is applied every time a new
pulse is positioned, pulse-doubling is eliminated automatically.
MPLPC with Pitch Prediction
A basic multi-pulse coder produces satisfactory speech quality at medium
bit rates. However as the bit rate is lowered, degradations in the speech
quality become noticeable. This is especially true for the higher-pitched
voiced regions which usually occur with female speakers. This is due to a
limited number of pulses being available, the majority of which are used to
model the fundamental pitch pulses and hence relatively few pulses remain
for the modelling of the remaining excitation signal. With the introduction of
a pitch predictor into the AbS loop as shown in Figure 7.20 such effects can
be reduced. In 1989, Singhal and Atal [12] proposed a closed-loop solution

236
Analysis by Synthesis LPC Coding
s(n)
e(n)
s(n)
sw(n)
+
^
−
+
Pulse
Generator
W(z)
1/P(z)
Error
Minimization
1/Aw(z)
Figure 7.20
Block diagram of MPLPC AbS procedure with LTP
0
10
8
6
4
2
Number of pulses / 32 samples
0.0
5.0
10.0
15.0
SegSNR (dB)
(a)
(b)
Figure 7.21
Performance of MPLPC (a) with and (b) without a pitch predictor
which gave optimum values of the pitch predictor where the delay in the
predictor was integer multiples of the pitch. The aim of pitch prediction was
to model the long-term similarities in speech and hence it was also called
the long-term predictor (LTP). The most popular pitch prediction used in
MPLPC is the one-tap predictor. Figure 7.21 shows the objective performance
of the MPLPC with a (one-tap closed-loop) pitch predictor and without a
pitch predictor. It is clear that pitch prediction provides higher segSNR at all

Code-Excited Linear Predictive Coding
237
pulse rates. At high pulse rates, the subjective difference between the MPLPC
with and without LTP saturates, since at higher rates the pulses from the
multi-pulse excitation can model the fundamental pitch pulses accurately.
Pulse Position Coding
The coding of pulse positions is usually performed by enumerative source
coding techniques [6]. The number of possibilities for placing M pulses in a
subframe of L samples is given by,

 =
 L
M

=
L!
M!(L −M)!
(7.61)
Hence the minimum number of bits required for coding these positions is
Bmin = ⌈(log2 
)⌉
(7.62)
where ⌈.⌉is the nearest integer greater than or equal to 
. Such methods
of pulse-position coding can be considered as a vector quantization of the
pulse positions. These techniques are not very favourable in bad channel
conditions and alternative coding methods are pursued. Another method is
the independent coding of the pulse positions. Although this method leads
to a higher bit rate, it is more robust to channel errors. The number of bits
required for independent pulse position coding is given by,
Bmin = M × ⌈(log2 L)⌉
(7.63)
Figure 7.22 shows the comparison between combinational and independent
coding of pulse positions for L = 32. Clearly, combinational coding is by far
the more efﬁcient at very high pulse rates, but at very low number of pulses
the difference is small (i.e. 3 bits). At such coding rates, the disadvantage of
an extra few bits per analysis frame for independent coding is reﬂected in the
complexity and the coder robustness.
Pulse Amplitude Coding
Efﬁcient normalization is necessary for coding of the pulse amplitudes,
because the pulse amplitudes have a large dynamic range and direct quan-
tization requires a large number of bits. Normalization can be carried out
by the rms of the amplitudes. In such methods, the rms value must also
be included in the transmission. This inevitably leads to higher bit rates.
In most MPLPC designs, quantization of the ﬁrst pulse is accomplished by
incorporating a large number of nonuniform quantizer levels (usually ﬁve
bits or more) and the rest of the pulses in the subframe are normalized with

238
Analysis by Synthesis LPC Coding
0
5
10
15
20
Number of pulses / 32 samples
0.0
20.0
40.0
60.0
80.0
Number of bits
(a)
(b)
Figure 7.22
Comparison between (a) independent and (b) combinational pulse-
position coding
the ﬁrst pulse magnitude and coded using fewer quantization levels (typi-
cally three bits) [6]. This assumes that the ﬁrst pulse usually has the largest
magnitude and, if independent pulse position coding is used, this can be
advantageous. Otherwise the largest magnitude pulse may be used to limit
the pulses within ±1.
7.3.4 Codebook Excitation
The vectors contained in the excitation codebook form a very important part
of the CELP coding algorithm. They serve two main purposes:
• They provide the start-up information to the LTP memory, including
any sudden changes in the speech not adequately tracked by the pitch
prediction.
• They supply the ‘ﬁlling-in’ information that the pitch predictor has omitted.
This is especially the case during unvoiced regions.
Thus, how the codebook of a CELP is populated and the method by which
the optimum vector is computed are very important issues as indicated by
the many publications on this subject [20–22]. Another related issue is the
computational cost and storage of the codebook search procedure. The search
process for the best vector in CELP can be broken down into four stages.

Code-Excited Linear Predictive Coding
239
1. Synthesis of the codebook vector to obtain the output ˆsk(n).
2. Calculation of the cross-correlation between the reference ˜s(n) (LPC, per-
ceptual weighting and pitch effects removed) and the synthetic estimate
ˆsk(n).
3. Calculation of the autocorrelation of the synthetic estimate ˆsk(n).
4. Testing for the minimum error, or the maximum normalized correlation.
To reduce complexity and memory, and improve quality, many versions
of the codebook excitations have been developed and used. Here, we will
consider three secondary excitation types.
Gaussian Excitation
Almost all of the early versions of CELP used a form of Gaussian code-
book as the source of secondary excitation. This is mainly because speech,
after inverse ﬁltering with the LPC followed by the pitch, has residual
characteristics similar to Gaussian. One problem with this approach is the
size of the memory required to store the Gaussian codebook vectors. For
example, if a 10-bit codebook is used to match L reference samples, the
number of storage locations will be 210 × L. Assuming L = 40, this will
correspond to 40 960 storage locations, which can be very large for real-
time memory-restricted implementations. In order to overcome this problem,
the Gaussian vectors are represented as a one-dimensional array, where
most of the L samples of two consecutive vectors are common. The most
popular versions of this overlapping codebook are those with one or two
shifts. In other words, to generate a new vector, one or two samples at
the end of the previously-used vector are dropped and one or two new
samples are introduced at the beginning for one or two shifts respectively.
An overlapping codebook with single shift can be represented as an LTP
ﬁlter where the minimum and maximum delays are L −1 and C + L −1
respectively, assuming that the locations start from 0 and the size of the
codebook is C.
In some coders, a centre-clipped version is used with a clipping threshold
of 1.2 for a unit variance vector. This is found to produce sharper output
speech. One reason for this is that, when matching the codebook vectors with
the reference vector, a few higher-magnitude vector elements dominate the
selection causing errors in the lower-magnitude vector elements. By making
the smaller vector elements zero, as well as minimizing the error they cause,
the matching of the larger magnitude samples is improved in the absence of
the erroneous smaller samples in the vector.
Training Gaussian codebooks does not improve the quality signiﬁcantly
if the codebook size is eight bits or more. Therefore, they can simply be
populated by using a Gaussian random number generator. Some applica-
tions [23] use ternary codebooks, where each Gaussian number amplitude

240
Analysis by Synthesis LPC Coding
is set to 1 if it is positive, left as zero if it is zero and set to −1 if it is
negative. This is especially useful in ﬁxed point implementations. In addition
to the above versions, one other popular version of the standard codebooks
is called the sparse codebook, where each nonzero vector element is fol-
lowed by a ﬁxed number of zeros. This is very similar to regular pulse
excited LPC.
Overlapping codebooks are useful in reducing the computation of the
codebook search as well as requiring less storage. The fact that the adjacent
vectors have similarities can be used to reduce the convolution (ﬁltering)
process to generate the synthetic output. If h(n) represents the combined
impulse response of the LPC and perceptual-weighting ﬁlters, the synthesized
vector ˆsk(n) due to the kth excitation vector xk(n) of a single shift codebook is,
ˆsk(n) =
n

i=0
xk(n −i)h(i)
(7.64)
(When the pitch ﬁlter is implemented as an adaptive codebook for D < L,
it can be assumed to be parallel with the ﬁxed codebook since it is not
considered during the codebook search and hence will not affect the code-
book search process.) In a single shift codebook, the difference between two
consecutive vectors is only one sample at either end of the two vectors and
the synthesized vector ˆsk+1 can be written in terms of ˆsk as,
ˆsk+1(n) = xk+1(0)h(n) + ˆsk(n −1)
(7.65)
where ˆsk(−1) = 0. As can be seen from the above expression, by shifting
the previous output by one sample and adding this to the impulse response
h(n) scaled by the new sample, most of the convolution computations can be
simpliﬁed. As the number of shifts in the codebook increases, however, com-
plexity increases, and when the shift equals the vector size, the overlapping
codebook becomes a standard codebook containing independent vectors.
Note that if centre-clipping is used, the zero values of xk(n) in equation (7.64)
will not need multiplication with h(i), and hence will reduce computational
complexity. However, once the ﬁrst vector is fully synthesized, more sav-
ings will be made using equation (7.65). Every time a zero-valued excitation
sample helps to produce the new vector, the ﬁrst term of equation (7.65)
will be zero, which means that the new synthetic vector is simply the shifted
version of the previous with its ﬁrst sample set to zero. Using an unquantized
CELP coder as deﬁned by Table 7.3, four different versions of the standard
Gaussian codebook were compared. The results are shown in Table 7.4 (the
overlapping codebooks have a two-sample shift). Subjective listening shows
that the speech quality is generally improved with centre-clipping compared
with standard Gaussian codebooks. The difference between the overlap-
ping and nonoverlapping codebooks of the same type is negligible. Coupled

Code-Excited Linear Predictive Coding
241
Table 7.3
CELP
coder
parameter
definition for the comparison test
Parameter
Update rate
Sampling
8 kHz
LPC analysis
160 samples
LTP 1- & 3-tap
40 samples
10-bit codebook
40 samples
Weighting
γ = 0.9
Table 7.4
Performance of four standard codebooks
Scheme
1-tap (dB)
3-tap (dB)
Storage (words)
Gaussian
11.11
12.52
1024 × 40
Centre-clipped Gaussian
11.20
12.53
1024 × 40
Overlapping Gaussian
11.16
12.49
(2 × 1023) + 40
Overlapping centre-clipped Gaussian
11.18
12.55
(2 × 1023) + 40
with the objective results in Table 7.4, the overlapping centre-clipped Gaus-
sian codebook is very attractive for its reduced memory and computational
requirements.
Vector Sum Excitation
In the normal ﬁltering approach of CELP, ˜s(n) is matched by exhaustively
searching a ﬁnite number of sequences ˆsk(n) and the best match, ˆsopt(n), is
the sequence which gives the minimum mean square error between ˜s(n)
and ˆsk(n). How good a match between ˜s(n) and ˆsk(n) is determined by the
degree of freedom in ˆsk(n), i.e. the size and characteristics of the codebook.
In the method previously described, the freedom in ˆsk(n) was obtained by
synthesizing many versions of ˆsk(n), i.e. the degree of freedom in ˆsk(n) is
limited by x(n) at the residual side of the analysis. However, it can be noted
that if the same degree of freedom can be achieved at the synthetic signal side
of the analysis whilst retaining the fact that all candidate ˆsk(n) are spectrally-
shaped by the LPC and perceptual-weighting ﬁlters, then less complexity
and equal performance could be obtained. Therefore, the aim is to limit the
amount of synthesis operations and perform the vector combinations to give
the necessary freedom in ˆsk(n) at the output side of the analysis. One such
method is vector sum excitation (VSE) [24].
As for the majority of speech-coding analysis, in VSE, the mean squared
error approximation is used. The formulation of VSE to derive the opti-
mum excitation xopt(n) and hence ˆsopt(n) is as follows. Let each candidate

242
Analysis by Synthesis LPC Coding
synthesized signal be given by,
ˆsk(n) =
M

i=1
aiˆsi(n),
n = 0, 1, . . . , L −1
(7.66)
where the sequences ˆsi(n) are derived from exciting the combined LPC and
perceptual-weighting ﬁlter with M different excitation sequences, xi(n); ai are
variable scaling factors; and L is the excitation subframe size. An optimum
synthesized signal ˆsopt(n) can be derived by choosing the set of coefﬁcients
ai, i = 1, 2, . . . , M which minimizes the weighted mean squared error between
ˆs(n) and ˜s(n) given by,
E =
L−1

n=0
˜s(n) −ˆsk(n)2 =
L−1

n=0

˜s(n) −
M

i=1
aiˆsi(n)
	2
(7.67)
This minimization is achieved by solving a set of equations produced by the
partial derivatives of equation (7.67), with respect to each of the variables ai,
to be zero,
∂
∂aj
L−1

n=0
[˜s(n) −
M

i=1
aiˆsi(n)]2
	
= 0
(7.68)
which can be simpliﬁed into,
M

i=1
aiR(i, j) = φ(j),
j = 1, 2, . . . , M
(7.69)
where,
R(i, j) =
L−1

i=0
ˆsi(n)ˆsj(n)
(7.70)
and,
φ(j) =
L−1

n=0
˜s(n)ˆsj(n)
(7.71)
Assuming that the vectors ˆsi(n) are made independent and orthogonal, si(n),
then,
R(i, j) =
L−1

n=0
si(n)sj(n)

 = 0
for i ̸= j
̸= 0
for i = j
(7.72)

Code-Excited Linear Predictive Coding
243
Thus, substituting equation (7.72) into equation (7.69) and rearranging, the ai
can be calculated as,
ai = φ(i)/R(i, i),
i = 1, 2, . . . , M
(7.73)
The best estimate for ˜s(n) is then given by,
ˆsopt(n) =
M

i=1
aisi(n),
n = 0, 1, . . . , L −1
(7.74)
Theaboveprocedurecomprisesaminimummean square error approximation
in which the optimum solution can be derived only if the set of sequences ˆsi(n)
are linearly independent (i.e. form a basis) and are orthogonal to each other.
A popular method for achieving this orthogonalization is the Gram–Schmidt
procedure, summarized below.
Consider a set of m + 1 vectors pi(n) each of length L which form a basis.
The objective is to construct orthogonal vectors qi(n) so that
L−1

n=0
qi(n)qj(n)

 = 0
for i ̸= j
̸= 0
for i = j,
i, j = 0, 1, . . . , m
(7.75)
Let q0(n) = p0(n) and deﬁne q1(n) as a linear combination of q0(n) and p1(n),
then
q1(n) = p1(n) −α01q0(n)
(7.76)
Then for q1(n) to be orthogonal to q0(n),
L−1

n=0
q1(n)q0(n) = 0
(7.77)
i.e.
L−1

n=0
p1(n)q0(n) −
L−1

n=0
α01q2
0(n) = 0
(7.78)
and the correlation α01 is given by,
α01 =
L−1

n=0
p1(n)q0(n)
L−1

n=0
q2
0(n)
(7.79)

244
Analysis by Synthesis LPC Coding
With this α01, the two functions q0(n) and q1(n) are now orthogonal. In order
to build the other functions qi(n), linearly independent pi(n) are added one at
a time until all are constructed. This can be formulated in general as
qk(n) = pk(n) −
k−1

j=0
αjkqj(n)
(7.80)
where,
αjk =
L−1

n=0
pk(n)qj(n)
L−1

n=0
q2
j (n)
,
j = 0, 1, . . . , k −1
(7.81)
Therefore, the basic task in VSE is to obtain ˆsopt(n) by deriving the set of
orthogonal vectors si(n) and their optimum scaling factors ai. The optimum
excitation xopt(n) can then be derived by passing ˆsopt(n) through the combined
LPC and perceptual-weighting inverse ﬁlter.
In VSE, it is very important to construct the basis vectors in a perceptu-
ally advantageous way and training is required. As in CELP with Gaussian
excitation, in VSE linear prediction (VSELP), the pitch ﬁlter is treated as an
adaptive codebook for lag values less than the subframe size and, hence,
it is not included in the codebook search process. Finally, the total excita-
tion is obtained by adding the gain-scaled secondary excitation Gxopt(n) to
the pitch predictor excitation. A block diagram of vector processing of a
VSELP coder where the scaling factors ai are assumed to be ±1 is shown in
Figure 7.23.
The M basis vectors, [xi(n)]M
i=1, are ﬁrst synthesized to give M synthetic
basis vectors [ˆsi(n)]M
i=1. These are then made orthogonal to the pitch predictor
contribution signal via the Gram–Schmidt orthogonalization process to get
[si(n)]M
i=1. This ensures that the secondary excitation does not cover the vector
space that has already been covered by the pitch predictor contribution.
After orthogonalization, the individual basis vector scaling factors ai are
computed to form a Mth element vector, a. The vector a is then quantized
as ±1. The ﬁnal synthetic signal from the excitation, ˆsopt(n), is obtained by
ﬁrst summing up the properly signed (±1) orthogonal basis vectors and
then gain scaling the summed vector. In order to obtain the quantized
secondary excitation, ˆsopt(n) is then inverse-ﬁltered with the combined LPC
and perceptual-weighting ﬁlters at the encoder. At the decoder, as there is
no need for the perceptual-weighting ﬁltering, only the LPC inverse ﬁltering
is used. The overall scaling factor is obtained as in standard CELP. The ﬁnal

Code-Excited Linear Predictive Coding
245
−1,   1
−1,   1
−1,   1
Original Vectors
Filtered Vectors
VSELP EXCITATION VECTOR PROCESSING
Pitch (LTP)
Output
Decorrelated Vectors
VSELP EXCITATION VECTOR GENERATION
Decorrelated Vectors
Binary
Weighting
Final
Excitation
Vectors
Decorrelator
LPC and
Weighting
Filters
+
Figure 7.23
Block diagram of VSELP vector processing
synthetic speech is obtained by adding the scaled secondary excitation and
the pitch predictor contribution and synthesizing this through the combined
LPC and perceptual-weighting ﬁlters with initial memories restored. Note
that the orthogonalization procedure and inverse ﬁltering is required both at
the encoder and the decoder.
Algebraic Codebook Excitation
As discussed earlier multi-pulse excitation is very useful in tracking the
changes in speech accurately. Its main disadvantage is the number of bits
required to encode its pulse positions as well amplitudes. Regular pulse exci-
tation, on the other hand, is a very restricted version of multi-pulse excitation
in terms of pulse positions which may degrade speech quality if decimation
factors of ﬁve or more are used. The codebook types discussed above are
restrictive both in terms of pulse positions and amplitudes, i.e. the codebook
contains preset vectors. The VSE makes an attempt to modify the vectors by
the pitch predictor contribution but still has ﬁxed basis vectors. Although
they are very efﬁcient in coding capacity, they may suffer from quality
degradations especially in speech transitions where the pitch predictor fails
to perform its usual function adequately. Algebraic codebooks have over-
come these problems by cleverly representing excitation pulses where they
have some freedom in position [25, 26]. In algebraic codebooks, only a small
number of pulses are used and they are positioned in interleaved tracts (for
efﬁcient coding); hence, although each pulse position is severely restricted,

246
Analysis by Synthesis LPC Coding
Table 7.5
Typical
5-pulse
algebraic
codebook
tracks for a 40-sample subframe
Track
Pulse number
Possible locations
1
i0
0,5,10,15,20,25,30,35
2
i1
1,6,11,16,21,26,31,36
3
i2
2,7,12,17,22,27,32,37
4
i3
3,8,13,18,23,28,33,38
5
i4
4,9,14,19,24,29,34,39
together they are able to form most of the combinations necessary for ade-
quate excitation. Since the selected pulse positions will usually correspond
to the remaining major pulses, which will usually have somewhat similar
magnitudes (expected after removing the pitch predictor contribution), the
pulse amplitudes are also restricted to having the same amplitude, usually set
to ±1. However, in order to have efﬁcient coding of the formations (indices)
of the excitation vectors and enable fast search, the overall combination of the
nonzero samples is usually restricted to four or ﬁve interleaved tracks. Only
one or two nonzero pulses with either positive or negative signs are placed in
each track. Table 7.5 shows typical ﬁve-pulse interleaved track positions in a
40-sample excitation subframe. Using the possibilities shown in Table 7.5, the
codebook vector x(n) is formed by setting only ﬁve unity pulses in a possible
40-sample vector with all other locations being set to zero.
x(n) = s0δ(n −m0) + s1δ(n −m1) + s2δ(n −m2) + s3δ(n −m3) + s4δ(n −m4),
n = 0, . . . , 39
(7.82)
where si and mi are the sign and position of the ith pulse and δ(0) represents
unity pulse amplitude.
The total possible number of excitation vector combinations that an alge-
braic codebook can produce is quite large. Therefore full searching of
all possible excitations becomes prohibitive for real-time implementations.
However, algebraic codebooks are designed to reduce this complexity sig-
niﬁcantly. Having got the synthetic output for each excitation vector, the
cross-correlation of the synthesized signal with the target signal (LPC and
perceptual-weighting ﬁlters’ memory response and the pitch predictor con-
tribution removed from weighted input speech) and the synthesized signal
energy need to be computed. The best excitation sequence is then selected by
maximizing:
Ak = (dtxk)2
xt
kxk
(7.83)

Code-Excited Linear Predictive Coding
247
where d = Ht˜s is the correlation matrix between the target signal ˜s(n) and the
combined LPC and perceptual-weighting ﬁlter impulse response h(n), xk is
the kth excitation vector, H is the lower triangular Toeplitz convolution matrix
with diagonal h(0), h(1), . . . , h(39), and  = HtH is the matrix correlation of
h(n). Before the ﬁxed codebook search starts, both d and  are computed,
d(n) =
39

i=n
˜s(i)h(i −n)
;
n = 0, . . . , 39
(7.84)
φ(i, j) =
39

n=j
h(n −i)h(n −j)
;
j ≥i
(7.85)
Once the correlation of the impulse response, h(n), and the target signal,
˜s(n), is computed for every possible pulse position, the overall correlation
(i.e. the numerator of equation (7.83)) becomes a simple summation of the
correlations at only nonzero excitation pulse positions.
C =
Np−1

i=0
pid(mi)
(7.86)
where Np is the number of pulses, pi = siδ(0), in the excitation. The denomi-
nator of equation (7.83) is given by:
D =
Np−1

i=0
φ(mi, mi) + 2
Np−2

i=0
Np−1

j=i+1
pipjφ(mi, mj)
(7.87)
However the above equation can be simpliﬁed signiﬁcantly if we assume
that the pulses pi have unity amplitudes. Before the codebook search, d(n) is
decomposed into its absolute value |d(n)| and sign sign[d(n)]. Using the sign
information, φ is modiﬁed,
φ′(i, j) = sign[d(i)]sign[d(j)]φ(i, j),
i = 0, . . . , 39, j = i + 1, . . . , 39.
(7.88)
The main-diagonal elements of φ are scaled to remove the factor of two in
equation (7.87).
φ′(i, i) = 0.5φ′(i, i)
i = 0, . . . , 39
(7.89)
The correlation in equation (7.86) can now be computed over the nonzero
pulses as (ﬁve nonzero pulses are assumed, to follow the example),
C = |d(m0)| + |d(m1)| + |d(m2)| + |d(m3)| + |d(m4)|
(7.90)

248
Analysis by Synthesis LPC Coding
since the sign is separately coded, the absolute value of d(n) is used
in the above equation. The denominator of equation (7.83) expressed in
equation (7.87) can be computed by;
E/2 = φ′(m0, m0)
+φ′(m1, m1) + φ′(m0, m1)
+φ′(m2, m2) + φ′(m0, m2) + φ′(m1, m2)
+φ′(m3, m3) + φ′(m0, m3) + φ′(m1, m3) + φ′(m2, m3)
+φ′(m4, m4) + φ′(m0, m4) + φ′(m1, m4) + φ′(m2, m4) + φ′(m3, m4)
(7.91)
Having simpliﬁed the search process shown above, further reductions in
search complexity are achieved by a focused search approach. A precomputed
threshold is tested before entering the last loop (in a nested search to locate
the pulses in ﬁve tracks). The threshold determines if the ﬁrst four pulses
have already produced a good combination and whether it is worthwhile
to continue into the last loop. The threshold is based on the correlation
C in equation (7.86). The maximum absolute correlation and the average
correlation produced by the ﬁrst four pulses, max4 and av4, are used to
compute the threshold,
TH4 = av4 + K4(max4 −av4) ; 0 ≤K4 < 1.
(7.92)
The last loop is entered (to search for the ﬁfth pulse) only if the absolute
correlation due to ﬁrst four pulses exceeds TH4. K4 controls the percentage
of combinations searched (set to 0.4 in G.729 for the fourth loop where four
nonzero pulses are used). Since this will result in variable search computation,
the last loop is entered only a predetermined ﬁxed number of times.
As explained above, an algebraic codebook is an excellent compromise
between the very restricted regular pulse excitation and multi-pulse exci-
tation. Algebraic codebooks are computationally very efﬁcient as well as
producing good performance. However the combinations of the interleaved
track positions still require a large number of bits. For example in a 40-sample
subframe using ﬁve tracks with eight locations in each, the pulse positions
would require three bits each plus a sign bit, giving a total of 20 bits per
subframe. This will result in 20 × 8000
40 = 4 kb/s which is only applicable to bit
rates of around 6 kb/s and above. Algebraic codebooks have been extensively
used in many CELP coders such as G.729, EFR, G.723.1, etc.
Pitch Adaptive Mixed Excitation
In the case of Gaussian codebooks, it is assumed that the size of the codebook
is large enough to cater for both voiced and unvoiced speech excitations. In
the VSE case, orthogonalization with respect to the pitch predictor output
enables the secondary excitation vectors to cover the space that is not covered

Code-Excited Linear Predictive Coding
249
by the pitch prediction, resulting in a better system. However at low bit-rates
(increased vector sizes), during voiced onsets and transitions where the pitch
cannot build up fast enough to track the changes, the speech quality dete-
riorates signiﬁcantly. The advantage of algebraic codebooks also reduces at
low bit-rates (i.e. at around 4.8 kb/s) as the number of pulse combinations
need to be severely restricted in order to allocate fewer bits for the secondary
excitation which results in distorted speech. Other important issue at low
bit-rates is the amount of noise added to speech from the secondary excitation
during steady state voiced regions. A constrained gain approach [27] helps to
produce cleaner voiced speech by limiting the power of secondary excitation
during steady state voiced regions. This section describes an adaptive code-
book excitation where the excitation pulse-positioning is made adaptive with
the pitch lag computed for the same subframe. This can be seen as a subset
of the algebraic codebook approach where the pulse positions are severely
restricted but made adaptive with respect to the pitch so as to increase their
chances of positioning them to locations where they are needed most.
In pitch adaptive mixed excitation (PAME), the static codebook is split into
two parts. The ﬁrst part is made adaptive with respect to the pitch lag as
follows. The excitation buffer is ﬁlled with a unit sample amplitude every D
samples starting from the ﬁrst location. The rest of the vector elements are set
to zero. During the search of the codebook, this vector is synthesized and its
phase position is determined by shifting its synthetic response one sample at
a time for D −1 times. Each phase position is then treated as a new excitation
vector. In order to guard against pitch-doubling errors in the LTP search, if the
lag D is greater than 2Dmin the same process is applied again by placing the
excitation pulses every D/2 samples. The total number of excitation vectors
searched is then found by adding the total phase positions considered. This
is similar to regular pulse excitation with the decimation factor of D and D/2.
After selecting the best excitation vector from the pitch-adaptive section of
the codebook using Ca phase positions, the search continues in the second
part of the codebook which is ﬁxed and contains centre-clipped overlapping
excitation. Here, a further Cf = C −Ca vectors are searched and the best
performing vector index from the overall search process is transmitted to
the receiver. At the receiver, after decoding the pitch lag, the corresponding
excitation vector is decoded.
By forcing the secondary excitation to have pitch structure, it is possible
to match voiced onsets more accurately. This is because the pitch predictor
memory builds more quickly to track the incoming periodicity more accu-
rately and the secondary excitation provides the required periodicity where
the pitch predictor fails. This, of course, depends on the accurate computation
of the periodicity by the pitch predictor in the ﬁrst place. Many other adapta-
tion schemes may be used to accurately place the secondary excitation pulses
every pitch period. The pitch predictor lag adaptation is useful because it

250
Analysis by Synthesis LPC Coding
does not require extra computation or bits. Encoding and decoding processes
of the codebook index in this algorithm have three possibilities: D ≥L;
L/2 ≤D < L; and D < L/2, assuming Dmin < L/2. The total phase positions
considered in each possibility can be calculated as follows:
1. In the case of D ≥L, there will be a single excitation pulse located in
the ﬁrst position of the secondary excitation vector, hence, a possible L
phase positions will be considered. If the submultiple is also greater than
L, then the process stops. However, if D/2 < L, then L −D/2 more phase
positions will be considered where the excitation vector will have an extra
pulse located at position D/2. Therefore, the total phase positions will
be 2L −D/2.
2. In the case of L/2 ≤D < L the secondary excitation vector will have two
pulses, placed at the ﬁrst and Dth positions. Therefore, the total phase
positions to be considered is D. If D/2 ≥Dmin then, a further D/2 phase
positions will be searched giving a total phase positions of 3D/2.
3. Finally, when D < L/2, the secondary excitation will have pulses at every
D samples starting from the ﬁrst position, resulting in a possible D phase
positions. If, however, D/2 ≥Dmin then a further D/2 phase positions are
considered giving a total of 3D/2.
The above possibilities are indicated to the receiver by the ﬁxed subframe
size and the decoded pitch predictor lag D.
In informal listening comparisons of VSE, centre-clipped Gaussian and
PAME, PAME produced the best result by making the overall speech sharper.
This was the result of the periodic excitation part of the secondary excitation
matching voiced speech faster and hence more accurately. This is illustrated
in Figure 7.24 where, the pitch of a voiced onset is better reproduced by the
pitch adaptive excitation. In this ﬁgure we can also see that PAME tracks
voice changes much faster. It must be noted, however, that the performance
of PAME can be affected if the pitch predictor lag is chosen wrongly in
the ﬁrst place. Therefore, it is important that during the LTP search, the
correct lag or its integer multiples are selected. The dependency of the PAME
performance on the pitch predictor lag can be removed if all the possible lags
(in a subframe) and phase positions are exhaustively searched. This, however,
requires more index values to be coded in the adaptive part of the codebook.
In this case, a set of primary excitation vectors are formed by placing a unit
amplitude pulse at the start of the excitation buffer x, and then after every P
samples. P is varied from Dmin (the smallest possible pitch) to L (the subframe
size) to get all primary vectors. Whilst Dmin is related to the minimum pitch,
it may also be varied to enhance ﬁdelity. Therefore, for each P, the primary

Code-Excited Linear Predictive Coding
251
0.0
500.0
1000.0
1500.0
-12000.0
-10000.0
-8000.0
-6000.0
-4000.0
-2000.0
0.0
2000.0
Amplitude
(a)
(b)
(c)
Time (Samples)
Figure 7.24
Speech waveforms of (a) original, (b) overlapping centre-clipped
Gaussian excited CELP output and, (c) PAME excited CELP output
candidate excitation is derived as follows:
xj(n) =

 1
n = iP < L,
i = 0, 1, 2, . . .
0
otherwise
(7.93)
In order to form all possible phase positions, for each primary vector xj,
P −1 further vectors xj+k, k = 1, 2, . . . , P −1, are derived by shifting as,
xj+k(n) =

 0
n = 0, 1, 2, . . . , k −1
xj(n −k)
n = k, k + 1, . . . , L −1
(7.94)
It should be noted that the number of candidate excitation vectors Ca
depends on L and Dmin such that,
Ca = L +
L/2

I=Dmin
I +
L/2 −1

I=1
I
(7.95)
Thus the number of bits required by the adaptive excitation index range is
I = ⌈log2 Ca⌉. If Ca does not correspond to an integer power of 2, a further
2I −Ca vectors are then searched in the ﬁxed codebook. As with all algebraic

252
Analysis by Synthesis LPC Coding
0.0
500.0
1000.0
1500.0
Time (Samples)
24000.0
26000.0
28000.0
30000.0
32000.0
34000.0
36000.0
38000.0
40000.0
42000.0
44000.0
Amplitude
(a)
(b)
(c)
(d)
(e)
(f)
Figure 7.25
Typical CELP waveform plots: (a) codebook contribution, (b) LTP (pitch)
contribution, (c) LPC memory contribution, (d) total output, (e) original speech, and
(f) final error
codebooks, the above codebook type does not require codebook storage and
all codebook search simpliﬁcations are applicable. Typical CELP waveforms
which use the above excitation are shown in Figure 7.25. The overall rate of
this coder was around 4.8 kb/s.
7.3.5 Joint LTP and Codebook Excitation Computation
In the above analysis, where the pitch predictor contribution and secondary
excitations are computed sequentially, the ﬁnal LPC excitation which is
formed by adding the pitch predictor contribution and secondary excitations
is suboptimum. When the pitch predictor is optimized, the effect of the
secondary excitation is assumed to be zero. Thus, when the secondary

Code-Excited Linear Predictive Coding
253
excitation is computed the pitch prediction excitation becomes suboptimum.
Since the whole idea of AbS is to compute an optimum excitation, the pitch
prediction and secondary excitations should ideally be computed jointly. As
this process would require a huge number of combination (or computation)
possibilities, it has not been applied in practice. However assuming that the
pitch delay D has been selected correctly during the pitch search (which
should be a reasonable assumption for voiced speech), an approximation to
joint optimization of pitch prediction contribution and codebook excitations
can be made by jointly computing their gains. Assuming a single-tap pitch
predictor, the weighted mean squared error for a subframe can be written as,
E(k, D) =
L−1

n=0

sw(n) −sm(n) −
n

i=0
gkxk(n −i)h(i) −
n

i=0
βDˆr(n −i −D)h(i)
	2
(7.96)
Since the zero input memory response of the LPC and perceptual-weighting
ﬁlter, sm(n), cannot be changed, and does not affect the selection of k and D,
we can substitute ˜s(n) in the place of sw(n) −sm(n). Thus,
E(k, D) =
L−1

n=0

˜s(n) −gk
n

i=0
xk(n −i)h(i) −βD
n

i=0
ˆr(n −i −D)h(i)
	2
(7.97)
The above equation is searched for Dmin ≤D ≤Dmax and 0 ≤k ≤C −1. To
further simplify the above equation, substitute,
ZD(n) =
n

i=0
ˆr(n −i −D)h(i)
(7.98)
and,
Vk(n) =
n

i=0
xk(n −i)h(i)
(7.99)
Thus,
E(k, D) =
L−1

n=0
[˜s(n) −gkVk(n) −βDZD(n)]2
(7.100)
In equation (7.100), the variables k and D should be jointly selected such that
with their optimum gains gk and βD, the overall error E(k, D) is minimized.

254
Analysis by Synthesis LPC Coding
To minimize E(k, D), we differentiate equation (7.100) with respect to gk and
βD which are functions of k and D respectively. Thus,
∂E(k, D)
∂gk
=
L−1

n=0
[˜s(n) −gkVk(n) −βDZD(n)] × (−2)Vk(n) = 0
(7.101)
and similarly,
∂E(k, D)
∂βD
=
L−1

n=0
[˜s(n) −gkVk(n) −βDZD(n)] × (−2)ZD(n) = 0
(7.102)
Rearranging the above equations we get
L−1

n=0
˜s(n)Vk(n) = gk
L−1

n=0
V2
k(n) + βD
L−1

n=0
ZD(n)Vk(n)
(7.103)
L−1

n=0
˜s(n)ZD(n) = gk
L−1

n=0
Vk(n)ZD(n) + βD
L−1

n=0
Z2
D(n)
(7.104)
In matrix form,
 gk
βD

=
 G1
G2
G2
G3
−1  K1
K2

(7.105)
where,
G1 =
L−1

n=0
V2
k(n)
(7.106)
G2 =
L−1

n=0
ZD(n)Vk(n)
(7.107)
G3 =
L−1

n=0
Z2
D(n)
(7.108)
K1 =
L−1

n=0
˜s(n)Vk(n)
(7.109)
K2 =
L−1

n=0
˜s(n)ZD(n)
(7.110)

Code-Excited Linear Predictive Coding
255
The gains gk and βD can then be found by solving the above equation as,
gk = K1G3 −K2G2
G1G3 −G2
2
(7.111)
βD = K2G1 −K1G2
G1G3 −G2
2
(7.112)
The variables k and D are searched through all combinations using the
above analysis and the combination giving the minimum error is selected for
transmission. Even though, the speech quality increases signiﬁcantly with
this joint computation of the pitch predictor and secondary excitations, the
search computation required is extremely high. Therefore the pitch lag D is
usually limited to a narrow range (i.e. ±2 samples) around the selected value
during the LTP search (and its submultiples) before the codebook vector is
selected in this joint optimization process.
7.3.6 CELP with Post-Filtering
As discussed earlier, the function of the perceptual weighting ﬁlter is to shape
the noise spectrum so as to hide it under the speech spectrum [2]. However,
at low rates such as 4.8 kb/s, where the average noise level is relatively large,
it is very difﬁcult to suppress the noise below the masking threshold at all
frequencies. Therefore, in order to improve CELP speech quality at lower
bit rates (or, indeed, all rates), further subjective noise reduction techniques
are required.
As CELP is essentially a waveform type speech coder, the coded speech
can be considered to be the original speech corrupted by additive Gaussian-
type noise. Therefore, any speech enhancement technique that deals with
this problem can be used to reduce the noise. One such method is that of
post-ﬁltering [28]. Adaptive post-ﬁltering (APF) has been used successfully in
enhancing ADPCM-coded speech and APC-type schemes [29]. For AbS-LPC
coders, the APF as reported by Chen [30] and given by equation (7.113), has
been widely accepted.
Hapf(z) =
(1 −µz−1)

1 −
p

i=1
aiβiz−i


1 −
p

i=1
aiαiz−i

(7.113)
The function of the APF is to attenuate the components in the spectral
valleys. However, to achieve this successfully, the simple all-pole APF used
in earlier schemes is not adequate. If an all-pole APF is used alone, then,

256
Analysis by Synthesis LPC Coding
g(n)
^so(n)
^sp(n)
^spo(n)
Power
Normal-
isation
Modified
Synthesis
Filter
Modified
Inverse
Filter
High-Pass
Filter
Figure 7.26
Block diagram of the adaptive post-filter
although the perceived noise level is lowered, the output speech is severely
low-pass, giving a mufﬂing effect. In order to compensate for this low-pass
effect the spectral tilt of the all-pole APF can be modiﬁed such that its response
is somewhere between an all-pass response and the signal spectrum. The best
APF combination was found to be that shown in Figure 7.26.
The simple high-pass ﬁlter in the ﬁrst stage provides a slightly high-pass
spectral tilt and thus helps to reduce mufﬂing. The pole-zero second-stage
ﬁlter provides ‘shaping’ of the spectral envelope. Finally, a gain control is
added to scale the post-ﬁltered speech such that it has roughly the same
power as the unﬁltered noisy speech. This is necessary as the cascaded
ﬁlters are not unity gain ﬁlters. One technique used to normalize the output
signal power is to estimate the power of the un-ﬁltered and ﬁltered speech
separately, then determine an appropriate scaling factor based on the ratio
of the two estimated power values. The speech power is estimated by an
exponential-average gain estimator, i.e. the two estimated power values δ2
o
and δ2
p are given by,
δ2
o(n) = ζδ2
o(n −1) + (1 −ζ)ˆs2
o(n)
(7.114)
δ2
p(n) = ζδ2
p(n −1) + (1 −ζ)ˆs2
p(n)
(7.115)
where ˆso(n) is the original synthetic speech and ˆsp(n) is the post-ﬁltered
speech. A suitable leakage factor ζ is 0.96. At each sampling instant, δ2
o(n) and
δ2
p(n) are computed as above, then the ratio and the square root are computed
in order to obtain the gain factor g(n) =

δ2o(n)/δ2p(n). Therefore, the ﬁnal
post-ﬁltered speech is given by
ˆspo(n) = g(n)ˆsp(n)
(7.116)
The above procedure is quite computationally-intensive as it requires a
divide and square root operation per sample. Therefore, instead of the

Code-Excited Linear Predictive Coding
257
0.0
1.0
2.0
3.0
4.0
Frequency (kHz)
3.0
23.0
43.0
Magnitude (dB)
Speech Spectrum
LPC Envelope
Postfilter Response
Figure 7.27
Typical response of APF together with the original LPC envelope and
speech spectrum (µ = 0.3, β = 0.6, and α = 0.9)
sample-by-sample normalization, block-wise normalization can be used, i.e.
sum the values of δ2
o(n) and δ2
p(n) for a block and use the average. Small
block sizes (e.g. 10 samples) generally produce indistinguishable results. The
effect of the APF of Figure 7.26 can be seen in Figure 7.27 alongside a typical
example of the original LPC envelope.
Generally, the following parameter ranges have been found to give reason-
able subjective results:
0.2 ≤µ ≤0.4
(7.117)
0.5 ≤β ≤0.7
(7.118)
0.8 ≤α ≤0.9
(7.119)
The factor µ controls the ‘brightness’ of the speech, and hence larger
values tend to bring in more high-frequency background noise. The factors
β and α control the degree of spectral ﬁltering, and the difference between
the parameters determines the ﬁltering effect. Subjectively, large differences
give quieter speech, but this is usually accompanied by an unnatural ‘deep’
voice effect. Applying APF with the correct subjectively-selected control
parameters to the coders produces signiﬁcant subjective noise reduction

258
Analysis by Synthesis LPC Coding
with almost negligible distortion in the speech. The post-ﬁltered speech is
characterized by its lack of background noise components (quiet room effect)
and increased smoothness for voiced speech. For lower-rate CELPs, this
enhancement to the subjective quality is particularly noticeable: the speech
sounds much cleaner and much more pleasant to listen to. As suggested
in [17], making the high-pass factor, µ, adaptive as
µ = ε|k1|
(7.120)
where |k1| is the modulus of the ﬁrst reﬂection coefﬁcient computed from the
quantized LP parameters and ε is a tuning factor with a typical value of 0.3,
improves the speech quality.
7.4 Summary
Analysis by synthesis coding of speech in the form of MPLPC and CELP has
been very popular for the past couple of decades. At bit rates of 6 kb/s and
above they produce good performance and the various versions reported in
the literature differ mainly on the way the secondary (codebook) excitation
is generated or represented. In early days, random Gaussian numbers were
used to populate the codebooks, but they were complex to store and search
and did not produce the best quality. The use of vector sum excitation
improved the situation both in terms of the cost of implementation and the
overall speech quality. However the most successful CELP coders have been
produced after the invention of algebraic codebooks. ACELPs are currently
being used in many international standards.
Although ACELPs have been very dominant at bit rates of 6 kb/s and above,
they rely heavily on objective measures (although perceptual weighting is
used) and as the bit rate is lowered their quality deteriorates rapidly. It would
therefore be very difﬁcult to produce a toll-quality 4 kb/s CELP coder unless
signiﬁcant modiﬁcations are made to the basic structure described above.
Bibliography
[1] B. Atal and M. Schroeder (1970) ‘Adaptive predictive coding of speech
signals’, in Bell Sys. Technical Journal, pp. 1973–87. October 1970.
[2] M. Schroeder and B. Atal (1979) ‘Predictive coding of speech signals
and subjective error criteria’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 27:247–54.
[3] R. Zelinski and P. Noll (1977) ‘Adaptive transform coding of speech sig-
nals’, inIEEE Trans. on Acoust., Speech and Signal Processing, 25(4):299–309.

Bibliography
259
[4] J. Tribolet and R. Crochiere (1979) ‘Frequency domain coding of speech’,
in IEEE Trans. On Acoust., Speech and Signal Processing, 27:512–30, Octo-
ber.
[5] N. Gouvianakis and C. Xydeas (1987) ‘Advances in analysis-by-synthesis
LPC speech coders’, in Journal of IERE, 57:272–86.
[6] P. Kroon and E. Deprettere (1988) ‘A class of analysis-by-synthesis
predictive coders for high quality speech coding at rates between 4.8
and 16 kbit/s’, in IEEE Journal on Selected Areas in Communications,
6(2):353–363.
[7] L. Rabiner and R. Schafer (1978) Digital Processing of Speech Signals.
Englewood Cliffs, NJ: Prentice-Hall
[8] B. Atal and J. Remde (1982) ‘A new model of LPC excitation for producing
natural-sounding speech at low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 614–17.
[9] J. Chung and R. Schafer (1989) ‘A 4.8 Kbit/s homomorphic vocoder
using analysis by synthesis excitation analysis’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing, pp. 144–7.
[10] M. Schroeder and B. Atal (1985) ‘Code excited linear prediction (CELP):
high quality speech at very low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 937–40. Tampa, FL
[11] R. Rose and T. Barnwell (1986) ‘The self-excited vocoder-alternative
approach to toll quality at 4800 bits/s’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 453–6.
[12] S. Singhal and B. Atal (1989) ‘Amplitude optimization and pitch predic-
tion in multipulse coders’, in IEEE Trans. On Acoust., Speech and Signal
Processing, 37(3):317–27.
[13] P. Kroon, E. Deprettere, and R. Sluyter (1986) ‘Regular-pulse excitation:
A novel approach to effective and efﬁcient multipulse coding of speech’,
in IEEE Trans. on Acoust., Speech and Signal Processing, 34:1054–63.
[14] Offer, D. Malah, and A. Dembo (1989) ‘A uniﬁed framework for LPC
excitation representation in residual speech coders’, in Proc. of Int. Conf.
on Acoust., Speech and Signal Processing, pp. 41–4.
[15] J. H. Chen (1990) ‘High quality 16 kbit/s speech coding with a one-way
delay less than 2 ms’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 453–6.
[16] R. P. Ramachandran and P. Kabal (1989) ‘Pitch prediction ﬁlters in
speech coding’, in IEEE Trans. On Acoust., Speech and Signal Processing,
37:467–78.
[17] W. B. Kleijn, D. J. Krasinski, and R. H. Ketchum (1988) ‘Improved speech
quality and efﬁcient vector quantisation in SELP’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing, pp. 155–8.

260
Analysis by Synthesis LPC Coding
[18] P. Kroon and B. Atal (1990) ‘Pitch predictors with high temporal res-
olution’, in Proc. of Int. Conf. on Acoust., Speech and Signal Processing,
pp. 661–4. Albuquerque, NM, USA
[19] P. Kabal and R. P. Ramachandran (1988) ‘Joint solutions for formant and
pitch predictors in speech processing’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 315–18.
[20] P. Dymarski, N. Moreau, and A. Vigier (1990) ‘Optimal and sub-optimal
algorithms for selecting the excitation in linear predictive coders’, in
Proc. of Int. Conf. on Acoust., Speech and Signal Processing, pp. 485–8.
[21] N. Ireton and C. Xydeas (1989) ‘On improving VEC through the use of
spherical lattice codebooks’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 57–60.
[22] R. Salami (1989) ‘Binary CELP: New approach to CELP coding of speech
without codebooks’, in IEE Electronics Letters, 26(6):401–403.
[23] J. Campbell et al. (1990) ‘The proposed Federal Standard 1016 4800 bit/s
voice coder: CELP’, in Speech Technology, pp. 58–64. April 1990.
[24] I. Gerson and M. Jasiuk (1990) ‘Vector sum excited linear prediction
(VSELP) speech coding at 4.8 kbps’, in Proc. of Int. Mobile Satellite Conf,
pp. 678–83. Ottawa
[25] J.-P. Adoul, P. Mabilleau, M. Delprat, and S. Morissette (1987) ‘Fast
CELP coding based on algebraic codes’, in Int. Conf. on Acoust., Speech
and Signal Processing, pp. 1957–60.
[26] C. Laﬂamme, J.-P. Adoul, H. Su, and S. Morissette (1990) ‘On reduc-
ing computational complexity of codebook search through the use of
algebraic codes’, in Int. Conf. on Acoust., Speech and Signal Processing,
pp. 177–80.
[27] Y. Shoham (1990) ‘Constraint-stochastic excitation coding of speech at
4.8 kb/s’, in Proc. of Int. Conf. on Spoken Language Processing, pp. 645–8.
Kobe
[28] N. Jayant and V. Ramamoorthy (1986) ‘Adaptive postﬁltering of 16 kbit/s
ADPCM speech’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 829–32.
[29] J. Chen and A. Gersho (1987) ‘Real-time vector APC speech coding at
4800 b/s with adaptive postﬁltering’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 2185–8. Dallas, USA
[30] J. H. Chen and A. Gersho (1987) ‘Real-time VAPC Speech Coding at
4800 Bit/s with Adaptive Postﬁltering’, in icassp, pp. 2185–8.

8
Harmonic Speech Coding
8.1 Introduction
A general sinusoidal analysis and synthesis concept was introduced by
McAulay [1] when he developed the Sinusoidal Transform Coder (STC) [2]
to demonstrate the applicability of the technique in low bit-rate speech
coding. Sinusoidal coding does not restrict the component sinusoids of the
synthesized speech to be harmonics of the fundamental frequency. The
frequency tracks of the sinusoids may vary independently of each other.
However in harmonic coding the higher frequency sinusoids are restricted
to be integer multiples of the fundamental frequency [3]. Therefore har-
monic coding can be seen as a subset of a generalized sinusoidal trans-
form coding. At low bit-rates, STC also restricts the frequency tracks to
be harmonics of the fundamental frequency, and deduces the harmonic
phases at the decoder, simply because the available bits are not sufﬁ-
cient to encode the large number of parameters of the general sinusoidal
representation.
The STC was introduced as an alternative to the source ﬁlter model, and
its analysis and synthesis was directly applied to the original speech signal.
The binary voicing decision of the source ﬁlter model is one of its major
limitations. The STC employs a more general mixed-voicing scheme by
separating the speech spectrum into voiced and unvoiced components, using
a voicing transition frequency above which the spectrum is declared unvoiced.
However, one of the most recent harmonic coders operates in the LPC
residual domain, i.e. Split Band LPC (SB-LPC) [4]. SB-LPC replaces the binary
excitation of the source-ﬁlter model with a more general mixed excitation, and
ﬁlters the excitation signal using an LPC ﬁlter. The LPC residual has a simpler
phase spectrum than the original speech. The residual harmonic phases
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

262
Harmonic Speech Coding
can be approximated by using the integrals of the component frequencies.
Moreover, LPC models the large variation in the speech magnitude spectrum
and simpliﬁes the harmonic amplitude quantization.
8.2 Sinusoidal Analysis and Synthesis
Figure 8.1 depicts block diagrams of the sinusoidal analysis and synthesis
processes introduced by McAulay. The speech spectrum is estimated by
windowing the input speech signal using a Hamming window and then
computing the Discrete Fourier Transform (DFT). The frequencies, ampli-
tudes, and phases corresponding to the peaks of the magnitude spectrum
become the model parameters of the sinusoidal representation. Employing a
pitch-adaptive analysis window length of two and a half times the average
pitch improves the accuracy of peak estimation. The synthesizer generates
the sine waves corresponding to the estimated frequencies and phases, and
modulates them using the amplitudes. Then all the sinusoids are summed
to produce the synthesized speech. The block edge effects are smoothed
out by applying overlap and add, using a triangular window. Overlap and
add is effectively a simple interpolation technique and, in sinusoidal synthe-
sis, it requires parameter update rates of at least every 10–15 ms for good
quality speech synthesis. At lower frame rates the spectral peaks need to
be properly aligned between the analysis frames to form frequency tracks.
The amplitudes of the frequency tracks are linearly interpolated, and the
instantaneous phases are generated using a cubic polynomial [1] as shown in
Figure 8.2.
DFT 
Magnitude
spectrum 
Phases of the
spectral peaks
Peak
picking
Window
Input speech 
Phases
Frequencies
Amplitudes
Sinusoidal speech analysis 
Sine wave
generator
Amplitudes
Frequencies
Phases
Sum all
sine waves
Overlap and
add
Sinusoidal speech synthesis
Synthetic speech
Figure 8.1
General sinusoidal analysis and synthesis

Parameter Estimation
263
Sine wave
generator
 
 
Amplitudes
Frequencies
Phases
(n)
Sum all
sine waves
Synthetic speech 
Linear
interpolation
Cubic phase
interpolation
A(n)
θ
Figure 8.2
Sinusoidal synthesis with matched frequency tracks
8.3 Parameter Estimation
Low bit-rate sinusoidal coders estimate the amplitudes at the harmonics of
the fundamental frequency. At low bit-rates, the harmonic phases are not
transmitted. Instead the phases are deduced from the spectral envelope on the
assumption that it is the gain response of a minimum phase transfer function
and added to the integrals of the component frequencies. STC implements
the harmonic phases explicitly and LPC-based coders implement the phases
implicitly through the time-domain LPC synthesis ﬁlter. Improved multi-
band excitation (IMBE) coders do not use any kind of phase information
and the phases are evolved as the integrals of the component harmonic
frequencies. Restricting the component frequencies to the harmonics and
modelling the phases at the decoder is well suited for stationary voiced
segments of speech. However, in general, the speech signal is not stationary
voiced and consists of a mixture of voiced and unvoiced segments. When
those segments are synthesized with the phase models described above,
the synthesized speech sounds buzzy. In order to remove this ‘buzzyness’
the concept of frequency-domain voicing was introduced into low bit-rate
harmonic coders [5]. Frequency-domain voicing allows the synthesis of mixed
voiced signals, by separating the speech spectrum into frequency bands
marked as either voiced or unvoiced.
Frequency-domain voicing decisions are usually made for each harmonic
of the speech spectrum. Therefore, an accurate pitch estimate is a prerequisite
of harmonic amplitude and voicing determination. The frequency-domain
voicing determination techniques based on spectral matching need a high
precision pitch estimate for good performance. A small error in the pitch
will cause large deviations at the high frequency harmonics, and subse-
quent declaration of them as unvoiced. Furthermore, female voices with
short pitch periods are more sensitive to small pitch error. In order to
reduce the complexity of a high-precision pitch estimation, an initial pitch
estimate is usually further reﬁned by performing a limited search around
the initial estimate. Having determined an accurate pitch the harmonic
coding usually proceeds with voicing and spectral amplitude estimation
processes.

264
Harmonic Speech Coding
8.3.1 Voicing Determination
There are many ways of performing the voicing classiﬁcation of speech,
which was discussed in Chapter 6, but here we brieﬂy summarize two
common techniques.
Multi-Band Approach
Harmonic voicing is estimated by computing the normalized mean squared
error of a synthetic voiced spectrum, ˆSw (ω, ω0), with respect to the speech
spectrum, Sw(ω), and comparing it against a threshold function for each
harmonic band [6]. The normalized mean squared error, Dk, of the kth
harmonic band is given by,
Dk =
(k+0.5)ω0

(k−0.5)ω0

Sw (ω) −ˆSw (ω, ω0)
2
dω
(k+0.5)ω0

(k−0.5)ω0
S2
w (ω) dω
for
k = 1, 2, . . . , K
(8.1)
where K = ⌊π/ω0⌋and ω0 is the normalized fundamental frequency.
Figure 8.3 illustrates Dk values of two speech spectra with the corresponding
synthetic spectra. If Dk is below the threshold function, i.e. a small error
and a good spectral match, the kth band is declared voiced. The initial multi-
0
(a)
(b)
1000
2000
3000
4000
frequency (Hz)
0
1
2
3
normalized spectral error
(a)
(b)
(c)
0
1000
2000
3000
4000
frequency (Hz)
0
1
2
3
normalized spectral error
(a)
(b)
(c)
Figure 8.3
Two speech spectra: (a) original spectrum Sw(ω), (b) synthetic spectrum
ˆSw(ω, ω0), and (c) normalized Dk

Parameter Estimation
265
band excitation (MBE) coders used a constant threshold for all the bands.
However the most recent versions use several heuristic rules to obtain a
better performance [7], e.g. as the frequency increases the threshold function
is decreased, if the same band of the previous frame was unvoiced, if the
high-frequency energy exceeds the low-frequency energy, and if the speech
energy approaches the energy of the background noise.
Sinusoidal Model Approach
McAulay et al. proposed a different voicing determination technique for his
sinusoidal transform coder (STC) [2]. The speech spectrum is divided into
two bands, determined by a voicing transition frequency above which the
spectrum is declared unvoiced. This method estimates the similarity between
the harmonically-synthesized signal, ˆs(n, ω0), and the original speech signal
s(n). The signal to noise ratio (SNR), δ, between s(n) and ˆs(n, ω0) is given by,
δ =
N−1

n=0
s2 (n)
N−1

n=0

s (n) −ˆs (n, ω0)
2
(8.2)
where N is the analysis frame length and ˆs(n, ω0) is given by
ˆs (n, ω0) =
K(ω0)

l=1
Al exp

jnlω0 + jθl

(8.3)
where the harmonic amplitudes, Al, are obtained from the spectral envelope
and θl are the harmonic phases. McAulay simpliﬁed equation (8.2) for reduced
computational complexity, and the simpliﬁed δ is given by,
δ =
L

l=1
A2
l
L

l=1
A2
l −2Nρ (ω0)
(8.4)
where Al are the harmonic-frequency spectral amplitudes of the original
signal as shown below,
s (n) =
L

l=1
Al exp

jnωl + jφl

(8.5)

266
Harmonic Speech Coding
and ρ (ω0) is given by,
ρ (ω0) =
K(ω0)

l=1
Al
	
maxl [AlD (ωl −kω0)] −1
2Al

(8.6)
where K (ω0) = ⌊π/ω0⌋,
D (ωl −kω0) =
sin

2π ωl−kω0
ω0

2π ωl−kω0
ω0
for
ωl −kω0
 ≤ω0
2
(8.7)
and D (ωl −kω0) = 0 otherwise.
The voicing level (probability), Lv(δ) (i.e. the ratio of the voiced bandwidth
to the speech bandwidth, 0 ≤Lv (δ) ≤1), is deﬁned as,
Lv (δ) =



1
δ > 13 dB
1
9 (δ −4)
4 dB ≤δ ≤13 dB
0
δ < 4 dB
(8.8)
The advantage of estimating the voicing for independent bands is that it essen-
tially removes the spectral tilt, i.e. all the components are equally weighted.
When the voicing is based on a single metric, i.e. δ, the large amplitudes
contribute more to the overall decision. If they have been corrupted by back-
ground noise, it may result in a large voicing error [2]. Therefore, the voicing
estimates based on independent bands are more robust against background
noise.
8.3.2 Harmonic Amplitude Estimation
The harmonic coding algorithms require the spectral amplitudes of the
harmonics, which can be estimated in a number of ways.
Peak-picking of the Magnitude Spectrum
Harmonic amplitudes may be estimated by simple peak-picking of the
magnitude spectrum and searching for the largest peak in each harmonic
band. The peak amplitude value, Sw(mk) should be normalized by a factor
depending on the window function used, as follows:
Ak = |Sw (mk)|
κ
for
−ω0
2 < 2π
N mk −kω0 < ω0
2
and k = 1, 2, . . . , K (8.9)

Parameter Estimation
267
where ω0 is the normalized fundamental frequency, K = ⌊π/ω0⌋, κ =
N−1

n=0
w (n), w(n) is the window function, N is the length of the window,
and Sw(m), the windowed speech spectrum, is given by,
Sw (m) =
N−1

n=0
s (n) w (n) e−j 2π
N mn
for m = 0, 1, 2, . . . , N
(8.10)
Spectral Correlation
Harmonic amplitudes may be estimated by computing the normalized cross-
correlation between the harmonic lobes of the speech spectrum and the main
lobe of the window spectrum. This method is based on the fact that the
spectrum of the windowed speech is equivalent to the convolution between
the speech spectrum and the window spectrum. It is also assumed that the
speech signal is stationary during the windowed segment and the spectral
leakage due to the side lobes of the window spectrum is negligible.
Ak =
bk−1

m=ak
Sw (m) W∗(2πm/N −kω0)
bk−1

m=ak
W2 (2πm/N −kω0)
for k = 1, 2, . . . , K
(8.11)
where ak = max

N
2π

k −1
2

ω0

, 0

and bk = min [ak+1, N/2], and W(ω) is
the spectrum of the window function, given by,
W (ω) =
N−1

n=0
w (n) e−jωn
(8.12)
In practice, W(ω) is computed with a high-resolution FFT, e.g. 214 samples,
by zero-padding the window function, and stored in a lookup table. The
high-resolution FFT is required because, in general, the spectral samples m of
Sw(m) do not coincide with the harmonic locations, kω0, of the fundamental
frequency. Hence W(ω) is shifted to the harmonic frequency and down-
sampled to coincide with the corresponding spectral samples of Sw(m), as
shown in equation (8.11). W(ω) is pre-computed and stored in order to reduce
the computational complexity.
The spectral cross-correlation-based amplitude estimation gives the opti-
mum gain of the harmonic lobes with respect to the main lobe of the window
spectrum, hence it is a more accurate estimate than the simple peak-picking.

268
Harmonic Speech Coding
However the cross-correlation-based method has a higher complexity and
requires a high-precision pitch estimate.
The unvoiced amplitudes are calculated as the rms spectral energy over the
unvoiced spectral bandwidth, given by,
Ak uv = 1
κ







bk

m=ak
S2
w (m)
bk −ak
(8.13)
The harmonic amplitude estimation techniques described may be applied to
either the speech spectrum or the LPC residual spectrum.
8.4 Common Harmonic Coders
This section describes three examples of low bit-rate harmonic coders: sinu-
soidal transform coding (STC) [2], improved multi-band excitation (IMBE) [8],
and split-band linear predictive coding (SB-LPC) [4]. The STC and IMBE apply
sinusoidal analysis and synthesis techniques to the original speech signal and
SB-LPC uses the LPC residual signal. All three examples restrict the synthesis
of sinusoidal components to be harmonics of the fundamental frequency.
8.4.1 Sinusoidal Transform Coding
The sinusoidal transform coding (STC) operating at 4.8 kb/s divides the
speech spectrum into two voicing bands using the sinusoidal model approach
described in Section 8.3.1. The lower part of the spectrum, which is declared
as voiced, is synthesized as follows:
ˆsv (n) =
Lv

l=1
A(lωk
0) exp

jlφ0 (n) + jφs

lωk
0

for −N/2 ≤n ≤N/2 (8.14)
where
φ0 (n) = nωk
0 + φk
0
(8.15)
and
φk
0 = φk−1
0
+

ωk−1
0
+ ωk
0

N′
2
(8.16)
where N + 1 is the frame length, ωk
0 is the normalized fundamental frequency
of the kth frame, N′ is the duration between the analysis points, A(ω) is
the spectral envelope obtained by interpolating the selected peaks of the
magnitude spectrum, φs(ω) is the phase spectrum derived from the spectral

Common Harmonic Coders
269
envelope on the assumption that it is the gain response of a minimum phase
transfer function, and Lv is the harmonic just below the voicing transition
frequency.
The upper part of the spectrum, which is declared as unvoiced, is synthe-
sized as follows:
ˆsuv (n) =
K

ωk
0


l=Lv+1
A

lωk
0

exp

jlφ0 (n) + jφs

lωk
0

+ jU [−π, π]

(8.17)
where K(ωk
0) =

π

ωk
0

and U [−π, π] denotes a uniformly distributed ran-
dom variable in the range −π and π. When a frame is fully unvoiced the
pitch estimate is meaningless and pitch frequencies greater than 150 Hz may
degrade the perceptual quality of unvoiced speech. In order to synthesize
the noise-like unvoiced speech with adequate quality, the number of sinu-
soids with random phases should be sufﬁciently large. Therefore, the pitch
frequency is set to 100 Hz for unvoiced speech. The synthesized speech of the
kth frame is then given by,
ˆs (n) = ˆsv (n) + ˆsuv (n)
(8.18)
The overlap and add method is used with a triangular window to produce
the ﬁnal speech output. Therefore, the frame length is equal to twice the
duration between the analysis points, i.e. N = 2N′. The frequency response
of the spectral envelope is given by,
H (ω) = A (ω) exp

jφs (ω)

(8.19)
which is approximated by an all-pole model,
H (ω) ∼=
g
1 −
p

i=1
aiz−i
for
|z| = 1
(8.20)
where g is the gain and ai are the predictor coefﬁcients. The conventional
time-domain all-pole LPC analysis is performed on the original speech signal
and the maximum ﬁlter order is usually limited to half the smallest pitch
period. The limitation is imposed so that the LPC models the formant spectral
envelope, since LPC ﬁlters with a large number of taps tend to resolve the
harmonic structure. However in the case of STC, all-pole modelling is applied
to the estimated spectral envelope. Hence, the ﬁlter order is not restricted
and can be increased depending only on the desired accuracy of the spectral

270
Harmonic Speech Coding
envelope and the bit rate. The 4.8 kb/s STC uses a 14th-order all-pole model
and quantizes the predictor coefﬁcients in the LSF domain. In addition to the
LSFs, the STC transmits gain, pitch, and voicing.
8.4.2 Improved Multi-Band Excitation, INMARSAT-M Version
Improved
multi-band
excitation
(IMBE)
operating
at
4.15 kb/s
for
INMARSAT-M divides the speech spectrum into several voiced and unvoiced
frequency bands, using the multi-band approach described in Section 8.3.1.
However, IMBE makes the voicing decisions for groups of three harmonics
and a single bit is allocated for each group. The total number of voicing bits
Bv is limited to a maximum of 12 and the harmonics beyond the coverage of
voicing are declared unvoiced. The reﬁned pitch is transmitted using eight
bits. The frame length is 20 ms giving 83 bits per frame at 4.15 kb/s and the
remaining bits, i.e. 83−8−Bv, are allocated for spectral amplitudes. The voiced
amplitudes are estimated using equation (8.11) and the unvoiced amplitudes
are estimated using equation (8.13). The voiced bands are synthesized as
follows:
ˆsv (n) =

k=voiced
Ak cos (kφ0 (n))
for n = 0, 1, 2, . . . , N −1
(8.21)
where N is the frame length and the fundamental phase evolution, φ0(n), is
deﬁned by the following equations:
φ0 (n) = φ0 (n −1) + ω0 (n)
(8.22)
ω0 (n) = 1
N (N −n) ωl−1
0
+ nωl
0
(8.23)
where φ0(−1) is φ0(N −1) of the previous frame and ωl
0 is the normalized
fundamental frequency estimated at the end of thelth frame. The amplitudes of
the voiced harmonics are linearly interpolated between the analysis points. If
the corresponding harmonic of one analysis point does not exist or is declared
unvoiced then its amplitude is set to zero and the harmonic frequency stays
constant (set to the frequency of the existing voiced harmonic). However
if the pitch estimate is not steady, neither the pitch nor the amplitudes are
interpolated for any harmonics; instead overlap and add method is used.
The unvoiced component is synthesized using ﬁltered white Gaussian
noise. White noise is generated in the time domain and transformed into the
frequency domain; the bands corresponding to the voiced components are
set to zero; and the unvoiced bands are scaled according to the unvoiced
gain factors. The inverse Fourier transform of the modiﬁed spectrum gives
the unvoiced component, ˆsuv(n), which is produced using the overlap and

Common Harmonic Coders
271
0
(a)
(b)
0.5
100
150
200
samples
(a)
(b)
(c)
amplitude
0
0.5
100
150
200
samples
amplitude
(a)
(b)
(c)
Figure 8.4
Harmonic speech synthesis: (a) original speech, (b) original harmonic
phases, and (c) IMBE
add method with the unvoiced part of the preceding frame. The synthesized
speech ˆs(n) is then given by,
ˆs (n) = ˆsv (n) + ˆsuv (n)
(8.24)
An interesting feature of the IMBE coder is its simple phase model. The
fundamental phase is computed as the integral of the linearly-interpolated
pitch frequency, and the multiples of the fundamental phase are used as the
harmonic phases. The effect of this phase model is illustrated in Figure 8.4.
The coherent phase model used in IMBE concentrates the speech energy at
the phase locations corresponding to the multiples of 2π of the fundamental
phase. For reference, the speech waveforms synthesized using the original
harmonic phases are also shown and they are very similar to the original
speech waveforms.
8.4.3 Split-Band Linear Predictive Coding
The split-band linear predictive coding (SB-LPC) coder operating at 4 kb/s
employs time-domain LPC ﬁltering and uses a multi-band type of excitation
signal. However the excitation signal of SB-LPC consists of only two bands,
separated by a frequency marker, below which the spectrum is declared
voiced and above which it is declared unvoiced. The estimation of the
frequency marker of SB-LPC is different from the technique used in STC.
The SB-LPC estimates a voicing decision for each harmonic band using
a similar multi-band approach described in Section 8.3.1. The estimated
voicing decisions are used to determine the voicing frequency marker, which
has eight possible equally-spaced locations in the spectrum, the ﬁrst being

272
Harmonic Speech Coding
fully unvoiced and the last being fully voiced. One method of deciding the
frequency marker is placing it at the end of the last voiced harmonic of
the spectrum, i.e. all the voiced harmonics are included in the voiced band
of the spectrum. A better solution for determining the frequency marker,
based on a soft decision process is described in [9]. The harmonic amplitudes
are estimated using equations (8.11) and (8.13) for voiced and unvoiced
harmonics respectively, however the LPC residual is used instead of the
speech signal. The LPC parameters are quantized and interpolated in the LSF
domain. The shape of the harmonic amplitudes is vector-quantized and the
gain is scalar-quantized separately.
At the receiving end, speech is synthesized with parameter interpolation
based on pitch cycle waveform (PCW). First, intermediate PCWs for the cur-
rent subframe are generated by interpolating the quantized model parameters
of the last and current subframes. The excitation signal ei(n), 0 ≤n < T0,i, for
the ith PCW is produced as
ei(n + ni) =
Vc

l=1
Ae,i(l) cos{lω0,i(n −ni)}
+
H

l=Vc+1
Ae,i(l) cos{lω0,i(n −ni) + U[−π, π]}
(8.25)
where H is the total number of harmonics, ω0,i = 2π/T0,i and U[−π, π]
denotes a random number with uniform distribution between −π and π. The
start position ni for the ith PCW is given by
ni = n0 +
i−1

j=0
T0,j
(8.26)
where n0 is the start position corresponding to the last position of the previous
subframe. The interpolated pitch T0,i for the ith PCW is calculated as
T0,i = αiT(t−1)
0
+ (1 −αi)T(t)
0
(8.27)
where T(t)
0 is the received pitch of the tth subframe. The interpolation factor αi
is deﬁned as
αi =
G(t)Ni
G(t−1)(N −Ni) + G(t)Ni
(8.28)
where N is the subframe size, G(·) is the received gain, and Ni is the PCW
position deﬁned by,
Ni = ni + 0.25(T(t−1)
0
+ T(t)
0 )
(8.29)

Common Harmonic Coders
273
The starting position n(t+1)
0
for the next subframe is updated as
n(t+1)
0
= (n(t)
I
+ T(t)
0,I)%N
(8.30)
where % is the modulo operator and I is the total number of PCWs. The
voicing cut-off index, Vc, is given by
Vc = max{V(t−1)
c
, V(t)
c }
(8.31)
The interpolated amplitude, Ae,i(l), for the lth harmonic is computed as
Ae,i(l) =



αiA(t−1)
e
(l) + (1 −αi)A(t)
e (l),
if V(t−1)(l) = V(t)(l),
A(t−1)
e
(l),
if V(t−1)(l) = 1 & V(t)(l) = 0
A(t)
e (l),
if V(t−1)(l) = 0 & V(t)(l) = 1
(8.32)
where V(·)(l) is the voicing information for the lth harmonic and 1 and 0 in
the voicing comparison denote voiced and unvoiced, respectively. The LPC
coefﬁcient for the ith PCW is interpolated in the same way as, obtaining
the interpolated pitch. Finally, the normalized speech signal ˜si(n) is recon-
structed by exciting the LPC synthesis ﬁlter hi(n) with the signal ei(n) in
equation (8.25), as
˜si(n) = ei(n) ∗hi(n)
(8.33)
where ∗is the convolution operator. In calculation of ˜si(n), the required
memory for ei(n), n < 0, can be obtained from ei−1(n) or the excitation signal
of the last subframe. The synthesized speech signal si(n) for the ith PCW is
produced by compensating for the gain as
si(n) =







T0,i
T0,i−1

n=0
˜s2
i (n)
Gi˜si(n)
(8.34)
where Gi is the interpolated gain based on the relative position of the PCW in
the subframe. Concatenation of each PCW in equation (8.34) forms the ﬁnal
speech signal.
The above description of excitation generation is based on the sinusoidal
synthesis of voiced and random noise generation of unvoiced parts of the
excitation. However, in practice, a DFT-based method (with the DFT size
equal to the pitch period), where the unvoiced frequencies would have
random phases, can be used to generate both voiced and unvoiced parts
jointly [10, 11].

274
Harmonic Speech Coding
LSFs
Voicing frequency
 marker
Amplitudes
Pitch 
LSF
Interpolation
Voiced
Excitation
Unvoiced
Excitation
LPC
Synthesis
Filter
LSF to LPC
Transform
Synthetic
Speech
Figure 8.5
SB-LPC decoder
Table 8.1
Bit allocation of 4 kb/s
SB-LPC coder for a 20 ms frame
Parameter
Bits
LSFs
23
Pitch
5 + 7
Parity bit
1
Voicing
3 + 3
Gain
5 + 5
Harmonic amplitudes
14 + 14
100
(a)
50
0
150
200
samples
100
(b)
50
0
150
200
samples
(a)
(b)
amplitude
(a)
(b)
amplitude
Figure 8.6
Harmonic speech synthesis: (a) original speech, and (b) SB-LPC
A block diagram of the SB-LPC decoder is shown in Figure 8.5 and the bit
allocation is shown in Table 8.1. Figure 8.6 illustrates the same waveforms
shown in Figure 8.4, but synthesized using the SB-LPC coder. The time-
domain LPC ﬁlter adds its phase response to the coherent excitation signal

Bibliography
275
of SB-LPC and disperses the energy of the excitation pulses. However the
waveform shape of the synthesized speech is different from the original
speech.
8.5 Summary
The fundamental sinusoidal speech analysis and synthesis techniques have
been brieﬂy discussed in this chapter. The basic sinusoidal model has been
modiﬁed to reduce the number of parameters in order to adapt it for low
bit-rates. At low bit-rates the frequencies of the sinusoids are restricted to
be harmonics of the pitch frequency and the harmonic phases are mod-
elled at the decoder. The concept of frequency-domain voicing is intro-
duced to achieve a compromise between the hoarseness and buzzyness of
harmonically-synthesized speech.
Three examples of low bit-rate harmonic coders have been presented:
sinusoidal transform coding (STC), improved multi-band excitation (IMBE),
and split-band linear predictive coding (SB-LPC). One of the main limitations
of low bit-rate harmonic coders is their inability to produce adequate quality
at the speech transitions.
Bibliography
[1] R. J. McAulay and T. F. Quatieri (1986) ‘Speech analysis/synthesis based
on a sinusoidal representation’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 34(4):744–54.
[2] R. J. McAulay and T. F. Quatieri (1995) ‘Sinusoidal coding’, in Speech
coding and synthesis by W. B. Kleijn and K. K. Paliwal (Eds), pp. 121–74.
Amsterdam: Elsevier Science
[3] L. B. Almeida and F. M. Silva (1984) ‘Variable frequency synthesis: an
improved harmonic coding scheme’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 27.5.1–4.
[4] I. Atkinson, S. Yeldener, and A. Kondoz (1997) ‘High quality split-band
LPC vocoder operating at low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 1559–62. May 1997. Munich
[5] J. Makhoul, R. Viswanathan, R. Schwartz, and A. W. F. Huggins (1978) ‘A
mixed source excitation model for speech compression and synthesis’,
in Proc. of Int. Conf. on Acoust., Speech and Signal Processing, pp. 163–6.
[6] D. Grifﬁn and J. S. Lim (1988) ‘Multiband excitation vocoder’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 36(8):1223–35.
[7] A. Kondoz, (1994) Digital Speech: coding for low bit rate communication
systems. New York: John Wiley & Sons Ltd

276
Harmonic Speech Coding
[8] DVSI (1991) INMARSAT-M Voice Codec, Version 1.7. September 1991.
Digital Voice Systems Inc.
[9] I. Atkinson (1997) ‘Advanced linear predictive speech compression at
3.0 kbit/s and below’, Ph.D. thesis, CCSR, University of Surrey, UK.
[10] T. Wang, K. Koishida, V. Cuperman, A. Gersho, and J. S. Collura (2002) ‘A
1200/24000 bps coding suite based on MELP’, in Proc. of IEEE Workshop
on Speech Coding, pp. 90–2.
[11] S. Villette (2001) ‘Sinusoidal speech coding for low and very low bit rate
applications’, Ph.D. thesis, University of Surrey, UK.

9
Multimode Speech Coding
9.1 Introduction
Harmoniccodersextract thefrequency-domainspeechparameters andspeech
is generated as a sum of sinusoids with varying amplitudes, frequencies and
phases. They produce highly intelligible speech down to about 2.4 kb/s
[1]. By using the unquantized phases and amplitudes, and by frequent
updating of the parameters, i.e. at least every 10 ms, they can even achieve
near transparent quality [2]. However this requires a prohibitive bit-rate,
unsuitable for low bit-rate applications. For example, the earlier versions
of multi-band excitation (MBE) coders (a typical harmonic coder) operated
at 8 kb/s with harmonic phase information [3]. However, harmonic coders
operating at 4 kb/s and below do not transmit phase information. The spectral
magnitudes are transmitted typically every 20 ms and interpolated during
the synthesis. The simpliﬁed versions used for low bit-rate applications are
well suited for stationary voiced segment coding. However at the speech
transitions such as onsets, where the speech waveform changes rapidly,
the simpliﬁed assumptions do not hold and degrade the perceptual speech
quality.
Figure 9.1 demonstrates two examples of harmonically-synthesized speech,
Figure 9.1a shows a stationary voiced segment and Figure 9.1b shows a
transitory speech segment. In both cases, (i) represents the original speech,
i.e. 128 kb/s linear pulse code modulation, and (ii) represents the synthesized
speech. The synthesized speech is generated using the split-band linear
predictive coding (SB-LPC) harmonic coder operating at 4 kb/s [4]. The
synthesized waveforms are shifted in the ﬁgures in order to compensate
for the delay due to look-ahead and the linear phase deviation due to loss
of phase information in the synthesis. The SB-LPC decoder predicts the
evolution of harmonic phases using the linearly interpolated fundamental
frequency, i.e. a quadratic phase evolution function. Low bit-rate harmonic
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

278
Multimode Speech Coding
0
50
100
150
samples
(i)
(ii)
amplitude
0
100
200
300
400
500
samples
(i)
(ii)
amplitude
(a) Stationary voiced speech
(b) Transitory speech
Figure 9.1
Harmonically-synthesized speech
coders cannot preserve waveform similarity as illustrated in the ﬁgures,
since the phase information is not transmitted. However, in the stationary
voiced segments, phase information has little importance in terms of the
perceptual quality of the synthesized speech. Stationary voiced speech has
a strong, slowly-evolving harmonic content. Therefore extracting frequency

Introduction
279
domain speech parameters at regular intervals and interpolating them in the
harmonic synthesis is well suited for stationary voiced segments. However
at the transitions, where the speech waveform evolves rapidly, this low bit-
rate simpliﬁed harmonic model fails. As depicted in Figure 9.1b, the highly
nonstationary character of the transition has been smeared by the low bit-rate
harmonic model causing reduction in the intelligibility of the synthesized
speech.
CELP-type coders, such as ACELP [5, 6], encode the target speech wave-
form directly and perform relatively better at the transitions. However, at
low bit-rates, analysis-by-synthesis (AbS) coders fail to synthesize stationary
segments with adequate quality. As the bit rate is reduced, they cannot main-
tain clear periodicity of the stationary voiced segments [7]. CELP-type AbS
coders perform waveform-matching for each frame or subframe and select
the best possible excitation vector. This process does not consider the pitch
cycles of the target waveform, and consecutive synthesized pitch cycles show
subtle differences in the waveform shape. This artifact introduces granular
noise into the voiced speech, perceptible up to about 6 kb/s. Preserving the
periodicity of voiced speech is essential for high quality speech reproduction.
Figure 9.2a shows a stationary voiced segment and 9.2b shows a transitory
segment synthesized using ACELP at 4 kb/s. In Figure 9.2a, the consecu-
tive pitch cycles have different shapes, which degrades the slowly-evolving
periodicity of voiced speech, compared to Figure 9.1a. Therefore despite the
fact that waveform similarity is less in Figure 9.1a, harmonically-synthesized
voiced speech is perceptually superior to waveform-coded speech at low bit-
rates. Figure 9.2b shows that ACELP can synthesize the highly nonstationary
speech transitions better than harmonic coders (see Figure 9.1b). ACELP
may also introduce granular noise at the transitions. However, the speech
waveform changes rapidly at the transitions, masking the granular noise of
ACELP, which is not perceptible down to about 4 kb/s. The above observa-
tions suggest a hybrid coding approach, which selects the optimum coding
algorithm for a given segment of speech: coding stationary voiced segments
using harmonic coding and transitions using ACELP. Unvoiced and silence
segments can be encoded with CELP [8] or white-noise excitation.
Harmonic coders suffer from other potential problems such as voicing and
pitch errors that may occur at the transitions. The pitch estimates at the
transitions, especially at the onsets may be unreliable due to the rapidly-
changing speech waveform. Furthermore, pitch-tracking algorithms do not
have history at the onsets and should be turned off. Inaccurate pitch estimates
also account for inaccurate voicing decisions, in addition to the spectral
mismatches due to the nonstationary speech waveform at the transitions.
These voicing decision errors declare the voiced bands as unvoiced and
increase the hoarseness of synthetic speech. Encoding the transitions using
ACELP eliminates those potential problems of harmonic coding.

280
Multimode Speech Coding
0
50
100
150
samples
amplitude
(i)
(ii)
0
200
400
600
samples
amplitude
(i)
(ii)
(a) Stationary voiced speech
(b) Transitory speech
Figure 9.2
Speech synthesized using ACELP
9.2 Design Challenges of a Hybrid Coder
The main challenges in designing a hybrid coder are reliable speech classiﬁca-
tion and phase synchronization when switching between the coding modes.
Furthermore, most of the speech-coding techniques make use of a look-ahead

Summary of Hybrid Coders
281
and parameter interpolation. Interpolation requires the parameters of the
previous frame; when switched from a different mode, those parameters may
not be directly available. Predictive quantization schemes also require the
previous memory. Techniques which eliminate these initialization/memory
problems are required.
9.2.1 Reliable Speech Classification
A voice activity detector (VAD) can be used to identify speech and silence
segments [9], while classiﬁcation of speech into voiced and unvoiced segments
can be seen as the most basic speech classiﬁcation technique. However, there
are coders in the literature which use up to six phonetic classes [10]. The
design of such a phonetic classiﬁcation algorithm can be complicated and
computationally complex, and a simple classiﬁcation with two or three
modes is sufﬁcient to exploit the relative merits of waveform and harmonic
coding methods. The accuracy of the speech classiﬁcation is critical for the
performance of a hybrid coder. For example, using noise excitation for a
stationary voiced segment (which should operate in harmonic coding mode)
can severely degrade the speech quality, by converting the high-voiced
energy of the original speech into noise in the synthesized speech; use of
harmonic excitation for unvoiced segments gives a tonal artifact. ACELP can
generally maintain acceptable quality for all the types of speech since it has
waveform-matching capability. During the speech classiﬁcation process, it is
essential that the above cases are taken into account to generate a fail-safe
mode selection.
9.2.2 Phase Synchronization
Harmonic coders operating at 4 kb/s and below do not transmit phase
information, in order to allocate the available bits for accurate quantization
of the more important spectral magnitude information. They exploit the fact
that the human ear is partially phase-insensitive and the waveform shape
of the synthesized speech can be very different from the original speech,
often yielding negative SNRs. On the other hand, AbS coders preserve the
waveform similarity. Direct switching between those two modes without
any precautions will severely degrade the speech quality due to phase
discontinuities.
9.3 Summary of Hybrid Coders
The hybrid coding concept has been introduced in the LPC vocoder [11],
which classiﬁes speech frames into voiced or unvoiced, and synthesizes the
excitation using periodic pulses or white noise, respectively. Analysis-by-
synthesis CELP coders with dynamic bit allocation (DBA), which adaptively

282
Multimode Speech Coding
distribute the bits among coder parameters in a given frame while maintaining
a constant bit rate, by classifying each frame into a certain mode, have also
been reported [12]. However, we particularly focus here on hybrid coders,
which combine AbS coding and harmonic coding. The advantages and
disadvantages of harmonic coding and CELP, and the potential beneﬁts of
combining the two methods have been discussed by Trancoso et al. [13].
Improving the speech quality of the LPC vocoder by using a form of multi-
pulse excitation [14] as a third excitation model at the transitions has also
been reported [15].
9.3.1 Prototype Waveform Interpolation Coder
Kleijn introduced prototype waveform interpolation (PWI) in order to
improve the quality of voiced speech [7]. The PWI technique extracts pro-
totype pitch cycle waveforms from the voiced speech at regular intervals of
20–30 ms. Speech is reconstructed by interpolating the pitch cycles between
the update points. The PWI technique can be applied either directly to the
speech signal or to the LPC residual. Since the PWI technique is not suit-
able for encoding unvoiced speech segments, unvoiced speech is synthesized
using CELP. Even though the motivation behind using two coding techniques
is different in the PWI coder (i.e. waveform coding is not used for transitions),
it combines harmonic coding and AbS coding. The speech classiﬁcation of
the PWI coder is relatively easier, since it only needs to classify speech into
either voiced or unvoiced.
At the onset of a voiced section, the previously estimated prototype wave-
form is not present at the decoder for the interpolation process. Kleijn suggests
three methods to solve this problem:
• Extract the prototype waveform from the reconstructed CELP waveform
of the previous frame.
• Set to a single pulse waveform (ﬁltered through LPC) with its amplitude
determined from the transmitted information.
• Use a replica of the prototype transmitted at the end of the current synthesis
frame.
The starting phase of the pitch cycles at the onsets can be determined
at the decoder from the CELP encoded signal. At the offsets, the linear
phase deviation between the harmonically synthesized and original speech
is measured and the original speech buffer is displaced, such that the AbS
coder begins exactly where the harmonic coder ended.
9.3.2 Combined Harmonic and Waveform Coding at Low Bit-Rates
This coder, proposed by Shlomot et al., consists of three modes: harmonic,
transition, and unvoiced [16, 17]. All the modes are based on the source ﬁlter

Summary of Hybrid Coders
283
model. The harmonic mode consists of two components: the lower part of
the spectrum or the harmonic bandwidth, which is synthesized as a sum of
coherent sinusoids, and the upper part of the spectrum, which is synthesized
using sinusoids of random phases. The transitions are synthesized using pulse
excitation, similar to ACELP, and the unvoiced segments are synthesized
using white-noise excitation.
Speech classiﬁcation is performed by a neural network, which takes into
account the speech parameters of the previous, current, and future frames,
and the previous mode decision. The classiﬁcation parameters include the
speech energy, spectral tilt, zero-crossing rate, residual peakiness, residual
harmonic matching SNRs, and pitch deviation measures. At the onsets,
when switching from the waveform-coding mode, the harmonic excitation
is synchronized by shifting and maximizing the cross-correlation with the
waveform-coded excitation. At the offsets, the waveform-coding target is
shifted to maximize the cross-correlation with the harmonically-synthesized
speech, similar to the PWI coder.
9.3.3 A 4 kb/s Hybrid MELP/CELP Coder
The 4 kb/s hybrid MELP/CELP coder with alignment phase encoding and
zero phase equalization proposed by Stachurski et al. consists of three modes:
strongly-voiced, weakly-voiced, and unvoiced [18, 19]. The weakly-voiced
mode includes transitions and plosives, which is used when neither strongly-
voiced nor unvoiced speech segments are clearly identiﬁed. In the strongly-
voiced mode, a mixed excitation linear prediction (MELP) [20, 21] coder
is used. Weakly-voiced and unvoiced modes are synthesized using CELP.
In unvoiced frames, the LPC excitation is generated from a ﬁxed stochastic
codebook. In weakly-voiced frames, the LPC excitation consists of the sum of a
long-term prediction ﬁlter output and a ﬁxed innovation sequence containing
a limited number of pulses, similar to ACELP.
The speech classiﬁcation is based on the estimated voicing strength and
pitch. The signal continuity at the mode transitions is preserved by trans-
mitting an ‘alignment phase’ for MELP-encoded frames, and by using ‘zero
phase equalization’ for transitional frames. The alignment phase preserves
the time-synchrony between the original and synthesized speech. The align-
ment phase is estimated as the linear phase required in the MELP-encoded
excitation generation to maximize the cross-correlation between the MELP
excitation and the corresponding LPC residual. Zero-phase equalization
modiﬁes the CELP target signal, in order to reduce the phase disconti-
nuities, by removing the phase component, which is not coded in MELP.
Zero phase equalization is implemented in the LPC residual domain, with a
Finite Impulse Response (FIR) ﬁlter similar to [22]. The FIR ﬁlter coefﬁcients
are derived from the smoothed pitch pulse waveforms of the LPC residual
signal. For unvoiced frames the ﬁlter coefﬁcients are set to an impulse so

284
Multimode Speech Coding
that the ﬁltering has no effect. The AbS target is generated by ﬁltering the
zero-phase-equalized residual signal through the LPC synthesis ﬁlter.
9.3.4 Limitations of Existing Hybrid Coders
PWI coders and low bit-rate coders that combine harmonic and waveform
coding use similar techniques to ensure signal continuity. At the onsets, the
initial phases of the harmonic excitation are extracted from the previous
excitation vector of the waveform-coding mode. This can be difﬁcult at
rapidly-varying onsets, especially if the bit-rate of the waveform coder is low.
Moreover, inaccuracies in the onset synchronization will propagate through
the harmonic excitation and make the offset synchronization more difﬁcult. At
the offsets, the linear phase deviation between the harmonically-synthesized
and original speech is measured and the original speech buffer is displaced,
such that the AbS coder begins exactly where the harmonic coder has ended.
This method needs the accumulated displacement to be reset during unvoiced
or silent segments, and may fail to meet the speciﬁcations of a system with
strict delay requirements.
Another problem arises when a transition occurs within a voiced speech
segment as shown in Figure 9.3, where there are no unvoiced or silent
segments after the transition to reset the accumulated displacement. Even
though the accumulated displacement can be minimized by inserting or
eliminating exactly complete pitch cycles, the remainder will propagate
into the next harmonic section. Furthermore, a displacement of a fraction
0
200
400
600
samples
amplitude
s(n)
r(n)
Figure 9.3
A transition within voiced speech

Synchronized Waveform-Matched Phase Model
285
of a sample can introduce audible high frequency distortion, especially in
segments with short pitch periods. Consequently, the displacements should
be performed with a high resolution. The MELP/CELP coder preserves signal
continuity by transmitting an alignment phase for MELP-encoded frames and
using zero phase equalization for transitional frames. Zero phase equalization
may reduce the beneﬁts of AbS coding by modifying the phase spectrum,
and it has been reported that the phase spectrum is perceptually important
[23–25]. Furthermore, zero phase equalization relies on accurate pitch pulse
position detection at the transitions, which can be difﬁcult.
Harmonic excitation can be synchronized with the LPC residual by trans-
mitting the phases, which eliminates the above difﬁculties. However this
requires a prohibitive capacity making it unsuitable for low bit-rate appli-
cations. As a compromise, Katugampala [26] proposed a new phase model
for the harmonic excitation called synchronized waveform-matched phase
model (SWPM). SWPM facilitates the integration of harmonic and AbS coders,
by synchronizing the harmonic excitation with the LPC residual. SWPM
requires only two parameters and does not alter the perceptual quality of the
harmonically-synthesized speech. It also allows the ACELP mode to target
the speech waveform without modifying the perceptually-important phase
components or the frame boundaries.
9.4 Synchronized Waveform-Matched Phase Model
The SWPM maintains the time-synchrony between the original and the
harmonically-synthesized speech by transmitting the pitch pulse loca-
tion (PPL) closest to each synthesis frame boundary [27, 28, 26]. The SWPM
also preserves sufﬁcient waveform similarity, such that switching between
the coding modes is transparent, by transmitting a phase value that indicates
the pitch pulse shape (PPS) of the corresponding pitch pulse. PPL and PPS are
estimated in every frame of 20 ms. SWPM needs to detect the pitch pulses only
in the stationary voiced segments, which is somewhat easier than detecting
the pitch pulses in the transitions as in [18]. The SWPM has the disadvantage
of transmitting two extra parameters (PPL and PPS) but the bottleneck of the
bit allocation of hybrid coders is usually in the waveform-coding mode. Fur-
thermore, in stationary voiced segments the location of the pitch pulses can
be predicted with high accuracy, and only an error needs to be transmitted.
The same argument applies to the shape of the pitch pulses.
In the harmonic synthesis, cubic phase interpolation [2] is applied between
the pitch pulse locations, setting the phases of all the harmonics equal
to PPS. This makes the waveform similarity between the original and the
synthesized speech highest in the vicinity of the selected pitch pulse locations.
However this does not cause difﬁculties, since switching is restricted to frame
boundaries and the pitch pulse locations closest to the frame boundaries

286
Multimode Speech Coding
are selected. Furthermore, SWPM can synchronize the synthesized excitation
and the LPC residual with fractional sample resolutions, even without up-
sampling either of the waveforms.
9.4.1 Extraction of the Pitch Pulse Location
The TIA Enhanced Variable Rate Coder (EVRC) [29], which employs relaxed
CELP (RCELP) [30], uses a simple method based on the energy of the LPC
residual to detect the pitch pulses. EVRC determines the pitch pulse locations
by searching for a maximum in a ﬁve-sample sliding energy window within
a region larger than the pitch period, and then ﬁnding the rest of the pitch
pulses by searching recursively at a separation of one pitch period. It is
possible to improve the performance of the residual-energy-based pitch
pulse location detection by using the Hilbert envelope of windowed LP
residual (HEWLPR) [31, 32]. A robust pitch pulse detection algorithm based
on the group delay of the phase spectrum has also been reported [33], however
this method has a very high computational complexity.
The SWPM requires a pitch pulse detection algorithm that can detect the
pulses at stationary voiced segments with a high accuracy and has a low
computational complexity. However the ability to detect the pitch pulses at
the onsets and offsets is beneﬁcial, since this will increase the ﬂexibility of
transition detection. Therefore an improved pitch pulse detection algorithm,
based on the algorithm used in EVRC, is developed for SWPM. Figure 9.4
depicts a block diagram of the pitch pulse location detection algorithm.
Initially, all the possible pitch pulse locations are determined by considering
the localized energy of the LPC residual and an adaptive threshold function,
Localized
Energy
Adaptive
Threshold
Probability
Function
History
Bias
r(n)
e(n)
e(n)
e(n)>t(n)
t(n)
np
Λp
Lp > 0.8
npw
nlst
Pitch
Pitch
Pitch
Figure 9.4
Block diagram of the pitch pulse detection algorithm

Synchronized Waveform-Matched Phase Model
287
t(n). The localized energy, e(n), of the LPC residual, r(n), is given by,
e (n) = 1
5
2

j=−2
r

n + j

for 2 ≤n < N −2
(9.1)
where N = 240 is the length of the residual buffer.
The adaptive threshold function, t(n), is updated for each half pitch period,
by taking 0.7 of the maximum of e(n) in the pitch period symmetrically-
centred around the half pitch period chosen to calculate t(n), and t(n) is
given by,
t

nk −τ1/4 + nτ/2

= 0.7 max

e

nk −τ1/2 + nτ

for 0 ≤nτ < τ1
and
0 ≤nτ/2 < τ1/2
(9.2)
where τ1 =

τ + 1
2

, τ1/2 =

τ
2 + 1
2

, τ1/4 =

τ
4 + 1
2

, nk = kτ1/2
for 1 ≤
k <

2N
τ

, and τ is the pitch period.
The exceptions corresponding to the analysis frame boundaries are given in,
e (0) = e (1) = e (N −2) = e (N −1) = 0
(9.3)
t (m) = 0.7 max

e

τ1/2

for 0 ≤m < τ1/4
(9.4)
n
2N
τ
 = N −τ1
(9.5)
The sample locations, for which e(n) > t(n), are considered as the regions
which may contain pitch pulses. If e(n) > t(n) for more than eight consecutive
samples, those regions are ignored, since in those regions the residual energy
is smeared, which is not a feature of pitch pulses. The centre of the each
remaining region is taken as a possible pitch pulse location, np. If any of
the two candidate locations are closer than eight samples (i.e. half of the
minimum pitch), the one which has the higher e(np) is taken.
Applying an adaptive threshold to estimate the pitch pulse locations from
the localized energy e(n) is advantageous, especially for segments where the
energy of the LPC residual varies rapidly, giving rise to spurious pulses.
Figure 9.5 demonstrates this for a male offset and a female onset. The male
speech segment has a pitch period of about 80 samples and the two high-
energy irregular pulses which do not belong to the pitch contour are clearly
visible. The female speech segment has a pitch of about 45 samples, which
also contains two high-energy irregular pulses. The energy function e(n) and
the threshold function t(n) are also depicted in Figure 9.5, shifted upwards for
clarity. The ﬁgures also show that e(n) at the irregular pulses may be higher
than e(n) at the correct pitch pulses. Therefore selecting the highest e(n) to
detect a pitch pulse location as in [34] may lead to errors. Since e(n) > t(n),
for some of the irregular pulses as well as for correct pitch pulse locations,

288
Multimode Speech Coding
0
100
200
300
400
samples
amplitude
r(n)
t(n)
e(n)
0
50
100
150
200
samples
t(n)
e(n)
r(n)
amplitude
(a) Male offset
(b) Female onset
Figure 9.5
Irregular pulses at the onsets and offsets
further reﬁnements are required. Moreover, the regions where e(n) > t(n),
gives only a crude estimation of the pitch pulse location. The algorithm relies
on the accuracy of the estimated pitch used for the computation of t(n) and
in the reﬁnement process described below. However SWPM needs only the
pitch pulses in the stationary voiced segments, for which the pitch estimate
is reliable.

Synchronized Waveform-Matched Phase Model
289
For each selected location np, the probability of it being a pitch pulse is
estimated, using the pitch and the energy of the neighbouring locations.
First, a total energy metric, Ep0 for the candidate pulse at np0 is computed
recursively as follows,
Ep0 =

l
e (nl)
(9.6)
where l = p0 and any q which satisﬁes the condition,
nl ± τ −nq
 < 0.15τ
(9.7)
For each term, +τ and −τ, if more than one q satisﬁes equation (9.7), only
the one which minimizes
nl ± τ −nq
 is chosen. Then further locations nq
that satisfy equation (9.7) are searched recursively, with any nq which have
already satisﬁed equation (9.7) taken as nl in the next iteration. Therefore, Ep0
can be deﬁned as the sum of e(np) of the pitch contour corresponding to the
location np0. This process eliminates the high-energy irregular pulses, since
they do not form a proper pitch contour and equation (9.7) detects them as
isolated pulses. The probability of the candidate location, np0, containing a
pitch pulse, p0, is given by,
p0 =
Ep0
max

Ep

(9.8)
If pitch pulse locations were detected in the previous frame and any of the
current candidate pitch pulse locations form a pitch contour which is a con-
tinuation of the previous pitch contour, a history bias term is added. Adding
the history bias term enhances the performance at the offsets, especially at the
resonating tails. Furthermore, the history bias helps to maintain the continu-
ity of the pitch contour between the frames, at the segments, where the pitch
pulses become less signiﬁcant, as shown in Figure 9.6. A discontinuity in the
pitch contour adds a reverberant character into voiced speech segments. The
biased term ′
l for any location nl which satisﬁes equations (9.10) or (9.11) is
given by,
′
l = l + 0.2
(9.9)
The initial value for l is given by equation (9.10), with ϵ being the minimum
possible integer value which satisﬁes equation (9.10). If more than one l
satisﬁes equation (9.10) with the same minimum ϵ, the one which maximizes
e(nl) is taken.
|nlst + ϵτ −nl| < 0.1τ
(9.10)
where nlst is the pitch pulse location selected in the last analysis frame. Then
any location nq which satisﬁes equation (9.11) is searched and further nl are

290
Multimode Speech Coding
0
50
100
150
200
samples
threshold
probability
r(n)
0.8
0.0
amplitude
0
100
200
300
400
samples
probability
threshold
r(n)
amplitude
0.0
0.8
0
100
200
300
400
samples
probability
threshold
r(n)
amplitude
0.0
0.8
s(n)
s(n)
0
50
100
150
200
samples
threshold
probability
r(n)
0.8
0.0
amplitude
(a) Onset with irregular pulses
(c) Resonance at the nasal sound ‘nd’
(b) High fundamental frequency vowel, ‘o’
(d) Resonance at the nasal sound ‘ng’
Figure 9.6
Some instances of difficult pitch pulse extraction
found recursively, with any nq which have already satisﬁed equation (9.11)
taken as nl in the next iteration. If more than one nq satisﬁes equation (9.11),
the one which minimizes
nl + τ + nq
 is chosen.
nl + τ + nq
 < 0.15τ
(9.11)
The ﬁnal probability of the candidate location np0 containing a pitch pulse
p0 is recalculated,
p0 =
′
p0
max
	
′p

(9.12)
A set of positions, npw which have probabilities, p > 0.8, are selected as
the pitch pulse locations, and they are further reﬁned in order to select
the pitch pulse closest to the synthesis frame boundary. Figure 9.6 shows

Synchronized Waveform-Matched Phase Model
291
some instances of difﬁcult pitch pulse detection along with the estimated
probabilities, p, and the threshold value. In Figures 9.6c and 9.6d, the
resonating speech waveforms are also shown.
The problem illustrated in Figure 9.6b can be explained in both the time
and frequency domains. In speech segments with a short pitch period, the
short-term LPC prediction tends to remove some of the pitch correlation
as well, leaving an LPC residual without any clearly distinguishable peaks.
Shorter pitch periods in the time domain correspond to fewer harmonics in
the frequency domain. Hence the inter-harmonic spacing becomes wider and
the formants of the short-term predictor tend to coincide with some of the
harmonics (see Figure 9.7). The speech spectrum in Figure 9.7 is lowered by
80 dB in order to emphasize the coinciding points of the spectra. The excessive
removal of some of the harmonic components by the LPC ﬁlter disperses the
energy of the residual pitch pulses. It has been reported that large errors in
the linear prediction coefﬁcients occur in the analysis of sounds with high
pitch frequencies [35]. In the case of nasal sounds, the speech waveform
has a very high low-frequency content (see Figure 9.6c). In such cases, the
LPC ﬁlter simply places a pole at the fundamental frequency. A pole in the
LPC synthesis ﬁlter translates to a zero in the inverse ﬁlter, giving rise to a
fairly random-looking LPC residual signal. The ﬁgures demonstrate that the
estimated probabilities, p exceed the threshold value only at the required
pitch pulse locations, despite those difﬁculties.
0
1000
2000
3000
4000
frequency (Hz)
−60
−40
−20
0
20
40
gain (dB)
LPC spectrum
speech spectrum
Figure 9.7
Speech and LPC spectra of a female vowel segment

292
Multimode Speech Coding
9.4.2 Estimation of the Pitch Pulse Shape
Figure 9.8 depicts a complete pitch cycle of the LPC residual, which includes
a selected pitch pulse, and the positive half of the wrapped phase spectrum
obtained from its DFT. The integer pitch pulse position is taken as the time
origin of the DFT, and the phase spectrum indicates that most of the harmonic
phases are close to an average value. This average phase value varies with
the shape of the pitch pulse, hence it is called pitch pulse shape (PPS). In the
absence of a strong pitch pulse, the phase spectrum becomes random and
varies between −π and π.
Figure 9.9 depicts a block diagram of the pitch pulse shape estimation
algorithm. This algorithm employs an AbS technique in the time domain to
estimate PPS. A prototype pulse, P(ns), is synthesized as follows:
p (ns) =
K

k=1
ak cos

kωns + αq

for
−4 ≤ns ≤4
(9.13)
where ω = 2π/τ, τ is the pitch period, K is the number of harmonics, ak are the
harmonic amplitudes, and the candidate pitch pulse shapes, αq, are given by,
αq = 2πq/8
for 0 ≤q < 8
(9.14)
Figure 9.10 depicts the synthesized pulses, p (ns), for two different candidate
pitch pulse shapes, i.e. values of αq. A simpler solution to avoid estimating
the spectral amplitudes, ak for equation (9.13) is to assume a ﬂat spectrum.
However, the use of spectral amplitudes, ak, gives the relative weight for
0
20
40
60
samples
amplitude
(a)
(b)
0
20
40
60
DFT samples
0
1
2
3
phase (radians)
(c)
(d)
Figure 9.8
(a) a complete pitch cycle of the LPC residual, (b) the pitch pulse
synthesized using PPS, (c) the positive half of the phase spectrum obtained from the
DFT, and (d) the estimated PPS

Synchronized Waveform-Matched Phase Model
293
Probability
Function
Shape
History
Final Pitch
Pulse
SNR
Computation
Cross
Correlation
Computation
Prototype
Pulse
Generation 
ω
ak
r(n)
r(n)
npw
r(n)
npw
Ej
Rj
nlst
lst
α
qw
npw
on/off 
α
Refine PPL
and PPS
ak
u
ω
n0
0
α
t0
t0
α
p(ns)
Figure 9.9
Block diagram of the pitch pulse shape estimation
0
10
20
30
samples
amplitude
0
10
20
30
samples
(a)    q  = 0
(a)    q  = π/2
amplitude
α
α
Figure 9.10
synthesized pulses, p (ns)
each harmonic, which is beneﬁcial in estimating the pitch pulse shape. For
example, if a harmonic component which is relatively small in the LPC
residual signal is given equal weight in the prototype pulse, p(ns), this
may lead to inaccurate estimates in the subsequent AbS reﬁnement process.
Considering the frequency domain, those relatively small amplitudes may be
affected by spectral leakage from the larger amplitudes, giving large errors in
the phase spectrum. However, since computing the spectral amplitudes for
each pitch pulse is a very intensive process, as a compromise, the same spectral
amplitudes are used for the whole analysis frame, and are also transmitted
to the decoder as the harmonic amplitudes of the LPC residual. Then the
normalized cross-correlation, Rj, and SNR, Ej, are estimated between the

294
Multimode Speech Coding
synthesized prototype pitch pulse p(ns) and each of the detected LPC residual
pitch pulses, at the locations np, where np ∈npw. Rj and Ej are estimated for
each candidate pitch pulse shape, αq,
Rj =
4

ns=−4
r

np + ns + j

p (ns)




4

ns=−4
p2 (ns)
4

ns=−4
r2 
np + ns + j

for −3 ≤j ≤3
(9.15)
Ej =
4

ns=−4
r2 
np + ns + j

4

ns=−4

p (ns) −r

np + ns + j
2
for −3 ≤j ≤3
(9.16)
The term j is introduced in Rj and Ej in order to shift the relative positioning
of the LPC residual pulse and the synthesized pulse. This compensates for the
approximate pitch pulse locations, np, estimated by the algorithm described
in Section 9.4.1, by allowing the initial estimates to shift around, with a
resolution of one sample. All the combinations of np, αq, and j for which
Ej ≤1.0 are excluded from any further processing. Ej ≤1.0 corresponds to
an SNR of less than or equal to 0 dB. Then probability of the candidate shape,
αq0, being the pitch pulse shape is estimated,
q0 =
Nq0
max

Nq

(9.17)
where Nq is the total number of residual pulses for a given q, for which
Rj > 0.5. If more than one j satisﬁes the condition Rj > 0.5, for a particular set
of q and np, Nq is incremented only once. The set of pitch pulse shape values,
αqw, which have probabilities, q > 0.7 are chosen for further reﬁnement.
If max Nq
 is zero, then all the q are set to zero, i.e. no pitch pulses are
detected. Figure 9.11 shows the LPC residual of an analysis frame and the
estimated probability density function (PDF) of αq in the range −π ≤αq < π.
The pitch pulses of the LPC residual in Figure 9.11a have similar shapes to
the shape of the synthesized pulse shown in Figure 9.10a. Consequently the
PDF is maximum around αq = 0, for the pitch pulse shape used to synthesize
the pulse shown in Figure 9.10a. If a history bias is used in pitch pulse
location detection, then the probability term, q is not estimated. Instead
the pitch pulse shape search is limited to three candidates, αL, around the
pitch pulse shape of the previous frame. During the voiced segments, the

Synchronized Waveform-Matched Phase Model
295
0
100
200
300
samples
amplitude
r(n)
centre of the
analysis frame
selected pulse
−4
−2
0
2
pitch pulse shape
(a) LPC residual
(b) PDF of 
pdf
4
q
α
Figure 9.11
An analysis frame and the probability density function of αq
pitch pulse shape is fairly stationary and restricting the search range around
the previous value does not reduce the performance. Restricting the search
range has advantages such as reduced computational complexity and efﬁcient
differential quantization of the pitch pulse shape. Furthermore, restricting the
search range avoids large variations in the pitch pulse shape. Large variations
in the pitch pulse shape introduce a reverberant character into the synthesized
speech.
αL = 2π

qL + δ

/8
for
−1 ≤δ ≤1
(9.18)
where,
qL =
αlst
2π 8 + 1
2

for 0 ≤αlst < 2π
(9.19)
Then Rj and Ej are estimated as before, substituting αq with αL, and all the
combinations of np, αL, and j for which Ej ≤1.0 or Rj ≤0.5 are excluded
from any further reﬁnements. If no combination of np, αL, and j are left, the
search is extended to all the αq, and q is estimated as before, otherwise the
remaining αL are chosen for further reﬁnement, i.e. the remaining αL form the
set αqw. The pitch pulse closest to the centre of the analysis frame, i.e. closest
to the synthesis frame boundary for which Rj > ξ is selected as the ﬁnal pitch
pulse. The threshold value, ξ, is given by,
ξ = 0.7 max

Rj

for αq ∈αqw
(9.20)
If more than one set of j and αq satisfy the condition Rj > ξ for the same
pitch pulse closest to the synthesis frame boundary, the set of values which
maximizes Rj is chosen. The pitch pulse shape and the integer pitch pulse
location are given by the chosen, αq and np+j respectively. Figure 9.11a shows

296
Multimode Speech Coding
the centre of the analysis frame and the selected pitch pulse. It is also possible
to select the pitch pulse closest to the centre of the analysis frame from the
set npw and estimate the shape of the selected pulse. However estimating
the PDF of αq for the whole analysis frame and including it in the selection
process improves the reliability of the estimates, which enables the selection
of the most probable αq. Then the integer pitch pulse location is reﬁned to a
0.125 sample accuracy, and the initial pitch pulse shape is reﬁned to a 2π/64
accuracy. In the reﬁnement process, a synthetic pulse pu(nu) is generated in
an eight times up-sampled domain, i.e. at 64 kHz. If the selected integer pitch
pulse location and shape are n0 and α0, respectively, then,
pu (nu) =
K

k=1
ak cos (kωunu + αi)
for −40 ≤nu < 40
(9.21)
where ωu = 2π/8τ, and αi is given by,
αi = α0 + 2πi/64
for −4 ≤i ≤4
(9.22)
Then equation (9.23) is used to compute the normalized cross-correlation Ri,j
for all i and j, and the indices corresponding to the maximum Ri,j are used
to evaluate the reﬁned PPS and PPL, as shown in equations (9.22) and (9.25)
respectively.
Ri,j =
4

nr=−4
r (n0 + nr) pj (nr)




4

nr=−4
p2
j (nr)
4

nr=−4
r2 (n0 + nr)
(9.23)
where pj(nr), is the shifted and down-sampled version of pu(nu) given by,
pj (nr) = pu

8nr + j

for
−8 ≤j < 8 and
−4 ≤nr ≤4
(9.24)
The ﬁnal PPL, t0, reﬁned to a 0.125 sample resolution is given by,
t0 = n0 −j/8
(9.25)
Fractional PPL is important for segments with short pitch periods and
when the pitch pulse is close to or at the synthesis frame boundary. When
the pitch period is short, a small variation in the pitch pulse location can
induce a large percentage pitch error. The pitch pulses closest to the synthesis
frame boundaries are chosen in SWPM in order to maximize the waveform
similarity at the frame boundaries, since the mode changes are limited to

Synchronized Waveform-Matched Phase Model
297
synthesis frame boundaries. However if the selected pitch pulse is on the
frame boundary or within a few samples of it, the pulse must be synthesized
smoothly across this boundary, in order to avoid audible artifacts. In such
cases, high resolution PPL and PPS are essential to maintain the phase
continuity across the frame boundaries. It is also possible to compute the
cross-correlation between pu(nu) and the eight times up-sampled residual
signal, in order to evaluate the best indices i and j. However this requires
more computations and an equally good result is obtained by shifting pu(nu)
in the up-sampled domain and then computing the cross-correlation in the
down-sampled domain, as shown in equations (9.23) and (9.24).
At the offsets, if no pitch pulses are detected, PPL is predicted from the PPL
of the previous frame using the pitch, and PPS is set to equal to the PPS of the
previous frame. This does not introduce any deteriorating artifacts, since the
encoder checks the suitability of the harmonic excitation in the mode selection
process. The prediction of PPL and PPS is particularly useful at offsets with a
resonant tail, where pitch pulse detection is difﬁcult.
9.4.3 Synthesis using Generalized Cubic Phase Interpolation
In the synthesis, the phases are interpolated cubically, i.e. by quadratic inter-
polation of the frequencies. In [2], phases are interpolated for the frequencies
and phases available at the frame boundaries. But in the case of SWPM the
frequencies are available at the frame boundaries and the phases at the pitch
pulse locations. Therefore a generalized cubic phase interpolation formula is
used, to incorporate PPL and PPS.
The phase θk (n) of the kth harmonic of the i+1th synthesis frame is given by,
θk (n) = θki + kωin + αkn2 + βkn3
for 0 ≤n < N
(9.26)
where N is the number of samples per frame and θki and ωi are the phase of
the kth harmonic and the fundamental frequency, respectively, at the end of
synthesis frame i, and αk and βk are given by,

t2
0
t3
0
2N
3N2
  αk
βk

=
 θt0 −θki −kωit0 + 2πMk
kωi+1 −kωi

(9.27)
where t0 is the fractional pitch pulse location (PPL), θt0 is the PPS estimated at
t0, and Mk represents the phase unwrapping and is chosen according to the
‘maximally smooth’ criterion used by McAulay [2]. McAulay chose Mk such
that f (Mk) is a minimum,
f (Mk) =
T

0
 ¨θk (t, Mk)
2 dt
(9.28)

298
Multimode Speech Coding
where θk (t, Mk) represents the continuous analogue form of θk(n), and
¨θk (t, Mk) is the second derivative of θk (t, Mk) with respect to t. Although
Mk is integer-valued, since f(Mk) is quadratic in Mk, the problem is most
easily solved by minimizing f(xk) with respect to the continuous variable xk
and then choosing Mk to be an integer closest to xk. For the generalized case
of SWPM, f(xk) is minimized with respect to xk and xkmin is given by,
xkmin = 1
2π

θki −θt0 + kωit0 + k

ωi+1 −ωi

t2
0
2N

(9.29)
Mkmin =

xkmin + 0.5

is substituted in equation 9.27 for Mk to solve for αk and
βk and in turn to unwrap the cubic phase interpolation function θk(n).
The initial phase θki for the next frame is θk(N), and the above computations
should be repeated for each harmonic, i.e. k. It should be noted that there is
no need to synthesize the phases, θk(n) in the up-sampled domain, in order
to use the fractional pitch pulse location, t0. It is sufﬁcient to use t0 in solving
the coefﬁcients of θk(n), i.e. αk and βk.
9.5 Hybrid Encoder
A simpliﬁed block diagram of a typical hybrid encoder that operates on a ﬁxed
frame size of 160 samples is shown in Figure 9.12. For each frame, the mode
that gives the optimum performance is selected. There are three possible
modes: scaled white noise coloured by LPC for unvoiced segments; ACELP
for transitions; and harmonic excitation for stationary voiced segments.
LPC, LSF &
Quantize
Inverse LPC
Filter
Fractional
Pitch
Initial
Classification
Analysis by
Synthesis
Classification 
Amps,
Pulse Shape
& Location 
Unvoiced
Gain
ACELP
Excitation
LSF
CL1
Excitation Parameters
Specific to Each Mode
CL2 
SW1
SW2
LPC
Residual  
Input Speech
LPC
Figure 9.12
Block diagram of the hybrid encoder

Hybrid Encoder
299
Any waveform-coding technique can be used instead of ACELP. In fact
this hybrid model [27] does not restrict the choice of coding technique for
speech transitions, it merely makes the mode decision and deﬁnes the target
waveform. In white noise excited mode, the gain estimated from the LPC
residual energy is transmitted for every 20 ms. The LPC parameters are
common for all the modes and estimated every 20 ms (with a 25 ms window
length), which are usually interpolated in the LSF domain for every subframe
in the synthesis process. In order to interpolate the LSFs, the LPC analysis
window is usually centred at the synthesis frame boundary which requires a
look-ahead.
A two-stage speech classiﬁcation algorithm is used in the above coder. An
initial classiﬁcation is made based on the tracked energy, low-band to high-
band energy ratio, and zero-crossing rate, and determines whether to use the
noise excitation or one of the other modes. The secondary classiﬁcation, which
is based on an AbS process, makes a choice between the harmonic excitation
or ACELP. Segments of plosives with high-energy spikes are synthesized
using ACELP. When the noise excitation mode is selected, there is no need
to estimate the excitation parameters of the other modes. If noise excitation is
not selected, the harmonic parameters are always estimated and the harmonic
excitation is generated at the encoder for the AbS transition detection. The
speech classiﬁcation is described in detail in Section 9.6.
For simplicity, details of LPC and adaptive codebook memory update are
excluded from the block diagram. The encoder maintains an LPC synthesis
ﬁlter synchronized with the decoder, and uses the ﬁnal memory locations for
ACELP and AbS transition detection in the next frame. Adaptive codebook
memory is always updated with the previous LPC excitation vector regardless
of the mode. In order to maintain the LPC and the adaptive codebook
memories, the LPC excitation is generated at the encoder, regardless of
the mode.
9.5.1 Synchronized Harmonic Excitation
In the harmonic mode, the pitch and harmonic amplitudes of the LPC residual
are estimated for every 20 ms frame. The estimation windows are placed at
the end of the synthesis frames, and a look-ahead is used to facilitate the
harmonic parameter interpolation. The pitch estimation algorithm is based
on the sinusoidal speech-model matching proposed by McAulay [36] and
improved by Atkinson [4] and Villette [37, 38]. The initial pitch is reﬁned to
0.2 sample accuracy using synthetic spectral matching proposed by Grifﬁn
[3]. The harmonic amplitudes are estimated by simple peak-picking of the
magnitude spectrum of the LPC residual.
The harmonic excitation eh (n) is generated at the encoder for the AbS tran-
sition detection and to maintain the LPC and adaptive codebook memories,

300
Multimode Speech Coding
which is given by,
eh (n) =
K

k=1
ak(n) cos (θk (n))
for 0 ≤n < N
(9.30)
where K is the number of harmonics. Since two analysis frames are inter-
polated to produce a synthesis frame, K is taken as the higher number of
harmonics out of the two analysis frames and the missing amplitudes of the
other analysis frame are set to zero. N is the number of samples in a synthesis
frame and θk(n) is given in equation (9.26) for continuing harmonic tracks,
i.e. each harmonic of an analysis frame is matched with the corresponding
harmonic of the next frame. For terminating harmonics, i.e. when the number
of harmonics in the next frame is smaller, θk(n) is given by,
θk (n) = θki + 2πkn/τi
(9.31)
where θki is the phase of the harmonic k and τi is the pitch at the end of
synthesis frame i. For emerging harmonics, θk(n) is given by,
θk (n) = θt0 + 2πk(n −t0)/τi+1
(9.32)
where t0 is the PPL, θt0 is the corresponding PPS, and τi+1 is the pitch, all at
the end of synthesis frame i+1. Continuing harmonic amplitudes are linearly
interpolated,
ak (n) = aki +

aki+1 −aki

n
N
for 0 ≤n < N
(9.33)
where aki is the amplitude estimate of the kth harmonic at the end of synthesis
frame i. For terminating harmonic amplitudes a trapezoidal window, unity
for 55 samples and linearly decaying for 50 samples, is applied from the
beginning of the synthesis frame,
ak (n) = aki
for 0 ≤n < 55,
ak (n) = aki
105 −n
50
for 55 ≤n < 105
(9.34)
For emerging harmonic amplitudes a trapezoidal window, linearly rising for
50 samples and unity for 55 samples, is applied starting from the 56th sample
of the synthesis frame,
ak (n) = aki+1
n −55
50
for 55 ≤n < 105, ak (n) = aki+1
for 105 ≤n < 160
(9.35)

Hybrid Encoder
301
9.5.2 Advantages and Disadvantages of SWPM
Figure 9.13 shows some examples of waveforms synthesized using the
harmonic excitation technique described in Section 9.5.1. In each example,
(i) represents the LPC residual or the original speech signal and (ii) represents
the LPC excitation or the synthesized speech signal. Figure 9.13a shows the
LPC residual and the harmonic excitation of a segment which has strong
pitch pulses and Figure 9.13b shows the corresponding speech waveforms.
It can be seen that the synthesized speech waveform is very similar to the
original. Figure 9.13c shows the LPC residual and the harmonic excitation
of a segment which has weak or dispersed pitch pulses and Figure 9.13d
shows the corresponding speech waveforms. The synthesized speech is time-
synchronized with the original, however the waveform shapes are slightly
0
100
200
300
400
500
samples
(i)
(ii)
amplitude
0
100
200
300
400
500
samples
(a) LPC residual and excitation signals
(b) Speech signals corresponding to (a)
0
100
200
300
400
500
samples
0
100
200
300
400
500
samples
(c) LPC residual and excitation signals
(d) Speech signals corresponding to (c)
(i)
(ii)
amplitude
(i)
(ii)
amplitude
(i)
(ii)
amplitude
Figure 9.13
synthesized voiced excitation and speech signals

302
Multimode Speech Coding
different, especially between the major pitch pulses. The waveform similarity
is highest at the major excitation pulse locations and decreases along the pitch
cycles. This is due to the fact that SWPM models only the major pitch pulses
and it cannot model the minor pulses present in the residual signal when
the LPC residual energy is dispersed. Furthermore, the dispersed energy of
the LPC residual, becomes concentrated around the major pitch pulses in the
excitation signal. The synthesized speech also exhibits larger variations in
the amplitude around the pitch pulse locations, compared with the original
speech.
In order to understand the effects on subjective quality due to the above
observations, an informal listening test was conducted by switching between
the harmonically-synthesized speech and the original speech waveforms at
desired synthesis frame boundaries. The informal listening tests showed
occasional audible artifacts at the mode transitions, when switching from
the harmonic mode to the waveform-coding mode. However there were
no audible switching artifacts when switching from waveform-coding to
harmonic-coding mode, i.e. at the onsets. It was found that this is due to
two reasons: difﬁculties in reliable pitch pulse detection and limitations
in representing the harmonic phases using the pitch pulse shape at some
segments. At some highly resonant segments, the LPC residual looks like
random noise and it is not possible even to deﬁne the pitch pulses. The
predicted pitch pulse location, assuming a continuing pitch contour, may be
incorrect at resonant tails. At such segments, the pitch pulse locations are
determined by applying AbS techniques in the speech domain, such that the
synthesized speech signal is synchronized with the original, as described in
the next subsection. In the speech segments illustrated using Figure 9.13c,
it is possible to detect dominant pitch pulses. However the LPC residual
energy is dispersed throughout the pitch periods, making the pitch pulses
less signiﬁcant, as described in Section 9.4.1. This effect reduces the coherence
of the LPC residual harmonic phases at the pulse locations and the DFT phase
spectrum estimated at the pulse locations look random. Female vowels with
short pitch periods show these characteristics. A dispersed phase spectrum
reduces the effectiveness of the pitch pulse shape, since the concept of pitch
pulse shape is based on the assumption that a pitch pulse is the result of
the superimposition of coherent phases, which have the same value at the
pitch pulse location. This effect is illustrated in Figure 9.14. The synthesized
pitch pulse models the major pulse in the LPC residual pitch period and
concentrates the energy at the pulse location. This is due to the single phase
value used to synthesize the pulse, as opposed to the more random-looking
phase spectrum of the original pitch cycle. This phenomenon introduces
phase discontinuities, which accounts for the audible switching artifacts.
However the click and pop sounds present at the mode transitions in speech
synthesized with SWPM are less annoying than those in a conventional

Hybrid Encoder
303
0
10
20
30
samples
amplitude
(a)
(b)
0
10
20
30
DFT samples
−2
−1
0
1
2
phase (radians)
(c)
(d)
Figure 9.14
PPS at a dispersed pitch period: (a) a complete pitch cycle of the LPC
residual, (b) the pitch pulse synthesized using PPS, (c) the positive half of the phase
spectrum obtained from the DFT, and (d) estimated PPS
50
0
100
150
200
samples
50
0
100
150
200
samples
(a) LPC residual signals
(b) Speech signals
amplitude
LPC residual
SWPM
SB-LPC
original speech
SWPM
SB-LPC
amplitude
Figure 9.15
Speech synthesized using PPS
zero-phase excitation, even if the pitch pulse locations are synchronized. This
is because SWPM has the additional ﬂexibility of choosing the most suitable
phase value (PPS) for pitch pulses, such that the phase discontinuities are
minimized. Figure 9.15 illustrates the effect of PPS on the LPC excitation
and the synthesized speech signals. For comparison, it includes the original
signals and the signals synthesized using the SB-LPC coder [4] which assumes
a zero-phase excitation.
The absence of audible switching artifacts at the onsets is an interesting
issue. There are two basic reasons for the differences between switching
artifacts at the onsets and at the offsets: the nature of the excitation signal

304
Multimode Speech Coding
and the LPC memory. At the onsets, even though the pitch pulses may be
irregular due to the unsettled pitch of the vocal cords, they are quite strong
and the residual energy is concentrated around them. Resonating segments
and dispersed pulses do not occur at the onsets. Therefore the only difﬁculty
at the onsets is in identifying the correct pulses and, as long as the pulse
identiﬁcation process is successful, SWPM can maintain the continuity of the
harmonic phases at the onsets. The pitch pulse detection algorithm described
is capable of accurate detection of the pitch pulses at the onsets as described
in Section 9.4.1. Furthermore at the onsets, waveform coding preserves the
waveform similarity, which also ensures the correct LPC memory, since LPC
memory contains the past synthesized speech samples. Therefore the mode
transition at the onsets is relatively easier and SWPM guarantees a smooth
mode transition at the onsets. However at the offsets, the presence of weak
pitch pulses is a common feature and the highly resonant impulse response
LPC ﬁlter carries on the phase changes caused by the past excitation signal,
especially when the LPC ﬁlter gain is high. Therefore, the audible switching
artifacts remain at some of the offset mode transitions. These need to be
treated as special cases.
At the resonant tails the LPC residual looks like random noise, and the
pitch pulses are not clearly identiﬁable. In those cases AbS techniques can be
applied directly on the speech signal to synchronize the synthesized speech.
This process is applied only for the frames, which follow a harmonic frame
and have been classiﬁed as transitions.
Synthesized speech is generated by shifting the pitch pulse location (PPL)
at the end of the synthesis frame, ±τ/2 around the synthesis frame boundary
with a resolution of one sample, where τ is the pitch period. The location
which gives the best cross-correlation between the synthesized speech and
the original speech is selected as the reﬁned PPL. The pitch pulse shape
is set equal to the pitch pulse shape of the previous frame. The excitation
and the synthesized speech corresponding to the reﬁned PPL are input to
the closed-loop transition detection algorithm, and form the harmonic signal
if the transition detection algorithm classiﬁes the corresponding frame as
harmonic, otherwise waveform coding is used.
9.5.3 Offset Target Modification
The SWPM minimizes the phase discontinuities at the mode transitions, as
described in Section 9.5.2. However at some mode transitions such as the
offsets after female vowels, which have dispersed pulses, audible phase
discontinuities still remain. These discontinuities may be eliminated by trans-
mitting more phase information. This section describes a more economical
solution to remove those remaining phase discontinuities at the offsets,
which does not need the transmission of additional information. The pro-
posed method modiﬁes some of the harmonic phases of the ﬁrst frame of

Hybrid Encoder
305
the waveform-coding target, which follows the harmonic mode. The remain-
ing phase discontinuities can be corrected within the ﬁrst waveform-coding
frame, since SWPM keeps the phase discontinuities at a minimum and the
pitch periods are synchronized.
As a ﬁrst approach the harmonic excitation is extended into the next frame
and the synthesized speech is linearly interpolated with the original speech
at the beginning of the frame in order to produce the waveform-coding
target. Listening tests were carried out with different interpolation lengths.
The waveform-coding target was not quantized, in order to isolate the
distortions due to switching. The tests were extended in order to understand
the audibility of the phase discontinuities with the frequency of the harmonics,
by manually shifting one phase at a time and synthesizing the rest of
the harmonics using the original phases. Phase shifts of π/2 and π are
used. Listening tests show that for various interpolation lengths the phase
discontinuities below 1 kHz are audible, and an interpolation length as small
as 10 samples is sufﬁcient to mask distortions in the higher frequencies.
Furthermore, male speech segments with long pitch periods, around 80
samples and above, do not cause audible switching artifacts. Male speech
segments with long pitch periods have well-resolved short-term and long-
term correlations, and produce clear and sharp pitch pulses, which can be
easily modeled by SWPM. Therefore only the harmonics below 1 kHz of the
segments with pitch periods shorter than 80 samples are considered in the
offset target modiﬁcation process.
The harmonic excitation is extended beyond the mode transition frame
boundary, and the synthesized speech is generated in order to estimate the
harmonic phases at the mode transition frame boundary. The phase of the kth
harmonic of the excitation is computed as follows:
θki+1 (n) = θki + 2πkn/τi
for 0 ≤n < N
(9.36)
where θki is the phase of the kth harmonic and τi is the pitch at the end of
synthesis frame i. The excitation signal is given by,
ehi+1 (n) =
K

k=1
aki cos

θki+1 (n)

(9.37)
where K is the number of harmonics and aki is the amplitude of the kth
harmonic estimated at the end of the synthesis frame i. The excitation signal
is ﬁltered through the LPC synthesis ﬁlter to produce the synthesized speech
signal, with the coefﬁcients estimated at the end of the synthesis frame i.
The LPC memories after synthesizing the ith frame are used as the initial
memories. The speech samples synthesized for the ith and i + 1th frames are

306
Multimode Speech Coding
concatenated and windowed with a Kaiser window of 200 samples (β = 6.0)
centred at the frame boundary. The harmonic phases, ϕki, are estimated using
a 512 point FFT.
Having analysed the synthesized speech, the original speech is windowed
at three points: at the end of the synthesis frame i, at the centre of the synthesis
frame i+1, and at the end of the synthesis frame i+1, using the same window
function as before. The corresponding harmonic amplitudes, Aki, Aki+1/2, Aki+1
and the phases φki, φki+1/2, φki+1 are estimated using 512 point FFTs. Then
the signal component sl(n), which consists of the harmonics below 1 kHz, is
synthesized by,
sl (n) =
L

k=1
Ak (n) cos (k (n))
for 0 ≤n < N
(9.38)
where L is the number of harmonics below 1 kHz at the end of the ith synthesis
frame, Ak (n) is obtained by linear interpolation between Aki, Aki+1/2, and Aki+1,
and k (n) is obtained by cubic phase interpolation [2] between φki, φki+1/2,
and φki+1. Then the signal sm(n), which has modiﬁed phases is synthesized.
sm (n) =
L

k=1
Ak (n) cos (k (n))
for 0 ≤n < N
(9.39)
and, ﬁnally, the modiﬁed waveform-coding target of the i + 1th synthesis
frame is computed by,
st (n) = s (n) −sl (n) + sm (n)
(9.40)
where k (n) is obtained by cubic phase interpolation between ϕki and
φki+1. Thus the modiﬁed signal, sm(n) has the phases of the harmonically-
synthesized speech at the beginning of the frame and the phases of the original
speech at the end of the frame. In other words, ˙k (n) (the rate of change
of each harmonic phase) is modiﬁed such that the phase discontinuities are
eliminated, by keeping ˙k (n) equal to the harmonic frequencies at the frame
boundaries. There is a possibility that such phase modiﬁcation operations
induce a reverberant character in the synthesized signals. However, large
phase mismatches close to π are rare, because SWPM minimizes the phase
discontinuities. Furthermore, the modiﬁcations are applied only for the
speech segments, which have pitch periods shorter than 80 samples, thus
a phase mismatch is smoothed out in a few pitch cycles. The listening
tests conﬁrm that the synthesized speech does not possess a reverberant
character. Limiting the phase modiﬁcation process for the segments with
pitch periods shorter than 80 samples also improves the accuracy of the
spectral estimations, which use a window length of 200 samples. Figure 9.16

Hybrid Encoder
307
0
50
100
150
200
samples
amplitude
(a)
(b)
(c)
(d)
Figure 9.16
Offset target modification: (a) s(n), (b) st(n), (c) sl(n), and (d) sm(n)
illustrates the waveforms of equation (9.40). It can be seen that the phases
of the low frequency components of the original speech waveform, s(n), are
modiﬁed in order to obtain st(n). The waveforms in Figures 9.16c and 9.16d
depict sl(n) and sm(n), respectively, the low frequency components, which
have been modiﬁed. The phase relationships between the high-frequency
components account more for the perceptual quality of speech [25], and the
high-frequency phase components are unchanged in the process.
Some speech signals show rapid variations in the harmonic structure at the
offsets, which may reduce the efﬁciency of the phase modiﬁcation process.
In order to limit those effects the spectral amplitude and phase estimation
process is not strictly conﬁned to the harmonics of the fundamental frequency.
Instead the amplitude and phase corresponding to the spectral peak closest
to each harmonic frequency are estimated. The frequency of the selected
spectral peak is taken as the frequency of the estimated amplitude and phase.
When ﬁnding the spectral peaks closest to the harmonic frequencies, the
harmonic frequencies are determined by the fundamental frequency at the
end of the ith synthesis frame, since the pitch estimates at the transition
frame are less reliable. In fact the purpose of the offset target modiﬁcation
process is to ﬁnd the frequency components corresponding to the harmonics
of the harmonically-synthesized frame in the ith frame and change the phase
evolution of those components such that the discontinuities are eliminated.
Moreover, the same set of spectral peak frequencies and amplitudes are used
when synthesizing the terms sl(n) and sm(n), hence there is no need to restrict
the synthesis process to the pitch harmonics.

308
Multimode Speech Coding
Another important issue at the offsets is the energy contour of the syn-
thesized speech. The harmonic coder does not directly control the energy of
the synthesized speech, since it transmits the residual energy. However the
waveform coders directly control the energy of the synthesized speech, by
estimating the excitation gain using the synthesized speech waveform. This
may cause discontinuities at the offset mode transition frame boundaries,
especially when the LPC ﬁlter gain is high. The ﬁnal target for the waveform
coder is produced by linear interpolation between the extended harmonically
synthesized speech and the modiﬁed target, st(n) at the beginning of the
frame for 10 samples. The linear interpolation ensures that the discontinuities
due to variations of the energy contour are eliminated as well as the phase
discontinuities, which are not accounted for in the phase modiﬁcation process
described above.
9.5.4 Onset Harmonic Memory Initialization
The harmonic phase evolution described in Section 9.4.3 and the harmonic
excitation described in section 9.5.1 interpolate the harmonic parameters in
the synthesis process, and assume that the model parameters are available
at the synthesis frame boundaries. However, at the onset mode transitions,
when switching from the waveform-coding mode, the harmonic model
parameters are not directly available. The initial phases θki, the fundamental
frequency ωi in the phase evolution equation (9.26), and the initial harmonic
amplitudes aki in equation (9.33) are not available at the onsets. Therefore,
they should be estimated at the decoder from the available information. The
signal reconstructed by the waveform coder prior to the frame boundary
and the harmonic parameters estimated at the end of the synthesis frame
boundary are available at the decoder. The use of a waveform-coded signal
in estimating the harmonic parameters at the onsets may be unreliable
due to two reasons: the speech signal shows large variations at the onsets
and, at low bit-rates, the ACELP excitation at the onsets reduces to a few
dominant pulses, lowering the reliability of spectral estimates. Therefore the
use of waveform-coded signal in estimating the harmonic parameters should
be minimized. The waveform-coded signal is used only in initializing the
amplitude quantization memories.
Since preserving the waveform similarity at the frame boundaries is impor-
tant, the pitch is recomputed such that the previous pitch pulse location can
be estimated at the decoder. Therefore the transmitted pitch represents the
average over the synthesis frame. The other transmitted harmonic model
parameters are unchanged, and are estimated at the end of the synthesis
frame boundary. Let’s deﬁne the pitch, τi+1 and pitch pulse location, t0i+1, at
the end of the i + 1th synthesis frame, and the pitch pulse location at the end
of the ith synthesis frame, t0i. The number of pitch cycles nc between t0i and

Hybrid Encoder
309
t0i+1 is given by,
nc =
t0i+1 −t0i
τi+1
+ 1
2

(9.41)
The recomputed pitch, τr, is given by,
τr = t0i+1 −t0i
nc
(9.42)
Then τr and t0i+1 are transmitted, and t0i is computed at the decoder, as
follows,
t0i = t0i+1 −τr
t0i+1 −t′
τr
+ 1
2

(9.43)
where t′ is the starting frame boundary and t0i is the pitch pulse location
closest to t′. The pitch pulse shape, θ0i, at the end of the ith synthesis frame
is set equal to the pitch pulse shape, θ0i+1, at the end of the i + 1th synthesis
frame. The initial phases θki in equation (9.26) are estimated as follows,
θki = θ0i −2πkt0i
τr
(9.44)
Both fundamental frequency terms, ωi and ωi+1, in equation (9.27) are com-
puted using τr, i.e. ωi = ωi+1 = 2π/τr. The harmonic amplitudes aki in
equation (9.33) are set equal to aki+1. Therefore, the phase evolution of the
ﬁrst harmonic frame of a stationary voiced segment becomes effectively linear
and the harmonic amplitudes are kept constant, i.e. not interpolated.
9.5.5 White Noise Excitation
Unvoiced speech has a very complicated waveform structure. ACELP can be
used to synthesize unvoiced speech and it essentially matches the waveform
shape. However, a large number of excitation pulses are required to synthesize
the noise-like unvoiced speech. Reducing the number of ACELP excitation
pulses introduces sparse excitation artifacts in noise-like segments [39]. The
synthesized speech also shows the sparse nature, and the pulse locations
are clearly identiﬁable even in the LPC-synthesized speech. In fact, during
unvoiced speech the short term correlation is small and the LPC ﬁlter gain
has little effect.
Sinusoidal excitation can also be used to synthesize unvoiced segments,
despite the fact that there is no harmonic structure. Speech synthesized by
generating the magnitude spectrum every 80 samples (100 Hz) and uniformly-
distributed random phases for unvoiced segments can achieve good quality

310
Multimode Speech Coding
[40]. This method suits sinusoidal coders using frequency domain voicing
without an explicit time-domain mode decision, since it facilitates the use
of the same general analysis and synthesis structure for both voiced and
unvoiced speech. However, this hybrid model classiﬁes the unvoiced and
silence segments as a separate mode, and, hence, uses a simpler unvoiced
excitation generation model, which does not require any frequency-domain
transforms. It has been shown that scaled white noise coloured by LPC can
produce unvoiced speech with quality equivalent to µ-law logarithmic PCM
[41, 42], implying that the complicated waveform structure of unvoiced speech
has no perceptual importance. Therefore in terms of the perceptual quality, the
phase information transmitted by ACELP is redundant and higher synthesis
quality can be achieved at lower bit-rates using scaled white-noise excitation.
Figure 9.17 shows a block diagram of the unvoiced gain estimation process
and Figure 9.18 shows a block diagram of the unvoiced synthesis process. The
band pass ﬁlters used are identical and have cut-off frequencies of 140 Hz and
3800 Hz. The transfer function of the fourth-order inﬁnite impulse response
(IIR) band pass ﬁlters is given by,
Hbp (z) =
0.8278 −1.6556z−2 + 0.8278z−4
1 −0.0662z−1 −1.6239z−2 + 0.0451z−3 + 0.6855z−4
(9.45)
and the unvoiced gain, guv, is given by,
guv =






N−1

n=0
r2
bp (n)
N
(9.46)
where rbp (n) is the band-pass-ﬁltered LPC residual signal and N is the length
of the residual vector, which is 160 samples including a look-ahead of 80
samples to facilitate overlap and add synthesis at the decoder.
LPC
inverse
filter
Band pass
filter
Unvoiced
gain
s(n)
r(n)
guv
Figure 9.17
Unvoiced gain estimation
Band pass
filter 
Scale
Overlap
and add
LPC
synthesis
filter
r(n)
ˆ
s(n)
ˆ
guv
us (n)
u(n)
Figure 9.18
Unvoiced synthesis

Speech Classification
311
White noise, u(n), is generated by a random number generator with a Gaus-
sian distribution (a Gaussian noise source has been found to be subjectively
superior to a simple uniform noise source). The scaled white-noise excitation,
us(n), is obtained by,
us (n) = ubp (n)
guv





Z

n=0
ubp (n)
Z
(9.47)
where ubp(n) is the band-pass-ﬁltered white noise and Z is the length of the
noise vector, 240 samples. For overlap and add, a trapezoidal window is used
with an overlap of 80 samples. For each synthesis frame the ﬁltered noise
buffer, ubp, is shifted by 80 samples and a new 160 samples are appended,
this eliminates the need for energy compensation functions to remove the
windowing effects [43]. In fact the overlapped segments are correlated, and
the trapezoidal windows do not distort the rms energy.
No attempt is made to preserve the phase continuity when switching to
or from the noise excitation. When switching from a different mode, the
unvoiced gain, guv, of the previous frame is set equal to the current value.
The validity of these assumptions are tested through listening tests and the
results conﬁrm that these assumptions are reasonable and do not introduce
any audible artifacts. The average bit rate can be further reduced by the
introduction of voice activity detection (VAD) and comfort noise generation
at the decoder for silence segments [9, 44].
9.6 Speech Classification
The speech classiﬁcation or mode selection techniques can be divided into
three categories [45].
• Open-loop mode selection: Each frame is classiﬁed based on the obser-
vations of parameters extracted from the input speech frame without
assessing how the selected mode will perform during synthesis for the
frame concerned.
• Closed-loop mode selection: Each frame is synthesized using all the modes
and the mode that gives the best performance is selected.
• Hybrid mode selection: The mode selection procedure combines both
open-loop and closed-loop approaches. Typically, a subset of modes is ﬁrst
selected by an open-loop procedure, followed by further reﬁnements using
closed-loop techniques.

312
Multimode Speech Coding
Closed-loop mode selection has two major difﬁculties: high complexity
and difﬁculty in ﬁnding an objective measure which reﬂects the subjective
quality of synthesized speech [46]. The existing closed-loop mode selection
coders are based on CELP, and select the best conﬁguration such that the
weighted MSE is minimized [47, 48]. Open-loop mode selection is based
on techniques such as: voice activity detection, voicing decision, spectral
envelope variation, speech energy, and phonetic classiﬁcation [10]. See [49]
for a detailed description on acoustic phonetics.
In the following discussion, a hybrid mode selection technique is used, with
an open-loop initial classiﬁcation and a closed-loop secondary classiﬁcation.
The open loop initial classiﬁcation decides to use either the noise excitation or
one of the other modes. The secondary classiﬁcation synthesizes the harmonic
excitation and makes a closed loop decision to use either the harmonic
excitation or ACELP. A special feature of this classiﬁer is the application of
closed-loop mode selection to harmonic coding. The SWPM [26] preserves
the waveform similarity of the harmonically-synthesized speech, making it
possible to apply closed-loop techniques in harmonic coding.
9.6.1 Open-Loop Initial Classification
The initial classiﬁcation extracts the fully unvoiced and silence segments of
speech, which are synthesized using white-noise excitation. It is based on
tracked energy, the low-band to high-band energy ratio, and the zero-crossing
rate of the speech signal. The three voicing metrics are logically combined to
enhance the reliability, since a single metric alone is not sufﬁcient to make
a decision with high conﬁdence. The metric combinations and thresholds
are determined empirically, by plotting the metrics with the corresponding
speech waveforms. A statistical approach is not suitable for deciding the
thresholds, because the design of the classiﬁcation algorithm should consider
that a misclassiﬁcation of a voiced segment as unvoiced will severely degrade
the speech quality, but a misclassiﬁcation of an unvoiced segment as voiced
can be tolerated. A misclassiﬁed unvoiced segment will be synthesized using
ACELP, however a misclassiﬁed voiced segment will be synthesized using
noise excitation.
The tracked energy of speech, te is estimated as follows:
te = 0.00025eh + e
0.01eh + e
(9.48)
where e is the mean squared speech energy, given by,
e =
N−1

n=0
s2 (n)
N
(9.49)

Speech Classification
313
where N, the length of the analysis frames, is 160 and eh is an autoregressive
energy term given by,
eh = 0.9eh + 0.1e
if 8e > eh
(9.50)
The condition 8e > eh ensures that eh is updated only when the speech energy
is sufﬁciently high and eh should be initialized to approximately the mean
squared energy of voiced speech. Figure 9.19a illustrates the tracked energy
over a segment of speech. The low-band to high-band energy ratio, γω, is
estimated as follows:
γω =
1/4

0
S2
 ω
ωs

d
 ω
ωs

1/2

1/4
S2
 ω
ωs

d
 ω
ωs

(9.51)
where ωs is the sampling frequency and S(ω) is the speech spectrum. The
speech spectrum is estimated using a 512-point FFT, after windowing 240
speech samples with a Kaiser window of β = 6.0. Figure 9.19b illustrates the
low-band to high-band energy ratio over a segment of speech, where the
speech signal is shifted down for clarity.
The zero-crossing rate is deﬁned as the number of times the signal changes
sign, divided by the number of samples used in the observation. Figure 9.20a
illustrates the zero-crossing rate over a segment of speech, where the speech
0
1000
2000
3000
samples
−0.5
0
0.5
1
tracked energy
s(n)
0
1000
2000
3000
4000
samples
(a) Tracked energy, te
−200
−100
0
100
200
low band to high band energy
s(n)
(b) Low-band to high–band energy ratio, γω
Figure 9.19
Voicing metrics of the initial classification

314
Multimode Speech Coding
0
1000
2000
3000
4000
5000
samples
−1
−0.5
0
0.5
1
zero crossing rate
s(n)
0
1000
2000
3000
4000
5000
samples
(a)
(b)
s(n)
unvoiced
unvoiced
voiced
amplitude
Figure 9.20
(a) Zero-crossing rate and (b) Voicing decision of the initial classification
0
1000
2000
3000
4000
samples
−1
−0.5
0
0.5
1
metric value
tracked energy
zero-crossing rate
s(n)
0
1000
2000
3000
4000
samples
−200
−100
0
100
200
metric value
s(n)
low-band to high-band energy ratio
voiced
unvoiced
Figure 9.21
Voicing metrics of the initial classification
signal is shifted down for clarity. Figure 9.20b depicts the voicing decision
made by the initial classiﬁcation. Figure 9.21 depicts the three metrics used
and the ﬁnal voicing decision over the same speech segment.
Even though the plosives have a signiﬁcant amount of energy at high
frequencies and a high zero-crossing rate, synthesizing the high energy
spikes of the plosives using ACELP instead of noise excitation improves
speech quality. Therefore we need to detect the plosives, which are classiﬁed
as unvoiced by the initial classiﬁcation, and switch them to ACELP mode.
A typical plosive is depicted at the beginning of the speech segment in
Figure 9.20b.

Speech Classification
315
9.6.2 Closed-Loop Transition Detection
AbS transition detection is performed on the speech segments [26, 27] that
are declared voiced by the open-loop initial classiﬁcation. A block diagram of
the AbS classiﬁcation process is shown in Figure 9.22. The AbS classiﬁcation
module synthesizes speech using SWPM and checks the suitability of the
harmonic model for a given frame. The normalized cross-correlation and
squared error are computed in both the speech domain and the residual
domain for each of the selected pitch cycles within a synthesis frame. The
pitch cycles are selected such that they cover the complete synthesis frame.
The mode decision between harmonic and ACELP modes is then based on
the estimated cross-correlation and squared error values. The squared error
of the ith pitch cycle, Ei, is given by,
Ei =
T−1

j=0

s

iT + j

−ˆs

iT + j
2
T−1

j=0
s2 iT + j
for 0 ≤i < I
(9.52)
The normalized cross-correlation of the ith pitch cycle, Ri, is given by,
Ri =
T−1

j=0
s

iT + j
 ˆs

iT + j





T−1

j=0
s2 
iT + j
 T−1

j=0
ˆs2 
iT + j

for 0 ≤i < I
(9.53)
where T = ⌊τ + 0.5⌋, τ is the pitch period, I = ⌊N/τ + 1⌋, and N is the
synthesis frame length of 160 samples.
Harmonic
Parameters
CL 2
Harmonic
Synthesis
Using SWPM
LPC
Synthesis
Filter
Pitch Cycle
Based SNR &
Correlation 
LPC
Inverse
Filter
Pitch Cycle
Based SNR &
Correlation
Decision
Logic
Input
Speech
ˆs(n)
s(n)
r(n)
ˆr(n)
Figure 9.22
Analysis by synthesis classification

316
Multimode Speech Coding
0
200
400
600
samples
−6
−4
−2
0
2
cross-correlation
squared error
original speech
synthesized speech
0
200
400
600
samples
(a) Stationary voiced speech
(b) Transitory speech
−10
−8
−6
−4
−2
0
2
4
cross-correlation
squared error
original speech
synthesized speech
0
100
200
300
samples
−2
 −1
0
1
squared error
cross-correlation
LPC residual
LPC excitation
0
100
200
300
samples
(c) Stationary voiced LPC residual
(d) Transitory LPC residual
−2
−1
0
1
2
3
squared error
cross-correlation
LPC residual
LPC excitation
Figure 9.23
Squared error, Ei, Eir, and cross-correlation, Ri, Rir, values
In order to estimate the normalized residual cross-correlation, Rir, and
residual squared error, Eir, equations (9.52) and (9.53) are repeated with s(n)
and ˆs(n) replaced by r(n) and ˆr(n) respectively. Figure 9.23 depicts Ei, Ri,
original speech s(n), and synthesized speech ˆs(n). Ei and Ri are aligned with
the corresponding pitch cycles of the speech waveforms, and the speech
waveforms are shifted down for clarity. Examples of the residual domain
signals, LPC residual r(n), LPC excitation ˆr(n), Eir, and Rir are also shown in
the ﬁgure.
For stationary voiced speech, the squared error, Ei, is usually much lower
than unity and the normalized cross-correlation, Ri, is close to unity. How-
ever, the harmonic model fails at the transitions, which results in larger errors
and lower correlation values. The estimated normalized cross-correlation and

Speech Classification
317
squared error values are logically combined to increase the reliability of the
AbS transition detection. The combinations and thresholds are determined
empirically by plotting the parameters with the corresponding speech wave-
forms. This heuristic approach is superior to a statistical approach, because it
allows inclusion of the most important transitions, while the less important
ones can be given a lower priority. AbS transition detection compares the
harmonically synthesized speech with the original speech, veriﬁes the accu-
racy of the harmonic model parameters, and decides to use ACELP when the
harmonic model fails.
The cross-correlation and squared error values are estimated on the pitch
cycle basis in order to determine the suitability of the harmonic excitation for
each pitch cycle. Estimating the parameters over the complete synthesis frame
may average out a large error caused by a sudden transition. In Figure 9.23a,
the speech waveform has a minor transition. The estimated parameters
also indicate the presence of such a transition. These minor transitions are
synthesized using the harmonic excitation, and the mode is not changed
to waveform coding. Changing the mode for these small variations leads to
excessive switching, which may degrade the speech quality, when the bit-rate
of the waveform coder is relatively low, due to the quantization noise of the
waveform coding. Moreover, the harmonic excitation is capable of producing
good quality speech despite those small variations in the waveform. In
addition to maintaining the harmonic mode across those minor transitions,
in order to limit excessive switching, the harmonic mode is not selected after
ACELP when the speech energy is rapidly decreasing. Rapidly-decreasing
speech energy indicates an offset and at some offsets the coding mode may
ﬂuctuate between ACELP and harmonic, if extra restrictions are not imposed.
At such offsets, the accumulated error in the LPC memories through the
harmonic mode is corrected by switching to the ACELP mode, which in turn
causes a switch back to the harmonic mode. The additional measures taken
to eliminate those ﬂuctuations are described below.
In order to avoid mode ﬂuctuations at the offsets, extra restrictions are
imposed when switching to the harmonic mode after waveform coding. The
rms energy of the speech and the LPC residual are computed for each frame,
and a hysteresis loop is added using a control ﬂag. The ﬂag is set to zero
when the speech or the LPC residual rms energy is less than 0.75 times the
corresponding rms energy values of the previous frame. The ﬂag is set to one
when the speech or the LPC residual rms energy is more than 1.25 times the
corresponding rms energy values of the previous frame. The ﬂag is set to zero if
the pitch is greater than 100 samples, regardless of the energy. When switching
to harmonic mode after waveform coding, the control ﬂag should be one, in
addition to the mode decision of closed-loop transition detection. The ﬂag is
checked only at a mode transition, once the harmonic mode is initialized, the
ﬂag is ignored. This process avoids excessive switching at the offsets.

318
Multimode Speech Coding
The pitch is used to change the control ﬂag for different reasons. For male
speech with long pitch periods, ACELP produces better quality than the
harmonic coders even at stationary voiced segments. When the pitch period
is long, ACELP needs fewer pulses in the time domain to track the changes
in the speech waveform while the harmonic coders have to encode a large
number of harmonics in the frequency domain. Furthermore, it is well-known
that speech-coding schemes which preserve the phase accurately work better
for male speech, while the harmonic coders which encode only the amplitude
spectrum result in better quality for female speech [24].
9.6.3 Plosive Detection
The unvoiced synthesis process described in Section 9.5.5 updates the
unvoiced gain every 20 ms. While this is sufﬁcient for fricatives, it reduces the
quality of the highly nonstationary unvoiced components such as plosives.
The listening tests show that synthesizing plosives using ACELP preserves
the sharpness of the synthesized speech and improves the perceptual quality.
Therefore a special case is required to detect the plosives, which are classiﬁed
as unvoiced by the initial classiﬁcation, and synthesize them using ACELP.
Plosives are characterized by isolated pulse-like signals with a sharp rise
in energy, and this feature is used to distinguish them from the fricatives.
The rms energy, ej, of the speech signal is computed for every 10 samples as
follows:
ej =






9

n=0
s2 10j + n
10
for 0 ≤j < 15
(9.54)
A plosive detection metric, pj, is deﬁned as,
pj =
ej
8ej−1
(9.55)
where e−1 is the ﬁnal energy term of the previous frame. A frame is classiﬁed
as containing a plosive if pj > 1 for at least one j. This algorithm may signal
a plosive even when the overall energy level is very low, for example at a
silence segment, if it detects a large ﬂuctuation in energy. Those low-energy
segments are completely ignored when using the tracked energy term, te, in
the open-loop initial classiﬁcation.
It should be noted that the scope of this plosive detection algorithm is
reduced to unvoiced segments, since the segments that include the unvoiced
plosives are already identiﬁed by the initial classiﬁcation. The plosive detec-

Hybrid Decoder
319
0
200
400
600
800
samples
–6
–4
–2
0
2
plosive detection metric
s(n)
0
200
400
600
800
samples
(a)
(b) 
original speech
SB-LPC
ACELP
amplitude
noise excitation
Figure 9.24
Plosive (a) detection and (b) synthesis
tion algorithm may erroneously identify the highly nonstationary onsets and
the speech signal near the glottal excitation of the long pitch period segments
as plosives, if applied to voiced speech. Figure 9.24a illustrates the plosive
detection metric pj and an example of a plosive. Figure 9.24b illustrates the
detected plosive synthesized using 4 kb/s SB-LPC and 3.7 kb/s ACELP (with-
out LTP). ACELP is used only for the frame that has the plosive and the rest of
the segment is synthesized using white-noise excitation. SB-LPC synthesizes
the speech segment using noise excitation, which cannot adequately represent
the plosive.
9.7 Hybrid Decoder
A simpliﬁed block diagram of the hybrid decoder is shown in Figure 9.25. The
decoder extracts the excitation parameters from the data bit stream according
to the mode and uses the appropriate excitation generation. The synthesized
excitation is then fed into the LPC synthesis ﬁlter, which produces the ﬁnal
synthetic speech output. The LPC parameters are common for all the modes
and linearly interpolated in the LSF domain with an update interval of 5 ms.
The excitation vector is also fed into the ACELP excitation and harmonic
excitation generators. The ACELP excitation updates the long term predic-
tion (LTP) buffer with the previous LPC excitation. The harmonic excitation
uses the previous excitation at the onsets to initialize the interpolation and
prediction parameters. In Figure 9.26 the results are shown for the original
and synthesized speech together with the mode used for each synthesis
frame. The frame boundaries are also shown by dashed lines.

320
Multimode Speech Coding
Received Excitation
Parameters
White Noise
Excitation
Harmonic
Excitation
ACELP
Excitation
CL2
CL1
CL2
CL1
LSF 
LPC
Synthesis
Filter
LSF to LPC
Conversion
sˆ(n)
LSF
Interpolation
Figure 9.25
Block diagram of the hybrid decoder
0
200
400
600
800
1000
samples
original speech
synthesized speech
A
N
N
N
A
H
amplitude
Figure 9.26
Synthesized speech and classification: A (ACELP), H (harmonic), and N
(noise excitation)
9.8 Performance Evaluation
The hybrid coder [26] described above has been tested to evaluate its perfor-
mance. The major tasks were developing a reliable classiﬁcation technique
and preserving the phase continuity when switching between the coding
modes. The classiﬁcation algorithm is tested using 64 seconds of modiﬁed

Performance Evaluation
321
IRS-ﬁltered speech, by comparing the mode decision against manually-
classiﬁed waveforms. Eight English sentence pairs uttered by four male and
four female speakers, taken from the Nippon Telegraph and Telephone [50]
speech database are used as the test material. The silence segments are
excluded from the analysis and synthesized using white-noise excitation.
The initial classiﬁcation detects all the voiced frames. Therefore the worst
possible classiﬁcation error, i.e. classifying a voiced frame as unvoiced, is
eliminated. More than 90 % of the unvoiced frames are also detected and the
rest of the unvoiced frames are misclassiﬁed as voiced. This bias towards
voiced is preferable to misclassifying voiced frames as unvoiced, since the
misclassiﬁed unvoiced frames will be classiﬁed as ACELP by the secondary
classiﬁcation, while a misclassiﬁed voiced frame will be synthesized using
white-noise excitation. The plosive detection algorithm detects all the plosives
in the unvoiced frames and does not misclassify other unvoiced frames as
plosives.
The transition frames are manually marked by observing the waveforms,
in order to test the closed-loop transition detection algorithm. Speech frames
which have irregular pitch periods and show large variations in the energy
are identiﬁed as transitions. The closed-loop transition detection classiﬁes the
frames already classiﬁed as voiced by the initial classiﬁcation into transitory
and harmonic. Consequently, all the frames classiﬁed as voiced by the initial
classiﬁcation are included in the test and the unvoiced frames that are
classiﬁed as voiced are marked as transitions, since they are expected to be
synthesized using ACELP. When testing the transition detection algorithm,
the use of waveform coding for pitch periods longer than 100 samples is not
activated. The transition detection algorithm detects more than 90 % of the
transition frames and the rest of the transitions are classiﬁed as harmonic
frames. It also detects more than 90 % of the harmonic frames and the rest of
the stationary voiced frames are classiﬁed as transitions.
Misclassiﬁcations may restrict the maximization of the speech quality
because of not choosing the best coding algorithm. However misclassiﬁcations
of the secondary classiﬁcation do not degrade the speech quality, due to its
closed-loop nature. A misclassiﬁcation of a stationary voiced segment as a
transition indicates a harmonic parameter estimation error and such frames
are synthesized using ACELP, perhaps a better solution than synthesizing
with the inaccurate harmonic parameters. A misclassiﬁcation of a transition as
stationary voiced indicates that the harmonic mode is capable of synthesizing
the particular transitory frame. This may be possible at some transitions,
particularly offsets, which usually have a steady pitch contour and a smooth
energy variation, where the harmonic interpolation model can ﬁt in.
The phase continuity is tested by listening to the synthesized speech,
without introducing quantization. The tests verify the validity of the hybrid
model and there are no perceptible discontinuities. The speech synthesized

322
Multimode Speech Coding
Table 9.1
Unquantized hybrid model vs 128 kb/s linear PCM
Better
Slightly better
Same
Slightly worse
Worse
Male (%)
0.0
19.3
51.9
26.9
1.9
Female (%)
0.0
5.8
69.2
17.3
7.7
Average (%)
0.0
12.5
60.6
22.1
4.8
Table 9.2
Unquantized hybrid model vs 8 kb/s G.729
Better
Slightly better
Same
Slightly worse
Worse
Male (%)
1.9
30.8
51.9
15.4
0.0
Female (%)
0.0
34.7
44.2
17.3
3.8
Average (%)
1.0
32.7
48.1
16.3
1.9
also indicates the upper bound of the quality achievable by the designed
hybrid model. An informal listening test was conducted using 128 kb/s linear
pulse code modulation (PCM), which is the best narrow-band speech quality,
and 8 kb/s ITU G.729, a toll-quality speech coder, as the reference coders
[26]. The speech material used for the test consists of eight sentences, four
from male and four from female talkers, ﬁltered by the modiﬁed IRS ﬁlter
and a pair of headphones was used to conduct the test. Twelve listeners were
asked to indicate their preferences for the randomized pairs of synthesized
speech. Both experienced and inexperienced listeners participated in the test.
The subjective test results are shown in Tables 9.1 and 9.2. As indicated by
these results, the unquantized hybrid model performs better than G.729 and
worse than 128 kb/s linear PCM. Therefore the quality of the unquantized
hybrid model can be classiﬁed as being higher than toll quality and lower
than transparent quality. In general, the speech encoded and decoded with
unquantized hybrid coder model parameters does not sound too different
from the original speech material. The perceived speech quality shows only
a slight degradation, even after quantizing the harmonic mode parameters at
4 kb/s and white-noise excitation at 1.5 kb/s, with unquantized transitions (at
128 kb/s linear PCM). The hybrid coder achieves toll quality when transitions
are quantized with 6 kb/s ACELP.
9.9 Quantization Issues of Hybrid Coder Parameters
9.9.1 Introduction
The above hybrid speech-coding model can be adopted for various applica-
tions with different quality requirements by quantizing the model parameters
at different bit-rates. For applications which support variable bit rates, the

Quantization Issues of Hybrid Coder Parameters
323
model parameters of different modes may be quantized at different bit-rates,
allocating the minimum number of bits required for each mode to maintain
adequate quality.
In the example, here the LPC parameters are common for all the modes,
and quantized using a ﬁxed number of bits per frame. This is advantageous
under noisy channel conditions, since the LPC parameters can be decoded
correctly even when the mode bits are in error. The LPC parameters are
quantized in the LSF domain using a multi-stage vector quantiﬁer (MSVQ),
with a ﬁrst order moving average (MA) prediction [37]. Having quantized
the LSFs, the excitation of the three modes are quantized differently.
9.9.2 Unvoiced Excitation Quantization
The hybrid coding algorithm synthesizes unvoiced speech using scaled white
Gaussian noise as the LPC excitation. Therefore, only a gain term is required
in addition to the LPC parameters to synthesize unvoiced speech. In order
to synthesize the unvoiced plosives with adequate quality, the gain term
should be updated at least every 5 ms. However listening tests show that
synthesizing plosives using ACELP gives better perceptual quality. Therefore
the plosives are synthesized using ACELP. The energy of the fricatives does
not show rapid ﬂuctuations and updating at the frame rate of every 20 ms is
adequate to synthesize high-quality unvoiced fricatives.
The unvoiced gain guv is quantized using a logarithmic scalar quantizer.
The quantized unvoiced gain guvi is given by,
guvi = k
gmax + k
k

i
N−1
−k
for i = 0, 1, 2, . . . , N −1
(9.56)
where N is the number of quantizer levels, gmax, deﬁnes the upper limit of guvi,
and k is a constant which controls the gradient of the exponential function.
All the guv values larger than gmax are clipped at gmax. The constant k is set as
16 and 32 quantizer levels were sufﬁcient to produce high quality unvoiced
speech. Hence ﬁve bits are required to transmit the quantized unvoiced gain,
guvi. Figure 9.27 depicts a typical plot of the unvoiced gain quantizer levels
where the maximum gmax = 904.
9.9.3 Harmonic Excitation Quantization
The stationary voiced speech segments are synthesized using the synchro-
nized harmonic excitation model described earlier. The model parameters
of the harmonic excitation with SWPM are pitch period, pitch pulse loca-
tion (PPL), pitch pulse shape (PPS), harmonic amplitudes, and gain. The
AbS transition detection algorithm synthesizes the harmonic excitation using
SWPM at the encoder to evaluate the suitability of the harmonic mode.

324
Multimode Speech Coding
0
10
20
30
40
quantizer index (i)
0
200
400
600
800
1000
quantized gain
Figure 9.27
Unvoiced gain quantizer levels
Therefore, quantized or unquantized harmonic parameters may be used for
the transition detection at the encoder. Generally, AbS algorithms include
the quantization in the error minimization loop, so that the quantization
noise is also accounted for in the parameter estimation process. However in
this case, the solution is not straightforward, since the decision is between
two modes, rather than the best set of parameters of a unimodal coder.
One solution to this problem is to perform a full closed-loop mode decision
with quantized parameters, i.e. synthesizing the speech frames with all the
modes and selecting the best mode. A weighting factor may be required
in the mode selection process, since the harmonic excitation with SWPM
may give superior perceptual quality even with a slightly lower SNR com-
pared to ACELP. However such a solution is computationally demanding,
since ACELP excitation should be computed for all the frames, excluding
the silence and unvoiced frames. Furthermore, deﬁning a suitable weighting
factor which reﬂects the perceptual quality is a difﬁcult task.
A more practical solution is to decide the inclusion of the harmonic
parameter quantization in the mode decision loop based on ACELP bit
rate. The inclusion of the harmonic quantization in the closed-loop mode
decision increases the number of ACELP mode frames. However, occasionally
switching to ACELP between harmonic frames may degrade the perceptual
quality, when the bit rate of the ACELP mode is below 8 kb/s, due to
the sudden discontinuities introduced in the voiced harmonics. In general,
ACELP operating at 8 kb/s or higher is capable of synthesizing perceptually-
superior speech compared to harmonic coding (with no phase transmission),

Quantization Issues of Hybrid Coder Parameters
325
even at the stationary voiced segments. Therefore the harmonic quantization
can be included in the closed-loop mode decision without worrying about
the quality of ACELP coded frames between the harmonic frames (except, of
course, the bit rate will be higher). However when the bit rate of ACELP mode
is low, the quantization noise becomes audible; hence, trying to eliminate
the quantization noise of the harmonic mode by switching to ACELP mode
does not improve the perceptual quality. Therefore, in all the tests described
here, harmonic parameter quantization is not included in the transition
detection loop.
The sensitivity of AbS transition detection is different for each parameter.
The sensitivity is high for the pitch period and PPL. Changes in these
parameters dramatically reduce the cross-correlation of the original and the
synthesized speech, due to the resulting time shifts. The spectral amplitudes
and the LPC parameters are least sensitive. In fact, quantized and unquantized
LPC parameters both produced the same classiﬁcation decisions for the
test speech material. The LPC memory locations of the transition detection
algorithm are initialized for each frame with the memory locations of the LPC
synthesis ﬁlter. This avoids drifting the LPC synthesis ﬁlter of the transition
detection algorithm from the synthesized speech.
Pitch Quantization
The pitch period, τ, is quantized using a nonlinear scalar quantizer, reﬂecting
the high sensitivity of the human ear to the pitch deviations at shorter pitch
periods. A logarithmic scale is used for the pitch values from 16 to 60 samples
and a linear scale is used for the pitch values from 60 to 160 samples (see
Figure 9.28). The quantized pitch τi is given by,
τi = τmin
 τ0
τmin

i
N0−1
for i = 0, 1, 2, . . . , N0 −1
(9.57)
τi = τ0 + τmax −τ0
N −N0
(i −N0 + 1)
for i = N0, N0 + 1, . . . , N −1 (9.58)
where τmin is 16, τmax is 160, τ0 is 60, N0 is 156, and N is 256. Therefore eight
bits are required to transmit the quantized pitch period.
Pitch Pulse Location Quantization
The pitch pulse location (PPL) is the location of the pitch pulse closest to the
centre of the analysis frame. PPL may be deﬁned as the distance to the pitch
pulse concerned from the centre of the analysis frame, measured in samples.
Assuming that the maximum possible pitch is 160 samples, PPL varies
between −80 and 80. However the pitch pulse location may be normalized

326
Multimode Speech Coding
0
100
200
300
quantizer index (i)
0
50
100
150
200
quantized pitch
Figure 9.28
Pitch quantizer levels
with respect to the pitch so that the PPL varies between −0.5 and 0.5.
Normalization of the PPL with respect to the pitch ensures the efﬁcient use
of quantizer dynamic range regardless of the pitch.
The accuracy of the PPL is more important when it is close to the centre
of the analysis frames or the synthesis frame boundaries, i.e. PPL values
close to zero. This is due to the fact that the mode changes between ACELP
and harmonic excitation may take place at the synthesis frame boundaries.
Preserving the continuity of the high-energy pitch pulses occurring at or close
to the switching frame boundaries is essential to eliminate audible switching
artifacts. Therefore the normalized PPL is quantized using a logarithmic scale,
quantizing the PPL values close to zero more accurately. The quantized PPL,
ti is given by,
ti = k
0.5 + k
k
 i−N/2−1
N/2
−k
for i = N/2 −1, N/2, . . . , N −1
(9.59)
ti = tN−2−i
for i = 0, 1, . . . , N/2 −2
(9.60)
where N is the number of quantizer levels and k is a constant that controls the
gradient of the exponential function. The constant k is set to 0.125, and 128
quantizer levels are sufﬁcient to eliminate audible switching artifacts. Hence
seven bits are required to transmit the quantized normalized PPL. PPL is
normalized using the quantized pitch so that the decoder can denormalize the

Quantization Issues of Hybrid Coder Parameters
327
0
50
100
150
quantizer index (i)
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
quantized normalized PPL
Figure 9.29
PPL quantizer levels
received PPL value accurately. Figure 9.29 depicts a plot of the normalized
PPL quantizer levels.
Pitch Pulse Shape Quantization
Large variations in the PPS introduces a reverberant character into the
synthesized speech, regardless of the PPS value. Therefore, in terms of the
perceptual quality, all the PPS values are equally important and a linear
quantizer is employed to quantize the PPS using 16 values. The quantized
PPS, θi, is given by,
θi = 2π
N i −π
for i = 0, 1, . . . , N −1 and
−π ≤θi < π
(9.61)
where N, the number of quantizer levels, is 16 and four bits are required to
quantize PPS.
Harmonic Amplitude Quantization
Harmonic amplitudes of the LPC residual are quantized using Switched
Predictive Mel-scale-based Vector Quantization (SP-MVQ) [51]. SP-MVQ
(see block diagram in Figure 9.30) converts the variable-dimension spectral-
amplitude vectors into ﬁxed-dimension vectors by warping the frequency
axis using a logarithmic scale. The warping process emphasizes the low

328
Multimode Speech Coding
+ 
−
Σ
+ 
+ 
Σ
+ 
+ 
Σ
Mel to
Linear
First Order
Predictor
a
Perceptual
Weighting
Mean Square Error
Minimization
aˆ
ML-MVQ
P-MVQ 
zˆm
zˆr
zˆp
zˆr
ω0
Figure 9.30
Block diagram of SP-MVQ
frequencies, taking into account the perceptual preferences of the human
auditory system. The ﬁxed dimension spectral vector, ˆz, is decomposed into
a predicted vector, ˆzp, and a prediction residual vector, ˆzr, as follows:
ˆz = ˆzp + ˆzr
(9.62)
where the predicted vector, ˆzp, is obtained using a ﬁrst-order autoregressive
method, given by,
ˆzp = 
ˆz−1 −ˆzm

+ ˆzm
(9.63)
where ˆz−1 is the most recently quantized ˆz, ˆzm is the mean vector, and 
denotes a diagonal matrix of prediction coefﬁcients. The prediction residual,
ˆzr is quantized using a typical vector quantizer such as MSVQ [52]. The
quantization becomes memoryless Mel-scale-based vector quantization (ML-
MVQ) if all the prediction coefﬁcients are zero, and autoregressive predictive
MVQ (P-MVQ) otherwise. The predictive scheme is effective in stationary
regions, and may increase spectral distortion at the transitions; therefore, a
switching scheme is introduced to switch between P-MVQ and ML-MVQ. The
decision between P-MVQ and ML-MVQ is made using AbS techniques and

Quantization Issues of Hybrid Coder Parameters
329
based on a weighted spectral-distortion measure. Therefore the quantization
scheme is called switched predictive Mel-scale-based vector quantization
(SP-MVQ). Moreover, the switching scheme restricts error propagation under
noisy channel conditions.
SP-MVQ quantizes spectral amplitudes every 10 ms using 14 bits. The har-
monic analysis/synthesis scheme described estimates the harmonic param-
eters every 20 ms. However there are sufﬁcient bits for the allocation of
28 bits per 20 ms frame for spectral amplitudes at 4 kb/s (see Table 9.5).
Therefore the harmonic analysis/synthesis scheme is modiﬁed to update
the spectral amplitudes every 10 ms. However the pitch is transmitted only
every 20 ms, and linearly interpolated to compute the number of harmonics
corresponding to the centre of the synthesis frame or the ﬁrst subframe,
at the decoder. The spectral amplitude quantization uses the quantized
(second subframe) or quantized and interpolated (ﬁrst subframe) pitch to
compute the number of harmonics, in order to ensure the correct dequan-
tization of the spectral amplitude vectors. In the spectral amplitude quan-
tization of the ﬁrst subframe, if the actual number of harmonics is greater
than the computed number of harmonics by interpolation, the higher har-
monics are ignored. If the actual number of harmonics is less than the
computed number of harmonics by interpolation, the amplitude vector is
zero-padded. Usually the pitch values of the stationary voiced segments are
fairly unchanged and linear interpolation of the number of harmonics gives
a good approximation.
Harmonic Gain Quantization
The spectral amplitude vectors are normalized before the quantization, in
order to improve the dynamic range. The shape components of the vectors
are quantized using SP-MVQ, as described above, and the gain component is
scalar quantized.
The normalized amplitude, akn, of the kth harmonic is given by,
akn = ak
g
(9.64)
where ak is the spectral amplitude estimated for the kth harmonic and g is the
normalization factor, given by,
g =






K

k=1
a2
k
K
(9.65)

330
Multimode Speech Coding
where K is the total number of harmonics. Normalization factor of the second
subframe, g2, is quantized using a logarithmic scale, given by,
g2i = k
gmax −gmin + k
k

i
N−1
−k + gmin
for i = 0, 1, . . . , N −1
(9.66)
where k is eight (which controls the gradient of the exponential function), N
(the number of quantizer levels) is 32, i.e. ﬁve bits are required to quantize
the gain of the second subframe, and gmax and gmin are the maximum and
minimum possible quantized normalization factors, respectively. The gain
values beyond gmax and gmin are clipped by the quantizer. The term gmin is
introduced in equation (9.66), because only the stationary voiced segments
are synthesized using the harmonic excitation and the minimum gain is
nonzero.
The normalization factor of the ﬁrst subframe, g1, is differentially quantized
with respect to the mean of the adjacent two quantized g2 values, as follows:
δ = g1 −g2 + g2−1
2
(9.67)
where g2−1 is the gain of the second subframe of the previous frame, i.e. the
previous g2, and δ is quantized using three bits. Finally the spectral amplitude
vectors are denormalized by multiplying with the quantized normalization
factors.
Onset Harmonic Parameter Quantization
The harmonic synthesis process interpolates the parameters between the
synthesis frame boundaries. However, at the onsets, when switching from
waveform-coding mode, the harmonic parameters of the initial synthesis
frame boundary are not directly available. The pitch, PPL, and PPS are
estimated, as described in Section 9.5.4, and quantized as described in the
preceding sections.
The spectral amplitudes of the ACELP excitation signal used before the
harmonic mode are estimated by windowing it using an asymmetric window
function given by,
w (n) = 0.54 −0.46 cos

π
n
n1 −1

for
0 ≤n < n1
(9.68)
w (n) = 0.08 + 0.92 cos
π
2
n −n1
n2 −1

for n1 ≤n < n1 + n2
(9.69)
where n1 is 140 and n2 is 20. The asymmetric window function emphasizes
the excitation signal close to the switching frame boundary.

Variable Bit Rate Coding
331
The spectral amplitude vector of the windowed ACELP excitation signal
is obtained by peak-picking of the magnitude spectrum, using the received
pitch value for the harmonic frame. The rms normalization factor of the
estimated spectral vector is used as g2−1 of the harmonic frame. The amplitude
quantization memory, ˆz−1 is initialized by quantizing the normalized shape
vector, while forcing SP-MVQ to use memoryless quantization.
9.9.4 Quantization of ACELP Excitation at Transitions
The transitions are quantized using algebraic code excited linear predic-
tion (ACELP). The pulse innovation of ACELP is capable of synthesizing
highly nonstationary transitions. The long term prediction (LTP) is not very
efﬁcient at the onsets, since the LTP memory buffer has no information
regarding the onsets. However LTP is employed, because it reduces the
sparse excitation artifacts [39] and synthesizes a signiﬁcant amount of the
excitation at the offsets. Moreover, at the resonance offsets, where the gain
of the excitation signal is small, the LTP gain acts as an adaptive gain term
and compensates for an inadequate gain quantization dynamic range of the
innovation pulses. Multi-tap and fractional delay LTP ﬁlters [53] are useful
only for stationary voiced segments, consequently, only integer delays and
single-tap ﬁlters are used to encode transitions.
The LTP gain is close to unity during the stationary voiced segments.
However at the transitions, LTP gain shows large variations, due to the large
variations in the speech energy. Therefore the LTP gain is quantized using
a larger dynamic range. A drawback in allowing gain values larger than
unity is that the LTP ﬁlter may become unstable under erroneous channel
conditions. The high-energy pulses of plosives are synthesized using only the
innovation sequence of ACELP. However the plosives are not classiﬁed as a
separate mode; instead, when a plosive is detected, the LTP gain is forced to
be zero.
9.10 Variable Bit Rate Coding
When using a 4 kb/s harmonic coder for steady state voiced segments and
unvoiced segments quantized at 1.5 kb/s (as detailed in Table 9.5) with
unquantized transitions, the synthesized speech quality shows only a slight
degradation when compared with using the unquantized model parameters,
which is nearly transparent. The quality versus the bit-rate limitation of this
hybrid coder is therefore dependent on transition quantization by ACELP.
Informal listening tests show that quantizing the transitions at 6 kb/s is
sufﬁcient to achieve toll quality. Three versions of the coder are tested and
compared with standard coders by quantizing the transitions at 4, 6 and
8 kb/s.

332
Multimode Speech Coding
9.10.1 Transition Quantization with 4 kb/s ACELP
The 4 kb/s version uses 10 ms subframes. For each subframe the LTP delay,
LTP gain, locations, signs, and the gain of two innovation pulses are trans-
mitted. The innovation gain terms of the two subframes are normalized
with respect to the quantized rms energy of the speech signal and the nor-
malization factor is transmitted for each 20 ms frame. The normalization
reduces the dynamic range required to quantize the innovation sequence
gain. Table 9.5 shows the bit allocation of the 4 kb/s ACELP parameters.
The LTP delay range is from 20 to 147, and only integer delays are allowed,
needing seven bits for the index. The LTP gain is quantized using four bits
(see Table 9.3). The two innovation pulses cover only the ﬁrst 64 locations of
each 80-sample subframe. Each pulse is chosen from 32 possible locations,
either even or odd, and ﬁve bits are required to transmit the location. The sign
of each pulse is transmitted using one bit. The pulse gain and the common
normalization factor of the frame are quantized using three bits each (see
Table 9.4).
9.10.2 Transition Quantization with 6 kb/s ACELP
The 6 kb/s version uses 5 ms subframes. For each subframe the LTP delay, LTP
gain, locations, signs, and the gain of two innovation pulses are transmitted.
The pulse gain terms of the four subframes are normalized with respect to
the quantized rms energy of the speech signal and the normalization factor
is transmitted for each 20 ms frame. Table 9.5 shows the bit allocation of the
6 kb/s ACELP parameters. The LTP delay and gain are quantized in the same
way to the 4 kb/s version, using seven bits and four bits respectively.
The two innovation pulses cover only the ﬁrst 32 locations of each 40-
sample subframe. Each pulse is chosen from 16 possible locations, either even
Table 9.3
LTP Gain quantizer table
Index
0
1
2
3
4
5
6
7
LTP Gain
0.00
0.15
0.30
0.40
0.50
0.60
0.70
0.80
Index
8
9
10
11
12
13
14
15
LTP Gain
0.90
1.05
1.20
2.00
3.50
5.50
8.00
10.00
Table 9.4
Innovation pulse gain quantizer table
Index
0
1
2
3
4
5
6
7
Pulse Gain
0.0
0.3
0.7
1.1
1.6
2.1
2.7
3.5
rms Gain
10
40
90
176
325
584
1030
1800

Variable Bit Rate Coding
333
Table 9.5
Bit allocation for a 20 ms frame
Parameters
White noise
Harmonic
ACELP 4k
ACELP 6k
LPC
23
23
23
23
Pitch
–
8
–
–
PPL
–
7
–
–
PPS
–
4
–
–
Amplitudes
–
14 + 14
–
–
Gain
5
3 + 5
3
3
LTP Delay
–
–
7 + 7
7 + 7 + 7 + 7
LTP Gain
–
–
4 + 4
4 + 4 + 4 + 4
Pulse Locations
–
–
10 + 10
8 + 8 + 8 + 8
Pulse Signs
–
–
2 + 2
1 + 1 + 1 + 1
Pulse Gain
–
–
3 + 3
3 + 3 + 3 + 3
Mode
2
2
2
2
Total
30
80
80
120
or odd, and four bits are required to transmit the location. The signs of the
two pulses are forced to be opposite in the error minimization process, hence
only the sign of the ﬁrst pulse is transmitted, using one bit. The pulse gain
and the common normalization factor of the frame are quantized using three
bits each (see Table 9.4).
9.10.3 Transition Quantization with 8 kb/s ACELP
The 8 kb/s version uses 5 ms sub frames. For each subframe the LTP delay,
LTP gain, locations, signs, and the gain of four innovation pulses are transmit-
ted. The pulse gain terms of the four subframes are normalized with respect
to the quantized rms energy of the speech signal and the normalization factor
is transmitted for each 20 ms frame. Table 9.8 shows the bit allocation of the
8 kb/s ACELP parameters. The LTP delay and gain are quantized in the same
way as the 4 kb/s version, using seven bits and four bits, respectively.
The locations and the signs of the four pulses are shown in Table 9.6.
The pulse gain of each subframe is quantized using four bits, as shown in
Table 9.7. The common normalization factor, i.e. the rms energy of the original
speech signal, in each frame is logarithmically quantized using seven bits,
and the quantized value, grmsi, is given by,
grmsi = k
gmax −gmin + k
k

i
N−1
−k + gmin
for i = 0, 1, . . . , N −1
(9.70)

334
Multimode Speech Coding
Table 9.6
Structure of the 17-bit algebraic codebook
Pulse
Amplitude
Position
Bits
0
±1
0, 5, 10, 15, 20, 25, 30, 35
1 + 3
1
±1
1, 6, 11, 16, 21, 26, 31, 36
1 + 3
2
±1
2, 7, 12, 17, 22, 27, 32, 37
1 + 3
3
±1
3, 8, 13, 18, 23, 28, 33, 38,
1 + 4
4, 9, 14, 19, 24, 29, 34, 39
Table 9.7
Innovation pulse gain quantizer table for 8 kb/s
ACELP
Index
0
1
2
3
4
5
6
7
Pulse Gain
0.0
0.15
0.3
0.45
0.6
0.8
1.0
1.2
Index
8
9
10
11
12
13
14
15
Pulse Gain
1.5
1.8
2.1
2.4
2.8
3.2
3.7
4.3
Table 9.8
Bit allocation of 8 kb/s ACELP
for a 20 ms frame
Parameters
ACELP 8k
LPC
23
Gain
7
LTP Delay
7 + 7 + 7 + 7
LTP Gain
4 + 4 + 4 + 4
Pulse Locations
13 + 13 + 13 + 13
Pulse signs
4 + 4 + 4 + 4
Pulse Gain
4 + 4 + 4 + 4
Mode
2
Total
160
Where k (a constant which controls the gradient of the exponential function)
is 80, N (the number of quantizer levels) is 128, and gmax and gmin are 2720.5
and 0.5 respectively.
9.10.4 Comparison
Three informal listening tests were conducted to assess the speech quality of
the hybrid coder, with transitions quantized at 4 kb/s, 6 kb/s, and 8 kb/s. The

Variable Bit Rate Coding
335
synthesized speech was compared against that from 5.3 kb/s ITU G.723.1,
6.3 kb/s ITU G.723.1, and 8 kb/s ITU G.729 coders. In all the tests, stationary
voiced segments were quantized at 4 kb/s, and silence and unvoiced segments
are quantized at 1.5 kb/s. The speech material used for each test consists of
eight sentences, four from male and four from female talkers, ﬁltered by
modiﬁed IRS ﬁlter; a pair of headphones was used to conduct the test.
Twelve listeners were asked to indicate their preferences for randomized
pairs of synthesized speech. Both experienced and inexperienced listeners
participated in the test. The subjective test results are shown in Tables 9.9,
9.10, and 9.11.
For the speech material used in the subjective tests, after discarding the
silence frames, about 64 % of the frames used harmonic excitation, 22 % used
ACELP, and 14 % used white-noise excitation. The 4 kb/s, 6 kb/s, and 8 kb/s
ACELP mode hybrid coders give average bit-rates of 3.65 kb/s, 4.1 kb/s, and
4.53 kb/s, respectively. The 4 kb/s ACELP version performs slightly better
than G.723.1 at 5.3 kb/s. The 6 kb/s ACELP version achieves similar quality
to G.723.1 at 6.3 kb/s. The quality of the 8 kb/s ACELP version is also similar
to G.729 at 8 kb/s, with an overall average bit rate of 4.53 kb/s.
Table 9.9
4 kb/s ACELP hybrid vs 5.3 kb/s G.723.1
Better
Slightly better
Same
Slightly worse
Worse
Male (%)
6.2
34.4
28.2
31.2
0.0
Female (%)
9.4
31.2
37.5
18.8
3.1
Average (%)
7.8
32.8
32.8
25.0
1.6
Table 9.10
6 kb/s ACELP hybrid vs 6.3 kb/s G.723.1
Better
Slightly better
Same
Slightly worse
Worse
Male (%)
0.0
31.3
43.7
18.8
6.2
Female (%)
6.3
28.1
37.5
21.9
6.2
Average (%)
3.2
29.7
40.6
20.3
6.2
Table 9.11
8 kb/s ACELP hybrid vs 8 kb/s G.729
Better
Slightly better
Same
Slightly worse
Worse
Male (%)
0.0
9.6
65.4
23.1
1.9
Female (%)
1.9
11.5
55.8
30.8
0.0
Average (%)
1.0
10.6
60.5
26.9
1.0

336
Multimode Speech Coding
9.11 Acoustic Noise and Channel Error Performance
Robustness to background noise and channel errors is an important factor for
any practical speech-coding algorithm. The speech coders designed for mobile
and military communication applications frequently encounter acoustic noise
and channel errors. The background noise may be suppressed before the
encoding process using a noise preprocessor [54]. However, this involves
additional complexity and delay, which may not be desirable for mobile
communication applications. Therefore the speech-coding algorithms are
expected to produce intelligible synthetic speech even in the presence of
background noise. Generally, AbS coders perform better than parametric
coders under noisy background conditions. This inherent robustness of AbS
coders is due to their waveform-matching process. The error minimization
process attempts to synthesize the input waveform regardless of its contents.
The model parameters estimated by the parametric coders may not be
accurate when the input speech signal is corrupted with noise. Inaccurate
model parameters may severely degrade the synthetic speech of a parametric
coder.
Channel errors are usually divided into two classes: random errors and
burst errors. A speech-coding algorithm should provide a reasonable output
even if a small proportion of the received bit stream is incorrect due to random
bit errors. Robustness against random channel errors can be increased by
means of index assignment algorithms [55, 56], through proper quantizer
design, and by adding redundancy into the transmitted information [57, 58,
59]. Unequal error protection techniques may be applied to provide a higher
degree of protection to the most sensitive bits. For example, in CELP coders,
the spectral envelope parameters are the most sensitive to errors, followed by
the ﬁxed codebook gain, the adaptive codebook index, the adaptive codebook
gain, the sign of the ﬁxed codebook gain, and the ﬁxed codebook index [60]. In
the case of sinusoidal coders, the gain is the most sensitive to errors, followed
by the voicing, the pitch, the spectral envelope parameters, and the spectral
amplitudes [61].
In the case of burst errors, error detection schemes are used to classify each
frame of received bits as usable or unusable. A similar problem encountered
in packet voice communication systems is lost packets due to transmission
impairments and excessive delays. In order to reduce the annoying artifacts
due to lost frames, concealment techniques based on waveform substitution
can be used [62]. The burst errors may also be converted to occur in a
more random fashion using interleaving techniques. The performance issues
speciﬁc to a hybrid coding algorithm are the robustness of the classiﬁcation
algorithm under acoustic noise and the channel bit error performance of the
coding mode; otherwise, the performance of hybrid coders will be similar to
either ACELP or harmonic coding.

Acoustic Noise and Channel Error Performance
337
9.11.1 Performance Under Acoustic Noise
The classiﬁcation algorithm was tested using 64 seconds of male and female
speech corrupted with either babble or vehicular noise. The SNR of the
corrupted speech is 10 dB.
Figure 9.31 depicts the classiﬁcation of the female speech. The initial clas-
siﬁcation declares only the strongly-unvoiced segments as unvoiced and all
0
500
1000
1500
samples
(a) Stationary voiced speech
(b) Unvoiced speech
A
H
H
H
H
H
H
A
A
amplitude
0
500
1000
1500
samples
H
A
A
A
N
N
A
A
A
amplitude
Figure 9.31
Classification of female speech corrupted by babble noise (10 dB SNR):
A (ACELP), H (harmonic), and N (noise excitation)

338
Multimode Speech Coding
0
500
1000
1500
samples
amplitude
A
H
H
H
H
H
H
H
A
0
500
1000
1500
samples
(a) Stationary voiced speech
(b) Unvoiced speech
amplitude
H
H
A
N
N
N
N
N
N
Figure 9.32
Classification of clean speech corresponding to Figure 9.31
the other frames are left to be encoded using either ACELP or harmonic
excitation (compare Figures 9.31b and 9.32b). The weakly-unvoiced segments
which have lower energy than the noise level are not detected as unvoiced.
When corrupted with babble or vehicular noise, the silence and the low-
energy unvoiced segments do not have the properties of unvoiced speech.
It can be seen that the energy of the noise component is comparable with
unvoiced speech and it has a signiﬁcant low-frequency component (see
Figure 9.35a). This is expected since babble noise is essentially attenuated
and superimposed speech components. Figure 9.33 shows the classiﬁcation
of the male speech and Figure 9.34 shows the corresponding clean speech
segments.

Acoustic Noise and Channel Error Performance
339
0
500
1000
1500
samples
H
H
H
H
H
H
A
A
H
amplitude
(a) Stationary voiced speech
0
500
1000
1500
samples
(b) Unvoiced speech (second frame)
A
A
N
H
H
H
H
H
H
amplitude
Figure 9.33
Classification of male speech corrupted by babble noise (10 dB SNR):
A (ACELP), H (harmonic), and N (noise excitation)
The secondary classiﬁcation performs very similarly under the clean speech
conditions, except for the occasional classiﬁcation of frames as ACELP, which
were originally classiﬁed as harmonic under the clean speech conditions
(compare Figures 9.31a and 9.32a). This is due to the inability of the harmonic
model to adequately synthesize the corrupted signal and the model parameter

340
Multimode Speech Coding
0
500
1000
1500
samples
A
H
H
H
H
H
H
H
A
amplitude
0
500
1000
1500
samples
(a) Stationary voiced speech
(b) Unvoiced speech (second frame)
A
N
N
H
H
H
H
H
H
amplitude
Figure 9.34
Classification of clean speech corresponding to Figure 9.33
estimation errors. Therefore, in general, in the presence of acoustic noise the
speech classiﬁcation algorithm declares more frames as ACELP. These include
the silence frames of the original clean speech, unvoiced segments with lower
energy than the noise level, and the stationary voiced frames with parameter
estimation and harmonic modelling difﬁculties.
Neither white-noise excitation nor harmonic excitation is suitable for syn-
thesizing the background noise. The spectra of babble and vehicular noise
are not white, even after discarding the spectral envelope. synthesizing them

Acoustic Noise and Channel Error Performance
341
0
1000
2000
3000
4000
frequency (Hz)
(a) A typical spectrum of babble noise
20
40
60
80
100
Magnitude (dB)
0
1000
2000
3000
4000
frequency (Hz)
(b) A typical spectrum of vehicular noise
0
20
40
60
80
100
Magnitude (dB)
Figure 9.35
Typical acoustic noise spectra
using white-noise excitation will degrade the perceptual quality by intro-
ducing an unnaturally noisy background. Therefore, in fact, the classiﬁcation
algorithm detects the most suitable mode, i.e. ACELP, to synthesize back-
ground noise. However the drawback is a high average bit-rate, which may
be reduced by using a robust voice activity detection (VAD) algorithm and
comfort noise generation at the decoder end [9].

342
Multimode Speech Coding
The correct classiﬁcation of the stationary voiced segments as harmonic
mode under noisy background conditions conﬁrms the robustness of SWPM,
since the AbS classiﬁcation algorithm synthesizes speech using SWPM. There-
fore, it can be concluded that the pitch pulse location (PPL) and the pitch
pulse shape (PPS) detection algorithms described in Section 9.4 perform well
under noisy background conditions.
An informal listening test was conducted to compare the speech quality
of the hybrid coder under noisy background conditions with white noise,
harmonic excitation, and ACELP quantized at 1.5 kb/s, 4 kb/s, and 6 kb/s,
as discussed before. The synthesized speech was compared against the same
noisy speech ﬁles synthesized using the 6.3 kb/s ITU G.723.1 coder. The
speech material used for each test consists of eight sentences, four from
male and four from female talkers, four corrupted with vehicular noise
and four corrupted with babble noise (10 dB SNR); a pair of headphones
was used to conduct the test. Twelve listeners were asked to indicate their
preferences for the randomized pairs of synthesized speech. Both experienced
and inexperienced listeners were participated in the test. The test results are
shown in Table 9.12.
The informal listening test shows a clear preference for the 6.3 kb/s ITU
G.723.1 coder. It was found that this is due to the metallic character of the
stationary voiced speech synthesized by the harmonic excitation: it is cleaner,
however, there is a pronounced metallic character. The test conﬁrms that
the listeners prefer more natural-sounding, noisy speech rather than metallic
speech.
The metallic character is not so pronounced in noisy speech synthesized
using a split-band LPC (SB-LPC) harmonic coder [4]. The SB-LPC coder
divides the speech spectrum into two bands using a voicing frequency
marker, where the upper band is declared unvoiced, and synthesized using a
ﬁltered noise excitation. For clean stationary voiced speech, most of the spec-
trum is declared voiced. However in the case of stationary voiced segments
of noisy speech, some frequency bands are declared unvoiced. Therefore
the voicing decision of SB-LPC reduces quality, synthesizing metallic sounds
under noisy background conditions. The harmonic excitation model described
in Section 9.5.1 was designed to synthesize stationary voiced segments and
the complete spectrum is synthesized using harmonically related sinusoids.
Table 9.12
Hybrid vs 6.3 kb/s G.723.1 for noisy speech
Better
Slightly better
Same
Slightly worse
Worse
Male(%)
0.0
9.6
38.5
40.4
11.5
Female(%)
0.0
21.2
21.2
42.3
15.3
Average(%)
0.0
15.4
29.9
41.3
13.4

Acoustic Noise and Channel Error Performance
343
Under noisy background conditions, there are strong spectral components
which are not related to the fundamental frequency of the speech. These noise
components change the harmonic amplitudes and are perceived as metallic
sounds in harmonically synthesized speech (see Figure 9.36). Introducing a
voicing frequency marker for the harmonic excitation, similar to SB-LPC,
improves the speech quality of the hybrid coder, especially in noisy back-
0
1000
2000
3000
4000
frequency (Hz)
(a) Spectrum of corrupted voiced speech
20
40
60
80
100
120
Magnitude (dB)
0
1000
2000
3000
4000
frequency (Hz)
(b) Harmonically synthesized speech
20
40
60
80
100
120
Magnitude (dB)
Figure 9.36
Speech corrupted with babble noise (10 dB SNR)

344
Multimode Speech Coding
ground conditions. The hybrid coding algorithm described has three modes,
and two bits are allocated to transmit the mode. Therefore an additional
mode may be added to further improve the speech quality. The quality of
speech corrupted by acoustic noise can be improved by using the additional
mode as another harmonic mode with a constant voicing frequency marker,
e.g. 80 % of the spectrum is voiced. Figure 9.37 depicts the spectrum of speech
0
1000
2000
3000
4000
frequency (Hz)
(a) Spectrum of corrupted voiced speech
20
40
80
100
120
Magnitude (dB)
0
1000
2000
3000
4000
frequency (Hz)
(b) synthesized speech, 80% voiced
20
40
60
80
100
120
Magnitude (dB)
60
Figure 9.37
Speech corrupted with babble noise (10 dB SNR)

Acoustic Noise and Channel Error Performance
345
Table 9.13
Hybrid vs 6.3 kb/s G.723.1 for noisy speech
Better
Slightly better
Same
Slightly worse
Worse
Male(%)
0.0
12.5
40.0
40.0
7.5
Female(%)
5.0
22.5
35.0
30.0
7.5
Average(%)
2.5
17.5
37.5
35.0
7.5
corrupted with babble noise (10 dB SNR) and the spectrum of the synthesized
speech, with 80 % of the spectrum declared voiced and the remaining high
frequency components synthesized using ﬁltered and scaled Gaussian noise.
The same informal listening test was conducted to compare the speech
quality. The informal test results are shown in Table 9.13. Comparing with
the results shown in Table 9.12, the introduction of the harmonic voicing
signiﬁcantly improves the performance under background noise which indi-
cates that there is still some room to retune the harmonic coder for the hybrid
coding operation. The same is perhaps true for ACELP, and it should be
designed speciﬁcally for hybrid operation.
9.11.2 Performance Under Channel Errors
The inherent robustness of the hybrid coder to mode bit errors was tested by
simulating all the possible mode errors. The hybrid coder has three modes,
hence there are six possible mode errors, i.e. each mode may be erroneously
decoded with the other two modes. The bit stream of the hybrid coder is
shown in Tables 9.14 and 9.15. For each parameter, the most signiﬁcant bit
(MSB) is transmitted ﬁrst. When erroneously decoding a lower-rate mode as
a higher-rate mode, e.g. decoding a white-noise excitation frame as harmonic,
the remaining bits are set to 1. Simulations show that setting the remaining
bits to 1 has the worst effect, since the higher indices are mapped to the
higher-energy levels in the gain quantizers. Using the LTP gain quantizer
shown in Table 9.3 results in blasts when the white noise or harmonic frames
are erroneously decoded as ACELP. Therefore the maximum LTP gain is
limited to 1.2.
All the modes quantize the LSFs using 23 bits, consequently they are
transmitted using the same bits. Therefore the LSFs are independent of the
mode and the mode bit errors can only affect the excitation parameters.
This is particularly attractive for the LSF interpolation and quantization with
ﬁrst-order moving average prediction. The most signiﬁcant bits of the gain
parameters are also transmitted using the same bits. However the gain of
each mode is estimated using different criteria. Hence the gain quantizers
of each mode have different dynamic ranges, and mode errors affect the
dequantization of the gain.

346
Multimode Speech Coding
Table 9.14
Transmission bit stream of the hybrid coder
Parameters
White noise
Harmonic
ACELP 6k
Mode
1–2
1–2
1–2
LSF
3–25
3–25
3–25
Gain (2nd subframe)
26–30
26–30
26–28
Gain (1st subframe)
–
31–33
–
Pitch
–
34–41
–
PPL
–
42–48
–
PPS
–
49–52
–
Amplitudes 1st subframe
–
53–66
–
Amplitudes 2nd subframe
–
67–80
–
Table 9.15
Bit stream of 6 kb/s ACELP subframes
Parameters
Subframe 1
Subframe 2
Subframe 3
Subframe 4
LTP Delay
29–35
52–58
75–81
98–104
LTP Gain
36–39
59–62
82–85
105–108
Pulse Sign
40
63
86
109
Pulse track 1
41–44
64–67
87–90
110–113
Pulse track 2
45–48
68–71
91–94
114–117
Innovation Gain
49–51
72–74
95–97
118–120
White Noise Excitation Mode Errors
Figure 9.38 illustrates erroneous decoding of white-noise excitation frames
as harmonic and ACELP. It shows that the errors are contained within the
frames which have mode errors. This is because the decoder does not inter-
polate the unvoiced gain at switching. The present gain is used to synthesize
the entire frame when switched from a different mode. However if the next
frame after decoding a noise excitation frame as ACELP is also ACELP, the
LTP memory propagates the errors, similar to the error propagation of CELP
coders [60]. The hybrid coding algorithm has the advantage of limiting the
error propagation, by switching to a different mode, which also refreshes the
LTP memory.
Harmonic Mode Errors
Figure 9.39 illustrates erroneous decoding of harmonic excitation frames as
unvoiced and ACELP. It shows that the errors are contained within the
frames which have mode errors. This is because the decoder reinitializes the
harmonic excitation memories when switched from a different mode, and

Acoustic Noise and Channel Error Performance
347
amplitude
A
N
N
A
H
A
H
N
A
H
(i)
(ii)
0
200
400
600
800
samples
(b) Decoding an unvoiced frame as ACELP
0
200
400
600
800
samples
(a) Decoding an unvoiced frame as harmonic
amplitude
(i)
(ii)
A
A
N
A
H
A
N
N
A
H
Figure 9.38
Erroneous decoding of white-noise excitation frames: (i) Original
speech, (ii) synthesized speech: A (ACELP), H (harmonic), and N (noise excitation)
use of the previous excitation vector is minimized. However if the next frame
after decoding a harmonic excitation frame as unvoiced is also unvoiced, the
unvoiced overlap and add process spreads the incorrect gain into the next
frame.

348
Multimode Speech Coding
ACELP Mode Errors
Figure 9.40 illustrates erroneous decoding of ACELP frames as unvoiced
and harmonic. In Figure 9.40a the error is contained within the frame which
has the mode error. For the next frame the harmonic mode reinitializes the
amplitude
H
H
H
H
H
H
H
N
H
H
(i)
(ii)
0
200
400
600
800
samples
(b) Decoding a harmonic frame as ACELP
0
200
400
600
800
samples
(a) Decoding a harmonic frame as unvoiced
amplitude
(i)
(ii)
H
H
A
H
H
H
H
H
H
H
Figure 9.39
Erroneous decoding of harmonic excitation frames, (i) Original speech,
(ii) synthesized speech: A (ACELP), H (harmonic), and N (noise excitation)

Acoustic Noise and Channel Error Performance
349
excitation memories. However in Figure 9.40b, the next frame after decoding
an ACELP frame as harmonic is also harmonic. Hence, the error propagates
into the next frame, due to the harmonic interpolation process.
The LPC ﬁlter may propagate the errors, when the ﬁlter response is highly
resonant. However the bandwidth expansion of the LPC coefﬁcients ensures
that the LPC impulse response dies away more quickly. Therefore all the mode
errors are localized and the output does not become unstable in the presence
of mode errors. This is mainly due to the independent memory initialization
procedures of the coding algorithm when switching between the modes.
The white-noise excitation mode always sets the previous gain equal to the
present one when switched from a different mode. The harmonic excitation
mostly depends on the received harmonic parameters when switched from a
different mode; only the amplitude quantizer memories are initialized using
the previous excitation vector. The LTP buffer is refreshed, regardless of the
mode, with the latest excitation vector.
9.11.3 Performance Improvement Under Channel Errors
During the experiments described in the preceding sections, the robustness
to mode-bit errors was improved by limiting the LTP gain to 1.2 and using
the same set of bits to transmit the LSFs of all the modes. The encoder and the
decoder cannot synchronize the random number generators at the presence
of mode-bit errors. This affects the performance of the LTP when switched
from white-noise excitation. However the exact content of the white-noise
excitation has no signiﬁcance and can be represented by any noise excitation
vector. Therefore, the performance of the LTP was also improved by always
reinitializing the LTP buffer to a ﬁxed stored noise excitation vector when
switching to ACELP from the white-noise excitation.
The robustness to mode-bit errors can be further improved by using error
detection and correction techniques. If a mode error is only detected and
not corrected, the concealment techniques based on waveform substitution
can be used to reduce the resulting annoying artifacts [62]. The decoded
parameters and the synthesized waveform may also be used to detect mode
errors. As can be seen in Figures 9.38, 9.39, and 9.40, mode errors generally
result in sudden changes in the waveform shape and the signal level, which
are unusual for speech signals. Moreover certain mode patterns are more
common than the others, e.g. for many speech utterances, ACELP to harmonic
and back to ACELP occur, while the silence segments before and after are
synthesized with the white-noise excitation. The transition from white noise to
harmonic mode is extremely rare, since generally the onsets request ACELP.
Consequently in order to assist in detecting mode errors, one can limit the
possible switching combinations.

350
Multimode Speech Coding
amplitude
A
A
A
H
H
A
A
N
H
H
(i)
(ii)
0
200
400
600
800
samples
(b) Decoding an ACELP frame as harmonic
0
200
400
600
800
samples
(a) Decoding an ACELP frame as unvoiced
amplitude
(i)
(ii)
A
A
H
H
H
A
A
A
H
H
Figure 9.40
Erroneous
decoding
of
ACELP
frames,
(i) Original
speech,
(ii) synthesized speech: A (ACELP), H (harmonic), and N (noise excitation)
9.12 Summary
In this chapter the principle techniques behind an advanced hybrid coding
algorithm, which integrates harmonic coding and waveform coding, have
been presented. The two important design issues are speech classiﬁcation

Bibliography
351
and, when mode-switching, proper coder synchronization. Provided that
these two processing stages are carried out successfully, the quality of
speech produced by a hybrid coding method is of good to toll quality at
around 3.5–5 kb/s (average). Simple informal subjective listening test results
conﬁrm that the hybrid model eliminates the limitations of the existing
single-model-based coders.
The robustness of the hybrid coding algorithm under acoustic noise and
channel error conditions is another important issue which requires signiﬁcant
research effort. The difﬁculties speciﬁc to hybrid coders are the speech
classiﬁcation under background noise, and the mode-bit errors due to random
channel errors. Although the classiﬁcation algorithm is capable of selecting
the best mode under noisy background conditions, there is a signiﬁcant bias
towards ACELP in the presence of noise compared to clean speech conditions.
This is due to the inability of the white-noise excitation or the harmonic
excitation to encode the corrupted signals. The noisy speech synthesized using
the harmonic mode sounds metallic, which can be improved by introducing
a proper voicing mixture classiﬁcation when harmonic mode is selected.
The robustness of the hybrid coder to mode errors has been tested by
simulating all the possible mode errors. The coder is capable of isolating
the mode errors and return to normal decoding almost immediately. This is
mainly due to the independent memory reinitialization of the modes when
switched from a different mode.
Finally it is important that each element or coding mode of the hybrid model
is redesigned with the knowledge that the noise, ACELP and harmonic
excitation models will be used during noise (or silence), transitions, and
steady state voiced speech parts respectively. In this case the LPC parameters
of ACELP and harmonic modes will have different vector quantizer tables
which will be trained over transitional and steady state voiced speech only
respectively, thus improving the quantization performance. In addition, using
the LTP in ACELP mode at the onsets may not be necessary. Instead more
pulses with phase spreading may be used to improve quality.
Bibliography
[1] R. J. McAulay and T. F. Quatieri (1995) ‘Sinusoidal coding’, in Speech
coding and synthesis by W. B. Kleijn and K. K. Paliwal (Eds), pp. 121–74.
Amsterdam: Elsevier Science
[2] R. J. McAulay and T. F. Quatieri (1986) ‘Speech analysis/synthesis based
on a sinusoidal representation’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 34(4):744–54.
[3] D. Grifﬁn and J. S. Lim (1988) ‘Multiband excitation vocoder’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 36(8):1223–35.

352
Multimode Speech Coding
[4] I. Atkinson, S. Yeldener, and A. Kondoz (1997) ‘High quality split-band
LPC vocoder operating at low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 1559–62. May 1997. Munich
[5] R. Salami, C. Laﬂamme, J.P. Adoul, A. Kataoka, S. Hayashi, T. Moriya,
C. Lamblin, D, Massaloux, S. Proust, P. Kroon, and Y. Shoham (1998)
‘Design and description of CS-ACELP: a toll quality 8 kbps speech coder’,
in IEEE Trans. Speech and Audio Processing, 6(2):116–30.
[6] C. Laﬂamme, J.-P. Adoul, H. Su, and S. Morissette (1990) ‘On reduc-
ing computational complexity of codebook search through the use of
algebraic codes’, in Int. Conf. on Acoust., Speech and Signal Processing,
pp. 177–80.
[7] W. B. Kleijn (1993) ‘Encoding speech using prototype waveforms’, in
IEEE Trans. Speech and Audio Processing, 1:386–99.
[8] M. Schroeder and B. Atal (1985) ‘Code excited linear prediction (CELP):
high quality speech at very low bit rates’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 937–40. Tampa, FL
[9] D. K. Freeman, G. Cosier, C. B. Southcott, and I. Boyd (1989) ‘The voice
activity detector for the pan-European digital cellular mobile telephone
service’, in Proc. of Int. Conf. on Acoust., Speech and Signal Processing,
pp. 369–72.
[10] S. Wang and A. Gersho (1992) ‘Improved phonetically segmented vector
excitation coding at 3.4 kbps’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, 1:349–352.
[11] T. E. Tremain (1982) ‘The government standard linear predictive coding
algorithm: LPC-10’, in Speech Technology, 1:40–9.
[12] P. Kroon and B. Atal (1988) ‘Strategies for improving CELP coders’, in
Proc. of Int. Conf. on Acoust., Speech and Signal Processing, 1:151–4.
[13] I. M. Trancoso, L. Almeida, and J. M. Tribolet (1986) ‘A study on the
relationships between stochastic and harmonic coding’, in Proc. of Int.
Conf. on Acoust., Speech and Signal Processing, pp. 1709–12.
[14] B. S. Atal and S. Singhal (1984) ‘Improving performance of multipulse
LPC coders at low bit rates’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 1.3.1–4.
[15] D. L. Thomson and D. P. Prezas (1986) ‘Selective modelling of the LPC
residual during unvoiced frames white noise or pulse excitation’, in Proc.
of Int. Conf. on Acoust., Speech and Signal Processing, pp. 3087–90.
[16] E. Shlomot, V. Cuperman, and A. Gersho (1998) ‘Combined harmonic
and waveform coding of speech at low bit rates’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing.
[17] E. Shlomot, V. Cuperman, and A. Gersho (1997) ‘Hybrid coding of speech
at 4 kbps’, in Proc. IEEE Workshop on Speech Coding for Telecom, pp. 37–8.

Bibliography
353
[18] J. Stachurski and A. McCree (2000) ‘Combining parametric and
waveform-matching coders for low bit-rate speech coding’, in X European
Signal Processing Conf.
[19] J. Stachurski and A. McCree (2000) ‘A 4 kb/s hybrid MELP/CELP coder
with alignment phase encoding and zero phase equalization’, in Proc. of
Int. Conf. on Acoust., Speech and Signal Processing, pp. 1379–82. May 2000.
Istanbul
[20] A. V. McCree and T. P. Barnwell (1995) ‘A mixed excitation LPC vocoder
model for low bit rate speech coding’, in IEEE Trans. Speech and Audio
Processing, 3(4):242–50.
[21] J. Stachurski, A. V. McCree, and V. R. Viswanathan (1999) ‘High quality
MELP coding at bit-rates around 4 kbps’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing.
[22] T. Moriya and M. Honda (1986) ‘Speech coder using phase equalisation
and vector quantisation’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 1701–4.
[23] J. Skoglund, W. B. Kleijn, and P Hedelin (1997) ‘Audibility of pitch-
synchronously modulated noise’, in Proc. IEEE Workshop on Speech Coding
for Telecom, pp. 51–2.
[24] H. Pobloth and W. B. Kleijn (1999) ‘On phase perception in speech’, in
Proc. of Int. Conf. on Acoust., Speech and Signal Processing.
[25] Doh-Suk Kim (2000) ‘Perceptual phase redundancy in speech’, in Proc.
of Int. Conf. on Acoust., Speech and Signal Processing.
[26] N. Katugampala, (2001) ‘Multimode speech coding below 6 kb/s’, Ph.D.
thesis, CCSR, University of Surrey, UK.
[27] N. Katugampala and A. Kondoz (2002) ‘Integration of harmonic and
analysis by synthesis coders’, in IEE Proc. on Vision Image and Signal
Processing, pp. 321–6.
[28] N. Katugampala and A. Kondoz (2001) ‘A hybrid coder based on a
new phase model for synchronization between harmonic and waveform
coded segments’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing.
[29] TIA/EIA (1997) Enhanced variable rate codec, speech service option 3 for
wideband spread spectrum digital systems, IS-127.
[30] W. Kleijn, P. Kroon, L. Cellario, and D. Sereno (1993) ‘A 5.85 kbps CELP
algorithm for cellular applications’, in Proc. of Int. Conf. on Acoust., Speech
and Signal Processing, 2:596–9.
[31] T. V. Ananthapadmanabha and B. Yegnanarayana (1979) ‘Epoch extrac-
tion from linear prediction residual for identiﬁcation of closed glot-
tis interval’, in IEEE Trans. on Acoust., Speech and Signal Processing,
27(4):309–19.

354
Multimode Speech Coding
[32] Y. M. Cheng and D. O’Shaughnessy (1989) ‘Automatic and reliable
estimation of glottal closure instant and period’, in IEEE Trans. On
Acoust., Speech and Signal Processing, 37(12):1805–15.
[33] P. Satyanarayana Murthy and B. Yegnanarayana (1999) ‘Robustness
of group delay based method for extraction of signiﬁcant instants of
excitation from speech signals’, in IEEE Trans. Speech and Audio Processing,
7(6):609–19.
[34] TIA/EIA (1997) ‘Enhanced variable rate codec, speech service option 3
for wideband spread spectrum digital systems’, IS-127.
[35] B. S. Atal and M. R. Schroeder (1974) ‘Recent advances in predic-
tive coding-applications to voiced speech synthesis’, in Speech Commun.
Seminar. Stockholm
[36] R. J. McAulay and T. F. Quatieri (1990) ‘Pitch estimation and voicing
decision based upon a sinusoidal speech model’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing, 1:249–52.
[37] S. Villette, Y. D. Cho, and A. M. Kondoz (2000) ‘Efﬁcient parameter quan-
tisation for 2.4/1.2 kbps split band LPC coding’, in Proc. IEEE Workshop
on Speech Coding for Telecom, pp. 32–4. September 2000. Wisconsin, USA
[38] M. Stefanovic, Y. D. Cho, S. Villette, and A. M. Kondoz (2000) ‘A
2.4/1.2 kb/s speech coder with noise pre-processor’, in Proc. European
Signal Processing Conference. Tampere, Finland
[39] R. Hagen, E. Ekudden, B. Johansson, and W. Kleijn (1998) ‘Removal of
sparse excitation artifacts in CELP’, in Proc. of Int. Conf. on Acoust., Speech
and Signal Processing.
[40] C. Li and V. Cuperman (1998) ‘Enhanced harmonic coding of speech
with frequency domain transition modeling’, in Proc. of Int. Conf. on
Acoust., Speech and Signal Processing, pp. 581–4.
[41] N. S. Jayant and P. Noll (1984) Digital Coding of Waveforms: Principles and
applications to speech and video. New Jersey: Prentice-Hall
[42] G. Kubin, B. S. Atal, and W. B. Kleijn (1993) ‘Performance of noise
excitation for unvoiced speech’, in Proc. IEEE Workshop on Speech Coding
for Telecom, pp. 35–6.
[43] I. Atkinson (1997) ‘Advanced linear predictive speech compression at
3.0 kbit/s and below’, Ph.D. thesis, CCSR, University of Surrey, UK.
[44] J. Sohn and W. Sung (1995) ‘A voice activity detection employing soft
decision based noise spectrum adaptation’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 365–8. Amsterdam
[45] A. Das, E. Paksoy, and A. Gersho (1995) ‘Multimode and variable rate
coding of speech’, in Speech coding and synthesis by W. B. Kleijn and K. K.
Paliwal (Eds), pp. 257–88. Amsterdam: Elsevier Science
[46] S. Wang, A. Sekey, and A. Gersho (1992) ‘An objective measure for
predicting subjective quality of speech coders’, in IEEE Journal on Selected
Areas in Communications, 10(5):819–829.

Bibliography
355
[47] S. V. Vaseghi (1990) ‘Finite state CELP for variable rate speech coding’,
in Proc. of Int. Conf. on Acoust., Speech and Signal Processing, pp. 37–40.
[48] T. Eriksson and J. Sjoberg (1993) ‘Evolution of variable rate speech
coders’, in Proc. IEEE Workshop on Speech Coding for Telecom, pp. 3–4.
[49] D. O’Shaughnessy (1987) Speech communication: human and machine. Addi-
son Wesley
[50] NTT Group Available at http://www.ntt.co.jp/index e.html.
[51] Y. D. Cho, S. Villette, and A. Kondoz (2001) ‘Efﬁcient spectral magnitude
quantization for sinusoidal speech coders’, in Proc. of Vehicular Technology
Conf.
[52] B. H. Juang and A. H. Gray (1982) ‘Multiple stage vector quantisation
for speech coding’, in Proc. of Int. Conf. on Acoust., Speech and Signal
Processing, pp. 597–600. Paris
[53] P. Kroon and B. S. Atal (1991) ‘On the use of pitch predictors with high
temporal resolution’, in IEEE Trans. Signal Processing, 39(3):733–5.
[54] Y. Ephraim and D. Malah (1985) ‘Speech enhancement using a minimum
mean square error log-spectral amplitude estimator’, in IEEE Trans. on
Acoust., Speech and Signal Processing, 33(2):443–5.
[55] K. Zeger and A. Gersho (1990) ‘Pseudo-gray coding’, in IEEE Trans. on
Communications, 38(12):2147–58.
[56] K. A. Zeger and A. Gersho (1987) ‘Zero redundancy channel coding in
vector quantisation’, in IEE Electronics Letters, 23(12):654–656.
[57] N. Farvardin (1990) ‘A study of vector quantisation for noisy channels’,
in IEEE Trans. Inform. Theory, 36:799–809.
[58] N. Farvardin and V. Vaishampayan (1991) ‘On the performance and
complexity of channel optimised vector quantisers’, inIEEE Trans. Inform.
Theory, 37:155–60.
[59] T. Eriksson, J. Linden, and J. Skoglund (1999) ‘Interframe LSF quantiza-
tion for noisy channels’, in IEEE Trans. on Speech and Audio Processing,
7(5):495–509.
[60] R. Cox, B. Kleijn, and P. Kroon (1989) ‘Robust CELP coders for noisy
backgrounds and noisy channels’, in Proc. of Int. Conf. on Acoust., Speech
and Signal Processing, pp. 739–42.
[61] S. Villette, M. Stefanovic, and A. Kondoz (1999) ‘Split band LPC based
adaptive multi rate GSM candidate’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing.
[62] D. Goodman, G. Lockhart, O. Wasem, and W. Wong (1986) ‘Waveform
substitution techniques for recovering missing speech segments in packet
voice communications’, in IEEE Trans. on Acoust., Speech and Signal
Processing, 34(6):1440–8.

10
Voice Activity Detection
10.1 Introduction
In voice communications, speech can be characterized as a discontinuous
medium because of the pauses which are a unique feature compared to other
multimedia signals, such as video, audio and data. The regions where voice
information exists are classiﬁed as voice-active and the pauses between talk-
spurts are called voice-inactive or silence regions. An example illustrating
active and inactive voice regions for a speech signal is shown in Figure 10.1.
A voice activity detector (VAD) is an algorithm employed to detect the
active and inactive regions of speech. When inactive regions are detected,
transmission is generally stopped and only a general description of the
background information is transmitted. At the decoder end, inactive frames
are then reconstructed by means of comfort noise generation (CNG), which
gives natural background sounds with smooth transitions from talk-spurts
to pauses and vice versa. To enhance the naturalness of the generated back-
ground signal, regular updates of the average information on the background
signal (especially necessary during noisy communication environments) is
transmitted by the comfort noise insertion (CNI) module of the encoder. The
overall structure of the silence compression scheme employing a VAD, CNG,
and CNI is shown in Figure 10.2.
Speech communication systems which operate a VAD for compression
of inactive speech regions provide various beneﬁts especially useful for
bandwidth-limited communication channels. These beneﬁts can be summa-
rized as given in the following list:
• Co-channel interference reduction in cellular communications: It is possible
to suppress co-channel interference in cell-based wireless communication
systems by decreasing transmission power during inactive regions (speech
pauses).
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

358
Voice Activity Detection
Clean speech
Noisy speech
(vehicle, 5 dB SNR)
Inactive
Active
Active
Inactive
Inactive
Figure 10.1
Voice active and inactive regions
CNI
Voice
Decoder
 
CNG
Input
speech
Output
speech
VAD
Active
Inactive
Active / Inactive
Voice
Encoder
Channel
Figure 10.2
Overall structure of a speech coding system with silence compression
• Improvement of the soft channel capacity in the code division multiple
access (CDMA) system: The theoretical capacity of a CDMA system is usu-
ally deﬁned by the possible combinations of the spreading code. However,
due to interference from other users, the CDMA capacity is limited to a
value smaller than this theoretical limit, i.e. due to interference from other
users, the error rates received by some users may be too high to enable
accurate decoding. By reducing the transmission power during speech
pauses, the interference on air can be reduced, which may automatically
allow more users on the system, hence achieving an increase in the CDMA
system’s capacity.
• Power-saving for mobile terminals: Mobile terminals do not have to trans-
mit radio signals during pauses. Thus, the battery life time of the terminals
can be extended by conserving power during speech inactive periods.

Introduction
359
• Increase in channel capacity by statistical multiplexing: A channel can be
granted just during talk-spurts and released during pauses. Once granted,
a user occupies a channel until the end of a talk-spurt and releases
it immediately after the last active speech frame. To get the channel
allocation again, the user makes a request at the start of the next talk-spurt.
This way the channel resources can be utilized in a more efﬁcient way
by the statistical multiplexing scheme, which allows a number of users to
communicate at the same time over limited channel resources. Note: in
statistical multiplexing, there is a possibility that there are no free channel
slots when a user makes a request. In this case, the new user may be
rejected after a time-out, which may cause information loss resulting in
some quality degradations.
• Reduction in packet losses when transmitting voice over packet-based
networks: A packet-based system can be overloaded with more pack-
ets than it can handle. The congestion of packet-based systems can be
reduced during voice communication by producing packets only during
active speech regions and cutting out packets for the inactive speech
regions.
• Bit-rate reduction: In addition to the bit-rate reduction achieved by speech
compression techniques, the use of a VAD together with silence compres-
sion (cutting out the inactive speech regions) gives additional reduction in
the bit-rate regardless of speech coders.
The VAD usually produces a binary decision for a given speech segment
(usually 10–20 ms long) indicating either speech presence or absence, which is
quite easy for clean background speech. For example, by checking the energy
level of the input signal, it is possible to obtain a high speech/nonspeech
detection performance. However, in real environments, the input signal may
be mixed with noise characteristics which may be unknown and changing
with time. In some cases where the background noise is signiﬁcantly high,
the speech may be obscured by this noise. Especially, the unvoiced sounds,
which are important for speech intelligibility, may be misdetected in such
noisy environments. Figure 10.1 shows an example for a noisy speech segment
with vehicle noise of 5 dB signal to noise ratio (SNR). As can be seen from the
ﬁgure, some low energy speech parts are fully submerged in noise, making
it very difﬁcult to discriminate these talk-spurts even by visual inspection.
Incorrect classiﬁcation of these talk-spurts can cause clipped sounds which
may result in signiﬁcantly degraded speech quality. On the other hand, the
increase in false detection of silence loses the potential beneﬁts of silence
compression. There is a trade-off in VAD performance, maximizing the
detection rate for active speech while minimizing the false detection rate of
inactive speech regions.

360
Voice Activity Detection
10.2 Standard VAD Methods
In order to exploit the advantages of silence compression, a number of VAD
algorithms have been proposed, some of which have been selected by stan-
dards organizations including ITU-T, ETSI, and TIA/EIA. ITU-T released
G.729 Annex B (G.729B) [1] and G.723.1 Annex A (G.723.1A) [2] as exten-
sions to the 8 kb/s G.729 [3] and 5.3/6.3 kb/s G.723.1 [4] speech coders for
performing discontinuous transmission (DTX). ETSI recommended GSM-FR,
-HR, and -EFR VAD methods for European digital cellular systems [5–7].
Recently, ETSI released two more VADs, adaptive multi-rate VAD option
1 (AMR1) and option 2 (AMR2) [8], with a view to using it in UMTS
(the third generation mobile communications). The North American stan-
dards organisation, TIA/EIA, released two VADs one for IS-96 [9] and the
other for IS-127 [10] and IS-733 [11] (the VADs suggested for IS-127 and
IS-733 have the same structure). Table 10.1 shows standard VADs classi-
ﬁed in terms of the input features mainly consisting of subband energies
and the spectral shape. For example, the TIA/EIA VADs use a small num-
ber of subbands whereas the IS-96 VAD uses the overall signal energy.
The IS-127 and IS-733 VAD, on the other hand, decomposes the input sig-
nal into two subbands only. Traditionally, ETSI VAD methods have been
based on a more accurate spectral shape of the input signal. The reason
behind this is that the energy of the predictive coding error increases when
the spectral shapes between the background and input signal mismatch
(i.e. when speech active). However, in the recent standard for AMR, ETSI
adopted two kinds of VAD algorithms both of which are based on the
spectral subband energies rather than the more accurate spectral shape.
The ITU-T VAD standards, G.729B and G.723.1A, conduct the detection
using four different features including both the spectral shape and subband
energies.
Table 10.1
Classification
of
standard
VAD
methods depending on input features; the val-
ues in parentheses indicate the number of spec-
tral subbands
Main features
VAD
Spectral shape
GSM-FR, GSM-HR, GSM-EFR
Sub-band energies
IS-96 (1), IS-127 (2), IS-733 (2)
AMR1 (9), AMR2 (16)
Others
G.729B, G.723.1A

Standard VAD Methods
361
10.2.1 ITU-T G.729B/G.723.1A VAD
As an extension to the G.729 speech coder, ITU-T SG16 released G.729 Annex
B in order to support DTX by means of VAD, CNI, and CNG. G.729B conducts
a VAD decision every frame of 10 ms, using four different parameters:
• a full-band energy difference, Ef = Ef −Ef
• a low-band energy difference, El = El −El
• a spectral distortion, LSF = 9
i=0(LSFi −LSFi)2
• a zero-crossing rate difference, ZC = ZC −ZC
where Ef, El, LSFi, and ZC are the full-band energy, low-band energy, ith
line spectral frequency, and zero-crossing rate of the input signal. Ef , El,
LSFi, and ZC are the noise characterizing parameters updated using the
background noise.
The block diagram of G.729B VAD is shown in Figure 10.3. The input
parameters for the VAD can be obtained from the input signal or from
the intermediate values of the speech encoder. Subsequently, the difference
parameters, Ef, El, LSF, and ZC, are computed from the input and
noise parameters. A decision of voice activity is conducted over a four-
dimensional hyper-space, based on a region classiﬁcation technique, followed
by a hangover scheme. The noise parameters are updated based on a ﬁrst
order autoregressive (AR) scheme, if the full-band energy difference is less
than a certain ﬁxed threshold. ITU-T G.723.1A VAD has a structure similar
to G.729B VAD.
10.2.2 ETSI GSM-FR/HR/EFR VAD
The VAD algorithms of ETSI GSM-FR, -HR, and -EFR have a common struc-
ture, in which the predictive residual energy is compared with an adaptive
Differential
Parameters
Computation
Multi-Boundary
VAD Decision
Ef < 15dB
Noise
Parameters
Update
Ef, El,
LSF, ZCR
Ef, El,LSF, ZCR
Ef,   El
LSF,
ZCR
Yes
No
Active/
Inactive
Hangover
∆
∆
∆
∆
Figure 10.3
Block diagram of ITU-T G.729B VAD

362
Voice Activity Detection
Energy
Computation
Smoothed
Autocorrelation
Threshold
Update
Active/
Inactive
Threshold
Autocorrelation
of LPC
Stationary
Check
VAD
Decision
Hangover
Pitch flag
Tone flag
Stationary flag
Autocorrelations
Figure 10.4
Block diagram of ETSI GSM-EFR VAD
threshold. The predictive residual energy is computed using the current and
smoothed autocorrelation values which describe the spectral characteristics
of the signal. The assumption is that if the signal is background noise only,
which is fairly stationary, the average spectral shape will be similar to the
current frame’s shape and hence result in smaller residual signal energy.
The threshold for VAD decision is updated during noise-only regions using
the most recent noise signals in order to reﬂect up-to-date noise characteristics.
A block diagram of the GSM-FR/HR/EFR VAD is shown in Figure 10.4.
10.2.3 ETSI AMR VAD
AMR1 decomposes the input signal into nine nonuniform subbands using
ﬁlter banks where lower frequency bands have smaller bandwidths and
higher frequency bands have larger bandwidths. Then it calculates each
subband energy followed by its corresponding SNR estimate. The energy of
the background noise used in calculating the SNR is computed by an adaptive
method based on a ﬁrst-order AR-model together with internal VAD logic.
Finally, VAD decision is conducted by comparing the sum of the subband
SNRs with an adaptive threshold, followed by a hangover. The block diagram
of AMR1 is shown in Figure 10.5.
AMR2 has a structure similar to AMR1 in that VAD is performed using
the subband energies together with the background noise energy. However,
AMR2 transforms the input signal into the frequency domain using FFT,
instead of the ﬁlter bank used in AMR1, and then calculates each subband
energy in which the number of bands is 16 with a nonlinear scale in band
grouping. Subsequently, SNRs for each subband are calculated using the input
and the background noise spectra. The background noise energy for each band
is adapted during noise frames using a ﬁrst-order AR-based scheme. In order
to prevent being over sensitive to nonstationary background noise conditions,
AMR2 increases the threshold for ﬁnal VAD decision for highly ﬂuctuating
signals,measured by thevarianceoftheirinstantaneousframe-to-frame SNRs.

Standard VAD Methods
363
Filter
Bank
Band Energy
Computation
Background
Noise
Adaptation
Threshold
Adaptation
VAD
Decision
Hangover
Band SNR
Sum
Active/
Inactive
Input
signal
Figure 10.5
Block diagram of ETSI AMR VAD option 1
FFT
Band Energy
Computation
Background
Noise
Adaptation
Noise
Update
Decision
VAD
Decision
Spectral
Deviation
Estimate
Band SNR
Estimate
Active/
Inactive
Input
signal
 
Peak-to-
Average
SNR Ratio
Pitch flag
Tone flag
Figure 10.6
Block diagram of ETSI AMR VAD option 2
Furthermore, noise adaptation may not be accurately performed by measur-
ing the spectral deviation when subband energies ﬂuctuate rapidly. Thus,
AMR2 changes the VAD threshold in an adaptive way together with the vari-
ation of burst and hangover counts. The hangover control is performed by
measuring the peak-to-average SNR, in which the average SNR is calculated
using AR-adaptation with the increased instantaneous SNR. In other words,
foranincreaseofthepeak-to-averageSNR,it decreasesthehangoverand burst
counts while increasing the VAD threshold. The block diagram of AMR2 is
shown in Figure 10.6.
10.2.4 TIA/EIA IS-127/733 VAD
CDMA-based digital cellular systems have a natural structure for incorporat-
ingVAD,calledaratedeterminationalgorithm(RDA),whichgivessubstantial

364
Voice Activity Detection
improvement in channel capacity by controlling the radio transmission power
to reduce co-channel interference. TIA/EIA released two kinds of RDA for IS-
96andIS-127,called8 kb/sQualcommcode-excitedlinearprediction(QCELP)
and enhanced variable rate codec (EVRC), respectively. In the North Ameri-
can CDMA standard, IS-127 RDA supports three rates: 1, 1/2, and 1/8. Active
speech is encoded in 1 or 1/2 rate and background noise is encoded in 1/8
rate. The RDA of IS-733, called 13 kb/s QCELP, is the same as IS-127.
As input parameters, IS-127 RDA uses two subband energies with the
long-term prediction gain. Firstly, it calculates the smoothed subband energy
using a ﬁrst-order AR-model. Subsequently, the signal and noise energies for
each subband are adapted depending on the long-term prediction gain. In
other words, the signal energy is actively adapted to the current input if the
prediction gain is relatively high. On the other hand, if the gain is relatively
low, it increases the noise adaptation rate. Using the two subband energies
of the signal and noise, each subband SNR is calculated. The ﬁnal rate is
determined by comparing the SNRs with adaptive thresholds depending on
the level of background noise and the SNR of the previous frame, followed
by a hangover. The block diagram of IS-127 RDA is shown in Figure 10.7.
10.2.5 Performance Comparison of VADs
The ﬁve standard VAD algorithms have been evaluated in terms of detection
error rates for speech and silence. The test data was 96 seconds of speech,
ﬁltered by the modiﬁed IRS, and then mixed with vehicle and babble noises
of 5, 10, 15, and 25 dB SNR. The active and inactive regions of the speech
material were marked manually. The proportions of the inactive and active
regions of the speech material were 0.43 and 0.57, respectively. The VAD
decision is carried out every 10 ms in the cases of G.729B and AMR2, and
every 20 ms in GSM-EFR, AMR1 and IS-127. With slight modiﬁcation to
the AMR2 source code, it is possible to obtain 10 ms results because AMR2
basically conducts the detection every 10 ms and then returns 20 ms results
using a logical combination of the two 10 ms results. In handling the multiple
High / Low
Band Energies
Active/ 
Inactive/
Mixed
Band
Energy
Smoothing
Signal
Energy
Update
Noise
Energy
Update
SNR
Computation
Threshold
Comparison
SNRs
Thresholds
Figure 10.7
Block diagram of TIA/EIA IS-127 RDA

Standard VAD Methods
365
rates of IS-127, the upper two rates, 1 and 1/2, have been assumed to be voice
active and the lowest rate, 1/8, is treated as voice inactive.
Performance in a vehicle noise environment are shown in Figures 10.8
and 10.9, and performance for babble noise are shown in Figures 10.10
0
5
10
15
20
25
30
35
40
5
10
15
20
25
Noise level (SNR dB)
Speech detection error rate (%)
G.729B
IS-127
GSM-EFR
AMR1
AMR2
Figure 10.8
Comparison of speech detection error rates against various vehicle
noise levels
0
5
10
15
20
25
30
5
10
15
20
25
Noise level (SNR dB)
Silence detection error rate (%)
G.729B
IS-127
GSM-EFR
AMR1
AMR2
Figure 10.9
Comparison of silence detection error rates against various vehicle
noise levels

366
Voice Activity Detection
0
5
10
15
20
25
30
35
5
10
15
20
25
Noise level (SNR dB)
Speech detection error rate (%)
G.729B
IS-127
GSM-EFR
AMR1
AMR2
Figure 10.10
Comparison of speech detection error rates against various babble
noise levels
0
10
20
50
60
30
40
5
10
15
20
25
Noise level (SNR dB)
Silence detection error rate (%)
G.729B
IS-127
GSM-EFR
AMR1
AMR2
Figure 10.11
Comparison of silence detection error rates against various babble
noise levels

Standard VAD Methods
367
and 10.11. G.729B exhibits the worst performance compared with other meth-
ods, especially for low SNRs. G.729B produces high speech detection errors,
which can cause severe clipping of speech. IS-127 exhibits relatively high error
rates for speech detection compared with those of ETSI VADs. However, it
produces quite reasonable performances in silence detection for babble noisy
speech. ETSI VAD methods, i.e. GSM-EFR, AMR1, and AMR2, exhibit similar
performances in speech detection, while giving quite variable performances
in silence detection. GSM-EFR produces the most desirable performances
for relatively high SNRs, i.e. greater than 15 dB. However, the error rates of
silence detection increase substantially for decreasing SNR. AMR2 produces
0
1
2
3
4
5
6
7
−2
0
2
× 104
Time (sec)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Figure 10.12
Comparison of VAD results over vehicle noise of 15 dB SNR: (a) noisy
input speech, (b) clean speech, (c) G.729B, (d) IS-127, (e) GSM-EFR, (f) AMR1, and
(g) AMR2

368
Voice Activity Detection
−2
0
2
0
1
2
3
4
5
6
7
× 104
Time (sec)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Figure 10.13
Comparison of VAD results over babble noise of 15 dB SNR: (a) noisy
input speech, (b) clean speech, (c) G.729B, (d) IS-127, (e) GSM-EFR, (f) AMR1, and
(g) AMR2
relatively consistent results regardless of the noise levels in silence detection
for vehicle noisy speech. The performance of AMR1 is between GSM-EFR and
AMR2. The characteristics of frame-wise voice activity decisions for various
noise sources and levels are shown in Figures 10.12 and 10.13.
10.3 Likelihood-Ratio-Based VAD
Sohn et al. have proposed a novel method which, unlike traditional VAD
methods, is based on a statistical model. They report that it can produce a
high detection accuracy [12]. The reason for the high performance is attributed

Likelihood-Ratio-Based VAD
369
to the adoption of Ephraim and Malah’s noise suppression rules [13] for the
voice activity decision rules.
A voice activity decision can be considered as a test of hypotheses: H0 and
H1, which indicate speech absence and presence, respectively. Assuming that
each spectral component of speech and noise has complex Gaussian distri-
bution [13], in which the noise is additive and uncorrelated with speech, the
conditional probability density functions (PDF) of a noisy spectral component
Yk, given H0,k and H1,k, are:
p(Yk|H0,k) =
1
πλN,k
exp

−|Yk|2
λN,k

(10.1)
p(Yk|H1,k) =
1
π(λN,k + λX,k) exp

−
|Yk|2
λN,k + λX,k

(10.2)
where k indicates the spectral bin index, and λN,k and λX,k denote the variances
of the noise and speech spectra, respectively.
The likelihood ratio (LR) of the kth spectral bin, k, is deﬁned from the
above two PDFs as [12]:
k = p(Yk|H1,k)
p(Yk|H0,k) =
1
1 + ξk
exp
(1 + γk)ξk
1 + ξk

(10.3)
where γk and ξk are the a posteriori and a priori SNRs deﬁned as, γk =
|Yk|2/λN,k −1 and ξk = λX,k/λN,k. Note that the deﬁnition of the a poste-
riori SNR is slightly different from the original one, γk = |Yk|2/λN,k [13].
The noise variance is assumed to be known through noise adaptation (see
Section 10.3.2). However, the variance of the speech is unknown, thus the a
priori SNR of the nth frame, ξ(n)
k
, is estimated using the decision-directed (DD)
method [13] as:
ˆξ(n)
k
= α
 ˆX(n−1)
k

2
λ(n−1)
N,k
+ (1 −α)MAX{γ (n)
k
, 0}
(10.4)
where α is a weighting term, e.g. 0.98, and the clean speech spectral amplitude,
| ˆXk|, is estimated using the minimum mean square error of the log spectral
amplitude estimator [14]. The decision about the voice activity is performed
by the geometric mean of the k over all spectral bins as:
 = exp

1
K
K

k=1
log k

(10.5)
where K denotes the number of spectral bins.

370
Voice Activity Detection
The a posteriori SNR γk ﬂuctuates highly from frame to frame because of
the high ﬂuctuation of the short-time spectral amplitude |Yk|. On the other
hand, the a priori SNR ˆξk changes slowly due to the smoothing effect. As
the value of α increases, ˆξk becomes smoother. The variations of γk and
ˆξk balance each other in the calculation of k and, consequently, result in
enhanced performance for the VAD. The DD estimator for the a priori SNR
is therefore useful not only for avoiding the musical noise phenomenon in
speech enhancement [15], but also for reducing the error rate in voice activity
detection.
10.3.1 Analysis and Improvement of the Likelihood Ratio Method
The behaviour of the LR in equation (10.3) with respect to the a priori and
a posteriori SNRs, is shown in Figure 10.14. The ML estimator [12] results
in lower performance in comparison with the DD estimator because of the
inherent high-ﬂuctuation of the a posteriori SNR. The LR employing the DD
estimator has the following properties:
• If the a posteriori SNR is very high, i.e. γk ≫1, and the range of the a priori
SNR is limited, the LR becomes very high, i.e. k ≫1.
• If the a posteriori SNR is low, i.e. γk < 1, the a priori SNR becomes a key
parameter in the calculation of the LR.
−15
−10
−5
0
5
10
15
−15
−10
−5
0
5
10
15
20
25
30
35
A priori SNR (dB)
Likelihood ratio (dB)
Figure 10.14
Likelihood ratio vs a priori SNR vs a posteriori SNR (the solid lines from
the top represent a posteriori SNRs of 15, 10, 5, 0, −5, −10, and −15 dB, respectively)

Likelihood-Ratio-Based VAD
371
In practice, the threshold of the LR is set between 0.2 dB and 0.8 dB, and
both the a posteriori and the a priori SNRs are bounded between −15 dB
and 15 dB.
Assuming that the noise characteristics change slowly, delay in estimation
of the noise variance λ(n−1)
N,k
in equation (10.4) does not seriously affect the a
priori SNR ˆξ(n)
k
. However, the spectral amplitude of the speech signal may
change abruptly, particularly in onset and offset regions, in which the power
of the spectral bins can increase and decrease rapidly, respectively. At the
offset region, γk can be low but ˆξk can be much higher than γk due to the delay
in | ˆX(n−1)
k
|2 as given in equation (10.4). Thus k becomes too low, according to
the second property above, and, consequently,  may become lower than the
threshold of VAD. On the other hand, the delay rarely causes a problem at the
onset regions, according to the ﬁrst property above, as γ (n)
k
in equation (10.3)
is usually large enough.
It is possible to consider an adaptive weighting factor in the estimation
of the a priori SNR in equation (10.4). In other words, a lower α can be
assigned for the active region, and a higher α for the inactive region. When
a low α is assigned at the offset region, it reduces the effect of the delay in
equation (10.4), producing a lower ˆξk, and therefore may prevent the abrupt
decay of k. However, it is not easy to design a generalized adaptive rule
that will result in good performance over various kinds of speech and noise
signals. Instead, Cho [16, 17] has suggested a smoothed likelihood ratio (SLR)
(n)
k
which is deﬁned as
(n)
k
= exp

κ log (n−1)
k
+ (1 −κ) log (n)
k
	
(10.6)
where κ is a smoothing factor and (n)
k
is deﬁned in equation (10.3) for the nth
frame. The decision of the voice activity is ﬁnally carried out by computing,
(n) = exp

1
K
K

k=1
log (n)
k

(10.7)
and comparing it against a threshold. An nth input frame is classiﬁed
as voice-active if (n) is greater than a threshold and voice-inactive
otherwise.
Examples of the LR and the SLR over a segment of speech are shown in
Figure 10.15. The SLR seems to overcome the problem outlined for the LR. As
shown in Figure 10.15b, the SLR is relatively higher than the LR at the offset
regions. The comparison over inactive frames is also shown in Figure 10.15c,
which indicates that the SLR ﬂuctuates less than the LR.

372
Voice Activity Detection
0.5
1
1.5
2
2.5
3
3.5
−80
−60
−40
−20
0
20
40
60
(a) Time (sec)
Likelihood ratio (dB)
Clean speech
Noisy speech
1.7
1.8
1.9
2
−20
−15
−10
−5
0
5
10
15
20
25
(b) Time (sec)
Likelihood ratio (dB)
3
3.2
3.4
3.6
 −1.5
−1
−0.5
0
0.5
1
1.5
2
(c) Time (sec)
Likelihood ratio (dB)
Figure 10.15
The LR (solid line) and SLR (dotted line) of a segment of vehicle noise
signals of 5 dB SNR; the dotted horizontal-line indicates the VAD threshold and the
boxed regions in (a) are enlarged in figures (b) and (c)

Likelihood-Ratio-Based VAD
373
10.3.2 Noise Estimation Based on SLR
Depending on the characteristics of the noise source, the short-time spectral
amplitudes of the noise signal can ﬂuctuate strongly from frame to frame. In
order to cope with time-varying noise signals, the variance of the noise spec-
trum is adapted to the current input signal by a soft decision-based method.
The speech absence probability (SAP) of the kth spectral bin, p(H0,k|Yk), can
be calculated by Bayes’ rule as:
p(H0,k|Yk) =
p(H0,k)p(Yk|H0,k)
p(H0,k)p(Yk|H0,k) + p(H1,k)p(Yk|H1,k) =
1
1 + p(H1,k)
p(H0,k)k
(10.8)
where p(H1,k) = 1 −p(H0,k), and the unknown a priori speech absence proba-
bility (PSAP), p(H0,k), is estimated in an adaptive manner given by:
ˆp(H(n)
0,k ) = MIN{MAX{β ˆp(H(n−1)
0,k
) + (1 −β)p(H(n)
0,k |Y(n)
k ), H(L)
0 }, H(U)
0
}
(10.9)
where β is a smoothing factor, e.g. 0.65. The lower and upper limits, H(L)
0
and
H(U)
0
, of the PSAP are determined through experiments, e.g. 0.2 and 0.8. Note
that, for SLR, k is applied to the calculation of the SAP instead of LR, k.
The variance of the noise spectrum of the kth spectral component in the nth
frame, λ(n)
N,k, is updated in a recursive way as:
λ(n)
N,k = ηλ(n−1)
N,k
+ (1 −η)E(|N(n)
k |2|Y(n)
k )
(10.10)
where η is a smoothing factor, e.g. 0.95. The expected noise power-spectrum
E(|N(n)
k |2|Y(n)
k ) is estimated by means of a soft-decision technique [18] as:
E(|N(n)
k |2|Y(n)
k ) = E(|N(n)
k |2|H0,k)p(H0,k|Y(n)
k ) + E(|N(n)
k |2|H1,k)p(H1,k|Y(n)
k )
= |Y(n)
k |2p(H0,k|Y(n)
k ) + λ(n−1)
N,k
p(H1,k|Y(n)
k )
(10.11)
where p(H1,k|Y(n)
k ) = 1 −p(H0,k|Y(n)
k ). During some tests, it is observed that
SLR-based adaptation is useful for the estimation of the noise spectra with
high variations, such as a babble noise source.
10.3.3 Comparison
The effect of the smoothing factor κ in equation (10.6) is shown in Figure 10.16.
Note that the case of κ = 0 reduces equation (10.6) to the LR-based method. It
is obvious from the results that the detection accuracy increases with increase
in κ, at the offset regions without serious degradations in the performance

374
Voice Activity Detection
0
(a)
(b)
10
20
30
40
50
60
0
.1
.2 .3
.4
.5
.6
.7
.8 .9 .95 .98
Smoothing factor of SLR (k)
Detection error rate (%)
0
10
20
30
40
50
60
0
.1
.2 .3
.4
.5
.6
.7
.8 .9 .95 .98
Smoothing factor of SLR (k)
Detection error rate (%)
silence
onset
offset
SAS
silence
onset
offset
SAS
Figure 10.16
Analysis of the smoothing factor κ of the SLR with respect to detection
error rates; the noise level is 10 dB SNR and the noise sources are (a) vehicle and
(b) babble; SAS indicates speech active sections
at the onset regions for both vehicle and babble noisy signals. In the case
of vehicle noisy signals, as κ increases, the false alarm rate in the inactive
frames increases gradually for κ < 0.9, and then substantially for κ > 0.9.
However, in the case of babble noisy signals, it can be seen that the error rate
decreases gradually as κ increases for κ < 0.9, and then increases like the case
of the vehicle noisy signal, for κ > 0.9. Therefore, if κ is selected properly,
SLR-based method can give signiﬁcantly improved performances over the
LR-based method.
Under various noise levels and sources, the performance of VAD methods
such as SLR-based VAD [16, 17], ITU-T G.729 annex B VAD (G.729B) [1], ETSI
AMR VAD option 2 (AMR2) [8], and LR-based VAD with and without the
hangover scheme [12] have been compared as shown in Table 10.2. Original
AMR2 produces the detection result every 20 ms by the logical OR operation
of two 10 ms detection results, thus the 10 ms result can be obtained easily
by slight modiﬁcation of the original code. Taking into account the results
in Figure 10.16, κ = 0.9 is selected for SLR-based VAD. G.729B generates
considerably high error rates at the active regions in comparison with other
methods. It is important to note that frequent detection errors of speech
frames lead to serious degradation in speech quality, thus the error rate of
speech frame detection should be as low as possible. LR-based VAD gives
consistently superior performance to G.729B, but VAD without the hangover
scheme produces relatively high detection error rates in the active regions.
The hangover scheme can considerably alleviate this problem, but the speech
detection error rate is still somewhat high in comparison with the results of
both SLR-based VAD and AMR2. The performance of SLR-based VAD and
AMR2 seems to be comparable.

Bibliography
375
Table 10.2
Comparison of speech and silence detection error rates of SLR-based,
LR-based, AMR2, and G.729B VADs
Detection error rate (%)
SNR
Vehicle noise
Babble noise
(dB)
VAD
Inactive
Onset
Offset
SAS
Inactive
Onset
Offset
SAS
5
SLR
13.87
6.42
7.51
0.00
29.40
2.43
4.93
0.52
LR
4.49
12.88
30.92
0.00
46.25
6.90
27.77
2.49
LR + HO∗
5.33
12.05
12.86
0.00
46.50
4.52
11.28
1.48
AMR2
18.64
9.13
0.00
0.00
41.66
4.75
0.26
0.00
G.729B
8.58
70.23
60.21
5.14
48.17
56.79
45.88
5.12
15
SLR
17.12
3.48
0.73
0.00
29.20
2.05
0.00
0.00
LR
5.07
5.34
19.85
0.00
41.76
3.70
16.83
0.08
LR + HO
7.52
4.75
6.80
0.00
42.67
3.32
4.18
0.00
AMR2
20.15
3.78
0.00
0.00
51.53
2.19
0.26
0.00
G.729B
8.57
31.19
39.41
0.00
49.79
25.90
32.73
0.00
25
SLR
23.01
2.82
0.00
0.00
30.77
1.54
0.00
0.00
LR
6.64
3.29
11.79
0.00
34.38
1.54
8.75
0.00
LR + HO
10.94
1.56
2.75
0.00
36.45
0.89
1.59
0.00
AMR2
20.28
2.68
0.00
0.00
20.61
2.31
0.12
0.00
G.729B
8.85
12.75
19.06
0.00
44.30
11.34
15.49
0.00
LR + HO means LR-based VAD with the hangover scheme
10.4 Summary
In this chapter, standard VAD techniques as well as LR- and SLR-based VAD
have been reviewed. Through performance evaluation of the standard VAD
methods, including G.729B, GSM-EFR, AMR1, AMR2, and IS-127, it has been
shown that both AMR1 and AMR2 produce relatively high and consistent
performances over various noise sources and levels. On the other hand,
statistical-model-based LR VAD, performs well but may have a problem at
the offset regions of speech signals which may be solved with a hangover
in the decision making. The SLR method newly-proposed by Cho [16, 17]
has overcome this problem without the need for a hangover. SLR VAD has
comparable performance to AMR2.
Bibliography
[1] ITU-T (1996) A silence compression scheme for G.729 optimised for terminals
conforming to ITU-T V.70, ITU-T Rec. G.729 Annex B.

376
Voice Activity Detection
[2] ITU-T (1996) Dual rate speech coder for multimedia communications transmit-
ting at 5.3 and 6.3 kbit/s. Annex A: Silence compression scheme, ITU-T Rec.
G.723.1 Annex A.
[3] ITU-T (1996) Coding of speech at 8 kbit/s using conjugate-structure algebraic-
code-excited linear prediction (CS-ACELP), ITU-T Rec. G.729.
[4] ITU-T (1996) Dual rate speech coder for multimedia communications trans-
mitting at 5.3 and 6.3 kbit/s, ITU-T Rec. G.723.1.
[5] ETSI (1998) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for full rate speech trafﬁc channels, GSM 06.32 (ETSI
EN 300 965 v7.0.1).
[6] ETSI (1999) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for full rate speech trafﬁc channels, GSM 06.42 (draft
ETSI EN 300 973 v8.0.0).
[7] ETSI (1997) Digital cellular telecommunications system; Voice activity detector
(VAD) for enhanced full rate (EFR) speech trafﬁc channels, GSM 06.82 (ETS
300 730), March.
[8] ETSI (1998) Digital cellular telecommunications system (phase 2+); Voice
activity detector (VAD) for adaptive multi-rate (AMR) speech trafﬁc channels,
GSM 06.94 v7.1.1 (ETSI EN 301 708).
[9] P. DeJaco, W. Gardner, and C. Lee (1993) ‘QCELP: The North American
CDMA digital cellular variable rate speech coding standard’, in IEEE
Workshop on Speech Coding for Telecom, pp. 5–6.
[10] TIA/EIA (1997) Enhanced variable rate codec, speech service option 3 for
wideband spread spectrum digital systems, IS-127.
[11] TIA/EIA (1998) High rate speech service option 17 for wideband spread
spectrum communication systems, IS-733.
[12] J. Sohn, N. S. Kim, and W. Sung (1999) ‘A statistical model-based voice
activity detection’, in IEEE Signal Processing Letters, 6(1):1–3.
[13] Y. Ephraim and D. Malah (1984) ‘Speech enhancement using a minimum
mean square error short-time spectral amplitude estimator’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 32(6):1109–20.
[14] Y. Ephraim and D. Malah (1985) ‘Speech enhancement using a minimum
mean square error log-spectral amplitude estimator’, in IEEE Trans. on
Acoust., Speech and Signal Processing, 33(2):443–5.
[15] O. Capp´e (1994) ‘Elimination of musical noise phenomenon with the
Ephraim and Malah noise suppression’, in IEEE Trans. Speech and Audio
Processing, 2(2):345–9.
[16] Y. D. Cho (2001) ‘Speech detection enhancement and compression for
voice communications’, Ph.D. thesis, CCSR, University of Surrey, UK.

Bibliography
377
[17] Y. Cho and A. M. Kondoz (2001) ‘Analysis and improvement of a
statistical model-based voice activity detector’, in IEEE Signal Processing
Letters, 8(10):276–8.
[18] J. Sohn and W. Sung (1995) ‘A voice activity detection employing soft
decision based noise spectrum adaptation’, in Proc. of Int. Conf. on Acoust.,
Speech and Signal Processing, pp. 365–8. Amsterdam

11
Speech Enhancement
11.1 Introduction
In voice communications, speech signals can be contaminated by environmen-
tal noise and, as a result, the communication quality can be affected making
the speech less intelligible. Furthermore, compression of the noisy speech
with a low bit-rate vocoder may result in considerable quality degradation
due to frequent estimation errors of speech production model parameters
required by the vocoder. This problem can be reduced signiﬁcantly by speech
enhancement (or noise cancellation), which may enable more pleasant voice
communication by suppressing the noise components in input signals.
Generally, it is assumed that the noisy speech signal is formed additively
by speech and noise signals in which the noise is generated by environ-
mental sources such as vehicles, street noise, babble, etc. Therefore, in real
environments, complete noise cancellation is not feasible as it is not possible
to completely track varying noise types and characteristics that change with
time. However, by assuming that the noise characteristics change slowly in
comparison with speech, it is possible to achieve signiﬁcant reduction in
the background noise levels producing more pleasant and intelligible speech
quality. Speech enhancement techniques can help the speech model param-
eter extraction process used in low bit-rate vocoders and hence they are
becoming an integral part of low bit-rate speech coding systems.
Speech enhancement techniques can be classiﬁed, depending on the number
of available microphones, into single and multiple channels. In the case of
a single channel, the reference noise is not available explicitly. The noise
statistics are typically characterized during voice-inactive regions between
talk spurts using a voice activity detector. On the other hand, when dual
channels are available, one microphone senses the noisy speech, but the
other can be used mainly to catch the noise. By eliminating the noise factor
collected by the second microphone from the ﬁrst, it would be possible to
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

380
Speech Enhancement
cancel the noise more efﬁciently. However, in real environments, the multiple
microphone scheme can be limited in its use. In the following, we consider
the single microphone case only.
Over the last three decades, many kinds of speech enhancement techniques
have been proposed [1–4], mostly based on transform domain techniques,
adaptive ﬁltering, and model-based methods. The transform-based technique
transforms the time-domain signal into other domains, suppresses noise com-
ponents, and then applies the corresponding inverse transform to reconstruct
enhanced speech signals. Discrete Fourier transform (DFT), discrete cosine
transform (DCT), Karhunen–Lo`eve transform (KLT), and wavelet trans-
form (WT) are widely-known transform methods. DFT-based techniques
have been intensively investigated based on short-time spectral amplitudes
(STSA). KLT-based techniques, called signal subspace-based methods [5],
decomposes the space into signal (or speech) and noise subspaces by means
of eigen decomposition, and then suppresses the noise component in the
eigenvalues. DCT-based techniques [6, 7] are of lower computational com-
plexity and higher frequency resolution than DFT-based methods. It is also
possible to consider WT-based methods in order to simultaneously exploit the
time and frequency characteristics of noisy speech signals. Adaptive ﬁltering,
on the other hand, cancels the noise using adaptive ﬁlters such as the Kalman
ﬁlter. A Kalman ﬁlter models noisy speech signals in terms of state space
and observation equations, which represent the speech production process
and the noise addition model together with channel distortion, respectively
[4]. Kalman ﬁlters normally assume a white Gaussian noise distribution;
however, Gibson et al. proposed a generalization of Kalman-ﬁltering over
coloured noise signals [8, 9]. Finally, model-based techniques classify the
noisy signal using an a priori speech model, such as hidden Markov and
voiced/unvoiced models, and then conducts the enhancement depending
on classiﬁed speech models [2]. This method can be useful for improving
noise reduction performance for various kinds of speech signals. However,
it requires extra training to build the model with intensive computation. In
addition, it may exhibit model selection errors which cause signiﬁcant speech
quality degradation. Fundamentally, it is not easy to handle complicated
speech signals with a ﬁnite number of speech models.
Amongst the speech enhancement techniques, DFT (or STSA)-based meth-
ods have been well investigated in the forms of spectral subtraction, Wiener
ﬁltering, maximum likelihood-STSA estimation, and minimum mean square
error STSA estimation. The reason for the popularity of STSA-based speech
enhancement is due not only to its computational simplicity but also to recent
technical advances in this technique producing signiﬁcant speech quality
improvement. In the following section, details of some of the most used
STSA-based speech enhancement techniques are reviewed.

Review of STSA-based Speech Enhancement
381
11.2 Review of STSA-based Speech Enhancement
Assuming that the noise d(n) is additive to the speech signal x(n), the noisy
speech y(n) can be written as,
y(n) = x(n) + d(n),
for 0 ≤n ≤K −1
(11.1)
where n is the time index. The objective of speech enhancement is to ﬁnd
the enhanced speech ˆx(n) given y(n), with the assumption that d(n) is
uncorrelated with x(n). The time-domain signals can be transformed to the
frequency domain as,
Yk = Xk + Dk,
for 0 ≤k ≤K −1
(11.2)
where Yk, Xk, and Dk denote the short-time DFT of y(n), x(n), and d(n),
respectively. The STSA-based speech enhancement ﬁlters out the noise by
modifying the spectral amplitudes of Yk in equation (11.2). Therefore, the
enhanced spectrum
ˆXk can be written in terms of the modiﬁcation factor
(gain) Gk and the noisy spectrum Yk as,
ˆXk = GkYk,
for 0 ≤Gk ≤1
(11.3)
The gain Gk is a function of a posteriori SNR,
γk ≡
|Yk|2
E(|Dk|2)
(11.4)
and a priori SNR,
ξk ≡E(|Xk|2)
E(|Dk|2)
(11.5)
where E(|Dk|2) and E(|Xk|2) are the statistical variances of the kth spectral
components of the noise and the speech, respectively. The function deﬁnition
of the gain Gk depends on speciﬁc enhancement methods. The a posteriori SNR
γk in equation (11.4) can be obtained easily as Yk is the input noisy spectrum
and E(|Dk|2) can be obtained through a noise adaptation procedure discussed
in Section 11.3. However, the speech variance E(|Xk|2) for the estimation of
ξk in equation (11.5) is not available. As a solution, Ephraim and Malah [10]
proposed the decision-directed (DD) method given by,
ˆξ(t)
k
= α | ˆX(t−1)|2
E(|D(t)
k |2)
+ (1 −α)MAX(γ (t)
k
−1, 0)
(11.6)
where 0 ≤α < 1 and t is the frame index.

382
Speech Enhancement
Window
z−1
y(n)
|Yk|
∠Yk
Gk
E(|Dk|2)
|Xˆ
k|
|Xˆ
k
(t−1)|
ˆx(n)
Overlap
& Add
DFT
DFT−1
Gain
Estimate
Noise
Adaptation
×
Figure 11.1
Block diagram of general STSA-based speech enhancement method
The main aim of speech enhancement can be stated as an optimization
problem, where the residual noise is minimized while maintaining the speech
quality. The optimization process therefore requires a trade-off between noise
reduction and speech quality. For example, over-estimation of the noise
statistics may degrade the speech quality or intelligibility. On the other hand,
estimation of noise may not be accurate, leading to considerable residual
noise. The most typical residual noise in speech enhancement is musical
noise, also called tonal noise, which is composed of narrowband signals
appearing and disappearing with time-varying amplitudes and frequencies.
The overall block diagram of a generalized STSA-based speech enhance-
ment method is shown in Figure 11.1. The noisy speech, y(n), is ﬁrst converted
into STSA, |Yk|, by a DFT with windowing. The enhanced spectral amplitude,
| ˆXk|, is estimated by multiplying the noisy signal spectral components, Yk,
with their corresponding estimated gains, Gk. Enhanced speech, ˆx(n), is then
reconstructed by applying the inverse DFT to the enhanced STSA, | ˆXk|, with
the noisy speech phase, ̸ Yk, followed by an appropriate overlap-and-add
procedure to compensate for the window effect and to alleviate abrupt sig-
nal changes between two consecutive frames. The most critical part of this
process is the accurate estimation of the gains, Gk, which is discussed next.
11.2.1 Spectral Subtraction
The noisy spectrum Yk in equation (11.2) can be converted to the power
spectrum as,
|Yk|2 = |Xk|2 + |Dk|2 + X∗
kDk + XkD∗
k
(11.7)
where X∗
k and D∗
k denote the complex conjugates of Xk and Dk, respectively.
In order to estimate |Xk|2 in equation (11.7), the statistically expected values
are applied since |Dk|2, X∗
kDk, and XkD∗
k are not available. We therefore get,
|Yk|2 = | ˆXk|2 + E(|Dk|2) + E(X∗
kDk) + E(XkD∗
k)
(11.8)

Review of STSA-based Speech Enhancement
383
where E(·) is the ensemble average and | ˆXk|2 is the enhanced power spectrum.
The expected noise E(|Dk|2) can be estimated by a noise adaptation procedure
as shown in Section 11.3. Due to the assumption that x(n) is uncorrelated with
d(n), E(X∗
kDk) = 0 and E(XkD∗
k) = 0. Thus, equation (11.8) can be rewritten as,
|Yk|2 = | ˆXk|2 + E(|Dk|2)
(11.9)
The enhanced power spectrum | ˆXk|2 can be estimated by subtracting E(|Dk|2)
from |Yk|2, which is called power spectral subtraction.
The spectral power subtraction can be generalized with an arbitrary spectral
order, called generalized spectral subtraction (GSS), as,
|Yk|ν = | ˆXk|ν + E(|Dk|ν)
(11.10)
where ν is the spectral order. In the cases of ν = 1 and ν = 2, GSS in
equation (11.10) can be reduced to the magnitude and power spectral sub-
tractions, respectively.
In practice, GSS-based speech enhancement may exhibit severe musical
noise due to the high ﬂuctuation of the STSA of noisy signals. In some
cases the estimated noise magnitude can be larger than the input spectral
magnitude, where the enhanced spectral magnitudes are clamped to zero in
order to prevent the spectral magnitude from being negative. The clamping
which happens irregularly with frequency and time leads to producing the
sound of musical tones.
Berouti et al. [11] proposed a method for alleviating the musical noise
phenomenon, where | ˜Xk|ν = |Yk|ν −αE(|Dk|ν) and Berouti’s GSS (GBSS) is
given by,
| ˆXk|ν =

| ˜Xk|ν
;
if | ˜Xk|ν > βE(|Dk|ν)
βE(|Dk|ν)
;
otherwise
(11.11)
where α and β are the spectral over-subtraction and ﬂoor factors, respectively,
with α ≥1 and 0 ≤β ≤1. Note that GBSS reduces to GSS when α = 1 and
β = 0, and to Power Spectral Subtraction (PSS) if ν = 2, α = 1, and β = 0.
GBSS is capable of reducing the overall residual noise level as well as typical
musical noises by appropriately adjusting α and β. The GBSS gain G(GBSS)
k
becomes,
G(GBSS)
k
=




1 −α
 1
γk
 ν
2
	 1
ν
;
if γk
ν
2 > α + β
β
1
ν
1
√γk
;
otherwise
(11.12)

384
Speech Enhancement
The noise ﬂoor factor β contributes to the reduction of musical noise sounds.
It has the effect of converting the narrowband musical noise into a wider
band noise. Although higher β values give less musical noise, if β is set too
high it may result in an increase of the level of other artifacts of residual
noise. The over-subtraction factor, α, is useful for reducing the overall level of
residual noise. Higher α values give lower levels of residual noise. However,
too high α values may cause distortion in perceived speech quality. Through
experiments, it is found that GBSS with ν = 2, α = 4 ∼8, and β = 0.1 give
a moderate level of musical noise reduction while maintaining the perceived
speech quality.
In GBSS, both spectral over-subtraction and ﬂoor factors are ﬁxed to
constant values. However, each set of parameters exhibits different noise
reduction performances depending on the selection of these two factors. There
are approaches to obtain the optimal factors based on the psycho-acoustic
model and a parametric formulation. In the psycho-acoustic approach, both α
and β change each frame depending on the psychoacoustic masking threshold
for each spectral component [12]. In the parametric formulation, α is derived
using the MMSE-based metric [13].
11.2.2 Maximum-likelihood Spectral Amplitude Estimation
In DFT-based speech enhancement, given Yk = Xk+Dk, the optimum estimate
of the speech magnitude |Xk| is obtained from the noisy spectrum Yk, in which
Xk = |Xk| exp (jθk) where θk is the phase of Xk. Assuming that the noise Dk
has complex Gaussian distribution, the probability density function (PDF) of
Yk conditioned over |Xk| and θk is,
p(Yk| |Xk|, θk) =
1
πE(|Dk|2) exp

−|Yk|2 −2|Xk|Re(e−jθkYk) + |Xk|2
E(|Dk|2)

(11.13)
McAulay [14] has shown that the maximum likelihood (ML) estimate of |Xk|
can be obtained from the derivative of PDF with respect to |Xk|, where the
ML estimate | ˆXk| is given by,
| ˆXk| = 1
2

|Yk| +

|Yk|2 −E(|Dk|2)

(11.14)
which can be written in terms of the gain as,
G(ML)
k
= 1
2 + 1
2

1 −1
γk
(11.15)

Review of STSA-based Speech Enhancement
385
11.2.3 Wiener Filtering
The Wiener ﬁlter (WF) is a minimum mean square error (MMSE) estimate
of a desired signal in the time domain [1, 4]. Given a noisy signal y(n), for
0 ≤n ≤N −1, the Wiener ﬁlter produces the MMSE estimate ˆx(n) of speech
x(n) as,


ˆx(0)
ˆx(1)
...
ˆx(N −1)





ˆx
=


y(0)
y(−1)
· · ·
y(1 −P)
y(1)
y(0)
· · ·
y(2 −P)
· · ·
· · ·
· · ·
· · ·
y(N −1)
y(N −2)
· · ·
y(N −P)





Y


w0
w1
...
wP−1





w
(11.16)
where wk are the ﬁlter coefﬁcients for 0 ≤k ≤P −1 with the ﬁlter order P.
Equation (11.16) can be rewritten in the algebraic form as,
ˆx = Yw
(11.17)
The Wiener ﬁlter error signal e is the difference between the desired and
estimated speech signals given by,
e = x −ˆx
(11.18)
The error metric ε is deﬁned as,
ε = eTe
(11.19)
= (x −Yw)T(x −Yw)
= xTx −wTYTx −xTYw −wTYTYw
The ﬁlter coefﬁcients w are derived by setting the derivative of ε to zero with
respect to w,
∂ε
∂w = −2(xTY −wTyTY) = 0
(11.20)
Then, the optimal w is given by,
w = (YTY)−1YTx
(11.21)
in which YTY and YTx are the autocorrelation matrix Ryy of y(n) and
the cross-correlation vector ryx between y(n) and x(n), respectively. Thus,
equation (11.21) can be written as,
w = R−1
yy ryx
(11.22)

386
Speech Enhancement
Note that because of the assumption that the speech is uncorrelated with
noise, Ryy = Rxx + Rdd and ryx = rxx. Thus, equation (11.22) becomes,
w = (Rxx + Rdd)−1rxx
(11.23)
Equation (11.23) can be interpreted in the frequency domain as,
G(WF)
k
=
E(|Xk|2)
E(|Xk|2) + E(|Dk|2)
=
ξk
1 + ξk
(11.24)
Note that the Wiener ﬁlter gain G(WF)
k
in equation (11.24) is deﬁned in terms
of the a priori SNR ξk only.
11.2.4 MMSE Spectral Amplitude Estimation
The Wiener ﬁlter is a time-domain MMSE estimation while McAulay’s
method is a frequency-domain ML estimation technique. Thus, it is possible to
consider the MMSE estimate of the spectral amplitude [10], which minimizes
ε = (|Xk| −| ˆXk|)2
(11.25)
The MMSE-STSA estimate, | ˆXk| given Yk, is,
| ˆXk| = E(|Xk| |Yk)
=
 ∞
0
 2π
0
αkp(Yk|αk, θk)p(αk, θk)dθkdαk
 ∞
0
 2π
0
p(Yk|αk, θk)p(αk, θk)dθkdαk
(11.26)
where,
p(Yk|αk, θk) =
1
πE(|Dk|2) exp

−|Yk −αkejθk|2
E(|Dk|2)

(11.27)
and,
p(αk, θk) =
αk
πE(|Xk|2) exp

−
α2
k
E(|Xk|2)

(11.28)

Review of STSA-based Speech Enhancement
387
in which αk and θk are dummy variables for the spectral amplitude and phase,
respectively, of Xk. The amplitude has the Rayleigh distribution given by,
p(αk) =
2αk
E(|Xk|2) exp

−
α2
k
E(|Xk|2)

(11.29)
and the phase has the uniform distribution given by,
p(θk) = 1
2π
(11.30)
Through derivation given in [10], equation (11.26) can be rewritten as,
| ˆXk| = 
(1.5)
√vk
γk
exp

−vk
2
 
(1 + vk)I0
vk
2

+ vkI1
vk
2

|Yk|
(11.31)
where 
(·) is the gamma function with 
(1.5) = √π/2, I0(·) and I1(·) denote
the modiﬁed Bessel functions of zero and ﬁrst order, respectively, and
vk ≡
ξk
1+ξk γk.
As a variant, Ephraim and Malah [15] proposed an MMSE log spectral
amplitude (MMSE-LSA) estimator, based on the well-known fact that a
distortion measure with the log spectral amplitudes is more suitable for speech
processing. The MMSE-LSA estimator minimizes the following distortion
measure,
ε =

log |Xk| −log | ˆXk|
2
(11.32)
with
| ˆXk| = exp

E{log(|Xk|) |Yk}

(11.33)
From [15], the ﬁnal estimate becomes,
| ˆXk| =
ξk
1 + ξk
exp
1
2
 ∞
vk
e−t
t dt

|Yk|
(11.34)
11.2.5 Spectral Estimation Based on the Uncertainty of Speech
Presence
The conventional speech enhancement methods can be extended by incorpo-
rating the uncertainty of speech presence [14, 15]. The absence and presence
of speech, H0 and H1, respectively, can be deﬁned as,
H0 : Yk = Dk
(11.35)
H1 : Yk = Xk + Dk
(11.36)

388
Speech Enhancement
Assuming that each spectral component of speech and noise has complex
Gaussian distribution, and that the noise is additive to and uncorrelated with
the speech signal, the conditional probability density functions observing a
noisy spectral component Yk, given H0 and H1, are
p(Yk|H0) =
1
πE(|Dk|2) exp

−
|Yk|2
E(|Dk|2)

(11.37)
p(Yk|H1) =
1
π(E(|Dk|2) + E(|Xk|2)) exp

−
|Yk|2
E(|Dk|2) + E(|Xk|2)

(11.38)
where k is the spectral bin index, 0 ≤k ≤K/2, and E(|Dk|2) and E(|Xk|2)
denote the variances of the kth spectral components of noise and speech,
respectively.
The probability of speech presence can be given by Bayes’ rule,
p(H1|Yk) =
p(Yk|H1)p(H1)
p(Yk|H0)p(H0) + p(Yk|H1)p(H1)
=
µ
1 + µk
(11.39)
where,
µ = p(H1)
p(H0)
(11.40)
in which p(H1) and p(H0) denote the a priori probability of speech presence
and absence, respectively. The likelihood ratio of the kth spectral bin k can
be deﬁned from the above two likelihood ratios,
k = p(Yk|H1)
p(Yk|H0)
=
1
1 + ξk
exp
(1 + γk)ξk
1 + ξk

(11.41)
The enhanced spectrum based on the probability of speech presence is
written as,
ˆXk = E(Xk|Yk, H0)p(H0|Yk) + E(Xk|Yk, H1)p(H1|Yk)
(11.42)

Review of STSA-based Speech Enhancement
389
where p(H0|Yk) denotes the probability of speech absence given Yk. Since the
expected speech spectrum under speech absence is zero, i.e. E(Xk|Yk, H0) = 0,
equation (11.42) can be simpliﬁed to,
ˆXk = E(Xk|Yk, H1)p(H1|Yk)
(11.43)
E(Xk|Yk, H1) and p(H1|Yk) can be computed by a conventional spectral esti-
mator and equation (11.39), respectively.
11.2.6 Comparisons
Objective speech qualities for voice-active regions are evaluated in terms
of both segmental SNR (SEGSNR) improvement and Itakura–Saito distor-
tion (ISD). The SEGSNR improvement indicates the difference between the
SEGSNRs of the enhanced speech and the noisy input signals, in which the
SEGSNR is deﬁned by,
SEGSNR(dB) = 10
M


M−1
"
m=0
log10



(m+1)N−1
"
n=mN
x2(n)
(x(n) −ˆx(n))2





(11.44)
where N and M are the frame size and the total number of frames, respectively.
The ISD is deﬁned as,
ISD(dB) = 10 log10

aT
xRˆxax
aT
ˆxRˆxaˆx

(11.45)
where ax and aˆx are the LPC coefﬁcients of the desired and estimated speech
signals, respectively, and Rˆx is the autocorrelation matrix of the estimated
signal.
For comparison, speech material of 64 seconds, mixed with vehicle and
helicopter noises of 0, 5 and 10 dB SNR were used. Enhancement processing
was applied every 10 ms in the frequency domain by the ﬁve types of spectral
estimator: PSS, GBSS, ML, WF, and MMSE-LSA. The MMSE-LSA is further
classiﬁed, depending on the adoption of the speech presence uncertainty,
into MMSE-LSA-HD and MMSE-LSA-SD in which HD and SD denote the
hard and soft decision methods, respectively. The reference (the best possible
processed signal) is obtained using the original spectral amplitudes with the
phases of the noisy signal, because the ideal speech enhancement is achieved
with the original speech spectral amplitudes and the phases of the noisy input
speech.
The SEGSNR improvement and ISD for the vehicle and the helicopter noisy
signals are shown in Figures 11.2, 11.3, 11.4, and 11.5. From the analysis, it is

390
Speech Enhancement
0
2
4
6
8
10
12
14
16
0
10
SNR (dB)
SEGSNR improvement (dB)
PSS
GBSS
ML
WF
MMSE-HD
MMSE-SD
Theoretical limit
5
Figure 11.2
SEGSNR improvements from STSA-based speech enhancement meth-
ods in vehicle noise environments
0
1
2
3
4
5
6
0
10
SNR (dB)
ISD (dB)
PSS
GBSS
ML
WF
MMSE-HD
MMSE-SD
Theoretical limit
Unprocessed
5
Figure 11.3
ISDs of STSA-based speech enhancement methods in vehicle noise
environments

Review of STSA-based Speech Enhancement
391
0
2
4
6
8
10
12
14
16
0
10
SNR (dB)
SEGSNR improvement (dB)
PSS
GBSS
ML
WF
MMSE-HD
MMSE-SD
Theoretical limit
5
Figure 11.4
SEGSNR improvements from STSA-based speech enhancement meth-
ods in helicopter noise environments
0
1
2
3
4
5
6
0
10
SNR (dB)
ISD (dB)
PSS
GBSS
ML
WF
MMSE-HD
MMSE-SD
Theoretical limit
Unprocessed
5
Figure 11.5
ISDs of STSA-based speech enhancement methods in helicopter noise
environments

392
Speech Enhancement
found that WF- and MMSE-based methods give better results than the other
methods.
For the noisy input signal in Figure 11.6, the spectrograms of different
enhancement methods, showing the characteristics of the residual noise, are
shown in Figures 11.9, 11.10, 11.11, 11.12, 11.13, and 11.14. The spectrograms
of the noise-free and reference signals are shown in Figures 11.7 and 11.8,
respectively. For the PSS and ML-based methods, severe musical noise gives
irregular spots in the spectrograms in Figures 11.9 and 11.11, respectively.
The GBSS method with ν = 2, α = 4, and β = 0.1 reduces the musical tones to
a moderate level (see Figure 11.10), compared with the PSS and ML methods.
The WF-based method gives a further reduction in the level of the residual
noise as shown in Figure 11.12. Using the MMSE-STSA-based method, it
is possible to further eliminate the musical noises (see Figure 11.13). Even
though the level of the overall residual noise of the MMSE-STSA is slightly
higher than that of the WF method, the sound quality of MMSE-STSA is
perceptually more comfortable than that of the WF method. The higher
speech quality is due to further reduction in tonal signals. Combining the
soft-decision technique with the MMSE-based method, it is possible to reduce
the overall level of the residual noise as shown in Figure 11.14.
11.2.7 Discussion
Ephraim and Malah’s speech enhancement method gives higher performance
mainly due to the DD-based a priori SNR estimation. Cappe [16] has shown
its usefulness for eliminating musical noise phenomena through behavioural
analysis. From interpretation of equation (11.6), it is not difﬁcult to see
that ˆξk is a smoothed version of γk. The a posteriori SNR γk shows high
ﬂuctuation from frame to frame, while ˆξk changes slowly. By exploiting the
characteristics of the two SNRs, γk and ˆξk, improved performance in speech
quality is achieved.
The WF produces better performance than either GBSS- or ML-based
methods. The reason behind this better performance is also due to the
DD-based a priori SNR estimation used in the gain function of the WF. The
usefulness of the DD-based a priori SNR can also be applied to a posteriori SNR-
based speech enhancement methods, such as GBSS- and ML-based spectral
estimators, by replacing the a posteriori SNR with the a priori SNR [17] as,
γk = ˆξk + 1
(11.46)
Although substantial reduction of musical noise is achieved by the WF-based
method, it is observed that the musical noise is not completely removed (see
Figure 11.12). It is also possible to show that the musical noise phenomenon
exists in the a priori SNR-based speech enhancement using equation (11.46).

Review of STSA-based Speech Enhancement
393
0.5
1
1.5
2
2.5
3
3.5
4
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.6
Noisy speech: (a) time waveform and (b) spectrogram at 5 dB SNR
vehicle noise

394
Speech Enhancement
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.7
Noise-free speech: (a) time waveform and (b) spectrogram

Review of STSA-based Speech Enhancement
395
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.8
Speech enhanced using a theoretical limit: (a) time waveform and
(b) spectrogram

396
Speech Enhancement
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.9
Speech enhanced by PSS: (a) time waveform and (b) spectrogram

Review of STSA-based Speech Enhancement
397
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.10
Speech enhanced by GBSS with ν = 2, α = 4, and β = 0.1: (a) time
waveform and (b) spectrogram

398
Speech Enhancement
0.5
1
1.5
2
2.5
3
3.5
4
−2
−2.5
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.11
Speech enhanced by ML-STSA estimation: (a) time waveform and
(b) spectrogram

Review of STSA-based Speech Enhancement
399
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.12
Speech enhanced by WF: (a) time waveform and (b) spectrogram

400
Speech Enhancement
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.13
Speech enhanced by MMSE-STSA estimation: (a) time waveform and
(b) spectrogram

Review of STSA-based Speech Enhancement
401
0.5
1
1.5
2
2.5
3
3.5
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
× 104
Time (sec)
(a)
Time
(b)
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 11.14
Speech enhanced by MMSE-STSA estimation with speech presence
uncertainty: (a) time waveform and (b) spectrogram

402
Speech Enhancement
Therefore, the following guidelines for designing a speech enhancement
algorithm can be stated:
• Proper combination of the a priori and a posteriori SNRs is important to
eliminate the musical noise while maintaining high speech quality.
• The soft-decision technique based on speech presence uncertainty is useful
for further suppressing the level of residual noise for voice-inactive regions.
11.3 Noise Adaptation
Frequency-domain speech enhancement focuses mainly on improved esti-
mation of spectral attenuation factors with the assumption of the given
noise statistics. However, in practice, the noise statistics exhibit frame to
frame ﬂuctuations which require robust estimation for good performance.
Noise estimation methods can be classiﬁed into two types: hard decision
(HD), which adapts the noise variance during voice-inactive regions by voice
activity detection (VAD), and soft decision (SD), which adapts the noise all
the time.
HD-based methods are quite successful when voice activity classiﬁcation
of speech is performed accurately. However, VAD itself is a complicated
technique to implement when high performances under various noise sources
and levels are required. Thus, speech detection errors due to VAD may cause
over-estimation or under-estimation of the noise statistics, which may lead to
degradation of speech quality. The performance of the HD-based method is
therefore heavily dependent on the performance of the VAD method used.
SD-based methods adapt the noise statistics based on the uncertainty of
speech absence, instead of the hard-limited function used in the HD-based
methods [18, 19]. SD-based methods do not rely on VAD decisions and update
the noise statistics even in the presence of speech. SD-based methods rely on
the accurate estimation of the mixture ratio between speech and noise. The
inaccurate measurement of speech absence (or presence), especially in voice-
active regions, can seriously distort the enhanced speech. Cho [20] proposed
a mixed-decision-based noise adaptation, combining the characteristics of the
HD- and SD-based methods.
11.3.1 Hard Decision-based Noise Adaptation
The HD-based method conducts noise adaptation during speech absence
regions only,
E(|D(t)
k |2) =

ηE(|D(t−1)
k
|2) + (1 −η)|Y(t)
k |2
if Y(t) ∈H0
E(|D(t−1)
k
|2)
otherwise
(11.47)

Noise Adaptation
403
where the superscript t indicates the frame index, η is the smooth adaptation
factor, e.g. 0.95, and Y is the noisy spectrum. In the case of speech presence,
usually indicated by a VAD, it does not update the noise variance. HD-based
noise adaptation has been widely used in speech enhancement.
11.3.2 Soft Decision-based Noise Adaptation
The SD-based noise estimation, the estimated noise given by Yk, is formu-
lated as,
E(Dk|Yk) = E(Dk|Yk, H0)p(H0|Yk) + E(Dk|Yk, H1)p(H1|Yk)
= {p(H0|Yk) + p(H1|Yk)GD,k}Yk
(11.48)
where E(Dk|Yk, H0) = Yk, E(Dk|Yk, H1) = GD,kYk. The probability of speech
presence p(H1|Yk) is deﬁned in equation (11.39) and p(H0|Yk) = 1 −p(H1|Yk).
The optimal noise gain GD,k can be derived from the Wiener estimator W in
the time domain. It can be shown that W = Rdd(Rxx +Rdd)−1, in which Rdd and
Rxx denote the covariance matrices of the noise and speech signals resulting
in the ﬁlter frequency response given by,
GD,k =
E(|Dk|2)
E(|Xk|2) + E(|Dk|2)
=
1
1 + ξk
(11.49)
where ξk is the a priori SNR which can be estimated using the decision-
directed method deﬁned in equation (11.6). Here, the estimation of noise gain
GD is an independent task within the noise estimation process which may
be used in other kinds of enhanced spectral estimation techniques, such as
MMSE, MMSE-LSA, etc. The noise variance of the SD-based method may be
estimated in a recursive manner as given below,
E(|D(t)
k |2) = ηE(|D(t−1)
k
|2) + (1 −η)|E(D(t)
k |Y(t)
k )|2
(11.50)
11.3.3 Mixed Decision-based Noise Adaptation
In order to alleviate the problems in the HD- and SD-based methods, the
MD-based method is proposed [20] for noise adaptation as
E(|D(t)
k |2) =



ηE(|D(t−1)
k
|2) + (1 −η)|Y(t)
k )|2
; if Y(t) ∈H0 and (t) ≤θ
ηE(|D(t−1)
k
|2) + (1 −η)|E(D(t)
k |Y(t)
k )|2
; if Y(t) ∈H0 and (t) > θ
E(|D(t−1)
k
|2)
; otherwise
(11.51)

404
Speech Enhancement
where (t) = {(K
k=1 (t)
k }1/K, as deﬁned in equation (11.41). The threshold θ
is set to a sufﬁciently small value, i.e. θ < 1, that rarely classiﬁes the speech
as silence.
11.3.4 Comparisons
In order to show the robustness of the noise adaptation techniques, speech
quality is compared in terms of both SEGSNR improvement and ISD with
respect to the speech-detection error-rate of VAD (Ed). Various Ed are cali-
brated by a voice activity detector [21], and then frame-by-frame VAD results
are given to each noise adaptation method. For the experiment, speech
material of 64 seconds was mixed with vehicle noise at 5 dB SNR, and then
processed every 10 ms in the frequency domain by the MMSE estimator [10]
employing noise adaptation methods. Finally, the enhanced speech signal
is obtained by the inverse DFT of the enhanced spectrum, followed by the
overlap-and-add procedure.
SEGSNR improvement and ISD between the clean and enhanced speech
signals for vehicle noisy speech signals of 0, 5, and 10 dB SNR are shown in
Figures 11.15, 11.16, and 11.17, respectively. The experiments conﬁrm that
• The SD-based method results in worse performance compared with both
the MD- and the HD-based methods, for low Ed.
• The HD-based method exhibits signiﬁcant degradation in performance
with increases in Ed.
• The MD-based method produces, regardless of the VAD performance,
robust and superior performance in comparison with the HD- and SD-
based methods.
Note that for very low Ed, i.e. 0.0 ≤Ed < 0.1, the performances of the MD
and HD are slightly worse than in the case of Ed = 0.2. This is caused by less
frequent adaptation of the noise frames because of the increased false alarm
rate of the VAD. n other words, VAD produces the low Ed at the expense of
an increased false alarm rate during pauses.
Results for helicopter noisy speech with levels of 0, 5, and 10 dB SNR are
shown in Figures 11.18, 11.19, and 11.20, respectively. They exhibit perfor-
mance patterns similar to the vehicle noisy signals despite differences in the
absolute values being measured.
In conclusion we can say that the STSA-based spectral enhancement tech-
niques including GSS, GBSS, ML, WF, and MMSE-based algorithms together
with the estimate of speech presence uncertainty have various advantages
and disadvantages. The MMSE-based STSA method combined with speech
presence uncertainty is perhaps the best currently available method for

Noise Adaptation
405
6
7
8
9
10
11
12
13
14
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
1
2
3
4
5
6
ISD
MD
HD
SD
Theoretical limit
Unprocessed
Figure 11.15
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for vehicle noisy speech of 0 dB SNR
noise reduction. In speech enhancement systems, accurate noise estima-
tion/adaptation is necessary to keep track of the noise characteristics. Noise
estimation and adaptation is the most important area that requires further
research for better speech enhancement techniques.

406
Speech Enhancement
2
4
6
8
10
12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
1
0
2
3
4
5
ISD
MD
HD
SD
Theoretical limit
Unprocessed
Figure 11.16
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for vehicle noisy speech of 5 dB SNR
11.4 Echo Cancellation
Echo in a telecommunications system is the delayed and distorted sound
which is reﬂected back to the source. In telecommunications, there are two
types of echo: acoustic echo, which results from the reﬂection of sound
waves and acoustic coupling between the microphone and loudspeaker,

Echo Cancellation
407
−2
0
2
4
6
8
10
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
1
0
2
3
4
5
ISD
MD
HD
SD
Theoretical limit
Unprocessed
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
Figure 11.17
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for vehicle noisy speech of 10 dB SNR
and electrical echo, generated at the two-to-four wire conversion hybrid
transformer due to imperfect impedance matching. Here, we will develop
cancellation for the electrical echo which will be equally applicable for acoustic
echo cancellation.
The source of electrical echo can be understood by considering a simpliﬁed
block diagram of a connection between a pair of subscribers, S1 and S2,

408
Speech Enhancement
6
7
8
9
10
11
12
13
14
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
1
0
2
3
4
5
6
ISD
MD
HD
SD
Theoretical limit
Unprocessed
Figure 11.18
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for helicopter noisy speech of 0 dB SNR
as shown in Figure 11.21. It can be seen from this block diagram that
each subscriber has a two-wire loop over which both the received signal
and transmitted signals travel. On the four-wire part of the line, the two
directions of transmission are separated. The speech from S1 travels on the
upper path and the speech from S2 travels on the lower path, as indicated
by the arrows. The converter device between the two- and four-wire sections

Echo Cancellation
409
0
2
4
6
8
10
12
14
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
ISD
MD
HD
SD
Theoretical limit
Unprocessed
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
Figure 11.19
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for helicopter noisy speech of 5 dB SNR
is called the hybrid. The role of the hybrid is to direct the signal energy
arriving from S1 or S2 to the upper or lower path of the four-wire circuit,
without allowing any leakage back to the source over the opposite direction
line. Because of impedance mismatching, however, some of the transmitted
signal returns to the original source, which hears a delayed version of its own

410
Speech Enhancement
−2
0
2
4
6
8
10
SEGSNR improvement (dB)
MD
HD
SD
Theoretical limit
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
ISD
MD
HD
SD
Theoretical limit
Unprocessed
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Speech detection error-rate of VAD, Ed 
Figure 11.20
Comparison of SEGSNR improvement and ISD against the speech-
detection error-rate of VAD for helicopter noisy speech of 10 dB SNR
speech. This is called the talker echo and its subjective effect depends on the
round trip delay around the loop. For short delays and reasonable attenuation
(6 dB or more) the talker echo cannot be distinguished from the normal side
tone of the telephone and hence does not cause problems. In applications
such as satellite communications however, as a consequence of high altitude,

Echo Cancellation
411
Hybrid
Echo
Two-wire
Subscriber loop
S1
Hybrid
Echo
Two-wire
Subscriber loop
Four-wire
Trunk
S2
Figure 11.21
Block diagram of duplex connection between two subscribers
a round trip delay of approximately 540 ms (270 ms each way) is possible; this
makes the echo very disturbing and may in fact make it impossible to carry
out a conversation. In such cases, it is essential to control or remove the echo.
Since the subjective disruption of echo is proportional to the round trip delay
as well as the echo energy level, the echo control techniques usually depend
on the circuit length.
Some international connections use a switch (called an echo suppressor)
operated by the voice activity, which attempts to impose an open circuit on
the return path from listener to talker when the listener is silent. However,
an echo suppressor cannot operate during double-talk and hence produces
choppy echo. For this reason, echo suppressors are now being replaced by
echo cancellers that are based on adaptive ﬁltering techniques.
11.4.1 Digital Echo Canceller Set-up
A block diagram of an echo canceller for one direction of transmission is
shown in Figure 11.22, where the far-end talker signal is denoted by y(i),
the unwanted echo signal by r(i), and the near-end talker signal by x(i). The
Hybrid
Echo
Canceller
+
u(i) = x(i) + r(i) − r(i)
x(i) + r(i)
y(i)
x(i)
Near-end
talker
From far-end
talker
r(i)
^
To far-end
listener
−
+
^
Figure 11.22
Block diagram of an echo canceller

412
Speech Enhancement
near-end signal and the echo are added together at the output of the hybrid.
Since the far-end signal is available as a reference for the echo canceller, the
replica of the echo ˆr(i) is estimated by matching the signals on both paths of
the four-wire section. The estimated echo is then subtracted from the total of
the returned echo and the near-end signal,
u(i) = x(i) + r(i) −ˆr(i)
(11.52)
The difference between r(i), the returned echo, and ˆr(i), the estimated echo,
should be as small as possible for good echo cancellation performance. The
echo canceller estimates the echo by using the far-end reference signal in a
transversal ﬁlter such as the one shown in Figure 11.23. This ﬁlter basically
acts as a tapped delay line. If the impulse response of the ﬁlter is same as the
echo path response, then the estimated echo and the returned echo become
identical, resulting in perfect echo cancellation. Since the echo path response
is not known in advance and may vary slowly with time, the coefﬁcients of
the transversal ﬁlter are adapted. In order to produce no distortion on the
near-end talker signal, the ﬁlter coefﬁcients are only updated when there is
no near-end activity.
The number of ﬁlter coefﬁcients, which may be very signiﬁcant from a
complexity point of view, is usually determined by the length of the echo
path impulse response, which typically lasts 2 to 4 ms, requiring 32 taps
(4 × 10−3/125 × 10−6) approximately. However, the impulse response of the
echo path may be delayed by some time depending on the distance between
the position of the echo canceller and the hybrid in the system. Moreover
more than a few taps may be needed to accurately model the response of the
hybrid. Therefore, the use of 64 or 128-tap ﬁlters are typical.
Z−1
Z−1
Z−1
Z−1
a0
a1
a2
ap
Echo
Reference Signal
y(i)
r(i)
^
x(i) + r(i)
x(i)  +r(i) − r(i)
To far-end
From far-end
^
−
+
+
Figure 11.23
Block diagram of a transversal filter used in echo cancellation

Echo Cancellation
413
+
−
+
Hybrid
Hybrid
+
Echo
Echo
Canceller
Canceller
C
H
A
N
N
E
L
+ −
Figure 11.24
Block diagram showing echo cancellation applied to both ends
In practice, echo cancellers are applied on both ends to cancel the echoes in
each direction as shown in Figure 11.24. An echo canceller should, in general,
satisfy the following fundamental requirements:
• Rapid convergence of the ﬁlter coefﬁcients when turned on.
• Very low echo when there is no near-end speech.
• Slow divergence when there is no far- or near-end speech.
• Little divergence when both near- and far-end signals are present.
The ITU-T G.165 recommendations [22], which summarize the above require-
ments, are as follows:
• After convergence with no near-end speech, with input noise level between
−10 dBm0 and −30 dBm0, ﬁnal echo return loss (ERL) should be −40 dB.
• After 500 ms of ﬁrst start up, the parameters should converge to give at
least 27 dB echo reduction with no near-end speech.
• Degradation of residual echo after 2 minutes from the time all signals are
removed from the fully-converged canceller should not be more than 10 dB.
• The returned echo level, 500 ms after interruption of the echo path, should
reach −40 dBm0.
11.4.2 Echo Cancellation Formulation
An echo canceller can be split into the following parts: adaptive transversal
ﬁlter, near-end speech detection, and residual error suppression.
Adaptive Transversal Filter
In a digital echo canceller both the reference and echo signals are available in
digital form. Therefore the echo path impulse response can be represented in
digital form by ak,
r(i) =
N−1
"
k=0
aky(i −k)
(11.53)

414
Speech Enhancement
Assuming the system is linear and the echo path impulse response is of ﬁnite
length N, then the echo canceller forms the replica of the returned echo using,
ˆr(i) =
N−1
"
k=0
aky(i −k)
(11.54)
When ak = hk, for k = 0, 1, . . . , N −1 the returned and estimated echoes are
identical resulting in no residual echo. The coefﬁcients of the transversal ﬁlter
are updated to match the slowly time-varying echo path impulse response
by minimizing the mean squared residual error given by:
e2(i) = [r(i) −ˆr(i)]2
(11.55)
When there is no near-end speech (x(i) = 0), the ﬁlter coefﬁcients are updated
in such a way that the residual error tends to a minimum. The update of the
coefﬁcients at each iteration is controlled by a step size β,
hk(i + 1) = hk(i) + 2βe(i)y(i −k)
(11.56)
The convergence of the algorithm is determined by the step size β and the
power of the far-end signal y(i). In general, making β large speeds up the
convergence, while a smaller β reduces the asymptotic cancellation error. It
has been shown that the convergence time constant is inversely proportional
to the power of y(i) and that the algorithm will converge very slowly for
low-signal levels [23]. To overcome this situation, the loop gain is usually
normalized by an estimate of the far-end signal power,
2β = 2β(i) =
β1
Py(i)
(11.57)
where β1 is a compromise value of the step size constant and Py(i) is an
estimate of the average power in y(i) at time i. The far-end signal power can
be estimated by
Py(i) = [Ly(i)]2
(11.58)
where,
Ly(i + 1) = (1 −ρ)Ly(i) + ρ|y(i)|
(11.59)
and a typical value of ρ = 2−7. The above equation is only an estimate
of the average signal level, which is updated for every sample using the
approximation for ease of implementation in real-time.

Echo Cancellation
415
Near-End Speech Detection
The quality of the echo canceller can be affected signiﬁcantly if the near-end
speech is not detected accurately. This is because the ﬁlter coefﬁcients will be
adjusted wrongly and hence will distort the near-end speech. Therefore, the
coefﬁcients are updated only when there is no near-end speech; they are kept
ﬁxed during near-end activity to prevent divergence. The power estimate ˆs(i)
of the near-end composite signal s(i) = x(i) + r(i) is usually compared with
the power estimate ˆy(i) of the far-end signal y(i) to decide if there is near-end
activity. The power estimate is computed as
ˆs(i + 1) = (1 −α)ˆs(i) + α|s(i)|
(11.60)
and,
ˆy(i + 1) = (1 −α)ˆy(i) + α|y(i)|
(11.61)
where a typical value for α is 1/32. Near-end speech is declared when
ˆs(i) ≥MAX[ˆy(i), ˆy(i −1), . . . , ˆy(i −N)]
(11.62)
In order to avoid continuous switching, every time near-end speech is
detected, it is assumed to last for some time (typically 600 samples).
Residual Echo Suppressor
Due to nonlinearities in the echo path, the convergence of the ﬁlter coefﬁcients
and hence the accuracy of the echo path modelling is limited to around 30
to 35 dB. In order to further enhance the performance of the echo canceller, a
residual echo suppressor can be used. This can be done simply by comparing
the returned signal power with a threshold relative to the far-end signal, and
completely eliminating it if it falls below the threshold. Again the returned
signal power is estimated using
Lu(i + 1) = (1 −ρ)Lu(i) + ρ|u(i)|
(11.63)
Whenever Lu(i)/Ly(i) < 2−4, the residual echo suppressor is activated. In
some applications however, it may be perceptually more acceptable to leave
a very low level of random signal to indicate that the line is not dead.
11.4.3 Improved Performance Echo Cancellation
Echo cancellation based on the NLMS algorithm (or other variants of the
general LMS algorithm) performs well with both acoustic and electrical
echoes, provided that near-end speech is not present. The performance,

416
Speech Enhancement
however, degrades when near-end speech is present (and is even worse
if near-end speech cannot be detected correctly). Signiﬁcant performance
degradation is also expected when echo is contaminated with background
noise.
Echo cancellers generally stop ﬁlter coefﬁcient adaptation when near-end
speech is present. An accurate near-end speech detector is therefore necessary
to avoid divergence of the ﬁlter coefﬁcients, which may have two drawbacks.
First, the cancellation performance strongly depends on the accuracy of the
near-end speech detector. The second drawback is related to the length of the
near-end speech presence. In cases where a near-end speech segment is long,
the echo characteristics may change considerably and if the ﬁlter coefﬁcients
are not continually adapted during those segments, then the ﬁlter will lose
synchronization with the echo path changes, leading to a large change when
ﬁlter coefﬁcient adaptation is resumed. This may result in temporary ﬁlter
divergence causing performance reduction.
An adaptive normalized least mean squared (ANLMS) algorithm has been
suggested by Al-Naimi [24] to overcome these problems. It is based on the
NLMS algorithm (with a 128-tap transversal adaptive ﬁlter [25]). The NLMS
of [25] differs from the general NLMS in that ﬁlter coefﬁcients are updated
less frequently with a thinning factor, M, resulting in
hk(i + 1) = hk(i) + β
M−1
"
m=0
e(i + M −m)y(i + M −m −k)
σ(i)2
(11.64)
The ANLMS includes a number of enhancements to the system in [25]
which are: increased robustness to noise contamination, continuous ﬁlter
coefﬁcient adaptation, and elimination of the need for a near-end speech
detector. The ANLMS is given by,
hk(i + 1) = hk(i) + wk(i)β
M−1
"
m=0
e(i + M −m)y(i + M −m −k)
ψ(i)2ρ(i)2
(11.65)
where ψ(i) and ρ(i) are given by,
ψ(i) = αeψ(i −1) + (1 −αe)
))y(i)
))
(11.66)
and,
ρ(i) = αeρ(i −1) + (1 −αe) |z(i)|
(11.67)

Echo Cancellation
417
respectively. The weighting function wk(i) is
wk(i) = exp


−
* ˆhk(i) −hk(i)
γβ
+2


(11.68)
where ˆhk(i) is the unweighted estimate and hk(i) is the average track of ﬁlter
coefﬁcient k at time i, given by,
ˆhk(i) = hk(i) + β
M−1
"
m=0
e(i + M −m)y(i + M −m −k)
ψ2(i)ρ2(i)
(11.69)
hk(i) = αhhk(i −1) + (1 −αh)hk(i).
(11.70)
Note that 0 ≤αe ≤1, 0 ≤αh ≤1 and γ > 0 are tuning parameters which need
to be optimized for a given application.
The performance improvement with the ANLMS method stems from the
soft-decision weighting function, wk(i). This weighting function removes the
need for a near-end speech detector and its associated problems. It also
provides a soft-decision means of continuous ﬁlter coefﬁcient adaptation so
as not to lose synchronization with echo path changes. In addition, it results
in increased robustness to background noise contamination.
At time i, the weighting function wk(i) depends, for its calculation, on
the average track of ﬁlter coefﬁcient k (as given in equation (11.70)) and
on the unweighted estimate of ﬁlter coefﬁcient hk(i) at time i (as given in
equation (11.69)). If the difference between the estimated and the related
average ﬁlter coefﬁcient track is large (which mostly occurs due to the
presence of noise, near-end speech or both), then the weighting will be small.
On the other hand, when the difference is small the wk(i) will be close to one.
The weighting wk(i) and the step size β determine the adaptive step size. The
adaptive step size is close to β for changes that follow smoothly the evolution
of each ﬁlter coefﬁcient track, whilst being much less than β for changes that
are generally not related to the echo path change over time. The variance of
the weighting function wk(i), i.e. how fast it will decay from the unity value,
is controlled by the value γβ.
Note that the performance of this method depends on correct estimation
of the average value of each ﬁlter coefﬁcient track and therefore requires an
initialization period that is dependent on the application. This initial period is
essential in getting a reliable average ﬁlter coefﬁcient track and for the overall
system convergence.
The ITU-T recommendation G.165 deﬁnes the echo canceller performance
requirements using band-limited white noise (300–3400 Hz) test signals for

418
Speech Enhancement
Table 11.1
ITU-T recommendations and ANLMS system performance
results
ITU-T recommendation G.165
ANLMS
Tests
Input levels
Recommendation
Results
Steady state
−30 dBm0
−48 dBm0
−83 dBm0
residual echo level
−20 dBm0
−42 dBm0
−72 dBm0
−10 dBm0
−36 dBm0
−60 dBm0
Convergence
−30 dBm0
attenuation ≥27 dB
30 dB
−20 dBm0
attenuation ≥27 dB
30 dB
−10 dBm0
attenuation ≥27 dB
30 dB
Leak rate
−30 dBm0
(For all input levels,
(Echo level
(i.e. slow divergence
−20 dBm0
residual echo level
increase of 6 dB
when no signal)
−10 dBm0
should not increase
was evident for
more than 10 dB)
all input levels)
Inﬁnite return
loss convergence
−30 dBm0
≤−37 dBm0
−78 dBm0
(i.e. rapid return to
−20 dBm0
≤−37 dBm0
−68 dBm0
convergence after
−10 dBm0
≤−37 dBm0
−57 dBm0
an interrupt to
echo path)
far-end and near-end ports. A test is devised (see [22]) for each of the require-
ments in Table 11.1, listed with the results obtained for the various tests.
The requirements for echo canceller performance for double-talk situations
is subdivided into two tests. The ﬁrst is related to the double-talk detection
part of the echo canceller. As there is no such double-talk detector used in
the ANLMS system this test is not performed. The second part of the test is
aimed at ensuring that, in double-talk situations, the divergence is low. The
requirement for this part is that only a 10 dB increase in residual echo level
of the results listed in the steady state test (Test No. 1 of [22]) are permitted.
The ANLMS is well within this requirement.
Note that this does not mean that systems based on either LMS or NLMS
do not satisfy the ITU-T requirements. On the contrary, they do satisfy them,
but the advantages of ANLMS are the continued ﬁlter coefﬁcient adaptation
even during cross-talk scenarios and that there is no need for switching or
VADs, which results in more consistency.
In order to improve the overall system performance, a noise suppressor
and an echo canceller can be used jointly. The noise suppressor may be
integrated either prior to the echo canceller or after it. Integrating prior to
the echo canceller in order to remove the noise from the near-end signal

Echo Cancellation
419
Echo
Canceller
Noise
Suppressor
+
^
+
−
r(i)
Far End
s(i)
y(i)
z(i)
Near End
x(i)
B
x(i) + r(i) + n(i)
A
Figure 11.25
Block diagram of cascaded echo cancellation and noise suppression
usually distorts the echo signal in a nonlinear manner, which may make echo
cancellation more difﬁcult. By placing the noise suppressor after the echo
canceller, to remove the residual echo error as well as noise, may therefore be
more appropriate as shown in Figure 11.25.
The performance of this set-up has been tested both subjectively and
objectively. Subjective testing was carried out through informal listening
tests, while objective testing was conducted through various ﬁlter coefﬁcient
convergence behaviours. Two different echoes were used for this purpose.
The ﬁrst was a simple echo resulting from a single delay and attenuation of the
far-end speech signal and the second was the sum of three different delayed
and attenuated versions of the far-end speech. Each echo was mixed with the
near-end speech signal along with vehicle noise contamination resulting in
SNRs of 0, 5, 10, 15 and 20 dB.
Results obtained using the simple echo case are shown in Figures 11.26–
11.31. The echo was generated by delaying the far end speech by 40 samples
and attenuated through a factor of 0.48. Part (a) of Figures 11.26–11.31 shows
the input to the cascaded system and the corresponding output signals and
part (b) shows the convergence track of ﬁlter coefﬁcients h40 and h0. The
robustness of the system under noisy conditions and the convergence of the
ﬁlter coefﬁcients (h40 and h0), even in the presence of near-end speech, are
quite evident in Figures 11.26–11.31. Note that, as also highlighted above,
neither a near-end speech detector nor a switch for ﬁlter coefﬁcient adaptation
is needed. All that is needed is an initial training period for which the wk(i)
are set to one. In this setup, the initial period is 1 second for which the near-
end speech is assumed to be absent. The weighting function is switched on
after that and is responsible for convergence of the ﬁlter coefﬁcients during
near-end speech presence and silences in the near-end signal. Based on the
average track of each ﬁlter coefﬁcient (i.e. hk(i)) and the selection of the γβ
value in the wk(i) deﬁnition, only the step changes that follow the average

420
Speech Enhancement
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.26
Performance of the noise-echo canceller for clean speech
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (x105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.27
Performance of the noise-echo canceller at 20 dB SNR

Echo Cancellation
421
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.28
Performance of the noise-echo canceller at 15 dB SNR
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude  (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.29
Performance of the noise-echo canceller at 10 dB SNR

422
Speech Enhancement
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000 40000 60000 80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.30
Performance of the noise-echo canceller at 5 dB SNR
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
Figure 11.31
Performance of the noise-echo canceller at 0 dB SNR

Summary
423
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (x105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.32
Performance of noise-echo canceller for clean speech
track have a considerable effect on the ﬁlter coefﬁcient adaptation. Otherwise
the overall step size due to wk(i) (i.e. wk(i)β) will be small, therefore not
changing the previous ﬁlter coefﬁcient value by much and thus reducing the
likelihood of divergence.
A similar result was obtained in the second experiment when a more
complex echo was used. The echo used for this setup was generated through
the sum of three different delays: 20, 40 and 60 samples with corresponding
attenuation factors of 0.2, 0.48 and 0.35 respectively. Figures 11.32–11.37
show the results obtained for the second setup which proves the effectiveness
of the new adaptation algorithm proposed by Al-Naimi [24].
11.5 Summary
With advanced signal processing algorithms and techniques it is possible
to improve the quality of speech communications signiﬁcantly. Both echo
and noise cancellation/suppression algorithms have been reasonably well
developed to tackle high levels of echo and noise present in communication
systems. It is, of course, important to adapt the existing algorithms to
speciﬁc communication systems to maximize their performances. When both
acoustic noise and echo are present it is important to tune the overall
enhancement algorithms (noise suppressor and echo canceller) jointly to
maximize performance. Another important issue is the convergence time of

424
Speech Enhancement
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.33
Performance of the noise-echo canceller at 20 dB SNR
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.34
Performance of the noise-echo canceller at 15 dB SNR

Summary
425
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.35
Performance of the noise-echo canceller at 10 dB SNR
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (x105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.36
Performance of the noise-echo canceller at 5 dB SNR

426
Speech Enhancement
0
20000
40000
60000
80000
Time (samples)
−2
−1
0
1
2
Amplitude (× 105)
0
20000
40000
60000
80000 1 × 105
Time (samples)
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Filter Coefficient
h0
h20
h35
h85
h92
Figure 11.37
Performance of the noise-echo canceller at 0 dB SNR
the adaptive ﬁltering used in the enhancement algorithms. It is crucial that
the ﬁlters converge rapidly and do not diverge under any circumstances
(any level of acoustic noise and echo). Normalized LMS algorithms usually
provide adequate performance. The newly-proposed adaptive normalized
LMS algorithm [24], discussed in this chapter, has shown robust performance
under signiﬁcant levels of acoustic noise and echo.
Bibliography
[1] J. S. Lim and A. V. Oppenheim (1979) ‘Enhancement and bandwidth
compression of noisy speech’, in Proc. IEEE, 67(12):1568–1604.
[2] Y. Ephraim (1992) ‘Statistical-model-based speech enhancement sys-
tems’, in Proc. IEEE, 80(10):1526–55.
[3] J. R. Deller, H. G. Proakis, and J. H. L. Hansen, (1993) ‘Speech enhance-
ment’, in Discrete-Time Processing of Speech Signals, Chapter 8. New York:
Macmillan
[4] S. V. Vaseghi (2000) Advanced digital signal processing and noise reduction,
2nd edition. Chichester: John Wiley & Sons Ltd
[5] Y. Ephraim and H. L. Van Trees (1995) ‘A signal subspace approach
for speech enhancement’, in IEEE Trans. Speech and Audio Processing,
3(4):251–66.

Bibliography
427
[6] I. Y. Soon, S. N. Koh, and C. K. Yeo (1998) ‘Noisy speech enhancement
using discrete cosine transform’, in Speech Communications, 24(3):249–57.
[7] I. Y. Soon and S. N. Koh (2000) ‘Low distortion speech enhancement’, in
IEE Proc. on Vision, Image and Signal Processing, 147(3):247–53, June.
[8] J. D. Gibson, B. Koo, and S. D. Gray (1991) ‘Filtering of colored noise
for speech enhancement and coding’, in IEEE Trans. Signal Processing,
39:1732–42.
[9] Z. Goh, K. C. Tan, and B. T. G. Tan (1999) ‘Kalman-ﬁltering speech
enhancement method based on a voicedunvoiced speech model’, in
IEEE Trans. Speech and Audio Processing, 7(5):510–24.
[10] Y. Ephraim and D. Malah (1984) ‘Speech enhancement using a minimum
mean square error short-time spectral amplitude estimator’, in IEEE
Trans. on Acoust., Speech and Signal Processing, 32(6):1109–20.
[11] M. Berouti, R. Schwartz, and J. Makhoul (1979) ‘Enhancement of speech
corrupted by acoustic noise’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 208–11.
[12] N. Virag (1999) ‘Single channel speech enhancement based on masking
properties of the human auditory systems’, in IEEE Trans. Speech and
Audio Processing, 7(2):126–37.
[13] B. L. Sim, Y. C. Tong, J. S. Chang, and C. T. Tan (1998) ‘A parametric
formulation of the generalised spectral subtraction method’, in IEEE
Trans. Speech and Audio Processing, 6(4):328–37.
[14] R. J. McAulay and M. L. Malpass (1980) ‘Speech enhancement using
a soft-decision suppression ﬁlter’, in IEEE Trans. on Acoust., Speech and
Signal Processing, 28(2):137–45.
[15] Y. Ephraim and D. Malah (1985) ‘Speech enhancement using a minimum
mean square error log-spectral amplitude estimator’, in IEEE Trans. on
Acoust., Speech and Signal Processing, 33(2):443–5.
[16] O. Capp´e (1994) ‘Elimination of musical noise phenomenon with the
Ephraim and Malah noise suppression’, in IEEE Trans. Speech and Audio
Processing, 2(2):345–9.
[17] P. Scalart and J. V. Filho (1996) ‘Speech enhancement based on a priori
signal to noise estimation’, in Proc. of Int. Conf. on Acoust., Speech and
Signal Processing, pp. 629–31. Atlanta, GA, USA
[18] J. Sohn and W. Sung (May 1998) ‘A voice activity detection employing
soft decision based noise spectrum adaptation’, in icassp, Seattle, WA,
USA, 365–8.
[19] N. S. Kim and J. H. Chang (2000) ‘Spectral enhancement based on global
soft decision’, in IEEE Signal Processing Letters, 7(5):108–110.
[20] Y. D. Cho (2001) ‘Speech detection enhancement and compression for
voice communications’, Ph.D. thesis, CCSR, University of Surrey, UK.

428
Speech Enhancement
[21] Y. D. Cho, K. Al-Naimi, and A. Kondoz (2001) ‘Improved voice activity
detection based on a smoothed statistical likelihood ratio’, in Proc. of Int.
Conf. on Acoust., Speech and Signal Processing. Salt Lake City, UT
[22] ITU-T (1993) Echo cancellers, ITU-T Rec. G.165.
[23] D. G Messerschmitt (1984) ‘Echo cancellation in speech and data
transmission’, in IEEE Journal on Selected Areas in Communications,
2(2):283–303.
[24] K. T. Al-Naimi (2002) ‘Advanced speech processing and coding tech-
niques’, Ph.D. thesis, CCSR, University of Surrey, UK.
[25] D. Messerschmitt, D. Hedberg, C. Cole, A. Haoui, and P. Winship (1989)
‘Digital voice echo canceller with a TMS32020’, Application Report:
SPRA129, in Digital Signal Processing Solutions, p. 32. Texas Instruments

Index
µ-Law PCM
32–3
3.7 kb/s ACELPC coder
319
4 kb/s ACELP coder
332
6 kb/s ACELP coder
332–3
8 kb/s ACELP coder
333–4
4.15 kb/s IMBE coder
270–1
2.4 kb/s MELP coder
113
4 kb/s MELP/CELP coder
283, 285
1.2 kb/s SB-LPC coder
128
4 kb/s SB-LPC coder
128, 271–5, 319
4.8 kb/s STC coder
268–70
1-tap pitch ﬁlter
81–2
3-tap pitch ﬁlter
82
A
AaS, see analysis-and-synthesis
AbS-LPC coder
block diagram
201
CELP
212, 213, 214, 219–60
excitation signal
200, 206–8, 208–12
generally
199–200
MPLPC
215–17, 218
perceptually-based error
minimization procedure
200,
203–6
procedure
201–2
RPELPC
217–19
SELP
212–15
time-varying ﬁlter
200, 202–3
weighting ﬁlter
204–5
AC, see autocorrelation
ACELP mode
298
ACELP mode error
348, 350
ACELP transition quantization
332–4
acoustic noise, performance of hybrid
coder
336, 337–45
acoustic noise, robustness of SWPM
342
active noise suppression
2
adaptive codebook
52, 53
adaptive differential pulse code
modulation (ADPCM)
1, 2, 8, 12, 13
adaptive multi-rate (AMR) coder
13, 14
adaptive multi-rate (ETSI) coding
standard
360, 362–3, 364–8, 374–5
adaptive normalized least mean squared
(ANLMS) algorithm
416–18
adaptive post-ﬁltering
257–8
adaptive quantizer
33–6, 36–9
adaptive sequential LMS method
100–1
adaptive transversal ﬁlter in echo
cancellation
413–14
ADPCM, see adaptive differential pulse
code modulation
A-Law PCM
32–3
algebraic codebook excitation
247–51
algorithm
adaptive normalized least mean
squared
416–18
Burg
74
Durbin
69, 72
frequency-domain pitch
determination
155–8, 158–66, 177
Digital Speech: Coding for Low Bit Rate Communication Systems, Second Edition. A. M. Kondoz
© 2004 John Wiley & Sons, Ltd. ISBN 0-470-87007-9 (HB)

430
Index
algorithm (continued)
K-means
43–4
normalized least mean squared
416
pitch measurement
81
time-domain pitch determination
151–5, 158–66, 177
voice activity detector
11, 281, 311,
341, 357–75
aliasing distortion
25
all-pole digital ﬁlter
90
all-pole modelling
269
all-pole synthesis ﬁlter
205
Al-Naimi
131, 416
AMBE speech coding standard
15, 16
AMDF, see average magnitude
difference function
AMR, see adaptive multi-rate
analogue speech signal bandwidth
25
analogue telephony system
1
analysis-and-synthesis (AaS) coder
199
analysis-by-synthesis coder, see
AbS-LPC coder
ANLMS, see adaptive normalized least
mean squared
anti-alias ﬁlter
130–46
APC
78
Application Speciﬁc Integrated Circuit
(ASIC) chip
11
Atal
78, 200, 204, 206
Atal, Singhal and
235
Atkinson
299
autocorrelation PDA
152–5
autocorrelation method
68–70
autocorrelation pitch measurement
algorithm
81
average magnitude difference function
(AMDF) PDA
81, 151–2
average spectral distortion
107
B
background noise
10
backward adaptation
34, 35, 37
Bartlett window function
58–9, 60, 61
Bayes
373
Berouti
383
binary search codebook
46–8
binary voicing, see hard decision voicing
bit rate conﬂict with speech quality
9
bit rate reduction
xiii
bit rate, VAD and
359
Blackman window function
59, 60, 61
block quantization, see vector
quantization
Burg’s algorithm
74
burst error
336
C
capacity increase, VAD and
358, 359
cascaded codebook
48–9, 53
Cattermole
32
CCITT regulatory body
9
CCSR, see Centre for Communication
Systems Research
CDMA, see code division multiple access
CELP
8, 12
CELP coder
codebook excitation
240–54
excitation behaviour
212, 213, 214
excitation signal
208–12
generally
206, 207, 279
LPC prediction
221–2
multi-pulse excitation
230–2, 233
operation
219–21
pitch prediction
222–8
post-ﬁltering
257–60
SNR
228–30
see also 4 kb/s MELP/CELP
Centre for Communication Systems
Research (CCSR), University of
Surrey
xiii
centre-clipped codebook
241–2, 243
centre-clipping PDA
169–72
Cepstrum pitch measurement algorithm
81
channel dependent mode decision
9
channel error
10, 54, 130, 336, 345–50
Chebyshev Series method
100
Chen
258
Cho
371
Cholesky decomposition
71, 81, 234
closed-loop mode selection of voicing
311–12, 315–18

Index
431
closed-loop optimization
199, 200
closed-loop prediction
235–6
CNG, see comfort noise generator
CNI, see comfort noise insertion
co-channel interference, VAD and
357
code division multiple access (CDMA)
14, 358
code-excited linear prediction, see CELP
codebook
adaptive
52, 53
binary search
46–8
cascaded
48–9, 53
centre-clipped
241–2, 243
comparison of codebooks
117–21
design
40, 43–4
design of simultaneous joint
116
full search
44–6
gain-shape
50–2
generally
40–2
multi-stage vector quantization
110,
113–16
optimization
43–6
overlapping
241, 242, 243
robustness
53–4
split vector quantization
49–50, 110,
111–12
testing
53
training
52, 116, 246
training database size
117
see also vector quantization
codebook excitation
algebraic
247–51
CELP coder
240–54
Gaussian
241–3
generally
206–7, 222
LTP and
255–7
pitch adaptive mixed
251–4
vector sum
243–7
codebook vector
40
coder
AbS-LPC
199–200, 200–2
analysis-and-synthesis
199–200
CELP
206, 207, 279
combined low bit-rate
282–3
enhanced variable rate
286
harmonic
277–9
hybrid
8–9, 280–5
hybrid encoder
298–311
improved multi-band excitation
268, 270–1
4.15 kb/s INMARSAT-M
270–1
low bit-rate
2
MBE
189
MELP
189
4 kb/s MELP/CELP
283, 285
MPLPC
230–7
multi-band excitation
264–5, 277
parametric
6, 7–8
prototype waveform interpolation
282
4.8 kb/s SB-LPC
128, 271–5
selection of
15–16
sinusoidal transform
261–75,
268–70
speech-speciﬁc
2
split-band LPC
261, 268, 271–5
4.8 kb/s STC
268–70
variable rate
9
waveform approximating
6
see also CELP coder, hybrid coder
coding harmonic speech
261–75
coding delay
10
comb ﬁlter
156
combined low bit-rate coder
282–3
comfort noise generator (CNG)
357
comfort noise insertion (CNI)
357
companded quantizer
32–3
complex root method
95
compression
µ-Law
32
A-Law
32–3
compression of signal
1–2
covariance method
70–1
Cox
90
D
DFT, see discrete Fourier transform
DFT–LSF method
99
differential quantizer (DQ)
36–9,
122–4
differential vector quantization
52
digital signal processor (DSP) chip
xiii,
11

432
Index
digital speech interpolation (DSI)
11
digitally-encoded speech, advantages of
1
direct expansion method
101–2
direct similarity measure
153
discrete cosine transform (DCT)
380
discrete Fourier transform (DFT)
380
distortion measure
42–3, 106
DoD speech coding standard
14–15
DQ, see differential quantizer
DQ predictor
122–4
DSI, see digital speech interpolation
DSP chip, see Digital Signal Processor
chip
Durbin’s algorithm
69, 72
E
echo cancellation
adaptive transversal ﬁlter
413–14
digital echo canceller
411–13
duplex connection
411
G.165 (ITU-T)
413, 417–18
generally
406–11
near-end speech detection
413, 415
performance
415–23
residual error suppression
413, 415
transversal ﬁlter
412
echo canceller with noise suppressor
418–9
EFR, see ETSI GSM enhanced full rate
EFR coder
251
EFR weighting method
109, 110, 120,
121
encoder, hybrid
298–311
enhanced variable rate coder (EVRC)
286
enhancing speech
379–426
environmental variability in signal
53–4
Ephraim and Malah
369, 381–387
European Telecommunications
Standards Institute (ETSI)
GSM enhanced full rate (EFR) speech
coding standard
13, 14, 360,
361–2, 364–8
GSM full rate (FR) speech coding
standard
13, 14, 360, 361–2,
364–8
GSM half rate (HR) speech coding
standard
13, 14, 360, 361–2,
364–8
speech coding standard
10, 13–14
UMTS speech coding standard
360
excitation of white noise
309–11
excitation signal, determination of
optimum
208–12
excitation signal in AbS-LPC coder
200, 206–8
F
FEC, see forward error correction
ﬁlter, ﬁnite impulse response
283
ﬁlter, ﬁnite length
226–8
ﬁlter memory
203
ﬁnite impulse response (FIR) ﬁlter
283
ﬁnite length ﬁlter
226–8
forward adaptation
33, 34, 37
forward error correction (FEC)
10
fractional-delay LTP
225–6
frame energy of speech
185–6
frame-to-frame interpolation
90
frequency-domain analysis
57–8
frequency-domain pitch determination
155–8, 158–66, 177–8
frequency-domain voicing
263
frequency response of LPC ﬁlter
77
frequency, sampling
25
FS-1015 speech coding standard
14, 15
FS-1016 speech coding standard
14, 15
full search codebook
44–6
G
G.165 (ITU-T) speech coding standard
413, 417–18
G.711 (ITU-T) speech coding standard
12, 13
G.721 (ITU-T) speech coding standard
12, 13
G.722 (ITU-T) speech coding standard
13
G.722.1 (ITU-T) speech coding standard
13

Index
433
G.723.1 (ITU-T) speech coding standard
12, 13
G.723.1 Annex A (ITU-T) speech coding
standard
360, 361, 364–8
G.723.1 coder
251
G.726 (ITU-T) speech coding standard
12, 13
G.728 (ITU-T) speech coding standard
12, 13
G.729 (ITU-T) speech coding standard
12, 13
G.729 Annex B (ITU-T) speech coding
standard
360, 361, 364–8, 374–5
G.729 coder
251
gain in SWPM
323
gain-shape codebook
50–2
Gaussian codebook excitation
241–3
generalized cubic phase interpolation
297–8
Gibson
380
Gram–Schmidt orthogonalization
process
247
Grifﬁn
299
group delay weighting method
109–10, 121
GSM, see ETSI speech coding standard
GSM enhanced full rate (EFR) ETSI
speech coding standard
13, 14, 360,
361–2, 364–8
GSM full rate (FR) ETSI speech coding
standard
13, 14, 360, 361–2, 364–8
GSM half rate (HR) ETSI speech coding
standard
13, 14, 360, 361–2, 364–8
GSS, see spectral subtraction, generalized
H
Hamming window
59, 60–5, 165, 190,
262
Hard-decision noise adaptation
402–3
hard-decision voicing
150, 178–89
harmonic amplitude
estimation
266–8
generally
272, 299–301
quantization in AbS
327–9
in SWPM
323
harmonic band
194
harmonic coder
8
see also coder
harmonic excitation mode
298
harmonic excitation quantization
amplitude
327–9
gain
329–30
onset parameter
330–1
pitch
324, 325, 326
pitch pulse location
325–7
pitch pulse shape
327
transition detection
324–5
harmonic excitation, synchronized
299–301
harmonic gain quantization in AbS
329–30
harmonic memory initialization at onset
308–9
harmonic mode error
346–7, 348
harmonic peak detection PDA
156
harmonic speech coding
261–75
harmonic voicing
264–6
hierarchical clustering, see binary search
Hilbert envelope
286
human speech emulation
7
hybrid coder
ACELP transition excitation
quantization
331
burst error
336
decoder
319–20
design
280–1
encoder
298–311
generally
6–7, 8–9
harmonic excitation quantization
323–31
limitations
284–5
LPC vocoder
281
performance
320–2
performance under acoustic noise
336, 337–45
performance under channel errors
336, 345–50
quantization issues
322–31
random channel error
336
speech classiﬁcation
311–19
speech quality
334–5
transition detection
315–18

434
Index
hybrid coder (continued)
unvoiced excitation quantization
323, 324
voicing
281
hybrid mode selection of voicing
311–12
I
improved multi-band excitation (IMBE)
coder
268, 270–1
improved multi-band excitation (IMBE)
speech coding standard
15, 16
impulse train generator
65
INMARSAT speech coding standard
15, 16
INMARSAT-M coder
270–1
inter-frame correlation
121–30
interpolation
generalized cubic phase
297–8
generally
281
linear
262
ﬁlter
227
technique overlap and add
262
inverse ﬁltering (LPC)
76, 77
inverse sine function
88, 90, 92
IS-54 (TIA/EIA) speech coding standard
14, 15
IS-96 (TIA/EIA) speech coding standard
360, 363–4, 364–8
IS-127 (TIA/EIA) speech coding
standard
360, 363–4, 364–8
IS-641-A (TIA/EIA) speech coding
standard
14, 15
IS-733 (TIA/EIA) speech coding
standard
360, 363–4, 364–8
Itakura
90
Itakura–Saito distortion
389
iterative sequential optimization
79–80, 116
ITU regulatory body
G.165 speech coding standard
413,
417–18
G.711 speech coding standard
12, 13
G.721 speech coding standard
12, 13
G.722 speech coding standard
13
G.722.1 speech coding standard
13
G.723.1 Annex A speech coding
standard
360, 361, 364–8
G.723.1 coder
251
G.723.1 speech coding standard
12,
13
G.726 speech coding standard
12, 13
G.728 speech coding standard
12, 13
G.729 speech coding standard
12, 13
G.729 Annex B speech coding
standard
360, 361, 364–8, 374–5
G.729 coder
251
generally
9, 12–13
P.862 measure of quality
18
J
Jayant (one word memory) quantizer
34–6
JQ-MSVQ quantizer
128–31
K
Kaiser window function
59, 60, 61
Kalman ﬁlter
380
Karhunen–Lo`eve transform (KLT)
380
Katugampala
285
Kleijn
282
K-means algorithm
43–4, 45–6
Kroon
227
L
lag, pitch
78, 81, 83, 175–7
LAR function, see log-area ratio function
lattice method
72–4
least mean square
67
likelihood ratio
368–75
line spectral frequency (LSF)
derivation
94–5
distribution plot
97
generally
87, 90
properties
103–5
line spectral pair (LSP)
87, 90
linear interpolation
262
linear prediction ﬁlter coefﬁcient
7
linear prediction vocoder
7
linear predictive analysis, see LPC
analysis

Index
435
log spectral distortion, see spectral
distortion
log-area ratio (LAR) function
88, 90, 91
logarithmic scalar quantizer
32–3
Log-PCM system
1, 2
long term prediction
202, 203
long term predictor in CELP
222–8
low bit-rate coder
2
low-band to full-band energy of speech
184
low-pass ﬁltering
134–46
LP ﬁlter coefﬁcient
7
LPC analysis
autocorrelation method
68–70
covariance method
70–1
generally
65–77, 90
lattice method
72–4
least mean square approach
67
maximum likelihood approach
67
in other ﬁelds
67
performance
74–7
LPC difference equation
66
LPC ﬁlter
87–90
LPC frequency response
77
LPC inverse ﬁltering
76, 77
LPC mode
298
LPC predictor
202–3, 221–2
LPC quantization process
87, 90, 94–5,
97
LPC residual
272
LPC spectral envelope
77
LPC synthesis
93–4, 102–3, 104
LPC transformation to LSF
adaptive sequential LMS method
100–1
Chebyshev Series method
100
complex root method
95
generally
90–101
ratio ﬁlter method
98–100
real root method
95–6
LSF estimation
130–46
LSF inverse distance weighting method
109, 110, 121
LSF prediction
122–4
LSF quantization process
105–10,
121–30, 128–30
LSF quantizer
107, 110–16
LSF transformation to LPC
101–2,
102–3, 104
LSF, see line spectral frequency
LSP, see line spectral pair
LTP analysis
228–30
LTP and codebook excitation
255–7
M
MA-MSVQ quantizer
129–31
Markov
380
Max quantizer
30–2
maximally smooth criterion
297
maximum likelihood
67
maximum likelihood pitch
measurement algorithm
81
maximum likelihood STSA estimation
380, 384, 389–92, 398
MBE mixed voicing
190–3
MBE-based coder
189
M-best tree search
115–16
McAulay
261, 262, 265, 297, 299, 384
mean opinion score (MOS) scale
17
mean square error distortion measure
42
mean square error measurement
106,
107–10
MELP coder
189
see also 4 kb/s MELP/CELP coder
memory, ﬁlter
203
meta-frame
128
minimum mean square error STSA
(MMSE- STSA) estimation
380,
386–7, 389–92, 400, 401
MIRS, see modiﬁed intermediate
response system
mixed decision noise adaptation
403–4
mode decision
9
mode error
345–50
modiﬁed intermediate response system
(MIRS)
164
moving average (MA) predictor
generally
123–4
joint quantization and
129–31
low-pass ﬁltering and
135–46
optimal order
124–5
performance
126–8

436
Index
moving average (MA) predictor
(continued)
prediction factor
125–6
training
125–6
MPLPC coder
amplitude, optimum excitation
232–5
excitation behaviour
215–17, 218
generally
230–2, 233
pitch prediction
235–7
pulse location
216, 217
MSVQ, see multi-stage vector
quantization
MSVQ quantizer
125–6
multi-band excitation (MBE) coder
149, 264–5, 277
multi-band voicing
264
Multimedia Communications Research
Group of CCSR
xiii
multimode coder, see hybrid coder
multi-pulse excitation
207–8, 230–2,
233
multipulse LPC (MPLPC)
200, 207
multi-stage vector quantization
codebook training
116
comparison with SVQ
117–21
generally
111
M-best tree search
115–16
2.4 kb/s MELP coder
113
performance
117–21
search strategy
114–16
N
narrowband speech coding standard
12–13
NATO speech coding standard
15
near-end speech detection in echo
cancellation
413, 415
network dependent mode decision
9
neural network
283
Nguyen
153
noise adaptation
hard decision
402–3
mixed decision
403–4
performance comparison of methods
404–6, 407, 408, 409, 410
soft decision
402, 403
voice activity detector
402
noise suppression
2
noise suppression rules
369
noise suppressor with echo canceller
418–9
noisy speech
189
non-uniform scalar quantizer
29–30
normalized least mean squared (NLMS)
algorithm
416
Nyquist criterion
xiii, 24, 25, 131
O
offset target modiﬁcation in SWPM
304–8
one word memory adaptation
34–6
one-shot optimization
79, 80–1
onset harmonic memory initialization
308–9
onset harmonic parameter quantization
in AbS
330–1
open-loop mode selection of voicing
311–14
open-loop search scheme
51
optimization closed-loop
199, 200
optimization of codebook
43–6, 116
optimization of pitch predictor
79–81
optimum scalar quantizer
29–32
ordering of LSF parameters
100, 103–5
outlier
107
see also performance
overlapping codebook
241, 242, 243
P
P.862 measure of quality
18
packet loss, VAD and
359
Paliwal–Atal weighting method
108,
110, 121
PAME, see pitch adaptive mixed
excitation
pan-European digital mobile radio
system, see GSM
Panter and Dite
30
parametric coder
6, 7–8
partial correlation (PARCOR) coefﬁcient
73, 87–90, 93–4

Index
437
pattern-matching quantization, see
vector quantization
PCM, see pulse code modulation
PDA, see pitch determination algorithm
peak detector
177
peakiness of speech
179–80
peak-picking of the magnitude spctrum
266–7
perceptual evaluation of speech quality
18
perceptually-based error minimization
procedure in AbS-LPC coder
200,
203–6
perceptually-determined distortion
measure
42–3
performance of LTP analysis methods
228–30
performance of echo canceller
415–23
performance of hybrid coder
320–2
performance of JQ-MSVQ quantizer
129
performance of low-pass ﬁltering
142–6
performance of LPC analysis
74–7
performance of MA-MSVQ quantizer
129, 130
performance of moving average
predictor
126–8
performance of multi-stage vector
quantization
117–21, 125–6
performance of noise adaptation
methods
404–6, 407, 408, 409, 410
performance of pitch determination
algorithms
164–6, 167, 168
performance of pitch tracking process
175
performance of speech coding standards
15–18
performance of speech enhancement
methods
389–402
performance of voice activity detector
(VAD)
364–8
periodicity in speech signal
77–8,
178–9
phase synchronization
281
pitch adaptive mixed excitation (PAME)
251–4
pitch determination
149, 150–78, 263
pitch determination algorithm (PDA)
autocorrelation
152–5
average magnitude difference
(AMDF)
151–2
centre-clipping
169–72
generally
149
harmonic peak detection
156
peak detector
177
performance comparison
164–6,
167, 168
spectral autocorrelation
158–62,
163–4
spectral synthesis
163–4
spectrum similarity
156–8
pitch determination preprocessing
166–77
pitch error
165, 177–8
pitch estimation, see pitch determination
pitch ﬁlter
81–2
pitch gain, optimum
153, 154
pitch lag
78, 81, 83, 175–7
pitch measurement algorithm
81
pitch period
generally
149, 150–1, 165
LP ﬁlter coefﬁcient
7
SWPM
323
pitch prediction
235–7
see also long term prediction
pitch predictor
77–83
see also long term predictor
pitch pulse location in AbS
325–7
pitch pulse location in SWPM
286–91,
302–4, 323
pitch pulse shape in AbS
327
pitch pulse shape in SWPM
292–7,
302–4, 323
pitch quantization in AbS
324, 325, 326
pitch smoothing
172–7
pitch tracking
172–7
pitch–LPC formulation model
79
plosive detection
318–19
polyphase structure
227
post-ﬁltering in a CELP coder
257–60

438
Index
power spectrum
99
power-saving, VAD and
358
PPL, see pitch pulse location
PPS, see pitch pulse shape
prediction gain
78, 80, 124–5, 140–1
prediction of LPC parameters in CELP
221–2
prediction of pitch in CELP
222–8
predictive vector quantization
52
predictor codebook
52
pre-emphasis of the signal
75
pre-emphasized energy of speech
183
probability density
33
prototype waveform interpolation (PWI)
coder
282
public switched telephone network
(PSTN)
5, 9, 10
pulse amplitude coding
237–8
pulse amplitude quantization, joint
238–40
pulse code modulation (PCM)
xiii, 5,
32–3
pulse excitation
202
pulse location
211–12, 216, 217, 248
pulse position coding
237
Q
quality measurements
16
quantization
23, 238–40
see also types of quantization:
differential vector, LSF, multi-stage
vector, predictive vector, scalar,
split vector, vector
quantization error
27–8, 29, 106
quantization issues of hybrid coder
322–31
quantization noise
7
quantization process
scalar
26–39, 106
vector
39–50, 106
see also LPC quantization process, LSF
quantization process
quantizer
adaptive
33–9
companded
32–3
differential
36–9
Jayant
34–6
JQ-MSVQ
128–31
logarithmic scalar
32–3
LPC
87, 90, 94–5, 97
LSF
107, 110–16
Max
30–2
see also scalar quantizer
quantizer input/output
31
quantizer step size
26
R
Rabiner
169, 172
random channel error
336
random noise excitation
202
random noise generator
65
ratio ﬁlter method
95–6, 98–100
real-time coder
108
real-time system
74
rectangular window function
58–9,
60–5, 165
Reeves
5
reference template
40
regular pulse excitation
207–8
regulatory body
9
residual error suppression in echo
cancellation
413, 415
rms energy
317, 318, 333
robustness
10
RPELPC coder
217–19
S
sampling
23–5
satellite telephony
15
SB-LPC, see split-band LPC
scalar quantization process
26–39, 106
scalar quantizer
54
scalar quantizer, logarithmic
32–3
scalar quantizer, non-uniform
29–30
scalar quantizer, optimum
29–32
scalar quantizer, uniform
26–9
secure communication
14–15
segmental SNR
389
self-excitation
207
SELP
207
SELP coder
212–15
sequential optimization
116

Index
439
Shlomot
282
short term predictor, see LPC analysis
short-time spectral analysis
57–65
signal compression
1–2
signal power LP ﬁlter coefﬁcient
7
signal processing
1–2
signal reconstruction
5
signal to noise ratio (SNR)
CELP coder
228–30
generally
7
RPELPC coder
218–19
segmental
389
signal variability
53–4
simultaneous joint codebook design
116
Singhal and Atal
235
sinusoidal analysis
262–3
sinusoidal coder
8, 261–75
sinusoidal model voicing
265–6
sinusoidal speech coder
149, 150, 156
sinusoidal speech-model matching
299
sinusoidal transform coder (STC)
149,
261–75
smoothed likelihood ratio (SLR)
371–2, 373, 374–5
SNR, see signal to noise ratio
soft-decision noise adaptation
402, 403
soft-decision voicing
150, 189–96
Sohn
368
Sondhi
169, 172
source dependent mode decision
9
source-ﬁlter model
65–7
speaker variability in signal
53
spectral analysis, short-time
57–65
spectral autocorrelation PDA
158–62,
163–4
spectral correlation
267–8
spectral distortion
106–7, 131–4
spectral envelope
77, 149, 202
spectral subtraction
380, 382–4,
389–92, 396, 397
spectral synthesis method
150
spectral synthesis PDA
163–4
spectral tilt of speech
182, 266
spectrum ﬂattening
166–72
spectrum similarity PDA
156–8
speech characteristic
frame energy
185–6
low-band to full-band energy
184
peakiness
179–80
periodic similarity
178–9
pre-emphasized energy
183
spectrum tilt
182
weighting
186–7
zero crossing rate
180–1
speech classiﬁcation in a hybrid coder
311–19
speech coder, see coder
speech coding standard
DoD
14–15
ETSI
13–14
INMARSAT
15, 16
ITU-T
12–13
NATO
15
performance
15–18
TIA/EIA
14, 15
speech enhancement
adaptive ﬁltering
380
discrete cosine transform (DCT)
380
discrete Fourier transform (DFT)
380
echo cancellation
406–23, 424–6
generally
379–80
guidelines
402
Kalman ﬁlter
380
Karhunen–Lo`eve transform (KLT)
380
maximum likelihood STSA
estimation
380, 389–92, 398
minimum mean square error STSA
estimation
380, 389–92, 400, 401
model-based
380
noise adaptation
402–6
performance comparison of methods
389–402
short-time spectral amplitude
381–402
spectral subtraction
389–92, 396, 397
transform domain
380
uncertainty of speech presence
387–9, 401

440
Index
speech enhancement (continued)
wavelet transform
380
Wiener ﬁltering
389–92, 399
speech presence, uncertainty of
387–9,
401
speech quality
xiii, 9, 10, 334 – 5
speech signal
average spectral distortion
107, 121
LPC analysis
65–77
outlier
107, 121
periodicity
77–8
requirements for good quality
107
spectral analysis
57–65
transition region
57
unvoiced
57, 58, 65
voiced
57, 58, 65
speech stationarity assumption
131
split-band LPC (SB-LPC) coder
128,
261, 268, 271–5, 342
split vector codebook
49–50
split vector quantization
111–12,
117–21
split-band mixed voicing
193–6
Stachurski
283
STANAG speech coding standard
15
statistical multiplexing, VAD and
359
STC, see sinusoidal transform coder
STP, see short term predictor
STSA, see short-time spectral analysis
STSA estimation
380, 389–92, 398, 400,
401
Sundberg
174
SWPM, see synchronized
waveform-matched phase model
synchronized harmonic excitation
299–301
synchronized waveform-matched phase
model (SWPM)
advantages
301–4
generally
285–98
offset target modiﬁcation
304–8
robustness to acoustic noise
342
T
tandem connection
11
telephony system, analogue
1
threshold function, voicing
191–2, 196
TIA regulatory body
enhanced variable rate coder (EVRC)
286
IS-54 speech coding standard
14, 15
IS-96 speech coding standard
360,
363–4, 364–8
IS-127 speech coding standard
360,
363–4, 364–8
IS-641-A speech coding standard
14,
15
IS-733 speech coding standard
360,
363–4, 364–8
time division multiple access (TDMA)
14
time-domain pitch determination
151–5, 158–66, 177
time-varying codebook
52
time-varying ﬁlter in AbS-LPC coder
200, 202–3
Toeplitz matrix
69, 249
training a codebook
52, 116, 246
training a moving average predictor
125–6
Trancoso
282
transcoding
11
transition
298
transition detection
315–18
transition quantization in ACELP
331,
332–4
transition region
150
transmission channel errors
54
tree search, M-best
115–16
tree search codebook, see binary search
codebook
U
UMTS (ETSI) speech coding standard
360
uncertainty of speech presence
387–9,
401
uniform scalar quantizer
26–9, 33
unvoiced excitation
202
unvoiced speech signal
57, 58, 65, 150,
298
up-sampling
226–8

Index
441
V
VAD, see voice activity detector
variable bit-rate coding
331–5
variable rate coder
9
vector quantization
harmonic amplitude
272
multi-stage
111, 113–21
split
111–12
see also codebook
vector quantization process
39–50, 106
vector quantizer
54
vector sum codebook excitation
243–7
Villette
299
vocoder
6–7, 149, 150, 202
voice activity detector (VAD)
beneﬁts
357–9
ETSI speech coding standards
360,
361–2, 362–3, 364–8, 374–5
hard decision noise adaptation
402
ITU-T speech coding standards
360,
361, 364–8,
374–5
likelihood ratio
368–75
performance
364–8
TIA/EIA speech coding standards
360, 363–4, 364–8
voicing decision
359
voice activity detector (VAD) algorithm
11, 281, 311, 341, 357
voiceband data handling
11–12
voiced excitation
202
voiced speech signal
57, 58, 65, 150, 298
voiced–unvoiced classiﬁcation
149
voicing
frequency-domain
263
generally
281
harmonic
264–6
multi-band excitation
264
sinusoidal model
265–6
threshold function
191–2, 196
W
waveform coder
6–7, 8
waveform, equation for sampled
23
wavelet transform
380
weighted mean square error
measurement
106
weighted mean square error distortion
measure
42
weighting ﬁlter of AbS-LPC coder
204–5
weighting method
EFR
109, 110, 120, 121
group delay
109–10, 121
LSF inverse distance
109–10, 121
Paliwal–Atal
108, 110, 121
performance
119–21
white noise excitation
309–11
white noise excitation mode error
346,
347
wideband speech coding standard
13
wide-sense stationary assumption
133
Wiener ﬁltering
380, 385–6, 389–92,
399
window length
81
window function
Bartlett
58–9, 60, 61
Blackman
59, 60, 61
generally
58–65, 75
Hamming
59, 60–5, 165, 190, 262
Kaiser
59, 60, 61
rectangular
58–9, 60–5, 165
window position test
132
Z
zero-crossing rate of speech
180–1, 313


FUNDAMENTALS OF 
COMPUTER 
ALGORITHMS 
COMPUTER SCIENCE PRESS 

FUNDAMENTALS OF 
COMPUTER 
ALGORITHMS 

COMPUTER SOFTWARE ENGINEERING SERIES 
ELLIS HOROWITZ, EDITOR 
University of Southern California 
WAYNE AMSBURY 
Structured BASIC and Beyond 
JEAN-LOUP BAER 
Computer Systems Architecture 
PETER CALINGAERT 
Assemblers, Compilers, and Program Translation 
M. S. CARBERRY, H. M. KHALIL, J. F. LEATHRUM, J. S. LEVY 
Foundations of Computer Science 
SHIMON EVEN 
Graph Algorithms 
W. FINDLAY and D. A. WATT 
Pascal: An Introduction to Methodical Programming, Second Edition 
ELLIS HOROWITZ and SART AJ SAHNI 
Fundamentals of Computer Algorithms 
ELLIS HOROWITZ and SARTAJ SAHNI 
Fundamentals of Data Structures 
ELLIS HOROWITZ and SARTAJ SAHNI 
Fundamentals of Data Structures in Pascal 
ELLIS HOROWITZ 
Fundamentals of Programming Languages, Second Edition 
ELLIS HOROWITZ, EDITOR 
Programming Languages: A Grand Tour 
TOM LOGSDON 
Computers and Social Controversy 
IRA POHL and ALAN SHAW 
The Nature of Computation: An Introduction to Computer Science 
DONALD D. SPENCER 
Computers in Number Theory 
JEFFREY D. ULLMAN 
Principles of Database Systems, Second Edition 
~ ~--------

FUNDAMENTALS OF 
COMPUTER 
ALGORITHMS 
ELLIS HOROWITZ 
University of Southern California 
SARTAJ SAHNI 
University of Minnesota 
COMPUTER SCIENCE PRESS 

Copyright © 1978 Computer Science Press, Inc. 
Printed in the United States of America 
All rights reserved. No part of this work may be reproduced, transmitted, 
or stored in any form or by any means, without the prior written consent of 
the Publisher. 
Computer Science Press 
1803 Research Blvd. 
Rockville, Maryland 20850 
IO 11 12 Printing 
Year 89 88 87 86 
This book represents a joint effort of both authors who fully col· 
laborated on all chapters. The names of the authors of this text 
have been listed in alphabetical order. This does not imply any 
senior-junior relationship. Dr. Sartaj Sahni was the primary author 
of Chapters 4, 5, 6, 8, 11, and 12 and Dr. Ellis Horowitz was the 
primary author of Chapters 1, 2, 9, and JO. 
Library of Congress Cataloging in Publication Data 
Horowitz, Ellis. 
Fundamentals of computer algorithms. 
(Computer software engineering series) 
Includes index. 
1. 
Electronic digital computers- -Programming. 
2. 
Algorithms 
I. Sahni, Sartaj, joint author. 
II. 
Title. 
QA76.6.H67 
519.4 
78·14735 
ISBN 0-914894-22-6 

dedicated to our wives 
Neeta Sahni 
Maryanne Horowitz 

PREFACE 
If we try to identify those contributions of computer science which will be 
long lasting, surely one of these will be the refinement of the concept called 
algorithm. Ever since man invented the idea of a machine which could per-
form basic mathematical operations, the study of what can be computed 
and how it can be done well was launched. This study, inspired by the 
computer, has led to the discovery of many important and clever algorithms. 
The discipline called computer science has embraced the study of algo-
rithms as its own. It is the purpose of this book to organize what is known 
about them in a coherent fashion so that students and practitioners can 
learn to devise and analyze new algorithms for themselves. 
But a book which contains every algorithm ever invented would be ex-
ceedingly large, and traditionally algorithms books have proceeded by ex-
amining only a small number of problem areas in depth. For each specific 
problem the most efficient algorithm for its solution is usually presented 
and analyzed. Having taught courses in this way for several years we were 
well aware that this approach has one major flaw. Though the student sees 
many fast algorithms and may master the tools of analysis, he remains un-
confident about how to devise good algorithms in the first place. 
The missing ingredient is a lack of emphasis on design techniques. A 
knowledge of design will certainly help one to create good algorithms, yet 
without the tools of analysis there is no way to determine the quality of 
the result. This observation that design should be taught on a par with 
analysis led us to a more promising line of approach: namely to organize 
our courses, and subsequently this book, around some fundamental strat-
egies of algorithm design. The number of basic design strategies is reason-
ably small. Moreover all of the algorithms one would typically wish to study 
can easily be fit into these categories; for example, mergesort and quick-
sort are perfect examples of the divide-and-conquer strategy while Kruskal's 
minimum spanning tree algorithm and Dijkstra's single source shortest 
path algorithm are straightforward examples of the greedy strategy. An 
understanding of these strategies is an essential first step towards acquiring 
the skills of design. 
vii 

viii 
Preface 
Though we both strongly feel that the emphasis on design as well as 
analysis is the appropriate way to organize the study of algorithms, a cau-
tionary remark is in order. First, we have not included every known design 
principle. One example is linear programming which is one of the most 
successful techniques, but is often discussed in a course of its own. Secondly, 
the student should be inhibited from taking a cookbook approach to algo-
rithm design by assuming that each algorithm must derive from only a 
single technique. This is not so. 
A major portion of this book, chapters 3 through 9, deal with the dif-
ferent design strategies. First each strategy is described in general terms. 
Typically a "program abstraction" is given which outlines the form that 
the computation will take if this strategy can be applied. Following this 
there are a succession of examples which reveal the intricacies and varieties 
of the general strategy. The examples are somewhat loosely ordered in terms 
of increasing complexity. The type of complexity may arise in several ways. 
Usually we begin with a problem which is very simple to understand and 
requires no data structures other than a one-dimensional array. For this 
problem it is usually obvious that the given design strategy yields a correct 
solution. Later examples may require a proof that an algorithm based on 
this design technique does work. Or, the later algorithms may require more 
sophisticated data structures (e.g. trees or graphs) and their analyses may 
be more complex. The major goal of this organization is to emphasize the 
arts of synthesis and analysis of algorithms. Auxiliary goals are to expose 
the student to good program structure and to proofs of algorithm correct-
ness. 
One of the most energetic areas of computer science research today is 
called computational complexity. That name denotes the study of what 
makes functions intrinsically difficult to compute. Two products of com-
putational complexity have been the development of algorithms with the 
lowest asymptotic computing time and facts concerning the minimum num-
ber of operations required to compute a given function. Many of these re-
sults can be found here. However our decision in writing this book was to 
emphasize algorithms which were not only of theoretical interest but which 
are practical to use. Unfortunately many of the "best" algorithms, from 
an asymptotic point of view, are quite hard to program and require such 
a great amount of overhead that their practical value is limited. We have 
avoided lengthy presentations of such algorithms and contented ourselves 
with pointing to the available literature. 
The algorithms presented here are written in SP ARKS, the name we 
have given to our ALGOL/PASCAL-like language which we first introduced 
in Fundamentals of Data Structures. The syntax of some statements has 

Preface 
ix 
been improved, but the changes are such that the meanings of all state-
ments is still immediately discernible. Chapter one presents the precise 
semantics of each statement via flowcharts and gives some drill in the art 
of program structuring. We hope that by studying well-written programs 
the student will apply these same principles to his or her own program 
composition. Another important aspect of this book is program testing. 
Though computer science still lacks an adequate formal treatment of this 
subject, for some algorithms we show how to devise a range of data sets 
which can be used for debugging and performance measurement. Also we 
have felt obliged to provide programs which are essentially complete in all 
details. Though this may complicate the presentation of the algorithms, it 
has as its virtue the fact that each algorithm can be quickly programmed 
and executed. Of course, subroutines are used to improve clarity. 
The material in this book does not correspond to any existing course 
within ACM's recommended Curriculum '68. However it does seem likely 
that the IEEE Computer Society will include an algorithms course within 
its new recommendations. As the course structure of many computer science 
programs is now firmly established, it has become harder to introduce new 
courses. Nevertheless we are confident that these subjects are of sufficient 
merit that many computer science educators will attempt to cover this ma-
terial. Thus we offer the arguments we used to get our own departments 
to adopt a course on The Design and Analysis of Algorithms. First and 
foremost, we argued that "algorithm" is a fundamental concept of com-
puter science and hence there should be a course devoted to its study. Sec-
ondly the skills of algorithm synthesis and analysis will improve both the 
students basic knowledge and his or her ability to comprehend more sophis-
ticated algorithms in later courses. Finally there now exists some important 
theoretical results (e.g. NP-Completeness which is discussed in Chapter 11) 
which deserve to be covered. 
We view the material presented here as ideal for a one semester or two 
quarter course given to juniors, seniors or graduate students. It does re-
quire prior experience with programming in a higher level language but 
everything else is self-contained. Practically speaking, it seems that a course 
on data structures is helpful, if only for the fact that the students have 
greater programming maturity. For a school on the quarter system, the 
first quarter might cover the basic design techniques as given in chapters 3 
through 8: divide-and-conquer, the greedy method, dynamic programming, 
search and traversal, backtracking, and branch-and-bound. The second 
quarter would cover the more theoretical subjects of chapters 10 through 
12: lower bound theory, NP-Completeness and approximation methods. 
For a semester schedule where the student has already encountered data 

x 
Preface 
structures and 0-notation, chapters 3 through 11 is about the right amount 
of material. This includes the major design strategies as mentioned above 
plus the fast Fourier transform, lower bound theory, and the chapter on 
NP-Complete problems. A slower pace more typical of undergraduates 
would cover chapters 1 through 7 and 11, allowing more time for an in-
troduction to the idea of algorithm analysis coupled with a review of the 
important data structuring techniques. 
One question we are often asked is what do you do in class. Typically 
we devote each period to a discussion of one, or at most two problems. 
For each problem we try to emphasize how the solution can be arrived at 
by considering a design principle and showing that it applies. Perhaps 
alternative strategies are investigated and discarded. A clean separation 
is made between how the computation will proceed and decisions about 
data representation when that is possible. The best case and the worst case 
data of the resultant algorithm is made clear. Then an analysis of the time 
and space requirements is done. This scenario is a bit idealized, but on the 
whole it is accurate. 
For homework there are numerous exercises at the end of each chapter. 
The most popular and instructive homework assignment we have found is 
one which requires the student to execute and time two programs using 
the same data sets. Since most of the algorithms in this book provide all 
of the implementation details they can easily be programmed in a variety 
of languages. The problem then reduces to devising suitable data sets and 
writing a main program which outputs the timing results. The timing re-
sults should agree with the asymptotic analysis that was done for the algo-
rithm. This is a nontrivial task which can be both educational and fun. 
Most importantly it emphasizes an aspect of this field that is often neglected, 
that there is an experimental side to the practice of computer science. 
Acknowledgments 
We gratefully acknowledge the help of Arnold Rosenthal who carefully 
read the entire manuscript. In addition, Gary Bloom and our many algo-
rithms students had many useful suggestions. Administrative credit goes 
to Martha Eul and typing assistance to Donald Aoki, Terrie Christian, 
Kathy Boyer and Sybil Wright. Also we thank the USC Information Sciences 
Institute for their computer resources and the inspiration of their program 
verification group including Ralph London, David Musser and Susan 
Gerhart. 

CONTENTS 
PREFACE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
vii 
1. 
INTRODUCTION 
1.1 
What is an algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
1 
1.2 Writing algorithms in SPARKS . . . . . . . . . . . . . . . . . . . . . . . . .. . . 
4 
1.3 Writing structured programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
14 
1.4 Analyzing algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
24 
References and selected readings , . . . . . . . . . . . . . . . . . . . . . . . . . 
40 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
41 
2. 
ELEMENTARY DATA STRUCTURES 
2.1 
Stacks and queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
48 
2.2 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
53 
2.3 Heaps and heapsort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
61 
2.4 Sets and disjoint set union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
70 
2.5 Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
79 
2.6 Hashing................................................ 
82 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . 
93 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
94 
3. 
DIVIDE-AND-CONQUER 
3 .1 
The general method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
98 
3.2 Binary search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
100 
3.3 Finding the maximum and minimum. . . . . . . . . . . . . . . . . . . . . . . . 
108 
3.4 Mergesort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
113 
3.5 Quicksort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
121 
3.6 Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
127 
3. 7 Strassen's matrix multiplication. . . . . . . . . . . . . . . . . . . . . . . . . . . . 
137 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . . 
140 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
141 
xi 

xii 
Contents 
4. 
THE GREEDY METHOD 
4.1 
The general method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
152 
4.2 Optimal storage on tapes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
153 
4.3 
Knapsack problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
157 
4.4 Job sequencing with deadlines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
161 
4.5 Optimal merge patterns................................... 169 
4.6 Minimum spanning trees.................................. 174 
4. 7 Single source shortest paths. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
183 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . . 
188 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
191 
5. 
DYNAMIC PROGRAMMING 
5.1 
5.2 
5.3 
5.4 
5.5 
5.6 
5.7 
5.8 
The general method ..................................... . 
Multistage graphs ....................................... . 
All pairs shortest paths .................................. . 
Optimal binary search trees ............................... . 
0/1 knapsack .......................................... . 
Reliability design ....................................... . 
The traveling salesperson problem ......................... . 
Flow shop scheduling .................. ·t" ............ .. 
References and selected readings.......... . .............. . 
Exercises .............................................. . 
6. 
BASIC SEARCH AND TRAVERSAL TECHNIQUES 
198 
203 
208 
211 
219 
228 
231 
234 
238 
240 
6.1 
The techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 
6.2 
Code optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
270 
6.3 AND/OR graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
286 
6.4 Game trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
290 
6.5 Biconnected components and depth first search . . . . . . . . . . . . . . . 302 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
311 
7. 
BACKTRACKING 
7 .1 
The general method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 
7.2 The 8-queens problem.................................... 337 
7.3 
Sum of subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
339 
7 .4 Graph coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 
7. 5 Hamiltonian cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 
7.6 Knapsack problem....................................... 350 

Contents 
xiii 
References and selected readings. . . . . . . . . . . . . . . . . . . . . . . . . . . 
359 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
363 
8. 
BRANCH-AND-BOUND 
8.1 
The method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
370 
8.2 
011 knapsack problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
390 
8.3 Traveling salesperson.................................. . . . 
403 
8.4 Efficiency considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
412 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . . 
415 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
417 
9. 
ALGEBRAIC SIMPLIFICATION AND 
TRANSFORMATIONS 
9.1 
The general method...................................... 422 
9.2 Evaluation and interpolation..... . . . . . . . . . . . . . . . . . . . . . . . . . . 
424 
9.3 The fast Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
431 
9.4 Modular arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
440 
9.5 Even faster evaluation and interpolation . . . . . . . . . . . . . . . . . . . . . 
447 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . . 
455 
Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
457 
10. 
LOWER BOUND THEORY 
10 .1 
Comparison trees for sorting and searching. . . . . . . . . . . . . . . . . 
461 
10.2 Oracles and Adversary Arguments . . . . . . . . . . . . . . . . . . . . . . . . 
469 
10.3 Techniques for algebraic problems.... . . . . . . . . . . . . . . . . . . . . . 
478 
10.4 Some lower bounds on parallel computation . . . . . . . . . . . . . . . . . 
488 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . 
494 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
497 
11. 
NP-HARD AND NP-COMPLETE PROBLEMS 
11.1 
Basic concepts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501 
11.2 Cook's theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 
11.3 NP-Hard graph problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
522 
11.4 NP-Hard scheduling problems............................ 532 
11.5 NP-Hard code generation problems.. . . . . . . . . . . . . . . . . . . . . . . 
538 
11.6 Some simplified NP-Hard problems. . . . . . . . . . . . . . . . . . . . . . . . 
545 
References and selected readings . . . . . . . . . . . . . . . . . . . . . . . . . . 548 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552 

xiv 
Contents 
12. 
APPROXIMATION ALGORITHMS FOR NP-HARD 
PROBLEMS 
12.1 Introduction........................................... 559 
12.2 Absolute approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
562 
12.3 E-approximations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
567 
12.4 Polynomial time approximation schemes................... 
578 
12.5 Fully polynomial time approximation schemes . . . . . . . . . . . . . . 
585 
12.6 Probabilistically good algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 
596 
References and selected readings. . . . . . . . . . . . . . . . . . . . . . . . . . 
599 
Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
604 
APPENDIX A. SPARKS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
614 
INDEX ..................................................... 622 

Chapter 1 
INTRODUCTION 
1.1 
WHAT IS AN ALGORITHM? 
The word algorithm comes from the name of a Persian author, Abu Ja'far 
Mohammed ibn Musa al Khowarizmi (c. 825 A.O.) who wrote a textbook 
on mathematics. An examination of the latest edition of Webster's dic-
tionary defines its meaning as "any special method of solving a certain 
kind of problem." But this word has taken on a special significance in 
computer science, where algorithm has come to refer to a precise method 
useable by a computer for the solution of a problem. This is what makes 
the notion of an algorithm different from words such as process, technique 
or method. 
An algorithm is composed of a finite set of steps, each of which may re-
quire one or more operations. The possibility of a computer carrying out these 
operations necessitates that certain constraints be placed on the type of 
operations an algorithm can include. For example, each operation must 
be definite, meaning that it must be perfectly clear what should be done. 
Directions such as "compute 5/0" or "add 6 or 7 to x" are not per-
mitted because it is not clear what the result is or which of the two pos-
sibilities should be done. Another important property each operation should 
have is that it be effective; each step must be such that it can, at least in 
principle, be done by a person using pencil and paper in a finite amount of 
time. Performing arithmetic on integers is an example of an effective oper-
ation, but arithmetic with real numbers is not, since some values may be 
expressible only by an infinitely long decimal expansion. Adding two such 
numbers would violate the effectiveness property. An algorithm produces 
one or more outputs and may have zero or more inputs which are externally 
supplied. 
Another important criterion we will assume about algorithms in this 
book is that they terminate after a finite number of operations. There is 
another word for an algorithm which obeys all of the above properties ex-
1 

2 
Introduction 
cept termination, and that is computational procedure. One important 
example of a computational procedure is the operating system of a digital 
computer. This procedure is designed to control the execution of jobs, such 
that when no jobs are available, it does not terminate, but continues in a 
waiting state until a new job is entered. Though computational procedures 
include important examples such as this one, we will restrict our study to 
those computational procedures which always terminate. 
A related consideration is that the time for termination should be rea-
sonably short. For example, an algorithm could be devised which, for any 
given position in the game of chess, decides if that is a winning position. 
The algorithm works by examining all possible moves and countermoves 
that could be made from the starting position. The difficulty with this algo-
rithm is that even using the most modern computers it may take billions 
of years to make the decision. Therefore, we will be very concerned with 
analyzing the efficiency of each of our algorithms. 
In order to help us achieve the criterion of definiteness, algorithms will be 
written in a programming language. Such languages are designed so that 
each legitimate sentence has a unique meaning. A program is the expression 
of an algorithm in a programming language. Sometimes words such as pro-
cedure or subroutine are used synonymously for program. Most readers 
of this book will have already programmed and run some algorithms on a 
computer. This is desirable because before one studies a concept in gen-
eral it helps if one has had some practical experience with it. Perhaps you 
have had some difficulty getting started in formulating an initial solution 
to a problem, or perhaps you were unable to decide which of two algorithms 
was better. The goal of this book is to teach you how to make these decisions. 
The study of algorithms includes many important and active areas of 
research. There are perhaps five distinct areas of study one can identify: 
(i} How to devise algorithms-The act of creating an algorithm is an 
art which may never be fully automated. A major goal of this book is to 
study various design techniques which have proven to be useful in that they 
have often yielded good algorithms. By mastering these design strategies, it 
will become easier for you to devise new and useful algorithms. Many of the 
chapters of this book are organized around what we believe are the major 
methods of algorithm design. The reader may now wish to glance back at 
the table of contents to see what these methods are called. Some of these 
techniques may already be familiar, and some have been found to be so 
useful that books have been written about them. Dynamic programming is 
one such technique. Some of the techniques are especially useful in fields 
other than computer science such as operations research and electrical 
engineering. In this book we can only hope to give an introduction to these 

What is an algorithm? 
3 
many approaches to algorithm formulation. All of the approaches we con-
sider have applications in a variety of areas including computer science. 
But some important design techniques such as linear, nonlinear and integer 
programming are not covered here as they are traditionally covered in other 
courses. 
(ii) How to express algorithms-The structured programming "move-
ment" has as its central concern the clear and concise expression of algo-
rithms in a programming language. We don't intend to give a tutorial on 
these subjects here and much good reading can be found in the books Struc-
tured Programming by Dahl, Dijkstra and Hoare (Academic Press), and 
The Elements of Programming Style by Kernighan and Plauger (McGraw-
Hill). Nevertheless, section 1.3 covers a few structuring topics which will 
be important for us, e.g. recursion. In addition we shall express all of our 
algorithms using the best principles of structuring we can muster. The 
process of reading well composed programs should serve as a positive form of 
stimulation to the reader to improve his or her own skills. 
(iii) How to validate algorithms-Once an algorithm is devised it is 
necessary to show that it computes the correct answer for all possible legal 
inputs. We refer to this process as algorithm validation. The algorithm 
need not as yet be expressed as a program. It is sufficient to state it in any 
precise way. The purpose of the validation is to assure us that this algo-
rithm will work correctly independent of the issues concerning the program-
ming language it will eventually be written in. Once the validity of the 
method has been shown, a program can be written and a second phase 
begins. This phase is referred to as program proving or sometimes as pro-
gram verification. This area is now the object of intensive study and is still 
very much in its infancy. A proof of correctness requires that the solution 
be stated in two forms. One form is usually as a program which is annotated 
by a set of assertions about the input and output variables of the program. 
These assertions are often expressed in the predicate calculus. The second 
form is called a specification and this may also be expressed in the predi-
cate calculus. A proof consists in showing that these two forms are equiv-
alent in that for every given legal input they describe the same output. A 
complete proof of program correctness requires that each statement of 
the programming language be precisely defined and that all basic opera-
tions be proved correct. All these details may cause a proof to be very 
much longer than the program. 
(iv) 
How to analyze algorithms-This field of study is called analysis 
of algorithms. As an algorithm is executed, it makes use of the computer's 
central processing unit ( cpu) to perform operations and it uses the memory 
(both immediate and auxiliary) to hold the program and its data. Analysis 

4 
Introducdon 
of algorithms refers to the process of determining how much computing 
time and storage an algorithm will require. This area is a challenging one 
which sometimes requires great mathematical skill. One important result 
of this study is that it allows one to make quantitative judgments about 
the value of one algorithm over another. Another result is that it allows 
us to predict if our software will meet any efficiency constraints which may 
exist. Questions such as how well does an algorithm perform in the best 
case, in the worst case, or on the average are typical. For each algorithm 
which is presented here, an analysis will also be given. The exact nature 
of this process is more fully described in section 1.4. 
(v) 
How to test a program-Testing a program really consists of two 
phases: debugging and profiling. Debugging is the process of executing 
programs on sample data sets to determine if faulty results occur and, if 
so, to correct them. However, as E. Dijkstra has pointed out, "debugging 
can only point to the presence of errors, but not to their absence." A proof 
of correctness is much more valuable than a thousand tests, (if that proof 
is correct), since it guarantees that the program will work correctly for all 
possible inputs. Profiling is the process of executing a correct program on 
data sets and measuring the time and space it takes to compute the re-
sults. These timing figures are useful in that they may confirm a previously 
done analysis and point out logical places to perform useful optimization. 
For some of the algorithms presented here we will show how to devise a 
range of data sets which will be useful for debugging and profiling. 
These five categories just serve to outline the questions we will be asking 
about algorithms throughout this book. As we can't hope to cover all of 
these subjects completely, we will content ourselves with concentrating on 
design and analysis, spending less time on program construction and cor-
rectness. One can see that the subject of algorithms is a very diverse and 
challenging one. 
1.2 WRITING ALGORITHMS IN SP ARKS 
Our choice of an algorithm description language was a difficult decision. 
We began by considering the use of some existing languages. Some names 
which came immediately to mind were ALGOL, ALGOL-W, APL, 
FORTRAN, LISP, PASCAL, and PL/I. Though some of these seemed 
more preferable than others, the choice of a specific language left us with 
many difficulties. First of all, we wished to be able to write our algorithms 
without dwelling on the idiosyncracies of a given language. Secondly, each 
language has its followers and its detractors. We would rather not have 

Writing algorithms in SPARKS 
5 
any individual rule us out simply because he did not know or, more partic-
ularly, disliked to use some specific language. 
Futhermore, it is not really necessary to write algorithms in a language 
for which a compiler exists. As long as the language is close enough to 
many of the languages mentioned before, a hand translation will be rela-
tively easy to accomplish. This encouraged us to develop a simple language 
which is tailored to describing the algorithms we wish to discuss. In this 
way we do not have to define many aspects of a programming language 
that we will never use here. We call our language SPARKS. It is close in 
form to ALGOL60 and PASCAL. Figure 1.1 shows how a SPARKS pro-
gram could be executed on any machine. For information about obtaining 
a SPARKS translator see Appendix A. 
Sparks 
Program 
Pre-
Processor 
or 
Hand 
Translation 
Program In 
Language X 
Compiler 
For X 
Figure 1.1 Translation of SPARKS 
Machine 
Code 
Some of you may already be familiar with SPARKS having read Funda-
mentals of Data Structures (Computer Science Press). Thus you may be 
surprised when you discover that the syntax of SPARKS has been changed, 
though modestly. The virtue of not being committed to a compiled lan-
guage with actual users is that one can improve the syntax as better ideas 
are realized, without concern about creating incompatibilities with previous 
versions. 
The primitive data types of SPARKS are integer, real, boolean and char-
acter. Variables may only hold values of a single type and this type can be 
declared by a statement of the form 
integer x,y; 
boolean a,b; 
char c,d. 
Identifiers having special significance in SPARKS are considered as re-
served and they are printed in boldface. The rule for naming variables is 
to begin with a letter, use no special characters, don't be too long, and do 

6 
Introducdon 
not duplicate any reserved words of statements. Several statements may be 
included on a single line if they are separated by a semi-colon. 
The method to accomplish assignment of values to variables is the as-
signment statement 
<variable> -
<expression> 
Contrary to FORTRAN and PL/I, the left arrow (-) denotes the act of 
assigning the value of its right-hand side to the variable on its left. 
There are two boolean values, 
true and false 
In order to produce these values, the logical operators 
and, or, not 
and the relational operators 
<, ::5, =, #-, =:::, > 
are provided. 
Multidimensional arrays are available with arbitrary integer lower and 
upper bounds. An n-dimensional array of integers with lower and upper 
bounds /;, u;, 1 ::5 i ::5 n may be declared by using the syntax integer A 
(l 1:u1, ... , Zn :un ). The /;s are optional and if an /; is not specified then the 
lower bound for that dimension is assumed to be 1. We have avoided in-
troducing a record or structure feature. This feature can be useful in many 
programming situations. However our need for it here is minimal and we 
prefer to keep the syntax of SPARKS simple. Thus, all data objects will 
be constructed using the array as the basic building block. 
A conditional statement has the form 
if cond then S1 
else S2 
endif 
or 
if cond then S 1 endif 
where cond is a boolean expression and S1. S2 are arbitrary groups of 
SPARKS statements. The meaning of this statement is given by the flow 
charts of figure 12: 

Writing algorithms in SPARKS 
7 
or 
Figure 1.2 H statement 
We will assume that conditional expressions are evaluated in "short cir-
cuit" mode: given the boolean expression (condl or cond2), if condl is true 
then cond2 is not evaluated; or given (condl and cond2), if condl is false 
then cond2 is not evaluated. Not all languages evaluate Boolean expressions 
in this way. 
Another statement within SPARKS is the case, which allows one to easily 
distinguish between several alternatives without using multiple if-then-else 
statements. It has the form 
case 
: cond 1: S1 
: cond 2: S2 
: cond n: Sn 
: else: Sn+! 
end case 
where the Si, 1 :s i :s n + 1 are groups of SPARKS statements and the 
else clause is optional. The semantics of this statement is described by the 
following flowchart: 
-----< cond false 
I 
. . . 
Figure 1.3 Case statement 

8 
Introduction 
To accomplish iteration several statements are available. One of them is 
while cond do 
s 
repeat 
where cond is as before, S is as S 1 before and the meaning is given by 
Figure 1.4 Whlle statement 
It is well known that all "proper" programs can be written using only 
the assignment, conditional and while statements. This result was obtained 
by Bohm and Jacopini (see CACM 1966). Though this is very interesting 
from a theoretical viewpoint, we should not take it to mean that this is 
the way to program. On the contrary, the more expressive our languages 
are, the more we can accomplish easily. So we will provide other statements 
such as a second iteration statement, the loop-until-repeat, 
which has the meaning 
loop 
s 
until cond repeat 
r s-;-i 
~ 
false J 
* ~1-------...- co~nd-~--
-
true 
.. 
Figure 1.5 Loop-until-repeat statement 
In comparison to the while statement, the loop-until-repeat guarantees 
that the statements of S will be executed at least once. Another iteration 
statement is called the for-loop, which has the form 

Writing algorithms in SPARKS 
9 
for vble -
start to finish by increment do 
s 
repeat 
vble is a variable, while start, finish and increment are arithmetic expres-
sions. A variable of type integer or real or a numerical constant is a simple 
form of an arithmetic expression. The clause "by increment" is optional 
and taken as + 1 if it does not occur. We can write the meaning of this 
statement in SP ARKS as 
vble -
start 
fin - finish 
incr -
increment 
while (vb le - fin) * incr ::5 0 do 
s 
vble -
vble + incr 
repeat 
Notice how the expressions are evaluated only once and stored as the 
value of the variables vble, fin, and incr (two of which are new). These 
three variables will be the same type as the expressions on the right hand 
side of the arrow. S represents a sequence of SPARKS statements that do 
not alter the value of the variable vble. 
A simpler form of the loop-until-repeat statement is given by 
which has the meaning 
loop 
s 
repeat 
Figure 1.6 Loop-repeat statement 
As it stands, this describes an infinite loop! However, we assume that this 
statement is used in conjunction with some test within S which will cause 
an exit. One way of exiting such a loop is by using a 
go to label 

10 
lntroducdon 
statement which transfers control to "label." A label may be attached to 
any statement by preceding that statement with an identifier and a colon. 
Though we will not normally need the go to statement, it will be useful 
when we translate recursive programs into iterative form. A more restricted 
form of the go to is the command 
exit 
which will cause a transfer of control to the first statement after the inner-
most looping statement which contains it. This looping statement may be a 
while-repeat, loop-repeat, loop-until-repeat or a for-repeat. exit can be 
used either conditionally or unconditionally, for instance 
will execute as 
loop 
S1 
if cond then exit endif 
S2 
repeat 
.. 
! ~1----·~folse 
~~~~t_r~u~e~~-~~~-• 
Figure 1. 7 Loop-repeat with exit 
Another statement which is a restricted form of the go to is the cycle 
statement. When encountered, it causes a transfer of control to the closing 
phrase of the innermost iteration statement which contains it. Thus 
will execute as 
loop 
S1 
if condl then cycle endif 
S2 
until cond2 repeat 

Writing algorithms in SPARKS 
11 
false 
Figure 1.8 Cycle statement 
The cycle statement works similarly when contained within the other loop-
ing constructs. 
A complete SPARKS program is a collection of one or more procedures, 
the first one taken as the main program. Execution begins with the main 
program. If any SPARKS procedure, say A, reaches an end or a return 
statement, then control passes back to the SPARKS procedure which called 
procedure A. If procedure A is the main one, then control passes back to 
the operating system. A single SPARKS procedure has the form 
procedure NAME(parameter list) 
declarations 
s 
end NAME 
A SPARKS procedure can be either a subroutine or a function. In either 
case the procedure is named and a set of formal parameters follow as a 
list within parentheses. The association of actual to formal parameters 
will be handled using the call-by-reference rule. This means that at run 
time the address of each parameter is passed to the called procedure. Para-
meters which are constants or values of expressions are assumed to be stored 
into internally generated words whose addresses are then passed to the 
procedure. 
When a function is being written, the value to be returned is shown by 
enclosing it in parentheses immediately following the return statement; e.g. 
return(expr) 
where the value of expr is delivered as the value of the function. For pro-
cedures, the execution of an end implies a return with no value associated 
with it. To halt execution there is the command stop. 
So far we have not said much about the declaration of variables other 
than to indicate how it might be done, for example integer i,j. Because 
SPARKS was devised as a language primarily for exposition we feel that 

12 
Introducdon 
the explicit declaration of all variables is extremely desirable though neces-
sarily tedious. Languages such as FORTRAN and PL/I allow variables 
to be implicitly declared, for example in FORTRAN a variable is implicitly 
defined as INTEGER or REAL depending upon the first letter of its name. 
A procedure can be said to contain three kinds of variables: local, global, 
and formal parameters. A local variable is one which is declared in the 
current procedure. A global variable is one which has already been declared 
as local to a procedure which contains the current procedure. A formal 
parameter is not really a variable since it never actually contains 'a value. 
It is an identifier which is contained in the parameter list following the 
name of the procedure. At execution time formal parameters are replaced 
by the actual parameters (as described before) which are listed in the in-
voking statement. 
As an example of how variables will be declared, consider the SPARKS 
procedure MAX which finds the maximum of n numbers, n > 0. 
procedure MAX( A, n, j) 
I /Setj so that A(j) is the maximum in A(l:n), n > 0.1 I 
xmax -A(l);j -
1 
for i -
2 to n do 
ifA(i) > xmax thenxmax -A(i);j -
i; endif 
repeat 
end MAX 
Algorithm 1.1 Finding the maximum of n items 
It is easy to see that after execution of MAX, the actual parameter re-
placing j will have as its value the position in the array of the maximum 
element. It is unclear from the above whether xmax is a local or global 
variable. Assuming it is a global variable makes sense because then its 
value at the end is the value of the largest element of A. The complete 
declaration for this procedure would appear as follows: 
procedureMAX(A, n,j) 
global real xmax; 
parameters integer j, n; real A (1 :n) 
local integer i; 
We shall often prefer not to give a complete declaration when the context 
of variables is obvious. Since global variables are used infrequently, SPARKS 
will assume that all variables are ejther local or parameters unless declared 

Writing algorithms in SPARKS 
13 
otherwise. Another advantage of being less strict about enforcing declara-
tions is that it allows a type of generalization called polymorphism. For 
example the way in which procedure MAX works applies equally well to 
A(l:n) whether A contains integers, reals, or character strings. Most pro-
gramming languages insist that we must specify the type of data that A 
contains and thus three separate procedures would have to be written. 
This makes no sense for us and as we are mostly interested in the algo-
rithm's mode of processing we will often not declare the type of data in 
the array. This lack of specification, from our point of view, is more desir-
able. 
Therefore the actual declarations which would appear in procedure MAX 
are simply global xmax; integer i,j, n. The type of A (1 :n) and xmax remain 
undeclared and the fact that A, n,j are parameters and i is a local variable 
may be easily discerned by scanning the program and eliminating global 
variables. 
Procedures may contain calls to other procedures which perform some 
task and then return to the next statement in the calling procedure. When 
a procedure includes a call to itself we refer to this as direct recursion. 
When a procedure calls another procedure which then causes the original 
procedure to be reinvoked, we call this indirect recursion. Both forms of 
recursion are permitted in SPARKS. Though recursion may carry with 
it some penalty at execution time, it remains an elegant way to describe 
many computing processes. This penalty will not deter us from using 
recursion. Many such programs are easily translatable so that the recur-
sion is removed and efficiency achieved. This is discussed further in the 
next section. 
For input and output we assume two functions 
read(argument list); 
print(argument list) 
and we shall refrain from giving any details about formatting. A comment 
may be placed anywhere in a program by surrounding it with double slashes, 
e.g. 
I I this is a comment I I 
The language SP ARKS as defined so far is not precise enough to pre-
sume to be complete. For example we have avoided rules about mixed mode 
arithmetic, about the formatting of 110, scope rules, even the complete 
character set has not been stated. But none of these issues will concern us 

14 
Introduction 
here so we needn't bother with them. Finally, there will be instances when 
it seems desirable to describe an operation by an English sentence, or by 
using familiar mathematical notation and we shall do so. Thus some of the 
algorithms in this book are actually written in what should be called pseudo-
SPARKS. 
1.3 WRITING STRUCTURED PROGRAMS 
Since most of the SPARKS programs will be read many more times than 
they will be executed, we have tried to make our code readable. This is a 
goal which should be aimed at by everyone who writes programs. The SPARKS 
language is rich enough so that one can create a good looking program by 
applying some simple rules of style. Some of the commonly accepted rules 
of style are: 
(i) Every procedure should carefully specify its input and output variables. 
(ii) The meaning of all local variables should be defined. 
(iii) The flow of the program should generally be forward except for 
normal looping or unavoidable instances. 
(iv) Indentation rules should be established and fol!owed so that com-
putational units of program text can more easily be identified. 
(v) 
Documentation should be short, but meaningful. Avoid comments 
like "i is increased by one." 
(vi) Use subroutines where appropriate. 
See the book The Elements of Programming Style by Kernighan and Plauger 
(McGraw-Hill) for more examples of good rules of programming. 
Which iteration statement to use 
Since SPARKS contains four different ways for getting a set of statements 
to be repeatedly executed, it is natural to ask under what circumstances 
we prefer to use one way rather than another. The four iteration statements 
are the (i) loop-repeat, (ii) while-repeat, (iii) loop-until-repeat, and (iv) 
for-repeat. Suppose we want to read a set of values until their sum exceeds 
a predefined limit, say n. This would naturally be expressed using the while 
loop as: 
y-0 
while y ::5 n do 
read(x) 
y -y + x 
repeat 

Wridng structured programs 
15 
On the other hand suppose we want to read in n values and process each 
one in some way. Using the while loop we might write: 
i -
1 
while i :s n do 
read(x) 
call PROCESS(x) 
i - i + 1 
repeat 
But in this case it is preferable to employ a for loop, as for example 
for i -
1 to n do 
read(x) 
callPROCESS(x) 
repeat 
The reason for favoring the for loop in this context is not so much because 
we save two statements (i -
1 and i -
i + 1) but because the number of 
iterations was fixed by n and independent of the data being read. When 
we know exactly how many times we want to iterate a group of statements 
then the for statement should be used. When we want to iterate some state-
ments until a certain condition becomes true or false then the while loop is 
favored. 
Now suppose we want to read a set of values and process them until we 
read an end-of-file marker. Using the while loop we can express this as: 
read(x) 
while x #- eof do 
call PROCESS(x) 
read(x) 
repeat 
However a better way would be using the loop-repeat: 
loop 
read(x) 
if x = eof then exit endif 
call PROCESS(x) 
repeat 

16 
Introducdon 
Now we have only one occurrence of the read statement which may prove 
useful if we have to modify this program segment. In fact suppose we now 
want to modify this program to test the result of call PROCESS(x), say 
PROCESS(xJI) and terminate processing if y = 0. Then the loop-until-
repeat can be fruitfully used: 
loop 
read(x) 
if x = eof then exit endif 
call PROCESS(x,y) 
until y = 0 repeat 
if x #- eofthen ... 
The Case for CASE 
There are two statements in SP ARKS for discriminating between alter-
natives: the if-then-else and the case. We could have gotten by with either 
one of these statements since we can simulate the case, as defined in Figure 
1.3, using the nested sequence of if-then-else's 
if condl then SJ 
else if cond2 then S2 
else if condJ then SJ 
else if condn then Sn 
else Sn + 1 
endif endif . . . endif 
The virtue of the case is that it eliminates the nesting of alternatives and 
brings the conditions out to the same level. The amount of syntax is reduced 
if many conditions are involved and the resulting program segment is easier 
to read. 
Functions versus Subroutines 
Most programming languages including SP ARKS permit the definition 
of both functions and subroutines. But very few language primers discuss 
when one is preferable to the other. Before we can get close to answering 
this question, let us first reconsider the way variables get used in a proce-
dure. In section 1.2 we noted that variables could be classified as either 
local, global, or parameter. There is another three category classification 
that pertains only to parameters and global variables. In one instance a 
variable may carry a value in to a procedure, but it remains unchanged 
--~-----~-- - - -
-----------
-
----

Writing structured programs 
17 
throughout execution. A second possibility is that a variable is undefined 
upon entry but it is assigned a value which is carried out when the proce-
dure ends. The third possibility is a variable which both brings a value in 
and (a possibly changed value) out. A language designer might even go 
so far as to insist that a programmer declare his variables in this way, as 
to whether they are in, out, or inout as this adds another measure of relia-
bility. However we will refrain from adding this declaration feature to 
SPARKS, at least for now. 
The reason for introducing these categories is to help us understand the 
notion of side effect. The words subroutine or pure procedure are used to 
denote a procedure which returns no function value but may alter either its 
parameters or global variables or both. A procedure which does alter one 
of its parameters or global variables is said to have a side effect. This is 
equivalent to a procedure which has at least an out or inout type of variable. 
Pure procedures work solely through side effects. A procedure which is a 
function may also have side effects. But in the interests of reliability SPARKS 
insists that one uses either functions without side effects or pure proce-
dures. 
In general we write a function when the value it returns will be used once 
in an expression. For example if we needed a procedure to determine 
whether two trees are equal we should create a function, say EQUAL(S, T), 
which returns either true or false (a Boolean function). Then in a program 
we could say 
if.EQUAL(S,T) then ... 
Or we might create a function for computing the greatest common divisor 
and use it in the assignment 
z - x *ylgcd(x,y) 
However if we need the gcd(x,y) more than once we can either assign its 
value to a variable (t -
gcd(x,y)) or we can make it a subroutine with a 
side effect (callgcd(x,y,t)). 
Recursion 
Recursion is a powerful programming technique which unfortunately is 
not employed to the extent it should. There are at least two reasons for this. 
One is the fact that FORTRAN does not permit recursion. Thousands of 
people who have learned the art of programming using FORTRAN have 
thus been unable to experience its benefits. Two is the fact that there is 

18 
Introducdon 
often a heavy penalty in terms of execution time when one uses recursion 
on some compilers. We shall be running some experiments later to see if 
we can quantify this penalty. 
Let's take a look at some examples, both good and bad, which make use 
of recursion. 
Example 1.1 The Fibonacci sequence l,1,2,3,S,8,l3,21,34, ... is defined 
as 
Fo = 0, Fi = 1, 
F; = F;-1 + F;-2, 
i > 
This mathematical definition might naturally lead to the recursive SPARKS 
procedure: 
procedure F(n) 
I !returns the nth Fibonacci number! I 
integern 
ifn :s 1 thenretum(n) 
endif 
endF 
else retum(F(n - 1) + F(n - 2)) 
Algorithm 1.2 Fibonacci numbers 
The virtue of this program is that it is almost syntactically identical to 
the mathematical definition. However it is atrociously inefficient from the 
standpoint of computing time. But the major source of the inefficiency 
does not arise because recursion is used. Rather it is because of the way 
the computation proceeds. Many values are recomputed many times; for 
example F(n -
2) is computed twice, F(n -
3) is computed three times, 
and F(n -
4) is computed five times. Other recursive versions can be con-
structed which are far more efficient (see the exercises). 
Example 1.2 Perhaps the oldest recorded nontrivial algorithm is due to 
Euclid. This algorithm is for computing the greatest common divisor of 
two nonnegative integers. The essential step which guarantees the validity 
of his method consists of showing that the greatest common divisor of a and 
b (a > b =:::: 0) is equal to a if b is zero and is equal to the greatest common 
divisor of band the remainder of a divided by b if bis nonzero. For exam-
ple: 

Writing structured programs 
19 
gcd(22,8) = gcd(8,6) = gcd(6,2) = gcd(2,0) = 2 
and 
gcd(21,13) = gcd(13,8) = gcd(S,S) = gcd(S,3) = gcd(3,2) = gcd(2,1) 
gcd(l,0) = 1 
Expressing this process as a recursive procedure one gets 
procedure GCD(a,b) 
I I assume a > b <::: 0/ I 
if b = 0 then retum(a) 
else retum(GCD(b, a mod b)) 
endif 
endGCD 
Algorithm 1.3 Greatest common divisor 
Example 1.3 One often gets the mistaken impression that recursion is 
only appropriate for computing "mathematical" functions. Here is a pro-
cedure which searches for x in A(l:n). 
procedure SEARCH(i) 
I /if there exists an index k such that A (k) = x in A (i: n )/I 
I !then the first such k is returned else zero is returned.I I 
global n,x,A(l:n) 
case 
: i > n : retum(O) 
: A (i) = x : retum(i) 
: else : retum(SEARCH (i + 1)) 
end case 
end SEARCH 
Algorithm 1.4 Searching for x in A (1: n) 
Normally we might have written this procedure using iteration. By using 
recursion the need for a looping statement has been removed. To determine 
if x is contained within A(l:n) this function is initially invoked as ans 
SEARCH(l ). (See the exercises for an improvement to SEARCH). 

20 
Introducdon 
Removing Recursion 
We are in a mild dilemma. Some design techniques are inherently re-
cursive and so recursion is a natural way to describe algorithms obtained 
from these techniques. Also, it is often easier to prove a recursive algorithm 
correct than it is to prove the corresponding iterative algorithm correct. 
Yet, many programming languages do not permit the use of recursion. 
Also, in some languages that do permit recursion, its cost is high. This is 
often due to the overhead of repeated procedure calls. But these difficulties 
should not prevent us from using recursion in the early stages of algorithm 
design. Once a recursive algorithm has been validated and we are satisfied 
that we have a good algorithm, the recursion may be removed by translating 
the algorithm into an equivalent one which uses only iteration. This trans-
lation may be accomplished using a simple set of rules. Then it is often pos-
sible to improve the efficiency of the resulting iterative procedure by making 
some simple transformations. 
First let us see how to translate a recursive procedure into an equivalent 
procedure which uses only iteration. This translation involves replacing all 
recursive procedure calls and return statements by equivalent nonrecursive 
code. We describe the translation process for the case of direct recursion. 
Only a slight modification is needed to handle indirect recursion. To trans-
late a directly recursive procedure one performs the following: 
(i) At the beginning of the procedure, code is inserted which declares 
a stack and initializes it to be empty. In the most general case, the 
stack will be used to hold the values of parameters, local variables, 
function value, and return address for each recursive call. 
(ii) The label Ll is attached to the first executable statement. 
Now, each recursive call is replaced by a set of instructions which do the 
following: 
(iii) 
Store the values of all parameters and local variables in the stack. 
The pointer to the top of the stack can be treated as global. 
(iv) 
Create the ith new label, Li, and store i in the stack. The value i of 
this label will be used to compute the return address. This label is 
placed in the program as described in rule (vii). 
(v) 
Evaluate the arguments of this call (they may be expressions) and 
assign these values to the appropriate formal parameters. 
(vi) 
Insert an unconditional branch to the beginning of the procedure. 

Writing structured programs 
21 
(vii) If this procedure is a function, attach the label created in (iv) to a 
statement which retrieves the function value from the top of the 
stack. Add code to use this value in the way described in the recur-
sive procedure. If this procedure is not a function then affix the 
label created in (iv) to the statement immediately following the 
branch of (vi). 
These steps are sufficient to remove all recursive calls in a procedure. We 
must now alter all return statements in the following way. In place of each 
return do the following: 
(viii) If the stack is empty then execute a normal return. 
(ix) 
Otherwise take the current values of all output parameters (explicitly 
or implicitly understood to be of type out or inout) and assign these 
values to the corresponding variables which are in the top of the 
stack. 
(x) 
Now insert code which removes the index of the return address from 
the stack if one has been placed there. Assign this address to some 
unused variable. 
(xi) 
Remove from the stack the values of all local variables and para-
meters and assign them to their corresponding variables. 
(xii) If this is a function, insert instructions to evaluate the expression 
immediately following retum and store the result in the top of the 
stack. 
(xiii) Use the index of the label of the return address to execute a branch 
to that label. 
By following these rules carefully one can take any recursive program and 
produce a program which works in exactly the same way, yet which uses 
only iteration to control the flow of the program. On many compilers this 
resultant program will be much more efficient than its recursive version. 
On other compilers the times may be fairly close. Once the transformation 
to iterative form has been accomplished, one can often simplify the program 
even further thereby producing even more gains in efficiency. 
Example 1.4. Here we consider a problem which is most often thought of 
as best solved using iteration. The recursive program is not really any more 
intelligible. But we will use this example to illustrate the translation from 
recurisve to iterative form. The problem is one we've seen before, to write 
a procedure which finds the maximum element in an array A(l:n). 

22 
Introducdon 
procedure MAXI (i) 
I /this is a function which returns the largest integer kl I 
I !such that A(k) is the maximum element in A(i:n)l I 
global Integer n,A (l:n ),j, k; 
integer i 
if i < n thenj - MAX1(i + 1) 
ifA(i) > A(j) then k -
i 
elsek - j 
endif 
else k -
n 
endif 
return(k) 
endMAXl 
Algorithm 1.5 Recursively finding the maximum 
This recursive version should be easy to follow, but you might try it on some 
data before you proceed. The overhead at run time which accompanies the 
procedure calls and the manipulation of the implicit stack naturally causes 
us to consider removing the recursion before compilation. 
procedure MAX2(i) 
local lntegerj,k; global Integer n, A(l:n); 
Integer i 
Integer STACK(1:2 * n); 
top -
0 
Ll:ifi<n 
then top - top + l; STACK(top) -
i 
top -
top + 1; STACK(top) -
2; 
i - i + 1 
go toLl 
L2:j - STACK(top); top - top - 1 
if A(i) > A(i) then k - i 
endif 
elsek -
n 
endif 
elsek - j 
if top = 0 then return (k) 
elseaddr -STACK(top); top - top - 1 
i - STACK(top); top - top -
1 
top - top + 1; STACK(top) -
k 
if addr = 2 then go to L2 endif 
endif 
I /rule (i)/ I 
I /rule (i)/ I 
I /rule (ii)/ I 
I /rule (iii)/ I 
I /rule (iv)// 
//rule (v)// 
I /rule (vi)/ I 
I /rule (vii)/ I 
I /rule (viii)/ I 
I /rule (x)/ I 
I /rule (xi)/ I 
I /rule (xii)/ I 
I /rule (xiii)/ I 
end MAX2 
Algorithm 1.6 Iterative equivalent of Algorithm 1.5 

Writing structured programs 
23 
As is often the case when we automatically remove recursion, the resulting 
program can look like a dish of spaghetti. But by religiously following the 
rules, we can have faith that the resulting version is semantically equivalent 
to the recursive version. Now we can begin to simplify the program by ex-
amining the way it operates. For example we needn't stack the return ad-
dress since there is only one place to which the procedure returns. This 
leaves only the function value in the stack. However, at any point in time 
there is only one value of the function, that is, the index of the current 
maximum. Thus we can store this value in a single variable and eliminate 
the stack entirely. Another simplification is to remove the loop created by 
the statement go to L 1. Equivalently we set i to n and use k to hold the 
index of the current maximum. The resulting simplified program follows. 
procedure MAX3(A ,n) 
integer i, k, n; 
i -k -n 
while i > 1 do 
i -
i -
1 
if A(i) > A(k) then k - i endif 
repeat 
return(k) 
endMAX3 
Algorithm 1. 7 A refined version of Algorithm 1.6 
This example may be somewhat long but you shouldn't get discouraged. 
After you have tried a few examples on your own, you will be both more 
familiar with the way recursive procedures operate and quick to take 
advantage of many shortcuts as you translate out the recursion. 
The rules just given are for the general case. Often there are occasions 
when simpler rules apply. For example if the last statement of a procedure 
is a recursive call, then remove it by simply evaluating the new values of 
the parameters and branching to the beginning. A stack is not needed. The 
gcd procedure is an example of this. Removing its recursion yields the 
following program: 
procedure GCD1(a,b) 
Ll: if b = 0 then return(a) 
endif 
endGCDl 
else t -
b; b -
a mod b; a -
t; go toLl 
Algorithm 1.8 Iterative equivalent of Algorithm 1.3 

24 
Introduction 
With a little cleaning up we get 
procedure GCD2(a, b) 
while b #- 0 do 
t -
b; b -
a mod b; a -
t 
repeat 
return( a) 
endGCD2 
Algorithm 1.9 A refmed version of Algorithm 1.8 
The objective of removing recursion is to produce a more efficient but 
computationally equivalent iterative program. The fourteen rules stated 
previously need not always be followed if it is clear that one or more steps 
are unnecessary. Further, if your compiler translates recursive procedures 
into efficient code, then you may not need these rules at all. We shall return 
to recursive procedures and their translation as we meet the need in 
later chapters. 
1.4 ANALYZING ALGORITHMS 
Why do we bother to analyze an algorithm? For some of us analyzing algo-
rithms is an intellectual activity that is fun. Another reason is the challenge 
of being able to predict the future and even though we are narrowing our 
predictions to algorithms, it is gratifying when we succeed. A third reason 
is because computer science attracts many people who enjoy being efficiency 
experts. Analyzing algorithms gives these people a chance to exhibit their 
skills by devising new ways of doing the same task even faster. This ten-
dency has a large payoff in computing where time means money and effi-
ciency saves dollars. 
Before we can talk about how to analyze an algorithm we need to make 
explicit our assumptions about the kind of computer we expect the algorithm 
to be executed on. The assumptions we make can have important conse-
quences with respect to how fast a problem can be solved. Though formal 
models of machines do exist (e.g. Turing machines or Random Access Ma-
chines), for most of this book it will be sufficient to consider our computer 
as a "conventional" one. By this we mean that the instructions of a pro-
gram are assumed to be carried out one at a time and the major cost of an 
algorithm depends upon the number of operations it requires. We assume 
that a random access memory is available which permits one to either ac-
cess or store any element in a fixed amount of time. 

Analyzing algorithms 
25 
We admit that there are reasons to believe that these assumptions may 
become outmoded with future generations of machines. Already computers 
such as ILLIAC IV or the CDC ST AR exist and offer a high degree of par-
allelism in the manner in which a sequence of operations can be executed. 
This invalidates to some extent the measurement of an algorithm's cost by 
the summing of its logical operations. A second though somewhat more 
remote factor is the dramatic decrease in the cost of logic circuits (micro-
processors) to the point where configurations of these processors cause the 
movement of data to be more expensive than the arithmetic and logical 
operations. If these trends continue, a new theory of computation will be 
required. But until such machines becomes more pervasive the model of 
counting and summing logical operations on a sequential processor remains 
the most accurate predictor of performance and the one we will use. 
Given an algorithm to be analyzed, the first task is to determine which 
operations are employed and what their relative costs are. These operations 
may include the four basic arithmetic operations on integers: addition, 
subtraction, multiplication and division. Other basic operations might in-
clude arithmetic on floating point numbers, comparisons, assigning values 
to variables and executing procedure calls. These operations typically take 
no more than a fixed amount of time and so we say that their time is 
bounded by a constant. This is not true of all operations of a computer. 
Some may be composed of an arbitrarily long sequence of more basic op-
erations. For example, a comparison of two character strings may use a 
character compare instruction which may, in turn, use a shift and bit-com-
pare instruction. The total time for the comparison of two strings will de-
pend upon their lengths, while the time for each character compare is 
bounded by a constant. 
The second task is to determine a sufficient number of data sets which 
cause the algorithm to exhibit all possible patterns of behavior. This is one 
of the important and creative tasks of algorithm analysis. It requires us to 
understand the workings of the algorithm well enough to concoct the data 
configurations which produce the best or worst or typical behavior. We will 
say more about this when we discuss particular algorithms. 
In producing a complete analysis of the computing time of an algorithm, 
we distinguish between two phases: a priori analysis and a posteriori testing. 
In a priori analysis we obtain a function (of some relevant parameters) 
which bounds the algorithm's computing time. In a posteriori testing we 
collect actual statistics about the algorithm's consumption of time and 
space, while it is executing. Suppose there is the statement x - x + y some-
where in the middle of a program. We wish to determine the total time 
that statement will spend executing, given some initial state of input data. 

26 
Introduction 
This requires essentially two items of information, the statement's frequency 
count (i.e. the number of times the statement will be executed) and the 
time for one execution. The product of these two numbers is the total time. 
Since the time per execution depends on both the machine being used and 
the programming language together with its compiler, an a priori analysis 
limits itself to determining the frequency count of each statement. This 
number can be determined directly from the algorithm, independent of the 
machine it will be executed on and the programming language the algorithm 
is written in. 
For example consider the three program segments a,b,c: 
x-x+y 
(a) 
fori -
1 ton do 
x-x+y 
repeat 
(b) 
fori -
1 ton do 
forj -
1 ton do 
x -x +y 
repeat 
repeat 
(c) 
For each segment we assume the statement x - x + y is contained within 
no other loop than what is already visible. Thus for segment (a) the fre-
quency count of this statement is 1. For segment (b) the count is n and for 
segment (c) it is n 2• These frequencies 1, n, n 2 are said to be different, in-
creasing orders of magnitude. An order of magnitude is a common notion 
with which we are all familiar; for example walking, bicycling, riding in a 
car and flying in an airplane represent increasing orders of magnitude with 
respect to the distance we can travel per hour. In connection with algorithm 
analysis, the order of magnitude of a statement refers to its frequency of 
execution, while the order of magnitude of an algorithm refers to the sum 
of the frequencies of all of its statements. Given three algorithms for solving 
the same problem whose orders of magnitude are n, n 2, and n 3, naturally 
we will prefer the first since the second and third are progressively slower. 
For example, if n = 10 then these algorithms will require 10, 100, and 
1000 units of time to execute respectively (assuming all basic operations are 
of equal duration). Determining the order of magnitude of an algorithm is 
very important and producing an algorithm which is faster by an order of 
magnitude is a significant accomplishment. The a priori analysis of algo-
rithms is concerned chiefly with order of magnitude determination. For-
tunately there is a convenient mathematical notation for dealing with this 
concept. 

Analyzing algorithms 
27 
Asymptotic Notation 
An a priori analysis of computing time ignores all of the factors which 
are machine or programming language dependent and concentrates on 
determining the order of magnitude of the frequency of execution of state-
ments. There are several kinds of mathematical notation which are very 
useful for this kind of analysis. One of these is the 0-notation. 
Definition: fin) = O(g(n)) (read as ''f of n equals big oh of g of n ") iff there 
exist two positive constants c and no such that I f(n) I :::;; c I g(n) I for all 
n ~no. 
Suppose we are determining the computing time, fin), of some algorithm. 
The variable n might be the number of inputs or outputs, their sum or the 
magnitude of one of them. Since fin) is machine dependent, an a priori 
analysis will not sllffice to determine it. However, an a priori analysis can 
be used to determine a g(n) such that fin) = O(g(n)). When we say that 
an algorithm has computing time O(g(n)) we mean that if the algorithm is 
run on some computer on the same type of data but for increasing values 
of n, the resulting times will always be less than some constant times I g(n) I . 
When determining the order of magnitude of fin) we shall always try to 
obtain the smallest g(n) such thatfin) = O(g(n)). 
Theorem 1.1: If A(n) = am nm + · · · + a 1 n + a 0 is a polynomial of 
degree m thenA(n) = O(nm). 
Proof: 
Using the definition of A(n) and a simple inequality 
IA(n)I $ lamlnm + · · · + la1ln + laol 
$ (laml + lam-1 lln + · · · + laol /nm)nm 
Choosing c = I am I + · · · + I a o I and no = 1 the theorem immediately 
follows. 
D 
Theorem 1.1 says that if we can describe the frequency of execution of a 
statement in an algorithm by a polynomial such as A(n), then that state-
ment's computing time is O(n m). However the constant in the above theorem 
is not the best possible. Actually we can show that any constant greater 
than I am I can be used (for sufficiently large n). 

28 
Introduction 
If an algorithm has k statements whose orders of magnitude are c1nmt, 
c2nm2, ... , Ctnmt then the order of magnitude of the entire algorithm is 
given by c 1nmt + ... + Ctnmt which by Theorem 1.1 is equal to O(nm) 
where m = max{m;}, 1 s i s k. 
If we have two algorithms which perform the same task on n inputs, and 
the first has a computing time which is O(n) and the second O(n2), which 
is superior? It is easy to see that for sufficiently large values of n, the time 
for the second algorithm will be larger than the time for the first. For ex-
ample, if the actual computing times for these algorithms are 2n and n2 re-
spectively, then algorithm one is faster (i.e. has a smaller value) than algo-
rithm two for all n > 2. On the other hand if the actual computing times 
are 104 n and n2 then algorithm two is faster for all n < 104• For n > 104 
algorithm one is faster. So, we cannot decide which of the two algorithms 
is better unless we know something about the constants associated with the 
orders of magnitude. If the constants are comparable then the lower order 
algorithm is better than the higher order algorithm. But this is not the 
whole story. The point at which one algorithm requires fewer operations 
than another also depends upon the low order terms. In practice these 
terms and their coefficients depend on many factors, such as the language 
and the machine one is using. Alas, it is far more difficult to derive the 
entire formula for the computing time than the leading term. Thus for a 
priori analysis, we content ourselves with determining the order of magni-
tude, and the establishment of its constant will be postponed until after the 
program has been written and executed. We will not usually derive any 
terms other than the order of magnitude, unless those terms significantly 
influence the comparison of two algorithms. 
As an example of the usefulness of improving an algorithm by an order 
of magnitude, suppose we have two algorithms for solving the same task 
which require n2 and n log n operations on n inputs. For n = 1024 they 
require 1,048,576 versus 10,240 operations. If it takes one microsecond to 
perform each operation then algorithm one requires about 1.05 seconds 
while algorithm two requires .01 seconds on the same input. If we double 
n to 2048, then the operation counts become 4,194,304 versus 22,528 or 
roughly 4.2 seconds versus .02 seconds. When then is doubled an O(n2) 
algorithm takes four times as long to complete while an O(n log n) algo-
rithm takes only a little more than twice as long to complete. Since an n 
of several thousand is not especially large, we see how important an order 
of magnitude improvement such as this can be. 
The most common computing times for algorithms we will see here are 
0(1) < O(log n) < O(n) < O(n log n) < 0(n2) < O(n 3 ) < 0(2") 

Analyzing al~orithms 
29 
0(1) means that the number of executions of basic operations is fixed and 
hence the total time is bounded by a constant. The first six orders of mag-
nitude have an important property in common, they are bounded by a poly-
nomial. O(n), O(n2), and O(n3) are themselves polynomials referred to by 
their degrees: linear, quadratic, and cubic. However, there is no integer 
m such that nm bounds 2n, or 
2 11 ~ O(nm) 
for any integer m. The order of this formula is 0(2n). 
An algorithm whose computing time is bounded below by 0(2
11 ) is said to 
require exponential time. As n gets large, there becomes a tremendous dif-
ference between exponential and polynomial time algorithms. If one finds 
an algorithm which reduces the time to solve a problem from exponential 
to polynomial, that is a great accomplishment. See Chapter 11 for a further 
discussion of polynomial versus exponential time algorithms. 
Figure 1.9 and Table 1.1 show how the computing times for six of the 
typical functions grow with a constant equal to one. Notice how the times 
O(n) and O(n log n) grow much more slowly than the others. For large 
data sets, algorithms with a complexity greater than O(n log n) are often 
impractical. An algorithm which is exponential will be practical only for 
very small values of n and even if we decrease the leading constant, say by 
a factor of 2 or 3, we will not improve the amount of data we can handle 
by very much. To see more precisely why a change in the constant, rather 
than to the order, of an algorithm produces very little improvement in 
running time we look at an example. 
2n 
n3 
65536 
32768 
16384 
8192 
4096 
2048 
1024 
n 
512 
256 
128 
64 
32 
16 
8 
4 
2 
I 
2 
4 
8 
16 
32 
64 
128 
Figure 1.9 Rate of growth of common computing time functions 

30 
Introduction 
/ogn 
n 
n log n 
n2 
nJ 
2n 
0 
1 
0 
1 
1 
2 
1 
2 
2 
4 
8 
4 
2 
1 
8 
16 
64 
16 
3 
~ 
24 
64 
512 
256 
4 
16 
64 
256 
4096 
65536 
5 
32 
160 
1024 
32768 
4294967296 
Table 1.1 Values for computing functions 
Example 1.5 Suppose the orders of magnitude of two algorithms are n 2 * 2n 
and n * 2n. Both algorithms are exponential, but in one case there is 
an extra factor of n. The leading constants are assumed to be one. The 
respective frequency counts are: 
n 
n •2" 
n2 • 2" 
5 
160 
800 
10 
10240 
102400 
15 
491520 
7372800 
20 
20971520 
419430400 
30 
3.2 x 1010 
9.6 x 1011 
Using the same assumption as before of one operation per microsecond, 
we observe that for n = 30 the times are roughly 8.9 hours versus 11 days. 
Though the extra linear factor does make a considerable difference, the 
exponential character of these times dominates and implies that they will 
both soon become intolerably long. If we were able to speed up the second 
algorithm by a factor often, so that the time is (1/10)n 22n, then for n > 10 
the first algorithm is still faster. Moreover, for n = 30 the time required 
by this faster version is still greater than 24 hours. The conclusion we draw 
from this example is this: exponential algorithms require so much time, 
that neither subsequent improvements in the speed of sequential computers 
nor improvements which effect even the leading constant of the computing 
time, will ever produce a much greater range of solvable problem size. One 
possible recourse is to devise new algorithms with much improved orders 
of magnitude. 
0 
So far we have concentrated on 0-notation as a means for describing an 
algorithm's performance. Whereas 0-notation is used to express an upper 
bound, we might also wish to determine a function which is a lower bound. 
What is needed is a mathematical notation for expressing a formula which 
is a lower bound on the computing time of an algorithm to within a con-
stant. 

Analyzing algorithms 
31 
Definition: f(n) = O(g(n)), (read as "f of n equals omega of g(n)") iff there 
exist positive constants c and no such that for all n > no, lf(n)I ~ clg(n)I. 
In some cases the time for an algorithm, f(n), will be such thatf(n) = 
O(g(n)) and f(n) = O(g(n)). For this circumstance we will use the 
following notation. 
Definition: f (n) = e (g(n )) iff there exist positive constants Ct, c2, and 
no such thatforalln >no, Ct lg(n)I :S lf(n)I :S c2 lg(n)I. 
If f(n) = e (g(n)) then g(n) is both an upper and lower bound on f(n). 
This means that the worst and best cases require the same amount of 
time to within a constant factor. As an example consider the algorithm 
which finds the maximum of n elements, Algorithm l. l. The computing 
time for this algorithm is both O(n) and O(n) since the for loop always 
makes n -
1 iterations. Thus, we say that its time is 8(n). The procedure 
of algorithm 1.4 searches an array of n elements for a single value. It has 
a computing time which is O(n) but 0(1). In the best case it might find 
the value on the first comparison, but in the worst case it will look at all 
elements once. 
An even stronger mathematical notation is given by the following. 
Definition: f(n) -
o(g(n)) (read as ''f of n is asymptotic to g(n)") iff 
limit f(n)/ g(n) -
1 as n -
oo 
Since the ratio in the limit is one, the functions fin) and g(n) must agree 
even closer than by a constant factor. If there is an algorithm whose exact 
computing time isf(n) and we can determine ag(n) such that/is asymptotic 
to g, then we will have a more precise description of the computing time 
than if we had used the big 0-notation. In practice it implies we will know 
both the order of the leading term and its constant. For example ifj(n) 
a kn 1c + . . . + a o then 
fin) = O(nk) 
and 

32 
Introduction 
Sums of Integen 
As we work to determine the frequency of execution of statements we 
shall often encounter expressions of the form 
E 
fli) 
g(n)sish(n1 ' 
(1.1) 
where fl.i) is a polynomial in i with rational number coefficients. The most 
common forms of this formula are 
E l, 
lsisn 
E i, 
lsisn 
E i2 
lsisn 
(1.2) 
which are the first three Bernoulli polynomials. Since these sums are finite 
there exist formulas, polynomials inn, which are equal to these sums. The 
value of the first sum is easily seen to be n. But how do we determine the 
values of the others? One method is by using interpolation. For example 
we can think of the second summation as describing the set of points in 
two dimensional space, (n,P(n)), which are: 
(1,1), (2,3), (3,6) (4,10), ... 
P(n) is the polynomial to be found. According to Lagrange's formula (see 
Chapter 9 for more details) we find that 
E i = n(n + 1)/2 = O(n2) 
(1.3) 
tsisn 
E i 2 = n(n + 1) (2n + 1)/6 = O(n3) 
lsisn 
(1.4) 
In general we will find that 
nk+t 
nk 
E ik = --- + -
+ lower order terms 
(1.5) 
lsisn 
k + 1 
2 
Thus we can conclude that 
E ik = O(nk+t) 
(1.6) 
lsisn 
or more precisely 
E ik -
o ---
( 
nk+I ) 
tsisn 
k + 1 
(1.7) 
------ ----

Proftllng 
33 
PROFILING 
Suppose we assume at this stage that a program solving some problem has 
been devised, coded, proved correct, and debugged on a computer. How 
do we go about producing a peiformance profile, that is determining the 
precise amounts of time and storage this program will consume? In order 
to determine exact times, our computer must be equipped with a clock 
whose time can be read. Using this timing capability there are many factors 
of the program's performance we can check. The most important test of a 
program is the one which confirms the earlier analysis of the order of mag-
nitude. A program whose time has been determined to be 8(n) or 8(n log n), 
etc. will have a performance profile which looks like the curves in Figure 
1.9. Using actual timing data we should be able to determine the exact 
shape of this curve given the programming language and the machine we 
are using. 
Let the program be called SOLUTION(X,Y) where X denotes the input 
and Y the output. When the initial analysis was first done, a consideration 
of possible data sets was made. This was necessary to determine at least 
the worst and best possbile cases of the algorithm. Let these data sets be 
created to be used as input to this procedure. Then a program to produce 
a timing profile has the following general form: 
procedure PROFILE 
I I this program outlines the form that a main program/ I 
I /will take when testing the program SOLUTION(X, Y)I I 
I /initialize any variables that may be needed for SOLUTION/ I 
print('Test of algorithm SOLUTION. Times in milliseconds') 
loop 
read(DATA) 
If DATA = end-of-file then exit endif 
print('A new data set =',DATA) 
call STIME(t) 
I /Procedure STIME initializes t to the current/ I 
I /value of the clock. Determining the time on a/ I 
//computer is machine dependent and varies greatly./ I 
//See a consultant at your computing center for further details./ I 
call SOLUTION(DAT A, OUTPUT) 
call STIME(s) 
print('Time = ', s - t) 
repeat 
end PROFILE 
Algorithm 1.10 Schema for producing a program's performance profile 

34 
Introduction 
The above procedure will print out the times that SOLUTION takes to 
process each data set. Notice that we don't bother to print the result Y, 
since we are assuming that SOLUTION is known to work properly. Also, 
we assume that neither SOLUTION nor any of its subroutines perform any 
input or output. To produce an order of magnitude curve the data sets are 
chosen so that they grow in size. The resulting timing data will show the 
performance profile of SOLUTION. For a program whose computing time 
is described not by 8(f(n)) but by O(f(n)), separate runs should be 
made using the worst, best and average data over a range of sizes. 
A second way to use the timing capability of your computer is to take 
two programs for performing the same task whose orders of magnitude are 
identical and run them as they process data. The resulting times will show 
which, if any, program is faster. Changes to one program which do not 
alter the order of magnitude but which purport to speed up the program 
can also be tested in this way. 
The procedure STIME is used to read the computer's clock. Reading 
a clock varies widely from computer to computer and determining the pre-
cise times that an algorithm takes is not entirely trivial. Much of the dif-
ficulty comes from the idiosyncracies associated with computer clocks. Often 
they are not very accurate or it may be difficult to access them. A survey 
of some popular computers revealed that their clocks measured time in the 
given units; see Table 1.2. 
IBM 370/158 
3.3 milliseconds 
UNIVAC 1108 
.2 milliseconds 
PDP 11/45 
16.7 milliseconds 
CDC Cyber 74 
1 
milliseconds 
HP3000 
milliseconds 
B 3700 
milliseconds 
B t;i700 
2.64 microseconds 
Table 1.2 Clocks and their accuracies 
Another difficulty in getting reliable clock times comes about if your 
computer's operating system is in multiprogramming or time sharing mode. 
For instance, on the PDP-10 under the TENEX operating system, the clock 
times always includes a certain fraction of the time needed to swap out 
the user's program on disk. This time will vary depending upon the num-
ber of users who are currently logged into the system, and there is no way 
of discerning how much time that takes. 
If we run an algorithm on an IBM 370 and the complete execution takes 
less than 1 millisecond, then the resulting timing figures will be just "noise", 

Profiling 
35 
i.e., totally unreliable. There are two ways to solve this problem. One way 
is to increase the size of the input until the total time required is large 
enough to give a reliable measurement. A second possibility is to repeatedly 
execute the algorithm r times for r sufficiently large and then divide the 
total time by r. 
In the remainder of this section we will take a specific problem, give 
several algorithms for its solution and compare their resulting running times. 
The problem we shall solve is a simple one. We are given n integers residing 
in the array A(l:n). These integers are already in sorted order. The mode 
of A is an element which occurs most often. We wish to write an algorithm 
which determines both the mode of A and the number of times (frequency) 
the mode occurs in A. Procedure MODE presents what might be termed 
a straightforward solution. 
procedure MODE(A,n, mode,freq) 
11 In array A(l:n), n ~ 1 which is already sorted,// 
I /the mode and its frequency are found. In case/ I 
//of a tie the first mode encountered is chosen.I I 
integer i ,n ,freq ,temp; 
mode - A(l);freq -
1; temp -
1 
for i -
2 to n do 
if A(i) ~ A(i -
1) then temp -
1 I la new element is encountered// 
else temp -
temp + 1 I /increase the frequency of the current ele-
ment// 
if temp > freq 
thenfreq -
temp; mode -
A(i) //new frequency; possibly a 
new mode// 
endif 
endif 
repeat 
end MODE 
Algorithm 1.11 Finding a mode and its frequency 
Now let us try to conceive of a recursive algorithm which finds the mode. 
Suppose we imagine that we already have a procedure RMODE(n,mJ) 
which finds the mode m and frequency f of the already sorted elements in 
A(l:n). Suppose we apply the procedure to the first n -
1 elements, using 
call RMODE(n -
1, m,f) and consider under what circumstances a new 
mode may occur by including A (n ). Thinking in this way leads to a recursive 
program and possibly to another mode finding program. Clearly, if A(n) -;it 

36 
Introduction 
A(n -
1), m andf needn't be changed. If A(n) = A(n -
1), how can we 
distinguish between the cases: (i) a new mode is found, (ii) the mode is un-
changed, but its frequency is increased, (iii) no change need be made to m 
or f? The answer comes by considering if A(n) = A(n - j), for then there 
are n -
(n - j) + 1 = f + 1 occurences of A(n) which makes it either a 
new mode or the same mode with a new frequency. Otherwise the mode 
needn't be changed. This leads to the elegant recursive procedure first 
given by M. Griffiths: 
procedure RMODE(i ,m J) 
I I the modem and its frequency fin A(l :i) are found; i ~ 11 I 
global A (1 :n ); integer i ,n /; 
lfi = 1 thenm -A(l);f-1 
else callRMODE(i - 1,m,f) 
If A(i) = A(i - j) 
thenm -A(i);f-f + 1 
endif 
endif 
endRMODE 
Algorithm 1.12 Recursively finding a mode and its frequency 
This program is very slick and needs to be studied. Initially RMODE is 
invoked by the statement call RMODE(n, mode, freq) which sets i to n 
and begins execution. If i is one, it is obvious that it works! Otherwise, as-
suming RMODE will work correctly for i -
1 elements (i :::;; n), we ask it 
to find the mode and frequency of the first n -
1 elements of A. It does 
this, returning the mode and frequency in m and f. If the frequency of the 
mode of the entire set A(l:n) is g then the frequency of the mode of the 
first n -
1 elements,/, is/ = g or f = g -
1. The latter case occurs only 
if the last element, A (n), is the mode. Then A (n - f) = A (n - f + 1) = 
... = A(n). The innermost If therefore correctly updates the mode. 
If we examine the way RMODE actually works we see that it continually 
calls itself until i = 1. It then computes the final result by examining, in 
tum, the second, third, fourth, ... , nth element and updating the mode 
appropriately. This realization makes it natural to consider a translation of 
RMODE which would work iteratively. Since there is only one recursive 
call, no return address need be stacked. After performing a translation 
according to the rules in section 1.3 and then simplifying, procedure RMOD El 
is obtained. 

procedure RMODEJ (A ,n ,m J) 
11 a non-recursive version of RMODEl I 
integer i ,n /; 
m -A(l);f- 1 
for i -
2 to n do 
ifA(i) = A(i - j) 
then m -A(i);f - f + 1 
endif 
repeat 
endRMODEJ 
Algorithm 1.13 A refined version of Algorithm 1.12 
Profiling 
37 
We now have three algorithms for finding the mode and its frequency in a 
sorted array. Which one is the best? RMODE is the shortest, but not by 
much. We might not have thought of RMODEl if we hadn't searched for 
a recursive solution first. Since RMODEl was derived from RMODE by 
removing the recursion, it will probably be faster. But how much faster? 
All three programs have computing times which are e(n) and an asymptotic 
analysis is unable to provide any more clues as to their relative efficiencies. 
The solution is to devise some data sets and determine a performance 
profile for these three programs. Which data set will cause these programs 
to work the hardest? Clearly, sets containing only one distinct element will 
give the worst case. The frequency will continually be updated and each 
of the algorithms will do the maximum amount of work that is possible on 
each iteration. A best case data set would be the one with all distinct ele-
ments. Determining a data set which exhibits some average behavior is 
more complex. Elements should be repeated with varying frequencies. If 
there are k distinct elements out of n, where the ith occurs with frequency 
n;, then n1 + ... + nk = n. This sum is called a k-partition of n. For 
random data sets we need to generate random k partitions of n for all values 
of k. 
Table 1.3 gives the computing times in milleseconds, determined when 
RMODE, RMODEl, and MODE were run on the same data set, all num-
bers distinct. RMODE is consistently slower, by a factor between three and 
four over its iterative equivalent RMODEl, MODE, the original solution, 
is also slower than RMODEl, but never by more than a factor of two. 
Notice that there is a severe penalty running a recursive program in PLIC 
(a diagnostic compiler for PL/1) versus its simplified iterative equivalent. 
We can use these times to estimate the constant of the leading term of the 
computing time. Let c1n, c2n, and c3n be the best case times for RMODE, 

38 
Introduction 
RMODEl, and MODE respectively where the Ci are constants we wish to 
determine. From the table the approximate values for these constants are 
11/10, 3/10, and 1/2 respectively. 
n 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000 
RM ODE 
110 
220 
340 
400 
640 
680 
720 
820 
940 
1050 
RM OD El 
40 
60 
80 
120 
150 
170 
200 
240 
260 
280 
MODE 
60 
100 
150 
180 
250 
300 
350 
410 
440 
500 
(Run on an IBM 370/158 in PL/C) 
Table 1.3 RMODE vs. RMODEl vs. MODE, distinct numbers or best case 
Another characteristic which we can test for is the difference in times 
obtained by writing an algorithm in different programming languages. To 
test this factor we wrote RMODEl in FORTRAN and PLl/X and ran it 
using data with one distinct element. PLl/X is an optimizing compiler for 
PL/I. The table below summarizes the results. 
n: 
500 
1000 1500 2000 2500 3000 3500 4000 4500 5000 
FORTRANG: 
3 
13 
16 
26 
33 
39 
41 
46 
56 
66 
PLIIX: 
9 
19 
33 
43 
56 
69 
76 
86 
99 
113 
(run on an IBM 370/158) 
Table 1.4 Times in milliseconds for RMODEl, 1 element or worst case 
Notice that despite the use of an optimizing compiler for PL/I the FORTRAN 
version is uniformly faster by slightly less than a factor of two. 
There are many more experiments one could devise using these programs. 
The timing of programs and the collecting of performance profiles is an 
integral part of the analysis of algorithms. We will see more computational 
experiments as we study the algorithms in this book. 
CONCLUSIONS 
The first chapter has given us a glimpse of all of the phases we will go 
through as we study an algorithm and its variations. Informally speaking 
we perform in order: design, validation, analysis, coding, verification, de· 
bugging, and timing. Often we have to go back and repeat a phase. Though 
the creation of algorithms may never be fully automated, in Figure 1.10 
we have even gone so far as to give a procedure for composing an algo-
rithm. Look at it carefully. Maybe someday someone will write a system 
which automaticaly generates correct and efficient algorithms. In that case 
the need for this book will be removed.But in the meantime the develop-
ment of algorithms remains both an art and a science. 

Conclusions 
39 
In the following chapters you will find some of the most clever, most use-
ful, and the most beautiful of algorithms that are known. The study of 
these algorithms is a fascinating adventure in itself. But the skills we hope 
you will learn here are useful for more than just recreation. They will have 
a payoff which you can apply to your own work and to the work of others. 
Write a complete 
specification 
of the problem 
Using one or more design methods 
conceive of o possible algorithm, T. 
If you can't do this then stop 
If you hove reached 
here twice, then stop 
NO 
CD 
Deterime best and worst case doto sets. 
Analyze the time and space requirements of T. 
Discover 
the bugs 
NO 
Code T properly 
Use the lest 
doto devised 
inCDond run 
the program 
Prove that the 
Time the program 
program agrees,____ ___ using the test doto 
with its 
from CD 
specification 
..._--'"'-------~ 
Figure 1.10 
YES 
Release the 
program 
for general 
use 

40 
Introduction 
The ability to effectively analyze, criticize, and improve the programs that 
your colleagues develop will be a sign that your computer science training 
has paid off. 
REFERENCES AND SELECTED READINGS 
The study of algorithms and their mathematical analysis was given great impetus 
by Donald Knuth, most specifically in his projected seven volume work. 
The Art of Computer Programming 
Volume 1, Fundamental Algorithms (1968) 
Volume 2, Seminumerical Algorithms (1969) 
Volume 3, Sorting and Searching (1973) 
Volume 4, Combinatorial Search and Recursion (to appear) 
Volume 5, Syntactical Algorithms (to appear) 
Volume 6, Theory of Languages (to appear) 
Volume 7, Compilers (to appear) 
Addison-Wesley, Reading, Massachusetts 
More entertaining are these popular articles 
"Algorithms" by D. E. Knuth, Scientific American, April 1977. 
"Computer science and its relation to mathematics", by D. E. Knuth, American 
Math Monthly, April, 1974. 
Other important textbooks on the theory and complexity of algorithms are 
The Design and Analysis of Computer Algorithms, by A. V. Aho, J.E. Hopcroft, 
J. D. Ullman, Addison-Wesley, Reading, Mass. (1974). 
The computational complexity of algebraic and numeric problems by A. Borodin 
and I. Munro, American Elsevier, New York, 1975. 
Several books have appeared whose major emphasis is on "combinatorial com-
puting." 
Elements of Combinatorial Computing, by M. B. Wells, Pergamon Press, Oxford 
(1971). 
Combinatorial Algorithms: Theory and Practice, by E. M. Reingold, J. Nievergelt, 
and N. Deo, Prentice-Hall, Englewood Cliffs, N.J. (1977). 

Exercises 
41 
Combinatorial algorithms by A. Nijenhuis and H. S. Wilf, Academic Press, New 
York (1975). 
Algorithmic combinatorics, by S. Even, Macmillan Company, New York, 1973. 
Combinatorial optimization, by E. Lawler, Holt, Rinehart, Winston, New York, 1976. 
More information on asymptotic analysis can be found in 
Asymptotic Methods in Analysis, by N. G. de Bruijn, North Holland Publishers, 
Amsterdam (1961). 
"Big Omicron and Big Omega and Big Theta," by D. E. Knuth, SIGACT News, 
ACM (April 1976). 
The idea of translating recursive programs into iterative form is discussed in 
"Structured Programming with Go-tos," by D. E. Knuth, Computing Surveys, vol. 
6, no. 4, (January 1975) 
"A system which automatically improves programs", by J. Darlington and R. M. 
Burstall, Proceedings of the Third International Conference on Artificial Intel/, 
Stanford, (1973), 479-485. See alsoJACM, vol. 24, no. 1, January 1977, pp. 44-67. 
"Toward automatic program systhesis" by Z. Manna and R. Waldinger, CACM, 
vol. 14, no. 3, (March 1971) 
For an eloquent plea about the future directions of computer science theory see 
"Microelectronics and computer science", by Ivan E. Sutherland and Carver A. 
Mead, Scientific American, September, 1977, 210-228. 
For a nice introduction to the fields of program data flow analysis and program 
verification plus many references see 
ACM Computing Surveys, vol. 8, no. 3, September 1976. 
EXERCISES 
1. Look up the words "algorism" and "algorithm" in your dictionary. 
2. The shortened name al-Khowarizmi (algorithm) literally means "from the 
town of Khowarazm". This city is now known as Khiva, and is located in the 
province of Uzbekistan, USSR. See if you can find this city in an atlas. 

42 
Introduction 
3. 
Rewrite the following program segments in a clearer way: 
i-n 
while i > 1 do 
y -
F(x) 
i-i-2 
repeat 
if a > b 
then if c > d 
then if e > /thenx -
1 
elsex -
2 
endif 
elsex -
3 
endif 
elsex -
4 
endif 
4. 
Write FORTRAN equivalents of the while, loop-until-repeat, and for state-
ments. Remember that according to the ANSI language standard, FORTRAN 
DO-loops always execute once and cannot count down. 
5. 
In an attempt to economize on the number of statements in SPARKS discuss 
the merits and demerits of removing the following statements: (i) while (ii) while 
and loop-until-repeat (iii) go to (iv) all iteration statements. 
6. 
Write a Boolean function which takes an array A(l:n), n ~ 1, of zeros and 
ones and determines if the size of every sequence of consecutive ones is even. 
What is the computing time of your algorithm? 
7. 
Write a recursive algorithm for problem 6 if you have not already done so. 
8. If t(n) is the time for procedure Fibonacci (n) as given in section 1.3 show that 
t(n) = 0(2n- 2). 
9. 
Another recursive procedure which computes the nth Fibonacci number is the 
one below. 
procedure Fl (n) 
//a function which returns the nth Fibonacci number./ I 
if n < 2 then retum(n) 
else return (F2(2,n,1,1)) 
endif 
end Fl 
procedure F2(i,n,x,y) 
if i :s; n 
then callF2(i + 1, n,y,x + y) 
endif 
return(y) 
endF2 

Exercises 
43 
Trace out the algorithm as it computes Fl(l), F1(2), F1(3), F1(4) and then 
compare its computing time to the time for procedure F(n), Algorithm 1.2. 
10. 
Simulate procedure MAXI, Algorithm 1.5 on the data set n = 5 andA(1:5) = 
10,20,12,18,16. 
11. 
Which of the following procedures correctly finds the maximum of then distinct 
elements in A(l :n)? 
procedure MAX4(i,j) 
global n, A(l:n) 
if i :s; n then if A(i) > A(j) thenj -
i endif 
endif 
endMAX4 
procedure MAX5(i,j) 
global n, A(l:n) 
call MAX4(i + 1,j) 
if i < n then call MAX5(i + 1,j) 
if A(i) > A(j) thenj -
i endif 
elsej - n 
endif 
endMAX5 
How is each procedure initially invoked? Does the correct procedure work in 
the same way as Algorithm 1.5 if the elements of A are not distinct? 
12. Take the five algorithms given in this chapter for finding the maximum and 
compare their computing times as you execute them on the data set whose 
values are in increasing order. 
13. 
Procedure SEARCH(i) in the text will work faster if we do the following: 
A(n + 1) -
x 
k - SEARCH(!) 
Show how to rewrite SEARCH so it takes advantage of the fact that x occurs 
at least once at the end of the array. 
14. Translate procedure SEARCH, Algorithm 1.4, into iterative form using the 
rules of section 1.3. First rewrite the recursive version so that there is only one 
return and then translate the result. 
15. Write a procedure which finds the mode and frequency of an unsorted array. 
Analyze its computing time. Is your method better than sorting? 

44 
lntroductlon 
16. 
Program procedure RMODEl in two different languages and run them on 
the same computer. Use a data set having all numbers distinct. Compare your 
results with Table 1.3. 
17. 
Observe that for MODE or RMODE to work properly we needn't assume the 
set is sorted, but simply clustered. Define this notion precisely and devise an 
algorithm for clustering. If possible your algorithm should not necessarily sort 
the elements. 
18. 
Devise an iterative version of mode finding which works faster than MODE 
(Algorithm 1.11). Your version will make fewer comparisons each time through 
the loop. 
19. For the following pairs of functions determine the smallest integer value of n <!:: 0 
for which the first function becomes greater than or equal to the second func-
tion. 
(i) n2, 10n 
(ii} 2n' 2n3 
(iii) n 2/log n, n(log n)2 
(iv) n 312, n2•81 
20. 
Write a recursive program which computes the binomial coefficient BINOM 
(n,m) using the recursive definition BINOM(n,m) = BINOM(n -
1,m) + 
BINOM(n - 1,m - 1) and BINOM(n,O) = BINOM(n,n) = 1. 
21. 
Compare the merits of computing binomial coefficients using the recursive 
program above with an iterative program based on factorials, BINOM(n,m) = 
n!l(m!(n - m)!). 
22. 
Prove that 1 + 2 + 3 + · · · + n = n(n + 1)/2. (Big hint: show that by 
grouping the terms as (1 + n) + (2 + n -
1) + (3 + n -
2) + · · · the 
formula holds.) 
23. 
Using your calculator (or by hand) augment Table 1.1 by adding the values 
for the following columns: log log n, n 2 log n, n 3 log n, and nn. 
24. 
Using your calculator (or by hand) extend Table 1.1 by adding rows for the 
following values of n: 64, 128, 256, 512 and 1024. Use approximation where-
ever it is necessary. 
25. 
In procedure MAX3(A,n), Algorithm 1.7, the frequency of execution of every 
statement is fixed by n except "k -
i". Determine the average number of 
times this statement is executed using the following set of hints. 

Exercises 
45 
(i) Assume the values in A(l :n) are distinct and each of the n! permutations 
are equally likely to occur. Let p(n,k) be the number of permutations 
of n which create an execution frequency of k divided by n!. 
(i) Determine the number of times that the frequency of execution of k -
i is either zero, one, or two when n = 3, namely find p(3,0), p(3,1), p(3,2). 
(ii) The average frequency is defined as the sum of kp(n,k) as k varies from 
zero to n - 1. Explain why 
p(n,k) = (lln)p(n - 1, k - 1) + ((n -
1)/n)p(n - 1,k) 
whereinitiallyp(l,k) = o(0,1), andp(n,k) = Oifk < 0. 
(iii) Let G(n,z) = p(n,0) + p(n,l)z + · · · + p(n,n -l)zn-l and G(l,z) = 
1. Using this definition and the previous formula show that 
G(n,z) = (zln)G(n -
l,z) + ((n -
1)/n)G(n - 1,z) 
= ((z + n - 1)/n)G(n - 1,z) 
(iv) Unwind the above formula to obtain 
(
z + n) 
G(n,z) = (1/(z + n))BINOM 
n 
(v) 
Now show that 
G'(n,z) = (lln)G(n - 1,z) + ((z + n - 1)/n)G'(n -
l,z) 
and 
G'(n,1) = 1/n + G'(n - 1,1) 
= .... = Hn - 1 
where H n is the nth Harmonic number, H n 
+ ... + 1/n 
1 + 1/2 + 1/3 + 1/4 
(vi) If you have gotton this far, you have no doubt observed that the average 
we are looking for is given by G'(n,1) so you are already done. 
These hints follow the derivation given by D. Knuth in Fundamental Algorithms. 

46 
Introduction 
26. 
In many cases it is advisable to compute the average time over a set of m trials 
when the input size is fixed. The test program to do this would look like 
initialize the timer 
for i -
1 to m do 
call SOLUTION(x,y) 
repeat 
average -
TIME/ m 
It would also be nice to know how close the average is to the actual readings. 
This is given by the standard deviation. Look up the definition of the standard 
deviation and show how to compute it within the above program. 
27. 
Another program for computing the mode and its frequency of a sorted set is 
procedure MODE2(A,n,mJ) 
/In> 011 
f -
O; count -
1; i -
2 
A(n + 1) - A(n) + 1 
loop 
if A(i) = A(i -
1) 
//We need to extend the array by one position/ I 
then count -
count + 1 
else if count > f 
endif 
then/ - count; m - A(i -
1); count -
1 
endif 
i - i + 1 
until i > n + 1 repeat 
endMODE2 
Devise and execute some experiments which compare the computing times of 
MODE2 with the other versions of mode finding in this chapter. 
28. 
Procedure F(n) of section 1.3 computes the nth Fibonacci number. How many 
times is F(i) computed for i = 1,2,3, · · · ,n? 
29. 
Why does procedure PROFILE (Algorithm 1.10) assume that the call to 
SOLUTION contains no input or output statements? 
30. 
Develop an algorithm which converts a Roman numeral into an Arabic integer. 
Note that I = 1, V = 5, X = 10, L = 50, C = 100, M = 1000. 
31. 
Develop an algorithm which converts a positive Arabic integer into its cor-
responding Roman numeral. 

Exercises 
47 
32. 
Design and test an algorithm which determines how long it takes your com -
puter to execute 2n, nn, and n ! additions for various values of n. Do the same 
for multiplications. 
33. 
Modify the "algorithm" of Figure 1.10 so that it handles the case when two 
competing algorithms are developed to solve the same problem. 

Chapter 2 
ELEMENTARY DATA STRUCTURES 
Now that we have presented the fundamental methods we need to express 
and analyze algorithms you might feel all set to begin. But alas we need to 
make one last diversion to which we devote this chapter, and that is a dis-
cussion of data structures. One of the basic techniques for improving algo-
rithms is to structure the data in such a way that the resulting operations 
can be efficiently carried out. Though we can't possibly survey here all of 
the techniques that are known, in this chapter we have selected several 
which we feel occur most frequently. Maybe you have already seen these 
techniques in a course on data structures (hopefully having used Fundamen-
tals of data structures). If so, you may either skip this chapter or scan it 
briefly. If you haven't been exposed to the ideas of stack, queues, sets, trees, 
graphs, heaps, or hashing then lets begin our study of algorithms right now 
with some interesting problems from the field of data structures. 
2.1 
STACKS AND QUEUES 
One of the most common forms of data organization in computer programs 
is the ordered or linear list, which is often written as A = (a i. a 2, ••• an). 
The a;s are referred to as atoms and they are chosen from some set. The 
null or empty list has n = 0 elements. A stack is an ordered list in which 
all insertions and deletions are made at one end, called the top. A queue 
is an ordered list in which all insertions take place at one end, the rear, 
while all deletions take place at the other end, the.front. 
<-Top 
Stack 
Front 
Rear 
t 
t 
A 8 
C DE 
Queue 
Figure 2.1 
Example of a stack and a queue 
48 

Stacks and Queues 
49 
The operations of a stack imply that ifthe elements A,B,C,D,E are inserted 
into a stack, in that order, then the first element to be removed/deleted 
must be E. Equivalently we say that the last element to be inserted into 
the stack will be the first to be removed. For this reason stacks are some-
times referred to as Last In First Out (LIFO) lists. The operations of a 
queue require that the first element which is inserted into the queue will be 
the first one to be removed. Thus queues are known as First In First Out 
(FIFO) lists. See Figure 2.1 for an example of a stack and a queue each 
containing the same five elements inserted in the same order. Note that the 
data object queue as defined here need not necessarily correspond to the 
concept of queue which is studied in queuing theory. 
The simplest way to represent a stack is by using a one-dimensional 
array, say STACK(l:n), where n in the maximum number of allowable en-
tries. The first or bottom element in the stack will be stored at STACK(l), 
the second at ST ACK(2) and the ith at ST ACK(i). Associated with the array 
will be a variable, typically called top, which points to the top element in 
the stack. To test if the stack is empty we ask "if top = O". If not, the top-
most element is at ST ACK(top ). Two more substantial operations are in-
serting and deleting elements. The corresponding procedures are given as 
algorithms 2.l(a) and (b). 
procedureADD(item, STACK, n, top) 
I /insert item into the ST ACK of maximum size n; top is the/ I 
I /number of elements currently in ST ACK/ I 
if top ~ n then call STACKFULL endif 
top - top + 1 
STACK(top) - item 
end ADD 
(a) Insertion of an element 
procedure DELETE(item, STACK, top) 
//remove the top element of STACK and store it// 
//in item unless STACK is empty// 
if top :5 0 then call STACKEMPTY endif 
item - STACK(top) 
top -
top - 1 
end DELETE 
(b) Deletion of an element 
Algorithm 2.1 Stacking operations 

50 
Elementary Data Structures 
Each execution of ADD or DELETE takes a constant amount of time 
and is independent of the number of elements in the stack. ST ACKFULL 
and STACKEMPTY are procedures which we leave unspecified since they 
will depend upon the particular application. Often a stack full condition 
will signal that more storage needs to be allocated and the program rerun. 
Stack empty is often a meaningful condition. 
Another way to represent a stack is by using links (or pointers). A node 
is a collection of data and link information. A stack can be represented 
by using nodes with two fields, possibly called DAT A and LINK. The data 
field of each node contains an item in the stack and the corresponding 
link field points to the node containing the next item in the stack. The link 
field of the last node is zero for we assume that all nodes have an address 
greater than zero. For example a stack with the items A,B,C,D,E inserted 
in that order, would look as in Figure 2.2. 
Stock 
A 
0 
DATA LINK 
Figure 2.2 Example of a 5 element, linked stack 
The variable STACK points to the topmost node (the last item inserted) 
in the list. The empty stack is represented by setting ST ACK = 0. Because 
of the way the links are pointing, insertion and deletion are easy to accom-
plish. For example to insert an item into the stack one should write the 
following: 
call GETNODE(T) 
DATA(T) -
item 
LINK(T) -
STACK 
STACK -
T 
Procedure GETNODE assigns to the variable T the address of an avail-
able node. If no more exist it will terminate the program. The next two 
assignments store appropriate values into the two fields of the node. Then 
the variable STACK is updated to point to the new top element of the list. 

Deletion would work as follows: 
if STACK = 0 then callSTACKEMPTYendif 
item - DATA(STACK) 
T-STACK 
STACK - LINK(STACK) 
call RETNODE(T) 
Stacks and Queues 
51 
If the stack is empty, then trying to delete an item will prodcue a call 
of the procedure STACKEMPTY. Otherwise the top element is stored as 
the value of the variable item, a pointer to the first node is saved, and 
ST ACK is moved to point to the next node. Procedure RETNODE is de-
signed to take a single node and place it into a list of available nodes for 
later use by GETNODE. 
The use of links to represent a stack requires more storage than the se-
quential array STACK(l:n). However, there is greater flexibility when 
using links, for many structures can simultaneously use the same pool of 
available space. Most importantly the times for insertion and deletion using 
either representation are a constant which is independent of the size of 
the stack. 
An efficient queue representation is obtained by taking an array, declared 
as Q(O:n -
1), and treating it as if it was circular. Elements are inserted 
by increasing the variable rear to the next free position. When rear = n - 1, 
the next element is entered at Q(O) in case that spot is free. front will 
always point one position counterclockwise from the first element in the 
queue. front = rear is and only if the queue is empty and initially we have 
front = rear = 0. Figure 2.3 illustrates two of the possible configurations 
for a circular queue containing the four elements J1-J4 with n > 4. 
(4) 
(3) 
(0) (n-1) 
front =O; rear "4 
(n-4) 
( n-3) 
(4) 
( 3) 
(0) 
(n-4) 
(n-3) 
front=n-4, rear=O 
Figure 2.3 Circular queue of capacity n containing four elements Jl,J2,J3,J4 

52 
Elementary Data Structures 
In order to insert an element, it will be necessary to move rear one posi-
tion clockwise. This can be done using the code: 
if rear = n - 1 then rear -
0 
else rear -
rear + 1 
end.if. 
A more elegant way to do this is to use the built-in modulo operator which 
computes remainders. Before doing an insert we would increase the rear 
pointer by saying rear - (rear + 1) mod n. Similarly, it will be necessary to 
movefront one posit~on clockwise each time a deletion is made. An exam-
ination of the algorithms (algorithm 2.2 (a) and (b)) indicates that by treat-
ing the array circularly, addition and deletion for queues can be carried out 
in a fixed amount of time or 0(1). 
procedure ADDQ(item, Q,n,jront,rear) 
//insert item in the circular queue stored in Q(O:n - 1);// 
I !rear points to the last item andfront is one position/ I 
I I counterclockwise from the first item in QI I 
rear -
(rear + 1) mod n 
I I advance rear clockwise/ I 
if front = rt:ar then call QUEUEFULL end.if 
Q(rear) - item 
//insert new item// 
endADDQ 
(a) Addition of an element 
procedure D ELETEQ(item, Q, n ,front, rear) 
I /removes the front element of the queue Q(O:n - 1)/ I 
I I and stores it in item .I I 
ifjront = rear then call QUEUEEMPTYendif 
front -
(front + 1) mod n //advance front clockwise// 
item -
Qifront) I !set item to front of queue// 
endDELETEQ 
(b) Deletion of an element 
Algorithm 2.2 Basic queue operations 
One surprising feature in these two algorithms is that the test for queue 
full in ADDQ and the test for queue empty in DELETEQ are the same. In 
the case of ADDQ, however, whenfront = rear there is actually one space 
-- --------------- ---------

Trees 
53 
free, Q(rear), since the first element in the queue is not at Q(jront) but is one 
position clockwise from this point. However, if we insert an item there, 
then we will not be able to distinguish between the cases full and empty, 
since this insertion would leave.front = rear. To avoid this, we signal queue 
full, thus permitting a maximum of n -
1 rather than n elements to be in 
the queue at any time. One way to use all n positions would be to use 
another variable, tag, to distinguish between the two situations, i.e. tag = 
O if and only if the queue is empty. This would however slow down the two 
algorithms. Since the ADDQ and DELETEQ algorithms will be used many 
times in any problem involving queues, the loss of one queue position will 
be more than made up for by the reduction in computing time. 
The procedures QUEUEFULL and QUEUEEMPTY have been used 
without explanation, but they are similar to STACKFULL and STACK-
EMPTY. Their function will depend on the particular application. 
Another way to represent a queue would be by using links. Figure 2.4 
shows a queue with the four elements A,B,C,D, entered in that order. 
DATA 
LINK 
I A I I 
_, 
B 
·I c 
~ D I 0 I 
J 
~ 
\ 
I 
front 
rear 
Figure 2.4 A linked queue with 4 elements 
As with the linked stack example each node of the queue is composed of 
the two fields DAT A and LINK. A queue is pointed at by two variables, 
front and rear. Deletions are made at the front and insertions at the rear. 
When front = 0 that signals an empty queue. Again, when using linked 
allocation one assumes the existence of procedures GETNODE and RET-
NODE which operate as they did for stacks. The procedures for insertion 
and deletion of linked queues are left as exercises. 
2.2 TREES 
Defmition A tree is a finite set of one or more nodes such that (i) there is 
a specially designated node called the root; (ii) the remaining nodes are 
partitioned into n ~ 0 disjoint sets Tl, ... , Tn where each of these sets is 
a tree. Tl, ... , Tn are called the subtrees of the root. 
There are many terms which are often used when referring to trees. Con-
sider the tree in Figure 2.5. This tree has 13 nodes, each data item of a 
node being a single letter for convenience. The root contains A, (though 

54 
Elementary Data Structures 
we will usually say node A), and we will normally draw trees with their root 
at the top. The number of subtrees of a node is called its degree. The degree 
of A is 3, of C is 1, and of F is 0. Nodes that have degree zero are called 
leaf or terminal nodes. The set {K, L, F, G, M, I, J} is the set of leaf nodes 
of Figure 2.5. The other nodes are referred to as nonterminals. The roots 
of the subtrees of a node, X, are the children of X. Xis the parent of its 
children. Thus the children of D are H, I, J; the parent of D is A. 
LEVEL 
2 
F 
J 
3 
K 
L 
4 
Figure 2.5 A sample tree 
Children of the same parent are said to be siblings. For example H, I, and 
J are siblings. We can extend this terminology if we need to so that we can 
ask for the grandparent of M which is D, etc. The degree of a tree is the 
maximum degree of the nodes in the tree. The tree in Figure 2.5 has degree 
3. The ancestors of a node are all the nodes along the path from the root to 
that node. The ancestors of Mare A, D and H. 
The level of a node is defined by initially letting the root be at level one. 
If a node is at level p, then its children are at level p + 1. Figure 2 .5 shows 
the levels of all nodes in that tree. The height or depth of a tree is defined 
to be the maximum level of any node in the tree. 
A forest is a set of n ~ 0 disjoint trees. The notion of a forest is very 
close to that of a tree because if we remove the root of a tree we get a forest. 
For example, in Figure 2.5 if we remove A we get a forest with three trees. 
Now how do we represent a tree in a computer's memory? If we wish to 
use linked lists where one node in the list corresponds to one node in the 
tree, then a node must have a varying number of fields depending upon 
the number of branches. However it is often simpler to write algorithms for 
a data representation where the node size is fixed. We can represent a 
tree using a fixed node size list structure. Such a list representation for 
the tree of Figure 2.5 is given in Figure 2.6. In this figure nodes have three 

~ 
@@G--lolGlol 
~ 
Figure 2.6 List representation for the tree of Figure 2.5 
'"' i 
Ill 
Ill 

56 
Elementary Data Structures 
fields: TAG, DATA, and LINK. DATA and LINK are used as before with 
the exception that when TAG = 1, DATA contains a pointer to a list 
rather than a data item. A tree is represented by storing the root in the first 
node followed by nodes which point to sublists and contain each subtree of 
the root. 
Binary Trees 
A binary tree is an important type of tree structure which occurs very 
often. It is characterized by the fact that any node can have at most two 
children, i.e. there is no node with degree greater than two. For binary trees 
we distinguish between the subtree on the left and on the right, whereas 
for trees the order of the subtrees was irrelevant. Furthermore a binary 
tree is allowed to have zero nodes while a tree must have at least one node. 
Thus a binary tree tree is really a different object than a tree. 
Deftnition: 
A binary tree is a finite set of nodes which is either empty or 
consists of a root and two disjoint binary trees called the left and right sub-
trees. 
Figure 2. 7 shows two sample binary trees. These two trees are special 
kinds of binary trees. The first is a skewed tree, skewed to the left and there 
is a corresponding one which skews to the right. Tree 2. 7(b) is called a 
complete binary tree. This kind of tree will be defined formally later on. 
Notice that for this tree all terminal nodes are on two adjacent levels. The 
terms that we introduced for trees such as: degree, level, height, leaf, par-
ent, and child all apply to binary trees in the natural way. 
LEVEL 
A 
A 
I 
B 
c 
2 
c 
F 
G 
3 
D 
H 
I 
4 
E 
5 
(al 
(bl 
Figure 2.7 Two sample binary trees 
---
-
- ----
~----- -----

Trees 
57 
lemma 2.1 
The maximum number of nodes on level i of a binary tree is 
2;- 1• Also the maximum number of nodes in a binary tree of depth k is 
2k - 1, k > 0. 
D 
The binary tree of depth k which has exactly 2k -
1 nodes is called a 
full binary tree of depth k. Figure 2.8 shows a full binary tree of depth 4. 
A very elegant sequential representation for full binary trees results from 
sequentially numbering the nodes, starting with the node on level one, then 
going to those on level two and so on. Nodes on any level are numbered 
from left to right (see Figure 2.8). A binary tree with n nodes and of depth 
k is complete iff its nodes correspoµd to the nodes which are numbered one 
to n in the full binary tree of depth k. A consequence of this definition is 
that in a complete tree, leaf nodes occur on at most two adjacent levels. 
The nodes of a complete tree may be compactly stored in a one dimensional 
array, TREE, with the node numbered i being stored in TREE(i). Thenext 
lemma shows us how to easily determine the locations of the parent, left 
child and right child of any node i in the binary tree without explicitly 
storing any link information. 
2 
3 
Figure 2.8 Full binary tree of depth 4 
lemma 2.2 If a complete binary tree with n nodes is represented sequen-
tially as described before then for any node with index i, 1 ::;; i ::;; n we 
have: 
(i) 
PARENT(i) is at Li/2j if i ~ 1. When i = 1, i is the root and has 
no parent. 

58 
Elementary Data Structures 
(ii) LCHILD(i) is at ]i if 2i s n. If 2i > n then i has no left child. 
(iii) 
RCHILD(i) is at 1i + 1 if 2i + 1 s n. If ]i + 1 > n then i has no 
right child. 
This representation can clearly be used for all binary trees though in 
most cases there will be a lot of unutilized space. For complete binary trees 
the representation is ideal as no space is wasted. For the skewed tree of 
Figure 2.7, however, less than a third of the array is utilized. In the worst 
case a right skewed tree of depth k will require 2 k -
1 locations. Of these 
only k will be occupied. 
(I) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
TREE 
A 
B 
c 
D 
(16) 
E 
TREE 
A 
B 
c 
D 
E 
F 
G 
H 
Figure 2.9 Sequential representation of the binary trees of Figure 2. 7 
While the sequential representation, as in Figure 2.9, appears to be good 
for complete binary trees it is wasteful for many other binary trees. In ad-
dition, the representation suffers from the general inadequacies of sequential 
representations. Insertion or deletion of nodes requires the movement of 
potentially many nodes to reflect the change in level number of the remain-
ing nodes. These problems can be easily overcome through the use of a 
linked representation. Each node will have three fields LCHILD, DATA, 
and RCHILD. While this node structure will make it difficult to determine 
the parent of a node, we shall see that for most applications it is adequate. 
In case it is often necessary to be able to determine the parent of a node, 
then a fourth field, PARENT, may be included with the obvious interpre-

Trees 
59 
tation. The representation of the binary trees of Figure 2. 7 using a three 
field structure is given in Figure 2.10. 
(a) 
( b) 
Figure 2.10 Linked representation for the binary trees of Figure 2. 7 
As an example of the use of binary trees suppose we wish to maintain a 
table which contains a subset of the reserved words of SPARKS. This would 
be called a symbol table, and it could be used by a compiler which translates 
SPARKS programs into some other, more primitive language (see Appendix 
A for more details). We will select 13 SPARKS reserved words and store 
them into the character array NAME(1:13): 
NAME: 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
case 
do 
else 
end 
endcase 
endif if 
NAME: 
(8) 
(9) 
(10) 
(11) 
(12) 
(13) 
loop 
procedure 
repeat 
return 
then while 
A binary tree will be used to help us search if a particular character string, 
X, is actually one of these reserved words. We insist that the binary tree be 
constructed in such a way that the data associated with any node P is both 

60 
Elementary Data Structures 
(i) alphabetically greater than the data in the nodes contained in the left 
subtree of P, and (ii) alphabetically less than the data in the nodes con-
tained in the right subtree of P. This must be true of all nodes in the binary 
tree. If so we refer to this data object as a binary search tree. Figure 2.11 
is one binary search tree for the data contained in NAME. 
The actual binary tree which is represented in our computer will not have 
these reserved words in the DATA field as shown in Figure 2.11, since the 
reserved words are of varying size while the node size is fixed. Rather 
DAT A(i) will be the index into array NAME of the reserved word of the 
ith node, The values in the LCHILD and RCHILD fields are also indices 
(or addresses) and they indicate the position of a node in the array (or in 
the memory). The actual representation of this tree in terms of arrays is 
given in Table 2.1. The linked representation shown in Table 2.1 requires 
13*3 = 39 locations for the binary tree. In general a binary tree with n 
nodes will require 3n locations. 
LC HILD 
DATA 
RC HILD 
(1) 
2 
7 
3 
(2) 
4 
3 
5 
(3) 
6 
10 
7 
(4) 
0 
1 
8 
(5) 
9 
5 
10 
(6) 
0 
8 
11 
(7) 
12 
12 
13 
(8) 
0 
2 
0 
(9) 
0 
4 
0 
(10) 
0 
6 
0 
(11) 
0 
9 
0 
(12) 
0 
11 
0 
(13) 
0 
13 
0 
Table 2.1 
Array representation of Figure 2.12 
It is easy to imagine how an algorithm would use a binary search tree to 
help it find out if X is present or not. This is asked for in the exercises. 
Sections 3.2 and 5.4 present more facts on this data structure. 
There is a natural generalization of binary trees to the concept of k-ary 
trees, k ~ 2. A node in a k-ary tree may have at most k children and these 
children are ordered. The sequential representation for binary trees can be 
extended to k-ary trees or a linked scheme using fixed size nodes can be 
used. 
Let us return to the data structure tree where the subtrees are unordered 
and vary in number. We have a tree T with root T 1 and subtrees T 11, T 12, 

Heaps and Heapsort 
61 
Figure 2.11 
A binary search tree 
... , T 1ko One way to represent this structure is to transform it into a binary 
tree. This is done by making T 1 the root of a binary tree, T 11 the left child 
and then T 1; becomes the right child of T 1,; -1 for 2 :s i :s k. Pictorially 
this looks like Figure 2.12. The virtue of this idea is that binary tree repre-
sentations are simpler to process than using linked allocation with variable 
size nodes. Therefore this transformation may often result in less space and 
simpler algorithms. 
We expect that most of our readers have already encountered the mate-
rial in sections 2.1 and 2.2 and therefore they may have skimmed these 
sections. The next sections may offer new material for you and if so you 
are cautioned to now slow down and read more closely. 
2.3 HEAPS AND BEAPSORT 
In this section we study a way of structuring data which permits one to 
insert elements into a set and also to find the largest element efficiently. 
A data structure which provides for these two operations is called a priority 
queue. Many algorithms need to make use of priority queues and so an 
efficient way to implement these operations will be very useful. 
We might first consider using a queue since inserting new elements 
would be very efficient. But finding the largest element would necessitate 
a scan of the entire queue. A second suggestion would be to use a sorted 
list which is stored sequentially. But an insertion could require moving all 
of the items in the list. What we want is a data structure which allows both 
operations to be done efficiently. 

62 
Elementary Data Structures 
( i) The general case 
A 
(ii) An example 
J 
Figure 2.12 Transforming a tree into a binary tree 
Deftnition: 
A heap is a complete binary tree with the property that the 
value at each node is at least as large as the values at its children (if they 
exist). 
This definition implies that a largest element is at the root of the heap. 
If the elements are distinct, then the root contains the largest item. The 
relation greater than or equal to may be reversed so that the parent node 
contains a value as small as or smaller than its children. In this case the 
root contains the smallest element. But clinging to historical tradition we 
will assume that the larger values are closer to the root. 
It is possible to take any binary tree containing values for which an or-
-
- - - - - - - - - - - - - -

Heaps and Heapsort 
63 
dering exists and move these values around so that the shape of the tree is 
preserved and the heap property is satisfied, see Figure 2.13. However, it 
is more often the case that we are given n items, say n integers and we are 
free to choose whatever shape binary tree seems most desirable. In this case 
the complete binary tree is chosen and represented sequentially, see Figure 
2.14. This is why in the definition of heap we insist that a complete binary 
tree is used. 
100 
118 
132 
112 
151 
132 
100 
Figure 2.13 A binary tree and a heap that preserves the tree's shape 
Now let us consider how to form a heap given n integers stored inA(l:n). 
One strategy is to determine how to insert one element at a time into an 
already existing heap. If we can do this then we can apply the algorithm n 
times, first inserting one element into an empty heap and continuing in that 
way until all n elements have been inserted. The solution is simple, one 
adds a new item "at the bottom" of the heap and then compares it with 
its parent, grandparent, greatgrandparent, etc. until it is less than or equal 
to one of these values. Procedure INSERT, Algorithm 2.3 describes this 
process in full detail. 
procedure INSERT(A, n) 
//inserts the value in A(n) into the heap which is stored/ I 
llatA(l)toA(n -
1)// 
integer i,j, n, ; 
j -
n; i -
Ln/2j; item -A(n) 
while i > 0 andA(i) < item do 
A(i) - A(i) 
//move the parent down// 
j -
i; i -
Li/2 J 
//the parent of A(i) is at A( Li/2 J )// 
repeat 
A(i) -
item 
I la place for A(n) is found// 
end INSERT 
Algorithm 2.3 Heap creation by inserting one item at a time 

64 
Elementary Data Structures 
Figure 2.14 shows one example of how INSERT would insert a new value 
into an existing heap of six elements. It is clear from the program and 
the figure that the time for INSERT can vary. In the best case the new 
element is correctly positioned initially and no values need be rearranged. 
In the worst case the number of executions of the while loop is proportional 
to the number of levels in the heap. 
-
0 
Figure 2.14 Action of INSERT inserting 90 as the seventh item into an existing heap 
n items in A(l:n) may be set up as a heap (which is also a complete bi-
nary tree) by the program segment 
for i -
2 to n do 
call INSERT(A,i) 
repeat 
Figure 2.15 shows how the data (40, 80, 35, 90, 45, SO, 70) is moved around 
until a heap is created. Trees in the left column represent the state of the 
array A(l:i) before each call of INSERT. Trees in the right column show 
how the array was altered by INSERT to produce a heap. The array is drawn 
as a complete binary tree for clarity. 

Heaps and Heapsort 
65 
Figure 2.15 Forming a heap from the set (40,80,35,90,45,50,70) 
Wont Case Analysis 
The data set which causes the heap creation method using INSERT to 
behave in the worst way is when the elements are inserted in ascending or-
der. Each new element will rise to become the new root. 

66 
Elementary Data Structures 
There are at most 2i-I nodes on level i of a complete binary tree, 1 :s; i :s; 
Jlog2(n + 1)1 . For a node on level i the distance to the root is i - 1. Thus 
the worst case time for heap creation using INSERT is 
t"' 
flog,(11+l)j 
~ (i -
1) 2i-I < Jlog2(n + 1)1 2 
= O(n log n) 
I :si:s pog, (n + Ol 
(2.1) 
Testing the Average Case 
A surprising fact about INSERT is that its average behavior on n random 
inputs is asymptotically faster than its worst case, O(n) rather than 
O(n log n). This implies that on the average each new value only rises a con-
stant number of levels in the tree. It is quite complex to prove that INSERT 
does have this behavior, and so we will not present the proof here. But in-
stead we will devise some average case test data to see if we can exhibit this 
behavior. 
To test this conjecture the array A(l:n) was filled with n random integers 
for n = 30 to 300 in increments of 30. In each case the average number of 
moves of data in the tree per call of INSERT over 100 trials was computed. 
Table 2.2 gives the results. One can see that the distance an element moved 
up in the tree was always less than 2. 
n: 
30 
60 
90 
120 
150 
180 
210 
240 
270 
300 
moves/insert: 
1.07 1.08 1.07 1.16 1.28 1.36 1.33 1. 76 1.28 1.52 
Table 2.2 Action of INSERT on random values 
There is another algorithm for creating a heap which has the nice prop-
erty that its worst case time is an order of magnitude faster than n - 1 calls 
of INSERT. This reduction is achieved by an algorithm which regards 
A(l:n) as a complete binary tree and works from the leaves up to the root, 
level by level. At each level, it will be the case that the left and right sub-
trees of any node are heaps. Only the value in the root node may violate 
the heap property. Hence it is sufficient to devise a method which converts 
a binary tree in which only the root may violate the heap property into a 
heap. Procedure ADJUST (Algorithm 2.4) does this for any binary tree 
whose root is at location i. The algorithm assumes that this binary tree is 
a subtree of a binary tree represented sequentially as discussed earlier. 

Heaps and Heapsort 
67 
procedureADJUST(A, i, n) 
I /The complete binary trees with roots A(2*i) and A(2*i + 1) are/ I 
I /combined with A(i) to form a single heap, 1 :5 i :5 n.I I 
I !No node has an address greater than n or less than 1/ I 
integer i,j, n; 
j -
2 * i; item - A(i) 
while j :s; n do 
ifj < n andA(j) < A(j + 1) 
//compare left and right child// 
thenj - j + 1 
I /j points to the larger child/ I 
endif 
if item :2:: A(j) 
then exit I I a position for item is found/ I 
else A ( Lj /2 J ) - A (j) 
I I move the larger child up a level/ I 
j -
2 *j 
end if 
repeat 
A( Lj/2 J ) - item 
end ADJUST 
Algorithm 2.4 Combining two heaps into a single heap 
Given n elements in A(l:n) we can create a heap by applying ADJUST. 
It is easy to see that leaf nodes are already heaps. So we may begin by call-
ing ADJUST for the parents of leaf nodes and then work our way up, level 
by level, until the root is reached. In Figure 2.16 we observe the action of 
HEAPIFY as it creates a heap out of the given seven elements. The initial 
tree is drawn in Figure 2.16(i). Since n = 7 the first call to ADJUST has 
i = 3. In Figure 2.16(ii) the three elements 118, 151, 132 are rearranged 
to form a heap. Subsequently ADJUST is called with i = 2 and i = 1 
yielding the trees in Figure 2.16(iii) and (iv). 
procedure HEAPIFY(A. ,n) 
I /Readjust the elements in A(l:n) to form a heap// 
integer n,i 
for i - L n/2 J to 1 by -1 do 
callADJUST(A., i, n) 
repeat 
endHEAPIFY 
Algorithm 2.5 Creating a heap out of n arbitrary elements 

I 
~ 
68 
Elementary Data Structures 
( il 
(ii) 
(iii) 
(iv) 
Figure2.16 ActionofHEAPIFY(A, 7)onthedataof(100, 119, 118, 171, 112, 
151, 132) 
For the worst case analysis of HEAPIFY let 2k-I s; n < 2k where k = 
Jlog(n + 1) l and recall that the levels of the n node complete binary 
tree are numbered 1 to k. The worst case number of iterations for ADJUST 
is k - i for a node on level i. Therefore the total time for HEAPIFY is pro-
portional to 
E 2i-l(k -
i) 
E 
i 2k-i- 1 s; n 
E 
i/2; < 2n = O(n) 
(2.2) 
l:si:sk 
l:si:sk-1 
t:si:sk-1 
Comparing HEAPIFY with repeated use of INSERT we see that the for-
mer is faster in the worst case, requiring O(n) versus O(n log n) operations. 
However, HEAPIFY requires that all of the elements be availal'-le before 
heap creation begins. Using INSERT we can add a new element into the 
heap at any time. 
In order to use a heap as a priority queue it is necessary to be able to 
insert or delete the largest element at any time. A simple way to accomplish 
------- --
-
------··· 

Heaps and Heapsort 
69 
this deletion is to remove the element at the root, A(l), (this is the largest 
element), and then move the element A(n) to A(l). Now we have a binary 
tree in which only the root may violate the heap property so ADJUST may 
be used to rt~create a heap. To insert elements we use procedure INSERT. 
Using the results of the analysis of ADJUST and INSERT we observe that 
both insertion into and deletion from a priority queue take O(log n) time. 
We have discussed a heap as a data structure with the property that the 
value in every node is at least as large as the values in the children nodes. 
It should be easy to see that a parallel discussion could have been carried 
out with a definition requiring the value in every node to be at least as 
small as the values in the children nodes. In this case it is possible to delete 
the smallest element in O(log n) time and also to insert an element in 
O(log n) time. Later when we use heaps, we will refer to these two cases 
as max and min heaps respectively. 
Heapsort 
The most well known example of the use of a heap arises in its applica-
tion to sorting. A conceptually simple sorting strategy is one which contin-
ually removes the maximum value from the remaining unsorted elements. 
A straightforward implementation of this idea leads to an algorithm whose 
worst case time is O(n 2). A heap allows the maximum element to be found 
and deleted in O(log n) time thus yielding a sorting method whose worst 
case time is O(n log n ). 
procedure HEAPSORT(A,n) 
I IA(l:n) contains n elements to be sorted./ I 
//HEAPSORT rearranges them in-place into nondecreasing order./ I 
I /first transform the elements into a heap/ I 
call HEAPIFY(A ,n) 
I /interchange the new maximum with the element at the/ I 
I I end of the tree/ I 
for i - n to 2 by - 1 do 
t - A(i); A(i) - A(l); A(J) -
t 
callADJUST(A, 1, i -
1) 
repeat 
end HEAPSORT 
Algorithm 2.6 Heapsort 

70 
Elementary Data Stmctures 
Though the call of HEAPIFY requires only O(n) operations, ADJUST 
possibly requires O(log n) operations for each invocation. Thus the worst 
case time is O(n log n). Notice that the storage requirements, besidesA(l:n) 
are only for a few simple variables. 
Final Comments on Priority Queues 
There are many other applications of priority queues besides sorting. 
For example, simulation programming languages are usually organized 
around an "event list" which is a summary of actions which must be per-
formed at different instants of simulated time. This event list is treated as 
a priority queue since new events with arbitrary times are inserted into this 
list and the next event to be deleted is the one with the earliest time. 
Another application of priority queues is for job scheduling according to a 
priority system. Jobs with priorities attached enter the system, which is 
continually looking for jobs to execute. The next job chosen is one with 
the largest priority. 
There are many other ways to represent priority queues besides heaps. 
But their complete presentation is beyond our scope. Historically a sorted 
linear linked list was the structure which was originally used to implement 
event lists. For this representation deletion reduces to removing the front 
element, while insertion is done by scanning the list until the proper posi-
tion is found. An additional property which is easily achieved by this repre-
sentation is the ability to treat events with equal times on a first-in-first-
out basis. 
Insertion into sorted lists can be speeded up by using the balanced tree 
idea of Adel'son-Velskii and Landis (A VL trees). Both insertion and dele-
tion can now be done in O(log n) steps, given n items in the tree. Unfortu-
nately the algorithms are quite complex. Other structures which can be used 
for priority queues are leftist trees, 2-3 trees, p-trees and binomial queues. 
More details about all of these structures can be found in the references. 
2.4 SETS AND DISJOINT SET UNION 
Suppose we have some finite universe of n elements, U, out of which 
sets will be constructed. These sets may be empty or contain any subset of 
the elements of U. A common way to represent such sets is to allocate a bit 
vector of length n, SET(l:n), such that SET(i) = 1 if the ith element of U 

Sets and Disjoint Set Union 
71 
is in this set and zero otherwise. This array is called the characteristic vector 
for the set. 
The advantage of this representation is that one can quickly determine 
whether or not any particular element i is present. Operations such as com-
puting the union and intersection of two sets can be carried out using the 
"logical-and" and "logical-or" operations of your computer. This is espe-
cially efficient when n is "small", as each operation can be done by a single 
machine instruction. The disadvantage of this representation is that it is 
inefficient when n is large (say larger than the number of bits in one word) 
and the size of each set is small relative to n. The time to perform a union 
or an intersection is proportional to n rather than to the number of elements 
in the two sets. 
An alternative representation for sets is to represent each set by a list 
of its elements. If there exists an ordering relation for these elements, then 
operations such as union and intersection can be done in time proportional 
to the sum of the lengths of the two sets. 
In this section we study the use of trees for the representation of sets. 
We shall assume that the elements of the sets are the numbers 1, 2, 3, ... , 
n. These numbers might, in practice, be indices into a symbol table where 
the actual names of the elements are stored. We shall assume that the sets 
being represented are pairwise disjoint; i.e. if S; and Sh i ;it. j, are two sets 
then there is no element which is in both S; and S j· For example, if we have 
10 elements numbered 1 through 10, they may be partitioned into three 
disjoint sets Si= {l, 7, 8, 9}; S 2 = {2, 5, 10} and SJ ={3, 4, 6}. 
The operations we wish to perform on these sets are: 
(a) Disjoint set union ... if S; and Sj are two disjoint sets, then their 
union S; U S j = 
{ all elements x such that x is in S; or S j}. Thus, 
S 1 U S 2 = {1, 7, 8, 9, 2, 5, 10}. Since we have assumed that all 
sets are disjoint, following the union of S; and Sj we can assume 
that the sets S; and Sj no longer exist independently, i.e. they are 
replaced by S; U Sj in the collection of sets. 
(b) Find (i) ... find the set containing element i. Thus, 4 is in set SJ 
and 9 is in set S 1. 
The challenge is to devise a data representation for disjoint sets such 
that these two operations can be carried out efficiently. The best we could 
hope for is to develop two algorithms whose times are both a constant, and 
so independent of the number of items in the sets. But we shall see that we 

' 
~ 
72 
Elementary Data Stmctures 
will be unable to do that well. The sets will be represented by trees. One 
possible representation for the sets Si, S 2 and S 3 is given in Figure 2.17: 
Figure 2.17 Representing disjoint sets by trees 
Note that the nodes are linked on the parent relationship, i.e. each node 
other than the root is linked to its parent. The advantage of this will be-
come apparent when we present the UNION and FIND algorithms. First, 
to take the union of S 1 and S 2 we simply make one of the trees a subtree 
of the other. S 1 U S 2 could then have one of the representations in Figure 
2.18: 
7 
5 
10 
Figure 2.18 The two tree representations of Sl U S2 
In order to find the union of two sets, all that has to be done is to set the 
parent field of one of the roots to the other root. In presenting the UNION 
and FIND algorithms we shall identify sets by the index of the roots of the 
trees. This will simplify the discussion. The transition to set names is easy 
and as we shall see, in many applications the set name is just the index 
of the root. The operation of FIND(i) now becomes: determine the root of 
the tree containing element i. UNION(i, j) requires two trees with roots i 
andj to be joined. We shall assume that the nodes in the trees are numbered 
1 through n so that the node index corresponds to the element index. Thus, 
element 6 is represented by the node with index 6. Consequently, each node 
needs only one field, the PARENT field to link to its parent. Root nodes 
have a PARENT field of zero. Based on the above discussion, our first 
attempt at arriving at UNION, FIND algorithms would result in the pro-
cedures U and F in Algorithm 2. 7. 
-
-
--
----~ 

Sets and Disjoint Set Union 
73 
procedure U(i, j) 
I /replace the disjoint sets with roots i andj, i ;it. j, by their union/ I 
integer i,j 
PARENT(i) - j 
endU 
procedure F(i) 
I /find the root of the tree containing element ii I 
integer i,j 
j - i 
while PARENT(j) > 0 do //PARENT(j) = 0 if this node is a root/ I 
j -
PARENT(j) 
repeat 
retum(j) 
endF 
Algorithm 2. 7 Simple union and find algorithms 
While these two algorithms are very easy to state, their performance 
characteristics are not very good. For instance, if we start off with n ele-
ments each in a set of its own, i.e. S; = {i}, 1 :::::; i :::::; n, then the initial 
configuration consists of a forest with n nodes and PARENT(i) = 0, 1 :::::; i 
~n. Now imagine that we process the following sequences of UNION-FIND 
operations: 
U(l, 2), F(l), U(2, 3), F(l), U(3, 4) 
F(l), U(4, 5), ... , F(l), U(n -
1, n) 
This sequence results in the degenerate tree of Figure 2.19: 
; 
• • • 
cb 
Figure 2.19 A worst case tree 

I .. 
74 
Elementary Data Structures 
Since the time taken for a union is constant, all the n -
1 unions can 
be processed in time O(n). However, each FIND requires following a chain 
of PARENT links from node 1 to the root. The time required to process a 
FIND for an element at level i of a tree is O(i). Hence, the total time needed 
to process the n - 2 finds is O(n2). It is easy to see that this example repre-
sents the worst case behavior of the UNION-FIND algorithms. We can do 
much better if care is taken to avoid the creation of degenerate trees. In 
order to accomplish this we shall make use of a Weighting Rule for UNION 
(i j). If the number of nodes in tree i is less than the number in tree j, then 
make j the parent of i, otherwise make i the parent of j. Using this rule on 
the sequence of set unions given before we obtain the trees in Figure 2.20. 
Remember that the arguments of UNION must both be roots. Now the 
time required to process all the n finds is only O(n) since in this case the 
maximum level of any node is 2. This however, is not the worst case. In 
lemma 2.3 we show that using the weighting rule, the maximum level for 
any node after any sequence of n union and find operations is L log n J + 1. 
CD® ···8 
initially 
UNION(l,2) 
UNION (2,3) 
~··0 ••• 
UNION(FIND(3), 4) 
UNION (FIND (n-1 ), n l 
Figure 2.20 Trees obtained using the weighting rule 
First, let us implement the weighting rule. We need to know how many 
nodes there are in any tree. To do this easily, we maintain a count field 
in the root of every tree. If i is a root node, then COUNT(i) = number of 
nodes in that tree. The count can be maintained in the PARENT field as 
a negative number. This is equivalent to using a one bit field to distinguish 
a count from a pointer. No confusion is created as for all other nodes the 
PARENT is positive. 

Sets and Disjoint Set Union 
75 
procedure UNION(i j) 
I !union sets with roots i andj, i ;it. j, using the weighting rule./ I 
//PARENT(i) = -COUNT(i) and PARENT(;) = - COUNT(j)./ I 
integer i,j,x 
x -
PARENT(i) + PARENT(j) 
if. PARENT(i) > PARENT(j) 
then PARENT(i) - j 
I Ii has fewer nodes// 
PARENT(j) - x 
else PARENT(j) -
i 
I I j has fewer nodes/ I 
PARENT(i) - x 
endif 
end UNION 
Algorithm 2.8 A more sophisticated union algorithm 
The time required to perform a union has increased somewhat but is 
still bounded by a constant. The FIND algorithm remains unchanged. The 
maximum time to perform a find is now determined by lemma 2.3. 
Lemma 2.3 Let T be a tree with n nodes created as a result of algorithm 
UNION. No node in T has level greater Llog nj + 1. 
Proof: The lemma is clearly true for n = 1. Assume it is true for all trees 
with i nodes, i :5 n -
1. We shall show that it is also true for i = n. Let 
T be a tree with n nodes created by the UNION algorithm. Consider the 
last union operation performed, UNION(k j). Let m be the number of nodes 
in tree j and n -
m the number in k. Without loss of generality we may 
assume 1 :5 m :5 n/2. Then the maximum level of any node in Tis either 
the same as that in k or is one more than that inj. If the former is the case, 
then the maximum level in T is :5 L log(n -
m) J + 1 :5 L log n J + 1. 
If the latter is the case then the maximum level in T is :5 L log m J + 2 :5 
Llog(n/2)j + 2 :5 Llog nj + 1. 
D 
Example 2.1 shows that the bound of lemma 2.3 is achievable for some 
sequence of unions. 
Example 2.1: 
Consider the behavior of algorithm UNION on the following 
sequence of unions starting from the initial configuration PARENT(i) 
- COUNT(i) = 
- 1, 1 :s; i :s; n = 23 
UNION(l, 2), UNION(3, 4), UNION(S, 6), UNION(7, 8) 
UNION(l, 3), UNION(S, 7), UNION(l, 5) 

76 
Elementary Data Stmctures 
The trees of Figure 2.21 are obtained. This example is easily generalized to 
obtain m node trees with L log m J + 1 nodes. 
D 
As a result of lemma 2.3, the maximum time to process a find is at most 
O(log n) if there are n elements in a tree. If an intermixed sequence of n 
UNION and m FIND operations is to be processed, then the worst case 
time becomes O(m log n). Surprisingly, further improvement is possible. 
This time the modification will be made in the FIND algorithm using the 
Collapsing Rule: If j is a node on the path from i to its root then set 
PARENT(j) -
root (i). The new algorithm is procedure FIND(Algorithm 
2.9): 
procedure FIND(i) 
I !Find the root of the tree containing element i. Use the/ I 
//collapsing rule to collapse all nodes from i to the rootj// 
j - i 
while PARENT(j) > 0 do I /find root/ I 
j - PARENT(j) 
repeat 
k - i 
while k ;it. j do //collapse nodes from i to rootj// 
t -
PARENT(k) 
PARENT(k) - j 
k -
t 
repeat 
retum(j) 
end FIND 
Algorithm 2.9 FIND using the collapsing rule 
This modification roughly doubles the time for an individual find. There-
fore one has to be very careful about claiming it is an improvement. For 
some applications, (e.g. when a lot of finds and few unions occur) this 
change to FIND may slow down the overall processing time. But in the 
worst case one can show that this change is a considerable improvement 
over just using the weighting rule. 
Example 2.2: Consider the tree created by algorithm UNION on the se-
quence of unions of example 2.1. Now process the following 8 finds: 
FIND(8), FIND(8), FIND(8), FIND(8) 
FIND(8), FIND(8), FIND(8), FIND(8) 

Sets and Disjoint Set Union 
77 
CD ® ® ® ® ® 0 ® 
initially 
I I I I 
UNION ( 1,2) 
UNION (3,4) 
UNION (5,6) 
UNION (7, 8) 
5 
UNION (I, 3) 
2 
6 
UNION (5,7) 
Figure 2.21 
A worst case tree using the weighting rule 
Using the old version of find, namely procedure F(8), requires going up 3 
parent link fields for a total of 24 moves to process all 8 finds. In algorithm 
FIND, the first FIND(8) requires going up 3 links and then resetting 3 
links. Each of remaining 7 finds requires going up only 1 link field. The 
total cost is now only 13 moves. 
D 
The worst case behavior of the UNION-FIND algorithms while processing 
a sequence of unions and finds is stated in Lemma 2.4. Before stating this 
lemma, let us introduce a very slowly growing function o:(m, n) which is 
related to a functional inverse of Ackermann's function A(p, q) with which 
you may already be familiar. We have the following definition for o:(m, n): 
o:(m, n) = min {z ~ 1 IA(z, 4 I min l) > log2n} 

• 
78 
Elementary Data Stmctures 
The definition of Ackermann's function used here is: 
2q p = 0 
0, 
q=Oandp~l 
A(p, q) = 
2 p ~ 1 and q = 1 
A(p -
1, A(p, q -
1)) p ~ 1 and q ~ 2 
The function A(p,q) is a very rapidly growing function. One may prove 
the following three facts: 
(a) A(p,q + 1) > A(p,q) 
(b) A(p + l,q) > A(p,q) 
(c) A(3,4) = 22/} 65, 536 two's 
If we assume m ;it. 0 then (a) and (b) together with the definition of a(m,n) 
imply that a (m,n) =::: 3 for log n < A(3,4). But from (c), A(3,4) is a very 
large number indeed! For all practical purposes we may assume log n < 
A(3,4) and hence a(m, n) =::: 3. In Lemma 2.4 n -
1 will be the number 
of UNIONs performed. 
Lemma 2.4: [Tarjan] Let T(m, n) be the worst case time required to pro-
cess an intermixed sequence of m ~ n FINDs and n -
1 UNIONs. Then 
k1ma(m,n) =::: T(m,n) =::: kzma(m,n) for some positive constants k1 and 
kz. 
D 
For a proof of this theorem see the paper by Tarjan, "Efficiency of a good 
but not linear set union algorithm,"JACM, (April 1975). 
Even though the function a(m, n) is a very slowly growing function, the 
complexity of UNION-FIND is not linear in m, the number of FINDs. As 
far as the space requirements are concerned, the space needed is one node 
for each element. 
Let us look briefly at an application of algorithms UNION and FIND; 
processing equivalence statements. The input is a set of pairs of the form 
i = j (i is equivalent toj). The goal is to be able to respond quickly to either 
new pairs or to questions which ask which equivalence class an element 
is currently in. This problem is an abstraction of what would have to be 
done to handle EQUIV ALEN CE statements in FORTRAN. The equivalence 
-------- -
-
---

Graphs 
79 
classes to be generated may be regarded as sets. These sets are disjoint as 
no variable can be in more than one equivalence class. To begin with all 
n variables are in equivalence classes of their own; thus PARENT(i) = -1, 
1 :::::; i :::::; n. If an equivalence pair, i = j, is to be processed, we must first 
determine the sets containing i and j. If these are different, then the two 
sets are to be replaced by their union. If the two sets are the same, then 
nothing is to be done as the relation i = j is redundant; i andj are already 
in the same equivalence class. To process each equivalence pair we need 
to perform at most two finds and one union. Thus, if we haven variables 
and m ~ n equivalence pairs, the total processing time is at most O(mcx 
(m, n)). The major advantage of this algorithm is that it works "on-line." 
This means that at any time it can answer questions about the equivalence 
class of an element rather than require all pairs to be presented to it first. 
In the following chapters we will see other fruitful uses of these two set 
manipulation algorithms. 
2.5 GRAPHS 
Now we consider the data object graph, an important structure which 
was first introduced by the mathematician L. Euler in 1736. A graph G 
consists of two sets called the vertices V and the edges E. Vis a finite non-
empty set of vertices (sometimes called nodes) usually numbered 1, 2, ... , n 
and E is a finite set of pairs of vertices. Each pair in E is an edge of G. 
If the pairs are ordered (i.e. the pair (i, .i) is different than the pair 
(j, i) then we call the graph directed. Otherwise we call it undirected. 
We will use angle brackets to denote directed edges and parentheses to de-
note undirected edges. Thus, (i, .i) represents a directed edge while (i, j) 
represents an undirected edge. Note that edges of the type (i,i) or (i,i) are 
not permitted. For many applications there is often a positive real number, 
called a cost, which is attached to each edge. Such a graph is called a net-
work. 
In an undirected graph we say that the vertex i is adjacent to vertexj if the 
edge (i ,j) exists. The degree of a vertex is the number of its adjacent vertices. 
For directed graphs we distinguish between the in-degree of a vertex i which 
is the number of edges with i as its second component, and the out-degree 
of i, the number of edges with i as the first component. If the directed edge 
(i ,j) is present, then i is adjacent-to j andj is adjacent-from i. 
A path from vertex vP to Vq is a sequence of vertices vp, v;i, v;2, ... , 
V;n, Vq such that (vp, V;1), (v;i, v;2), ... , (v;n, Vq) are edges in E(G). The 
length of a path is the number of edges on it. A simple path is a path in 

80 
Elementary Data Stmctures 
which all vertices except possibly the first and last are distinct. A cycle is a 
simple path in which the first and last vertices are the same. 
In Figure 2.22 we have an example of a directed and an undirected graph 
both containing 5 vertices and 5 edges. In the directed graph vertex 1 has 
zero as its in-degree and three as its out-degree. The degree of vertex 1 in 
the undirected graph is three. In the undirected graph there is a path be-
tween every pair of vertices, whereas in the directed graph there is no way 
to go from vertex 3 (or vertex 5) to any other vertex. In Figure 2.22 (ii) the 
edges (1,2) (2,3) form a simple path and the path (1,2) (2,3) (3,1) is a cycle. 
5 
(i) 
5 
(ii) 
Figure 2.22 Two sample graphs 
The last notion we will define before discussing representations of graphs 
is connectedness. An undirected graph is called connected if for every pair 
of vertices there exists a path between them. If a graph is not connected 
then we refer to its connected subgraphs separately. A subgraph of a graph 
is a subset of the vertices in V say VB, and a subset of the edges of E which 
connect vertices in VB. A subgraph G' = (V ', E ')is a connected component 
of the undirected graph G = (V, E) iff G' is connected and there exists no 
other subgraph G" = (V", E") of G which is also connected and either 
V' C V" or E' C E ". I.e., a connected component is a maximal connected 
subgraph. For directed graphs the connectedness idea is strengthened. If 
for every pair of vertices, i,j there exists a path from i toj and a path from 
j to i then we say that directed graph is strongly connected. 
There are two common ways to represent graphs. These may be thought 
of as the sequential and linked representations. The sequential form uses 
a square table with n rows and columns where n is the number of vertices. 
This table is called the adjacency matrix. For an undirected graph, the 
adjacency matrix, GRAPH(l:n, l:n), is defined such that GRAPH(i,j) = 1 
if the edge (i, j) is present and 0 otherwise. If the graph is a network then 
GRAPH(i, j) = the cost of edge (i, j). If (i, j) is not present the value of 
GRAPH(i, j) is + co. For a directed graph, GRAPH(i, j) = 1 iff (i,j) is 
an edge. Graph (i, j) is similarly defined in case of a directed network. 

Graphs 
81 
Table 2.3 shows the adjacency matrices for the directed and undirected 
graphs of Figure 2.22. Both matrices are 5 x 5 and have entries which 
are zero or one. Note how in both cases the diagonal elements are zero indi-
cating no "self-edges." The second matrix has a special structure which all 
undirected graphs will have, and that is that GRAPH(i,j) = GRAPH(j, i). 
Such a matrix is said to be symmetric. Though the adjacency matrix nor-
mally requires n 2 locations, for undirected graphs it would suffice to keep 
only an upper triangular matrix, or n(n -
1)/2 elements. Note that the 
main diagonal need not be stored as GRAPH(i, i) = 0. 
1 
2 
3 
4 
5 
1 
2 
3 
4 
5 
1) 
l~ 
!J 
[ 
!J 
2) 
0 
0 
0 
0 
3) 
0 
0 
0 
1 
0 
0 
4) 
0 
0 
0 
0 
0 
0 
5) 
0 
0 
0 
0 
0 
1 
Table 2.3 Adjacency matrices for Figure 2.22 
Before beginning any computation on a graph we will normally have to 
initialize an adjacency matrix so that it contains the graph we are going to 
operate on. This step will typically require at least O(n2) operations. Thus, 
the computing time of most any algorithm using this form of representation 
will be at least O(n 2). This will be true even if the graph has only O(n) 
edges! This fact leads us to consider an alternative representation. 
Given a graph, its adjacency list representation consists of n lists, one 
for each vertex i. The list for vertex i contains just those vertices adjacent 
from i. Because we often need to access the adjacent vertices of a random 
vertex we insist that the heads of the lists are stored sequentially. But the 
list of a vertex's neighbors may be linked together. Figure 2.23 shows the 
adjacency lists for the two graphs of Figure 2.22. 
For both graphs there are five sequential locations (head nodes) whose 
values are either zero (if no neighbors exist) or a pointer to a list of vertices. 
Each node on the list has two fields, a vertex and a pointer to the next ele-
ment on the list. The directed graph has 5 nodes and the undirected graph 
has 10. In general, a directed graph with n vertices and e edges will require 
n locations plus e nodes while an undirected graph will require n locations 
plus 2e nodes. This can be quite a bit better than the requirements of the 
adjacency matrix representation. 
In case no insertion or deletion of edges or vertices are to be performed 
on the graph, the adjacency lists may themselves be represented sequentially 

82 
Elementary Data Stmctures 
(I) 
(21 
(3) 
(4 ) 
(5) 
) 
(I 
(2) 
(3) 
(4 ) 
(5 ) 
HEAD 
VERTEX LINK 
2 I 
3 I 
-
3 I o I 
0 
-
5 I o I 
0 
2 I 
3 I 
= 
I I 
3 I o I 
-
I I 
2 I o I 
-
I I 
-
5 I o I 
4 I o I 
(ii) 
Figure 2.23 Adjacency lists for Figure 2.22 
4 I o I 
4 I o I 
in a one dimensional array VERTEX(l :p) where p = e if the graph is 
directed and p = 2e if the graph is undirected. HEAD(i), 1 ~ i ~ n gives 
the starting point for the adjacency list for vertex i. If we define HEAD 
(n + 1) = p + 1 then the vertices on the adjacency list for vertex i are 
stored in VERTEX(}), where HEAD(i) ~ j < HEAD(i + 1). If the list 
for vertex i is empty, then HEAD(i) = HEAD(i + 1). Figure 2.24 gives 
the sequential adjacency list representations corresponding to the linked 
representations of Figure 2.23. 
This concludes section 2.5. In the following chapters we will encounter 
many algorithms on graphs, so make sure that you are familiar with these 
representation schemes. 
2.6 HASHING 
A symbol table is a data structure which allows one to easily determine 
the presence or absence of an arbitrary element. It also permits easy inser-
tion and deletion of elements. In this section we present what is undoubtedly 
the most practical technique for maintaining a symbol table, hashing. 
Though many of the tree organizations of symbol tables (e.g. binary search 
trees) are useful when special information about the identifiers is known, 
in the absence of a priori statistical information, hashing is both conceptually 
simple and, as we shall see, very efficient. 

Hashing 
HEAD(I) 
HEAD(2) HEAr3l HEAD(4) 
VERTEX I t 
i 
~~0(5), 
2 
3 
4 
3 I 5 
HEAD(6)=6 
(I l 
(2) 
( 3) 
(4) 
(5) 
(i l Representation for the graph of Figure 2122 ( i l 
HEAD(I) 
HEAD(2) HEAD(3) HEAD(4) HEAD(5) HEA0(6)=1 I 
• 
• • • • 
VERTEX I 2 I 3 14 I I I 31 I I 2 j 1 I 5 I 41 
(I) (2) 
(3) (4) 
(5) (6) (7) 
(8) (9) (10) 
(ii l Representation for the graph of Figure 2.22(ii) 
Figure 2.24 Sequential adjacency lists 
83 
In tree tables, the search for an identifier key is carried out via a sequence 
of comparisons. Hashing differs from this approach in that the address or 
location of an identifier, X, is obtained by computing some arithmetic 
function, f, of X. fi.X) gives the address where X should be placed in the 
table. This address will be referred to as the hash address of X. The mem-
ory available to maintain the symbol table is assumed to be sequential. 
This memory is referred to as the hash table, abbreviated HT. The hash 
table is partitioned into b buckets, HT(O), ... , HT(b -
1). Each bucket 
is capable of holding s identifiers. Thus, a bucket is said to consist of s 
slots, each slot being large enough to hold 1 identifier. Usually s = 1 and 
each bucket can hold exactly one identifier. A hashing function, fi.X), is 
used to perform an identifier transformation on X. fi.X) maps the set of 
possible identifiers onto the integers 0 through b -
1. Let T be the size of 
the space from which the identifiers are drawn. This space is called the 
identifier space. For example if the identifiers are drawn from the set of 
all legal FORTRAN variable names then there are more than 1.6 x 109 
distinct possible values. Any reasonable program, however, would use 
far less than all of these identifiers. For a table containing n identifiers, the 
ration/Tis the identifier density, while a = nl(sb) is the loading density 
or loading factor. 
Since the number of identifiers in use is usually several orders of magni-
tude less than the total number of possible identifiers, T, the number of 

I .. 
84 
Elementary Data Stmctures 
buckets in the hash table is also chosen to be much less than T. Therefore, 
the hash function f will certainly map several different identifiers into the 
same bucket. Two identifiers I 1, I 2 are said to be synonyms with respect to 
f if f(l 1) = f(l 2). Synonyms are entered into the same bucket so long as all 
of the s slots in that bucket have not been used. An overflow is said to 
occur when a new identifier I is mapped or hashed by f into a full bucket. 
A collision occurs when two nonidentical identifiers are hashed into the 
same bucket. When the bucket size s is 1, collisions and overflows occur 
simultaneously. 
As an example, let us consider the hash table HT with b = 26 buckets, 
each bucket having exactly two slots, i.e., s = 2. Assume that there are 
n = 10 distinct identifiers in the program and that each identifier begins 
with a· letter. The loading factor, a, for this table is 10/52 = 0.19. The 
hash function f must map each of the possible identifiers into one of the 
numbers 1-26. If the internal binary representation for the letters A-Z 
corresponds to the numbers 1-26 respectively, then the function f defined 
by: j{X) = the first character of X, will hash all identifiers X into the hash 
table. The identifiers GA, D, A, G, L, A2, Al, A3, A4 and E will be hashed 
into buckets 7, 4, l, 7, 12, l, l, l, 1 and 5 respectively by this function. The 
identifiers A, Al, A2, A3 and A4 are synonyms. So also are G and GA. 
Figure 2.25 shows the identifiers GA, D, A, G, and A2 entered into the 
hash table. Note that GA and G are in the same bucket and each bucket 
has two slots. Similarly, the synonyms A and A2 are in the same bucket 
gets hashed into HT(l2). The next identifier, Al, hashes into the bucket 
HT(l). This bucket is full and a search of the bucket indicates that Al is 
not in the bucket. An overflow has now occurred. Where in the table should 
I 
2 
3 
4 
5 
6 
7 
• • • 
26 
A 
0 
0 
D 
0 
0 
GA 
• • • 
0 
AZ 
0 
0 
0 
0 
0 
G 
• • • 
0 
Figure 2.25 Hash table 

Hashing 
85 
Al now be entered so that it may be retrieved when needed? We will look 
into overflow handling strategies a little later. But before we do that we wish 
to say more about choosing a hashing function. 
The hash function in the previous example is not very well suited for the 
use we have in mind because of the very large number of collisions and 
resulting overflows that may occur. This is so because it is not unusual to 
find that a collection of symbols such as identifiers in a computer program 
contain many which begin with the same letter. Ideally, we would like to 
choose a function! which is both easy to compute and results in very few 
collisions. But since the ratio b/T is usually very small, it is impossible to 
avoid collisions altogether. 
In summary, hashing schemes perform an identifier transformation 
through the use of a hash functionf. It is desirable to choose a function! 
which is easily computed and also minimizes the number of collisions. Since 
the size of the identifier space is usually several orders of magnitude larger 
than the number of buckets and s is small, overflows necessarily occur. 
Hence a mechanism to handle overflows is also needed. 
Hash Function 
A hashing functions, f, transforms an identifier X into a bucket address 
in the hash table. As mentioned earlier the desired properties of such a 
function are that it is easily computable and that it minimize the number 
of collisions. We would like the function to depend upon all the characters 
in the identifier rather than upon one character. In addition, we would 
like the hash function to be such that it does not result in a biased use of 
the hash table for random inputs. If X is an identifier chosen at random 
from the identifier space, then we want the probability thatfiX) = i to be 
lib for all buckets i. Then a random X has an equal chance of hashing 
into any of the b buckets. A hash function satisfying this property will be 
termed a uniform hash function. Many kinds of hash functions are in use. 
We shall discuss only two. A more detailed discussion may be found in 
any of the relevant references at the end of this chapter. 
One simple and effective choice for a hash function is obtained by using 
the modulo (mod) operator. The identifier X is interpreted as an integer 
and it is divided by some number M and the remainder is used as the hash 
address for X. 
fv(X) = X modM 
This gives bucket addresses in the range 0 to M -
1 and so the hash table 
is at least of size b = M. The choice of Mis critical. If Mis a power of 2, 

I • 
1 • 
86 
Elementary Data Stmctures 
thenf v(X) depends only on the least significant bits of X. For instance, if 
each character is represented by six bits and identifiers are stored right 
justified in a 60-bit word with leading bits filled with zeros (Figure 2.26) 
then with M = 2i, i s 6 the identifiers Al, Bl, Cl, X41, DNTXYl all 
have the same bucket address. With M = 2i, i s 12 the identifiers AXY, 
BXY, WTXY, have the same bucket address. Since programmers have a 
tendency to use many variables with the same suffix, the choice of M as a 
power of two would result in many collisions. 
Figure 2.26 Identifier Al right and left justified and zero filled 
(6 bits per character) 
Choosing M a power of 2 would have even more disastrous results if the 
identifier X is stored left justified zero filled. Then, all 1 character iden-
tifiers would map to the zeroth bucket for M = 2i, i s 54; all 2 char-
acter identifiers would map to the zeroth bucket for M = 2i, i s 48, etc. 
As a result of this observation, we see that when the division operation f v 
is used as a hash function, the table size should not be a power of 2. An-
other problem about the choice of M is that if M is divisible by 2 then odd 
keys are mapped to odd buckets (as the remainder is odd) and even keys 
are mapped to even buckets. The use of the hash table is thus biased again. 
Further analysis indicates that when M contains factors, a biased use of 
the table results if many of the identifiers are permutations of each other. 
These difficulties can be avoided by choosing M to be a prime number. 
Then, the only factors of M are M and 1. Knuth has shown that when M 
divides rk + a or rk - a where k and a are small numbers and r is the radix 
of the character set, then X mod M tends to be a simple superposition of 
the characters in X. Thus, a good choice for M would be: Ma prime num-
ber such that M does not divide rk + a or rk -
a for small k and a. In 
practice it has been observed that it is sufficient to choose M such that it 
has no prime divisors less than 20. 
Another commonly used hash function is the "middle of the square" 
function. This function, f m• is computed by squaring the identifier and 
then using an appropriate number of bits from the middle of the squared 
number to obtain the bucket address; the identifier is assumed to fit into 
one computer word. Since the middle bits of the square will usually depend 
upon all of the characters in the identifier, it is expected that different 
identifiers would result in different hash addresses with high probability 
even when some of the characters are the same. The number of bits to be 
used to obtain the bucket address depends on the table size. If r bits are 

Hashing 
87 
used, the range of values is 2', so the size of hash tables is chosen to be a 
power of 2 when this kind of scheme is used. 
Overflow Handling 
In order to be able to detect collisions and overflows, it is necessary to 
initialize the hash table to represent the situation when all slots are empty. 
Assuming that no identifier has a value of zero, then all slots may be ini-
tialized to zero. When a new identifier gets hashed into a bucket already 
occupied, it is necessary to find another bucket for this identifier. The 
simplest solution would probably be to find the closest unfilled bucket. Let 
us illustrate this on a 26-bucket table with one slot per bucket. Assume 
the identifiers are GA, D, A, G, L, A2, Al, A3, A4, Z, ZA, E. For sim-
plicity we choose the hash function fiX) = first character of X. Initially, 
all the entries in the table are zero. fiGA) = 7, this bucket is empty, so 
GA (and any other information about this identifier) are entered into HT(7). 
D and A get entered into the buckets HT(4) and HT(l) respectively. The 
next identifier G hasfiG) = 7. This slot is already used by GA. The next 
vacant slot is HT(8) and so G is entered there. Lenters at HT(l2). A2 col-
lides with A at HT(l), the bucket overflows and A2 is entered at the next 
vacant slot HT(2). Al, A3 and A4 are entered at HT(3), HT(S) and HT(6) 
respectively. Z is entered at HT(26), ZA at HT(9), (the hash table is used 
circularly), and E collides with A3 at HT(S) and is eventually entered at 
HT(lO). Figure 2.27 shows the resulting table. This method of resolving 
overflows is known as linear probing or linear open addressing. 
In order to search the table for an identifier, X, it is necessary to first 
computefiX) and then examine keys at positions HT(j{X)), HT(j{X) + 1), 
... , HT{f{X) + j) such that HT{f{X) + j) either equals X(X is in the table) 
or 0 (X is not in the table) or we eventually return to HT{f{X)) (the table is 
full). The implementation of linear search is given in Algorithm 2.10. 
procedure LIN SR CH(X, HT, b,j) 
//search the hash table HT(O:b -
1) (each bucket has exactly 111 
//slot) using linear probing. If HT(j) = 0 then thej-th bucket/ I 
I /is empty and X can be entered into the table. Otherwise/ I 
I /HT(j) = X and X is already in the table. f is the hash function/ I 
i - fiX);j - i 
while HT(j) ~ X and HT(j) ~ 0 do 
j - (j + 1) mod b 
I /treat the table as circular/ I 
if j = i then call TABLE-FULL endif //no empty slots// 
repeat 
endLINSRCH 
Algorithm 2.10 Linear hashing 

.. 
88 
Elementary Data Structures 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
-----
A 
A2 
Al 
D 
A3 
A4 
GA 
G 
ZA 
E 
0 
L 
0 
0 
Figure 2.27 Hash table with linear probing. 26 buckets, 1 slot per bucket 
Our earlier example shows that when linear probing is used to resolve 
overflows, identifiers tend to cluster together, and moreover, adjacent clus-
ters tend to coalesce, thus increasing the search time. To locate the identifier, 
ZA, in the table of Figure 2.27, it was necessary to examine HT(26), HT(l), 
... , HT(9), a total of ten comparisons. This is far worse than the worst 
case behavior for tree tables. If each of the identifiers in the table of Figure 
2.27 was retrieved exactly once, then the number of buckets examined 
would be 1 for A, 2 for A2, 3 for Al, 1 for D, 5 for A3, 6 for A4, 1 for GA, 
2 for G, 10 for ZA, 6 for E, 1 for L and 1 for Z for a total of 39 buckets 
examined. The average number examined is 3.25 buckets per identifier. 
An analysis of this method in general shows that the expected average 
number of identifier comparisons, P, to look up an identifier is approxi-
mately (2 - a)/(2 - 2a) where a is the loading density. This is the average 
over all possible sets of identifiers yielding the given loading density and 
using a uniform hashing function f. In the above example a = 12/26 = 
0.46 and P = 1.42. Even though the average number of probes is small, 
the worst case can be quite large. 
One of the reasons linear probing and its variations perform poorly is 
that searching for an identifier involves comparison of identifiers with dif-

Hashing 
89 
ferent hash values. In the hash table of Figure 2.25, for instance, searching 
for the identifier ZA involved comparisons with the buckets HT(l) to HT(8), 
even though none of the identifiers in these buckets had a collision with 
HT(26) and so could not possibly be ZA. Many of these comparisons could 
be avoided if we maintained lists of identifiers, one list per bucket, where 
each list contains only the synonyms for that bucket. If this were done, a 
search would then involve computing the hash address fi.X) and examining 
only those identifiers in the list for f(X). Since the sizes of these lists is not 
known in advance, the best way to maintain them is as linked chains. Each 
chain will have a head node which will usually be much smaller than the 
other nodes since it has to retain only a link. Since the lists are to be accessed 
at random, the head nodes should be sequential. We assume they are num-
bered 1 to M if the hash function! has range 1 to M. 
Using chaining to resolve collisions and the hash function used to obtain 
Figure 2.27, the hash chains of Figure 2.28 are obtained. When a new 
identifier, X, is being inserted into a chain, the insertion can be made at 
either end. This is so because the address of the last node in the chain is 
known as a result of the search that determined X was not in the list for 
f(X). In the example of Figure 2.28 new identifiers were inserted at the front 
of the chains. The number of probes needed to search for any of the iden-
tifiers is now 1 for each of A4, D, E, G, L, and ZA; 2 for each of A3, GA 
and Z; 3 for Al; 4 for A2 and 5 for A for a total of 24. The average is now 
2.0 which is considerably less than for linear probing. Additional storage, 
however, is needed for links. 
procedure CHNSRCH(X,HT, b,j) 
//search the hash table HT(O:b -
1) for X. Either HT(i) = 011 
//or it is a pointer to the list of identifiers X such thatf(X) = i.11 
//List nodes have fields IDENT and LINK. Either} points// 
I Ito the node already containing X or j = 01 I 
j -
HT{f{X)) 
//compute head node address// 
//search the chain starting atj// 
while}~ 0 andJDENT(j) ~ X do 
j - LINK(j) 
repeat 
endCHNSRCH 
Algorithm 2.11 Hashing with chaining 
The expected number of identifier comparisons can be shown to be 
approximately equal to 1 + (a/2) where a is the loading density nib 

!.in.L 
IDENT LINK 
I 
H 
A4l 
H 
A3l 
H 
Al I 
H 
A2l 
H 
A I 0 I 
2 I 
o I 
3 C----oJ 
4 I 
HD I 0 I 
5 I 
H 
E I 0 I 
6 I 
o I 
7 I 
H 
G I 
H 
GA I 0 I 
a C----oJ 
9 
o I 
10 
o] 
II 
0 I 
12 
H LI 0 I 
• • • 
26 I 
H 
ZA H=zJ o] 
Hash table with 26 buckets; each bucket can hold a link. 
Figure 2.28 Hash chains corresponding to Figure 2.27 
~ 
t"1 
ID 
~ g 
t:i 
~ 
Cll 
~ i 
"' 

Hashing 
91 
(b = number of head nodes). For a = 0.5 this figure is 1.25 and for a = 1 
it is 1.5. This scheme has the additional advantage that only the b head 
nodes must be sequential and reserved at the beginning. Each head node, 
however, will be at most 1/2 to 1 word long. The other nodes will be much 
bigger and need to be allocated only as needed. This could represent an 
overall reduction in space required for certain loading densities despite the 
links. If each record in the table is five words long, n = 100 and a = 0.5, 
then the hash table will be of size 200 x 5 = 1000 words. Only 500 of these 
are used as a = 0.5. On the other hand, if chaining is used with one full 
word per link, then 200 words are needed for the head nodes (b = 200). 
Each head node is one word long. One hundred nodes of six words each 
are needed for the records. The total space needed is thus 800 words, or 20% 
less than when no chaining was being used. Of course, when a is close to 
1, chaining uses more space than linear probing. However, when a is close 
to 1, the average number of probes using linear probing or its variations 
becomes quite large and the additional space used for chaining can be 
justified by the reduction in the expected number of probes needed for 
retrieval. If one wishes to delete an entry from the table, then this can be 
done by just removing that node from its chain. The problem of deleting 
entries while using open addressing to resolve collisions is tackled in the 
exercises. 
The results of this section tend to imply that the performance of a hash 
table depends only on the method used to handle overflows and is indepen-
dent of the hash function so long as a uniform hash function is being used. 
While this is true when the identifiers are selected at random from the 
identifier space, it is not true in practice. In practice, there is a tendency 
to make a biased use of identifiers. Many identifiers in use have a common 
suffix or prefix or are simple permutations of other identifiers. Hence, in 
practice we would expect different hash functions to result in different hash 
table performance. The table of Figure 2.29 presents the results of an em-
pirical study conducted by Lum, Yuen and Dodd. The values in each column 
give the average number of bucket accesses made in searching eight dif-
ferent tables with 33,575; 24,050; 4909; 3072; 2241; 930; 762 and 500 
identifiers each. The table also gives the theoretical expected number of 
bucket accesses based on random keys. As expected, chaining outperforms 
linear open addressing as a method for overflow handling. In looking over 
the figures for the division and middle of square functions, we see that 
division is generally superior to middle of the square. Lum, Yuen, and 
Dodd have comparative figures for many other hash functions. Their con-
clusion is that division is generally the best hash function. For general 

92 
Elementary Data Structures 
applications, it is therefore recommended that the division method be used. 
The divisor should be a prime number, though it is sufficient to choose a 
divisor that has no prime factors less than 20. 
a = nib 
hash 
.5 
.75 
.9 
.95 
function 
c 
0 
c 
0 
c 
0 
c 
0 
MID SQ 
1.26 
1.73 
1.40 
9.75 
1.45 
27.14 
1.47 
37.53 
DIV 
1.19 
4.52 
1.31 
7.20 
1.38 
22.42 
1.41 
25.79 
THEO 
1.25 
1.50 
1.37 
2.50 
1.45 
5.50 
1.48 
10.5 
C = chaining, 0 = open linear addressing, a = loading density, MIDSQ = middle of square, 
DIV = division, THEO = expectation 
Figure 2.29 Average number of bucket accesses per identifier 
retrieved (condensed from Lum, Yuen and Dodd), "Key-to-Address 
Transform Techniques: A Fundamental Performance Study on Large 
Existing Formatted Files," CACM, April 1971, Vol. 14, No. 4, pp. 228-239. 
The experimental evaluation of hashing techniques indicates a very good 
performance over conventional techniques such as balanced trees. The 
worst case performance for hashing can, however, be very bad. In the worst 
case an insertion or a search in a hash table with n identifiers may take 
O(n) time. We now present a probabilistic analysis for the expected per-
formance of the chaining method and state without proof the result of a 
similar analysis for linear open addressing. First, we formalize what we 
mean by expected performance. 
Let HT(O:b - 1) be a hash table with b buckets, each bucket having one 
slot. Letf be a uniform hash function with range [0,b - 1). If n identifiers 
xi, x 2, ••• ' x n are entered into the hash table then there are b n distinct 
hash sequencesfiX 1),fiX 2), ... ,fiX n). Assume that each of these is equally 
likely to occur. Let Sn denote the expected number of identifier comparisons 
needed to locate a randomly chosen Xi, 1 s i s n. Then, Sn is the average 
number of comparisons needed to find thejth key, Xj; averaged over 1 s 
j s n with eachj equally likely and averaged over all bn hash sequences as-
suming each of these is also equally likely. Let Un be the expected num-
ber of identifier comparisons when a search is made for an identifier not 
in the hash table. This hash table contains n identifiers. The quantity Un 
may be defined in a manner analogous to that used for Sn· 
Theorem 2.1 Let a = nib be the loading density of a hash table using a 
uniform hashing functionf. Then: 

References and Select.ed Readings 
93 
(i) for linear open addressing 
Un -
21 (1 + 
1 
) 
(1 -
a)2 
~(l+ l~a) 
(ii) for chaining 
Sn -
1 + a/2 
Exact derivations of Un and Sn are fairly involved and can be found in 
Knuth's book: The Art of Computer Programming: Sorting and Searching. 
Here, we present a derivation of the approximate formulas for chaining. 
First, we must make clear our count for Un and Sn· In case the identifier 
X being searched for has fiX) = i and chain i has k nodes on it (not in-
cluding the head node) then k comparisons are needed if X is not on the 
chain. If X isj nodes away from the head node, 1 :5 j :5 k thenj compari-
sons are needed. 
When the n identifiers distribute uniformly over the b possible chains, 
the expected number in each chain is nib = a. Since, Un = expected 
number of identifiers on the chain, we get Un = a. 
When the ith identifier, Xi, is being entered into the table, the expected 
number of identifiers on any chain is (i -
1)/b. Hence, the expected num-
ber of comparisons needed to search for X; after all n identifiers have been 
entered is 1 + (i -
1)/b (this assumes that new entries will be made at 
the end of the chain). We therefore get: 
1 
Sn = n E (1 + (i -
1)/b) 
(2.3) 
I !ii s n 
1 + (n -
1)!(2b) -
1 + a/2 
D 
REFERENCES AND SELECTED READINGS 
A wide ranging examination of data structures and their efficient implementation 
can be found in 

• 
94 
Elementary Data Structures 
Fundamentals of Data Structures. By Ellis Horowitz and Sartaj Sahni, Computer 
Science Press, Potomac, Maryland, 1976. 
A complete analysis of many data structures including the topics discussed here 
can be found in the Knuth series, volumes I and III as cited in Chapter 1. Knuth's 
volume III and the data structures book by Horowitz and Sahni contain a more 
thorough discussion of hashing. 
The following paper contains an analysis showing that the average time for insert 
is 0(1): 
"Analysis of heap insertion", by Istvan and Porter, Computer Science Department, 
Stanford University, 1977. 
For more on priority queues see 
"The analysis of a practical and nearly optimal priority queue", by Mark R. Brown, 
Computer Science Dept. STAN-CS-77-600, Stanford University, March 1977 
"Priority queues with update and finding minimum spanning trees", by Donald B. 
Johnson Information Processing Letters, December 1975, 53-57. 
"Analysis of an algorithm for priority queue administration", by Ame Jonassen and 
Ole-Johan Dahl, BIT, 1975, 409-422. 
"A data structure for manipulating priority queues" by Jean Vuillemin, C.ACM, 
to appear. 
For more on the disjoint set union problem see 
"On the efficiency of a good but not linear set merging algorithm" by R. Tarjan, 
J.ACM, (22,2), April, 1975, 215-225. 
"On the average behavior of set merging algorithms" by Andrew C. Yao, Proc. 8th 
symposium on the theory of computing, ACM, May 1976, 192-195. 
"The expected linearity of a simple equivalence algorithm" by Donald E. Knuth 
and Arnold Schonhage, STAN-CS-77-599, Computer Science, Stanford University, 
March 1977. 
"Linear expected time of a simple UNION-FIND algorithm" by Jon Doyle and 
Ronald L. Rivest, Information Processing Letters, (1976) 146-148. 
EXERCISES 
1. 
Write algorithms for ADDQ and DELETEQ when the queue is represented as 
a linked list. 
2. A linear list is being maintained circularly in an array C(O:n -
1) with F and 
and R set up as for circular queues. 

Exercises 
95 
(a) 
Obtain a formula in terms of F, R and n for the number of elements in 
the list. 
(b) 
Write an algorithm to delete the kth element in the list. 
(c) 
Write an algorithm to insert an element Y immediately after kth e!>!ment. 
What is the time complexity of your algorithms for (b) and (c)? 
3. Let X = (x 1, ••• , x n) and Y = (y 1, ••• , y m) be two linked lists. Write an algo-
rithm to merge the two lists together to obtain the linked list Z = (x1, y 1, x2, y2, 
... ,Xm,ym,Xm+I, ... ,Xn) ifm :Sn andZ = (x1,y1,X2,y2, •.. ,Xm,yn, 
y n + 1, ••• ' y m) if m > n. 
4. A double ended queue (deque) is a linear list where insertions and deletions 
can occur at either end. Show how to represent a deque in a one dimensional 
array and write algorithms which insert and delete at either end. 
5. Consider the hypothetical data object X2. X2 is a linear list with the restriction 
that while additions to the list may be made at either end, deletions can be 
made from one end only. Design a linked list representation for X2. Write 
addition and deletion algorithms for X2. Specify initial and boundary conditions 
for your representation. 
6. Write an algorithm to search a binary search tree T for an identifier X. As-
sume that each node in T has three fields: LCHILD, DATA and RCHILD. 
What is the computing time of your algorithm? 
7. 
Write algorithms corresponding to ADJUST, HEAPIFY, INSERT and DELETE 
for the case of a min-heap represented as a complete binary tree. 
8. Devise a suitable representation for graphs so they can be stored on punched 
cards. Write an algorithm which reads in such a graph and creates its adjacency 
matrix. 
9. Write an algorithm which uses the external representation of exercise 8 to read 
in a graph and set up its adjacency lists. 
10. Is the directed graph below strongly connected? List all of its simple paths. 
Figure 2.30 A directed graph 

• 
96 
Elementary Data Structures 
11. Show how the graph above would look if represented by its adjacency matrix 
or adjacency lists. 
12. For an undirected graph G with n vertices and e edges show that the sum of 
the degrees of each vertex equals 2e. 
13. (a) Let G be a connected undirected graph on n vertices. Show that G must 
have at least n - 1 edges and that all connected undirected graphs with n - 1 
edges are trees. 
(b) 
What is the minimum number of edges in a strongly connected digraph 
on n vertices? What shape do such digraphs have? Prove your answer. 
14. For an undirected graph G with n vertices prove that the following are equiva-
lent: 
(a) G is a tree; 
(b) G is connected, but if any edge is removed the resulting graph is not con-
nected; 
(c) 
For every pair of distinct vertices u E V(G) and v E V(G) there is exactly 
one simple path from u to v; 
(d) G contains no cycles and has n - 1 edges; 
(e) 
G is connected and has n - 1 edges. 
15. Program and run algorithm 2.6, HEAPSORT and compare its time against 
your favorite sorting method. If HEAPSORT is your favorite sorting method, 
rewrite it as a purely recursive program and compare both versions on selected 
data. 
16. Verify for yourself that Algorithm 2.3, INSERT only uses a constant number of 
comparisons to insert a random element into a heap by repeating the experi-
ment described in Table 2.7. 
17. 
(a) 
(b) 
Equation 2.2 makes use of the fact that the sum E (iii) converges and is 
less than 2. Prove this fact. 
k 
Use induction to show that E i·1(k-i) = 2k·k-I, k ;;i: I. 
i=l 
18. Write an algorithm which prints all of the identifiers in a hash table in alpha-
betical order. How efficient is your algorithm? 
19. Another way of solving the disjoint set union problem is as follows: let NAME 
(i) be the name of the set containing i, NUMBER(j) be the number of items 
in setj, LIST(j) a pointer to a linked list containing the items of setj. The 
FIND(i) operation is trivially accomplished by examining NAME(i). The 
UNION(j,k) operation, wherej and k denote sets, is done by first comparing 
NUMBER(j) with NUMBER(k). If NUMBER(j) 
:s; NUMBER(k) then 
NAME(i) -
k for all i in LINK(j), LINK(j) is appended to LINK(k), and 
-
~-
-
- ------

Exercises 
97 
NUMBER(k) is increased by NUMBER(j). The new set isj. Prove that for 
a total of n items the time for all UNION operations is at most O(n log n). 
20. 
Knuth and Schonhage have shown that the average time to perform all unions 
by the method in the previous exercise is O(n). For given values of n, generate 
random pairs of integers i and j and compare the times necessary to union i 
andj using the algorithms in the text (2.8 and 2.9) versus the procedure out-
lined above. 
21. 
Write an algorithm to delete an identifier, X, from a hash table HT(O:b -
1) 
in which overflows are handled using linear probing. 
22. 
[T. Gonzalez] Design a symbol table representation which allows one to search, 
insert and delete an identifier X in 0(1) time. Assume thatX E [l, m] is integer 
valued that m + n units of space are available where n is the number of in· 
sertions to be made. (Hint: use two arrays A(l:n) and B(l:m) where A(i) will 
be the ith identifier inserted into the table. If X is the ith identifier inserted 
then B(X) = i). Write algorithms to search, insert and delete identifiers. Note 
that you cannot initialize either A or B to zero as this would take O(m + n). 
23. 
[T. Gonzalez] Let S = (x 1, ••• , x n) and T = (y 1, ••• , yr) be two sets. Assume 
1 s x; s m, 1 s is n and 1 s y; s m, 1 s is r. Allx;s andy;s are integers. Us-
ing the idea of exercise 22 write an algorithm to determine if S is contained in T. 
Your algorithm should work in 0 (n + r) time. Since S is equal to Tiff S is con-
tained in T and T is contained in S, this implies that one can determine in linear 
time if two sets are equal. How much space is needed by your algorithm? 

Chapter 3 
DIVIDE-AND-CONQUER 
3.1 THE GENERAL METHOD 
Given a function to compute on n inputs the divide-and-conquer strategy 
suggests splitting the inputs into k distinct subsets, 1 < k ~ n yielding k 
subproblems. These subproblems must be solved and then a method must 
be found to combine subsolutions into a solution of the whole. If the sub-
problems are still relatively large, then the divide-and-conquer strategy 
may possibly be reapplied. Often the subproblems resulting from a divide-
and-conquer design are of the same type as the original problem. For those 
cases the reapplication of the divide-and-conquer principle is naturally ex-
pressed by a recursive procedure. Now smaller and smaller subproblems of 
the same kind are generated, eventually producing subproblems that are 
small enough to be solved without splitting. 
To be more precise suppose we consider the divide-and-conquer strategy 
when it splits the input into two subproblems of the same kind as the orig-
inal problem. This splitting is typical of many of the problems we will see 
here. We can write a control abstraction which mirrors the way an actual 
program based upon divide-and-conquer will look. By a control abstraction 
we informally mean a procedure whose flow of control is clear, but whose 
primary operations are specified by other procedures whose precise meaning 
is left undefined. Let the n inputs be stored (or pointed at) by the array 
A(1 :n) and we will assume this array is global to Algorithm 3 .1. Procedure 
DANDC is a function which is initially invoked as DANDC(1, n). DANDC 
(p, q) solves a problem instance defined by the inputsA(p:q). 
98 

The General Method 
99 
procedure DANDC(p, q) 
global n, A(l:n); integer m,p,q; 
111 !5 p !5 q !5 n// 
if SMALL(p, q) 
then return (G(p, q )) 
else m -
DIVIDE(p,q) 
! Ip =s; m < qi I 
retum(COMBINE(DANDC(p,m), DANDC(m + 1,q))) 
endif 
endDANDC 
Algorithm 3.1 Control abstraction for divide-and-conquer 
SMALL(p, q) is a Boolean valued function which determines if the input 
size q - p + 1 is small enough so that the answer can be computed with-
out splitting. If this is so the function G is invoked. Otherwise the function 
DIVIDE(p, q) is called. This function returns an integer which specifies 
where the input is to be split. Let m = DIVIDE(p,q). The input is split 
so thatA(p:m) andA(m + l, q) define instances of two subproblems. The 
solutions x and y respectively of these two subproblems are obtained by 
recursive application of DANDC. COMBINE(x, y) is a function which de-
termines the solution to A(p: q) using the solutions x and y to the two sub-
problems A(p:m) andA(m + 1, q). If the sizes of the two subproblems are 
approximately equal then the computing time of DANDC is naturally de-
scribed by the recurrence relation 
{
g(n), 
T(n) = 
2T(n!2) + j(n), 
n small 
(3.1) 
otherwise 
where T(n) is the time for DANDC on n inputs, g(n) is the time to compute 
the answer directly for small inputs and f(n) is the time for DIVIDE and 
COMBINE. Recurrence relations will often arise for divide-and-conquer 
based algorithms and we will see how to work with them as they arise. 
For divide-and-conquer based algorithms which produce subproblems of 
the same type as the original problem it is very natural to first describe such 
an algorithm using recursion. But to gain efficiency it may be desirable to 
translate the resulting program into iterative form. Algorithm 3.2 shows 
the result of applying the translation rules of section 1.3 to Algorithm 3.1. 

100 
Dlvlde·and-Conquer 
procedure DANDCl (p ,q) 
I /iterative version of DANDC/ I 
I I declare a stack of appropriate size/ I 
locals, t 
top -
0 
I !set the stack to empty// 
Ll: while notSMALL(p, q) do 
m -
DIVIDE(p,q) //determine how to split the input/ I 
STACK gets p, q, m, 0, 2 
//process the first recursive call;/ I 
I /increment top/ I 
q-m 
repeat 
t -
G(p, q) 
while top ¢ 0 do 
p, q, m, s, ret removed from STACK 
ifret = 2 
I /decrement top appropri-/ I 
I lately! I 
then STACK gets p, q, m, t, 3 //process the second recursive call// 
p -m + 1 
go toLl 
else t -
COMBINE(s, t) //combine two solutions into one// 
endif 
repeat 
return(t) 
endDANDCl 
Algorithm 3.2 Iterative form of divide-and-conquer control abstraction 
3.2 BINARY SEARCH 
Let a;, 1 s i s n be a list of elements which are sorted in nondecreasing 
order. Consider the problem of determining whether a given element x is 
present in the list. In case x is present, we are to determine a value j such 
that a j = x. If x is not in the list then j is to be set to zero. Divide-and-
conquer suggests breaking up any instance I = (n, a1, ... , an. x) of this 
search problem into subinstances. One possibility is to pick an index k 
and obtain three instances: I1 = (k - 1, a 1, ••• , ak-1, x), I2 = (1, ak, x), 
and /3 = (n -
k, ak+h •.. , an, x). The search problem for two of these 
three instances is easily solved by comparing x with a k· If x = a k then j 
= k and 11 and /3 need not be solved. If x < ak then for /2 and /3, j = 0 
and only I1 remains to be solved. If x > ak then for /1 and 12,j = 0 and 

Binary Search 
101 
only 13 remains to be solved. After a comparison with a k, the instance 
remaining to be solved (if any) can be solved by using this divide-and-con-
quer scheme again. If k is always chosen such that a k is the middle element 
(i.e. k = L (n + 1)/2 J then the resulting search algorithm is known as 
binary search. 
Algorithm 3 .3 describes this binary search method using the language 
SPARKS. Procedure BINSRCH has three inputs, A, n and x, and one out-
put, j. The while loop continues processing as long as there are more ele-
ments left to check. The case statement permits the selection of the three 
alternatives. The first two conditions are checked for, and if they do not 
occur, the "else clause" is automatically executed. At the conclusion of 
the procedure eitherj = 0 if xis not present, or A(j) = x. 
procedure BINSRCH(A ,n ,x j) 
//given an array A(l:n) of elements in nondecreasing order,/ I 
I In ~ 0, determine if x is present, and if so, setj such that x = A(j)I I 
//elsej = 0.1 I 
integer low, high, mid, j, n; 
low -
1; high -
n 
while low :5 high do 
mid -
LC/ow + high)!2j 
case 
: x < A(mid): high - mid -
1 
: x > A(mid): low -
mid + 1 
: else :j -
mid; return 
end case 
repeat 
j-0 
endBINSRCH 
Algorithm 3.3 Binary Search 
Is BINSRCH an algorithm? We must be sure that all of the operations 
such as comparisons betweenx andA(mid) are well defined. If the elements 
of A are integers, reals, or character strings then the relational operators 
will correctly carry out the comparisons. This will be true for those languages 
which offer these data types. Does BINSRCH terminate? We observe that 
low and high are integer variables such that each time through the loop 
either x is found or low is increased by at least one or high is decreased by 

102 
Divide-and-Conquer 
at least one. Thus we have two sequences of integers approaching each 
other and eventually low will become greater than high causing termination 
in a finite number of steps if x is not present. 
Let us select the nine entries, 
-15, -6, 0, 7, 9, 23, 54, 82, 101 
place them in A(1:9), and simulate the steps that BINSRCH goes through 
as it searches for different values of x. Only the variables low, high and mid 
need to be traced as we simulate the algorithm. We shall try the following 
values for x: 101, -14, and 82 for two successful searches and one unsuc-
cessful search. 
x = 101 
low 
high 
mid 
x = -14 
low 
high 
mid 
1 
9 
5 
9 
5 
6 
9 
7 
4 
2 
8 
9 
8 
1 
1 
9 
9 
9 
2 
1 
not found 
found 
x = 82 
low 
high 
mid 
1 
9 
5 
6 
9 
7 
8 
9 
8 
found 
Table 3.1 
Three examples of binary search on nine elements 
These examples may give us a little more confidence about Algorithm 
3.3, but they by no means prove that it is correct. Proofs of programs are 
very useful because they establish the correctness of the program for all 
possible inputs, while testing gives much less in the way of guarantees. Un-
fortunately, program proving is a very difficult process and the complete 
proof of a program can be many times longer than the program itself. We 
shall content ourselves with an "informal proof' of BINSRCH. 
Theorem 3.1 Procedure BINSRCH(A,n,x,j) works correctly. 
Proof: We assume that all statements work as expected and that comparisons 
such as x > A(mid) are appropriately carried out. Initially low = 1, high 
= n, n ~ 0 andA(l) :S ... =s; A(n). Ifn = 0 the while loop is not entered 
andj is set to zero. Otherwise we observe that each time through the loop 
the possible elements to be checked for equality with x are A(low), 

Binary Search 
103 
A(low + 1), ... , A(mid), ... , A(high). If x = A(mid) then the algorithm 
terminates successfully. Otherwise the range is narrowed by either increasing 
low to mid + 1 or decreasing high to mid -
1. Clearly this narrowing of 
the range does not affect the outcome of the search. If low becomes greater 
than high then x is not present and hence the loop is exited. 
D 
Notice that in order to fully test binary search we need not concern our-
selves with the actual values of A(l:n). By varying x sufficiently, we can 
observe all possible computation sequences of BINSRCH without devising 
different values for A. To test all successful searches x must take on the n 
values in A. To test all unsuccessful searches x need only take on n + 1 
different values. Thus we might say that the complexity of testing BINSRCH 
is 2n + 1 for each n. 
Now let's analyze the execution profile of BINSRCH. The two relevant 
characteristics of this profile are the frequency counts and space required 
for the algorithm. For BINSRCH, storage is required for then elements of 
the array plus storage for the variables low, high, mid, x andj or n + 5 
locations. As for the time, there are three possibilities to consider: the best, 
average and worst case. 
Suppose we begin by determining the time for BINSRCH on the previous 
data set. We observe that the only operations in the algorithm are com-
parisons, some arithmetics and data movement. We will concentrate on com-
parisons between x and the elements in A recognizing that the frequency 
count of all other operations will be of the same order as that for these com-
parisons. Comparisons between x and elements of A will be referred to as ele-
ment comparisons. We assume that only one comparison is needed to deter-
mine which of the three possibilities of the case statement hold. The number 
of element comparisons needed to find each of the nine elements is: 
A: 
elements: 
comparisons: 
(1) 
(2) 
(3) 
-15 -6 
0 
3 
2 
3 
(4) 
(5) 
(6) 
(7) 
7 
9 
23 
54 
4 
1 
3 
2 
(8) 
(9) 
82 
101 
3 
4 
No element requires more than 4 comparisons to be found. The average is 
obtained by summing the comparisons needed to find all nine items and 
dividing by 9, yielding 25/9, or approximately 2. 77 comparisons per suc-
cessful search on the average. There are ten possible ways that an unsuc-
cessful search may terminate depending upon the value of x. If x < A(l), 
A(l) < x < A(2), A(2) < x < A(3), A(5) < x < A(6), A(6) < x < A(7), 
or A(7) < x < A(8) the algorithm requires 3 element comparisons to de-
termine that x is not present. For all of the remaining possibilities BINSRCH 

• 
104 
Divide-and-Conquer 
requires 4 element comparisons. Thus the average number of element com-
parisons for an unsuccessful search is (3 + 3 + 3 + 4 + 4 + 3 + 3 + 3 
+ 4 + 4)/10 = 34/10 = 3.4. 
The analysis just done applies to any sorted sequence containing nine 
elements. But the type of result we would prefer is a formula for n elements. 
A good way to derive such a formula plus a better way to understand the 
algorithm is to consider the sequence of values for mid that are produced 
by BINSRCH for all possible values of x. These values are nicely described 
using a binary decision tree in which the value in each node is the value of 
mid. For example, if n = 14 then Figure 3.1 contains a binary decision 
tree which traces the way in which these values will be produced by pro-
cedure BINSRCH. 
The first comparison is x with A(7). If x < A(7) then the next comparison 
is with A(3); similarly, if x > A(7) then the next comparison is with A(ll). 
Each path through the tree represents a sequence of comparisons in the 
binary search method. If x is present, then the algorithm will end at one 
of the circular nodes which lists the index into the array where x was found. 
If x is not present, the algorithm will terminate at one of the square nodes. 
Circular nodes are called internal nodes while squares nodes are referred to 
as external nodes. 
Theorem 3.2: If n is in the range [2k-•, 2k) then BINSRCH makes at most 
k element comparisons for a successful search and either k - 1 or k com-
parisons for an unsuccessful search. (In other words the time for a success-
ful search is O(log n) and for an unsuccessful search it is 8(log n)). 
Proof: Consider the binary decision tree describing the action of BINSRCH 
on n elements. All successful searches end at a circular node while all un-
successful searches end at a square node. If 2k - 1 ::; n < 2k then all circular 
nodes are at levels 1, 2, ... , k while all square nodes are at levels k and k 
+ 1 (note that the root is at level 1). The number of element comparisons 
needed to terminate at a circular node on level i is i while the number of 
element comparisons needed to terminate at a square node at level i is 
only i - 1. The theorem follows. 
D 
The previous theorem states the worst case time for binary search. To de-
termine the average behavior we need to look more closely at the binary 
decision tree, equating its size to the number of element comparisons in 
the algorithm. The distance of a node from the root is one less than its 
level. The internal path length, I, is the sum of the distances of all internal 
nodes from the root. The external path length, E, is defined analogously as 
the sum of the distance of all external nodes from the root. It is easy to 
show by induction that for any binary tree with n internal nodes E and I 
are related by the formula 

Binary Search 
105 
E=l+2n 
It turns out that there is a simple relationship between E, I and the average 
number of comparisons in binary search. Let S(n) be the average number 
of comparisons in a successful search and U(n) the average number of 
comparisons in an unsuccessful search. The number of comparisons needed 
to find an element represented by an internal node is one more than the 
distance of this node from the root. Hence, 
S(n) = 1 +Jin 
The number of comparisons on any path from the root to an external node 
is equal to the distance between the root and the external node. Since 
every binary tree with n internal nodes has n + 1 external nodes, it follows 
that 
U(n) = El(n + 1) 
Using these three formulas for E, S(n), and U(n) we find that 
S(n) = (1 + 1/n)U(n) -
1 
From this formula we see that S(n) and U(n) are directly related. The 
minimum value of S(n) (and hence U(n)) is achieved by an algorithm whose 
binary decision tree has minimum external and internal path length. This 
minimum is achieved by the binary tree all of whose external nodes are on 
adjacent levels, and this is precisely the tree which is produced by binary 
search. From Theorem 3.2 it follows that£ is proportional ton log n. Using 
this in the preceeding formulas, we conclude that S(n) and U(n) are both 
proportional to log n. Thus we conclude that the average and worst case 
number of comparisons for binary search is the same to within a constant 
factor. The best case analysis is easy. For a successful search only one ele-
ment comparison is needed. For an unsuccessful search, Theorem 3.2 
states that Llog n J element comparisons are needed in the best case. 
In conclusion we are now able to completely describe the computing 
time of binary search by giving formulas which describe the best, average 
and worst cases: 
successful searches 
8(1), e(log n), e(log n) 
best average worst 
unsuccessful searches 
e(log n) 
best average and worst 

106 
Divide-and-Conquer 
Can we expect another searching algorithm to be significantly better 
than binary search in the worst case? This question will be pursued rigor-
ously in chapter 10. But we can anticipate the answer here which is no. 
The method for proving such an assertion is to view the binary decision 
tree as a general model for any searching algorithm which depends upon 
comparisons of entire elements. Viewed in this way, we observe that the 
longest path to discover any element is minimized by binary search, and 
so any alternative algorithm will be no better from this point of view. 
Before we end this section there is an interesting variation of binary 
search which is useful for programming languages which require two com-
parisons to implement the case statement of procedure BINSRCH. This 
variation appears as Algorithm 3.4. The correctness proof of this algorithm 
is left as an exercise. 
procedure BINSR CH1 (A ,n ,x j) 
//Same specifications as BINSRCH except n > 0.1 I 
integer low, high, mid,j, n; 
low -
1; high -
n + 1 I /high is always one more than is possible/ I 
while low < high - 1 do 
mid -
L(low + high)/2j 
ifx < A(mid) 
//only one comparison in the loop// 
then high - mid 
else low - mid I Ix ~ A(mid)! I 
endif 
repeat 
if x = A(low) thenj - low 
elsej -
0 
I Ix is present/ I 
I Ix is not present/ I 
endif 
end BINSRCH1 
Algorithm 3.4 Binary search using one comparison per cycle 
The virtue of this procedure is that it uses only one comparison between 
x and A(mid) within the while loop. The case statement of BINSRCH can 
be implemented using the arithmetic-if statement in FORTRAN. In a 
language such as PL/ I or Pascal, it may be implemented by the code 
equivalent to: 
ifx < A(mid) then high -
mid -
1 
endif 
else if x > A(mid) then low -
mid + 1 
else j -
mid; return 
endJf 
---
------~-
--~ -

Binary Search 
107 
Figure 3.1 Binary decision tree for binary search, n = 14 
BINSRCH will sometimes make twice as many element comparisons as 
BINSRCHl (for example when x > A(n)). However, for successful searches 
BINSRCHl may in cases make (log n)/2 more element comparisons than 
BINSRCH (for example when x = A(mid)). The analysis of BINSRCHl is 
left as an exercise. It should be easy to see that the best, average and worst 
case times for BINSRCHl are 8(log n) for both successful and unsuccessful 
searches. 
These two algorithms were coded in FORTRAN and run on an IBM370/ 
158. Also a version of BINSRCH called CBINSRCH, which uses the arith-
metic IF statement was also coded and tested. The size of the arrays were 
5000, 10000, 15000, 20000, 25000, 30000. The first three rows represent 
the average time for a successful search over all n times. The second set of 
three rows are the average times for all possible unsuccessful searches. For 
successful searches BINSRCHl did marginally better than the other two 
methods. For unsuccessful comparisons BINSRCH was the winner. Based 
on this limited data sample and the usual inaccuracies of timing, the algo-
rithms are judged to be indistinguishable in terms of performance. 
Array sizes; 
5000 
10000 
15000 
20000 
25000 
30000 
CBINSRCH 
.356 
.402 
.421 
.445 
.456 
.448 
BINSRCH 
.330 
.410 
.442 
.462 
.478 
.447 
BINSRCHl 
.385 
.398 
.462 
.475 
.453 
.433 
(successful searches) 
CBINSRCH 
.373 
.412 
.450 
.432 
.430 
.422 
BINSRCH 
.350 
.377 
.438 
.382 
.369 
.386 
BINSRCHl 
.362 
.422 
.410 
.412 
.402 
.430 
(unsuccessful searches) 
(times in seconds) 
Table 3.2 Computing times for three binary search algorithms 

108 
Divide-and-Conquer 
3.3 FINDING THE MAXIMUM AND MINIMUM 
Let us consider another simple problem that can be solved by the divide-
and-conquer technique. The problem is to find the maximum and minimum 
items in a set of n elements. Though this problem may look so simple as 
to be contrived, it allows us to demonstrate divide-and-conquer in a simple 
setting. One straightforward algorithm looks like 
procedure STRAITMAXMIN<,A, n, max, min) 
//Set max to the maximum and min to the minimum of A(l:n)l I 
integer i, n; 
max - min - A.(1) 
for i -
2 to n do 
ifA(i) >max 
then max - A(i) endif 
if A(i) < min 
then min - A (i) endif 
repeat 
end STRAITMAXMIN 
Algorithm 3.5 Straightforward maximum and minimum 
In analyzing the time complexity of this algorithm, we shall once again 
concentrate on the number of element comparisons. The justification for 
this is that the frequency count for other operations in the above algorithm 
is of the same order as that for element comparisons. More importantly, 
when the elements in A(l:n) are polynomials, vectors, very large numbers, 
or strings of characters the cost of an element comparison is much higher 
than the cost of the other operations. Hence the time is determined mainly 
by the total cost of the element comparisons. 
It is easy to see that procedure STRAITMAXMIN requires 2(n -
1) ele-
ment comparisons in the best, average and worst cases. An immediate 
improvement is possible by realizing that the comparison A(i) < min is 
necessary only when A(i) > max is false. Hence we may replace the con-
tents of the for loop by: 
if A(i) > max then max - A(i) 
else if A.(i) < min then min - A(i) endif 
endif 
Now the best case occurs when the elements are in increasing order. 
The number of element comparisons is n - 1. The worst case occurs when 
-------
- ---

Finding the Maximum and Minimum 
109 
the elements are in decreasing order. In this case the number of element 
comparisons is 2(n -
1). The average number of element comparisons is 
less than (2n -
1). On the average, A(i) will be greater than max half the 
time, and so the average number of comparisons is 3n/2 -
1. 
A divide-and-conquer algorithm for this problem would proceed by divid-
ing any instance I = (n, A(l), ... , A(n)) into smaller instances. For example 
wemightdividelintothetwoinstancesll = ( Ln/2j ,A(l), .. . ,A( Lnl2J)) 
and 12 = (n - L n/2 J, A ( L n/2 J + 1), ... , A(n)). If MAX(J) and MIN(J) 
are the maximum and minimum of the elements in I then MAX(/) = the 
larger of MAX(Jl) and MAX(/2), and MIN(J) == the smaller of MIN(Jl) 
and MIN(J2). If I contains only one element then the answer can be com-
puted without any splitting. 
Algorithm 3.6 shows the procedure which results by applying the strategy 
just described. MAXMIN is a recursive procedure which finds the maximum 
and minimum of the set of elements {A(i), A(i + 1), ... , AU>}. The situa-
tion of set sizes one (i = j) and two (i = j -
1) are handled separately. 
For sets containing more than two elements, the midpoint is determined 
Gust as in binary search) and two new subproblems are generated. When 
the maximum and minimum of these subproblems is determined, the two 
maxima are compared and the two minima are compared to achieve the so-
lution for the entire set. max and min are considered to be built-in func-
tions which require one comparison each to compute their result. 
procedure MAXMIN(i, j, fmax, fmin) 
I !A is a global array containing n numbers in A(l), ... , A(n).I I 
I /Parameters i, j are integers: 1 s i s j s n. The effect is to/ I 
//assign to/max and/min the largest and smallest values in// 
I !A(i;j) respectively./ I 
integer i,j; global n, A(l:n) 
case 
: i = j :fmax - fmin -A(i) 
: i = j -
1 : ifA(i) < A(i) then/max - A(j);fmin - A(i) 
else/max - A(i);fmin - A(j) 
endJf 
: else: mid -
L(i + j)/2j 
call MAXMIN(i, mid, gmax, gmin) 
callMAXMIN(mid + l,j, hmax, hmin) 
fmax -
max(gmax, hmax) 
fmin -
min(gmin, hmin) 
endcase 
endMAXMIN 
Algorithm 3.6 Recursively finding the maximum and minimum 

110 
Divide-and-Conquer 
The procedure is initially invoked by the statement 
callMAXMIN(l, n, x,y). 
max and min are functions that find the larger and smaller of two elements 
respectively. Note that each of these functions uses only one comparison 
per call. Suppose we simulate procedure MAXMIN on the following nine 
elements 
A: 
(J) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
22 
13 
-5 -8 15 
60 
17 
31 
47 
A good way of keeping track of recursive calls is to build a tree so that a 
node is added each time a new call is made. For this program each node 
will have four items of information: i,j,fmax,fmin. On the array A above, 
the tree of Figure 3.2 is produced. 
Figure 3.2 Trees of recursive calls of MAXMIN 
Examining Figure 3.2 we see that the root node contains 1 and 9 as the 
values of i andj corresponding to the initial call to MAXMIN. This execu-
tion will produce two new calls to MAXMIN where i andj have the values 
1, 5 and 6, 9 respectively thus splitting the set into two subsets of approx-
imately the same size. From the tree we can immediately see that the max-
imum depth of recursion is four (including the first call). The circled num-
--~ - -------------

Finding the Maximum and Minimum 
111 
bers in the upper left hand comer of each node represent the order in 
which/max andfmin are assigned a value. 
Now what is the number of element comparisons needed for MAXMIN? 
If T(n) represents this number, then the resulting recurrence relation is 
{
T(Ln12j) + T(ln/21) + 2, n > 2 
T(n) = 
1, 
n = 2 
0, 
n = 1 
When n is a power of two, n = 2k for some positive integer k, then 
T(n) = 2T(n!2) + 2 
= 2(2T(nl4) + 2) + 2 
= 4 T(nl 4) + 4 + 2 
= 2k - I T(2) + 
E 2i 
Isis k -
I 
= 2k - I + 2k -
2 = 3 n/2 -
2 
(3.2) 
Note that 3n/2 -
2 is the best, average and worst case number of com-
parisons when n is a power of 2. 
Compared with the 2n -
2 comparisons for the straightforward method 
this is a savings of 25% in comparisons. It can be shown that no algorithm 
based upon comparisons uses less than 3n/2 -
2 comparisons. So in this 
sense procedure MAXMIN is optimal (see chapter 10 for more details). But 
does this imply that MAXMIN is truly better in practice? Not necessarily. 
In terms of storage MAXMIN is worse than the straightforward algorithm 
because it requires stack space for i, j, fmax andfmin. Given n elements 
there will be L log 2n J + 1 levels of recursion and we need to save five 
values for each recursive call (don't forget the return address is also needed). 
Of course we could remove the recursion using the translation rules of 
chapter 1. But even if we simplify the resulting iterative version, a stack 
whose depth is on the order log n is still needed. Another source of over-
head is the comparisons needed to check if i = j or i = j -
1. Perhaps 
it would be fairer not to distinguish between element comparisons and index 
comparisons. This is especially true when the A(i)s are themselves small 
numbers. 
Let us see what the count is when element comparisons have the same 

112 
Divide-and-Conquer 
cost as comparisons between i andj. Let C(n) be this number. First, we 
observe that the effect of the first two cases of the case statement can be 
achieved by deleting the case i = j and replacing i = j - 1 by i ~ j -
1. 
Hence, a single comparison between i andj -
1 is adequate to implement 
the modified case statement. Assuming n = 2k for some positive integer 
k, we get 
C(n) = {2C(n!2) + 3, 
2 
' 
Solving this equation we obtain 
C(n) = 2C(n!2) + 3 
= 4C(n!4) + 6 + 3 
n>2 
n = 2 
= 2k- 1 C(2) + 3 
E~-
2 2 1 
= 2k + 3*2k-I -
3 
= Sn/2 - 3 
(3.3) 
The comparative figure for STRAITMAXMIN is 3(n -
1) (including the 
comparison needed to implement the for loop). This is larger than Sn/2 - 3. 
Despite this, MAXMIN will be slower than STRAITMAXMIN because of 
the overhead of stacking i,j,fmax, andfmin for the recursion. 
Algorithm 3.6 makes several points. If comparisons among the elements 
of A are much more costly than comparisons of integer variables, then the 
divide-and-conquer technique has yielded a more efficient (actually an 
optimal) algorithm. On the other hand, if this assumption is not true, the 
technique yields a less efficient program. Thus the divide-and-conquer 
strategy is seen to be only a guide to better algorithm design which may 
not always succeed. Also we see that it is sometimes necessary to work out 
the constants associated with the computing time bound for an algorithm. 
Both MAXMIN and STRAITMAXMIN are 8(n) so the use of asymp-
totic notation is not enough of a discriminator in this situation. The recur-
sion of MAXMIN will make it run far slower than STRAITMAXMIN on 
most systems. Therefore for a fair comparison we might translate MAXMIN 
into an equivalent iterative program and test that against STRAITMAXMIN 
when the time for element comparisons is very long. Finally, see the ex· 
ercises for another way to find the maximum and minimum using only 
3n/2 - 2 comparisons which uses iteration but requires no stack. 

Mergesort 
113 
3.4 MERGESORT 
As another example of divide-and-conquer, we investigate a sorting algo-
rithm which has the nice property that in the worst case its complexity is 
O(n log2 n). This algorithm is called mergesort. We shall assume through-
out that the elements are to be sorted in nondecreasing order. Given a 
sequence of n. elements (also called keys) A(l), ... , A(n) the general idea 
is to imagine them split into two sets A(l), ... , A( Ln/2j) and A( Ln/2j 
+ 1), ... , A(n). Each set is individually sorted and the resulting sequences 
are merged to produce a single sorted sequence of n elements. Thus we 
have another ideal example of the divide-and-conquer strategy where the 
splitting is into two equal size sets and the combining operation is the 
merging of two sorted sets into one. 
Procedure MERGESORT describes this process very succinctly using 
recursion and a subprocedure MERGE which merges together two sorted 
sets. 
procedure MERGESORT(low, high) 
I IA(low : high) is a global array containing high -
low + 1 ~ 011 
//values which represent the elements to be sorted.I I 
integer low, high; 
if low < high 
then mid -
L(low + high)/2j 
//find where to split the set// 
call MERGESORT(low, mid) I !sort one subset/ I 
endJf 
call MERGESORT(mid + 1, high) I !sort the other subset// 
call MERGE(low, mid, high) I I combine the results/ I 
end MERGESORT 
Algorithm 3. 7 Mergesort 

• 
114 
Divide-and-Conquer 
procedure MERGE(low, mid, high) 
I I A(low:high) is a global array containing two sorted subsets I I 
//in A(low:mid) and in A(mid + l:high). 
11 
I /The objective is to merge these sorted sets into I I 
//a single sorted set residing inA(low:high). An auxiliary array Bis used// 
integer h, i,j, k, low, mid, high; 
I !low ~ mid < high! I 
global A (low :high); local B(low :high) 
h - low;i - low;j - mid+ l; 
while h ~ mid andj ~ high do //while both sets are not exhausted// 
ifA(h) ~ A(j) thenB(i) - A(h); h - h + 1 
endJf 
i - i + 1 
repeat 
else B(i) - A(j);j - j + 1 
if h > mid then fork - j to high do I /handle any remaining elements/ I 
B(i) - A(k); i - i + 1 
repeat 
else for k -
h to mid do 
B(i) - A(k); i - i + 1 
repeat 
endif 
fork - low to high do I I copy the merged sets back into Al I 
A(k) - B(k) 
repeat 
end MERGE 
Algorithm 3.8 Merging two sorted sets using auxiliary storage 
Before executing procedure MERGESORT, the n elements should be 
placed in A(l:n) and the auxiliary array B(l:n) should also be declared. 
Then call MERGESORT(l, n) will cause the keys to be rearranged into 
nondecreasing order in A. 
Consider the array of ten elements A = (310, 285, 179, 652, 351, 423, 
861, 254, 450, 520). Procedure MERGESORT begins by splitting A into 
two subfiles of size five. The elements in A(l :5) are then split into two 
subfiles of size three and two. Then the items in A(1:3) are split into sub-
files of size two and one. The two values in A(1:2) are split a final time into 
one element subfiles and now the merging begins. Note that no actual 
movement of data has yet taken place. A record of the subfiles is implicitly 

Mergesort 
115 
maintained by the recursive mechanism. Pictorially the file can now be 
viewed as 
(310l285l179l652, 3511423, 861, 254, 450, 520) 
with the vertical bars indicating the boundaries of subfiles. A(l) and A(2) 
are merged to yield 
(285, 310 I l 79 I 652, 351I423, 861, 254, 450, 520) 
Then A(3) is merged with A(1:2) producing 
(179, 285, 3101652, 3511423, 861, 254, 450, 520) 
Next, elementsA(4) andA(5) are merged 
(179, 285, 3101351, 6521423, 861, 254,450, 520) 
followed by the merging ofA(1:3) andA(4:5) to give 
(179, 285, 310, 351, 6521423, 861, 254, 450, 520) 
At this point the algorithm has returned to the first invocation of MERGE-
SORT and it is about to process the second recursive call. Repeated recur-
sive calls are invoked producing the following subfiles: 
(179, 285, 310, 351, 652l423l86ll254l450, 520) 
A(6) and A(7) are merged and then A(8) is merged with A(6:7) giving 
(179, 285, 310, 351, 6521254, 423, 8611450, 520) 
Next A(9) and A(lO) are merged followed by A(6:8) and A(9:10) 
(179, 285, 310, 351, 6521254, 423, 450, 520, 861) 
At this point there are two sorted subfiles and the final merge produces the 
fully sorted result 
(179, 254, 285, 310, 351, 423, 450, 520, 652, 861) 

116 
Divide-and-Conquer 
Figure 3.3 Tree of calls of MERGESORT(l, 10) 
Figure 3.3 is a tree which represents the sequence of recursive calls that 
are produced by MERGESORT when it is applied to 10 elements. The pair 
of values in each node are the values of the parameters low and high. Notice 
how the splitting continues until sets containing a single element are pro-
duced. Figure 3.4 is a tree representing the calls to procedure MERGE by 
MERGESORT. For example the node containing 1, 2, 3 represents the 
merging of the elements in A(1:2) with A(3). 
If the time for the merging operation is proportional to n then the com-
puting time for mergesort is described by the recurrence relation 
{
a, 
T(n) = 
2T(n/2) + en, 
n = 1, a a constant 
n > l, ea constant 
When n is a power of 2, n = 2k, we can solve this equation by successive 
substitutions, namely 
T(n) = 2(2T(n/4) + en/2) + en 
= 4T(nl4) + 2cn 
= 4(2T(n/8) + en/4) + 2cn 
= .. · = 2kT(l) + ken 
= an +en log n 
Itiseasytoseethatif2k < n s 2k+ 1 thenT(n) s T(2k+ 1). 
Therefore 
T(n) = O(n log2n). 

Further Ref"mements 
117 
Figure 3.4 Tree of calls of MERGE 
Further Refinements 
Though Algorithm 3. 7 nicely captures the divide-and-conquer nature 
of mergesort, there remain several inefficiencies which can and should be 
eliminated. In this subsection we present these refinements in an attempt 
to produce a version of mergesort which is good enough to execute. Despite 
these improvements the algorithm's complexity will remain O(n log n) in 
the worst case. We shall see in Chapter 10 that no sorting algorithm based 
upon comparisons of entire keys can do better. 
One complaint we might raise concerning mergesort is its use of 2n loca-
tions. The additional n locations were needed because one couldn't reason-
ably merge two sorted sets in place. But despite the use of this space the 
algorithm must still work hard, copying the result placed into B(low : high) 
back intoA(low:high) on each call of MERGE. An alternative to this copying 
is to associate a new field of information with each key. (The elements in 
A are called keys). This field will be used to link the keys and any associated 
information together in a sorted list (keys and related information are cal-
led records). Then the merging of the sorted lists proceeds by changing 
the link values and no records need be moved at all. A field which contains 
only a link will generally be smaller than an entire record so less space will 
also be used. 
Along with the original array A we define an auxiliary array LINK(l :n) 
which will contain integers in the range (0, n]. These integers will be inter-
preted as pointers to elements of A. A list will be a sequence of pointers 
ending with a zero. Below is one set of values for LINK which contains two 
lists: Q and R. Q = 2 and R = 5 denotes the start of each list. 
LINK: 
(1) 
6 
(2) 
4 
(3) 
7 
(4) 
1 
(5) 
3 
(6) 
0 
(7) 
8 
(8) 
0 

118 
Divide-and-Conquer 
The two lists are Q = (2, 4, 1, 6) and R = (5, 3, 7, 8). Interpreting these 
lists as describing sorted subsets of A(1:8) we conclude that A(2) s A(4) 
s A(l) s A(6) and A(S) s A(3) s A(7) s A(8). 
Another complaint one could raise about MERGESORT is the stack space 
which is necessitated by the use of recursion. Since mergesort splits each 
set into two approximately equal size subsets, the maximum depth of the 
stack is proportional to log n. The need for stack space seems necessitated 
by the "top-down" manner in which this algorithm was devised. The need 
for stack space can be eliminated if we build an algorithm which works 
"bottom-up", see the exercises for details. 
As can be seen from procedure MERGESORT and the previous example, 
even sets of size two will cause two recursive calls to be made. For small 
set sizes most of the time will be spent processing the recursion instead of 
actually sorting. This situation can be improved by not allowing the recur-
sion to go to the lowest level. In terms of the divide-and-conquer control 
abstraction we are suggesting that when SMALL is true for mergesort more 
work should be done than simply returning with no action. The work which 
would be helpful is to use a second sorting algorithm which works well 
on small size sets. 
Insertion sort works exceedingly fast on arrays of less than, say 16 ele-
ments, though for large n its computing time is 0(n2). Its basic idea for 
sorting the items in A ( 1 :n) is as follows: 
forj -
2 ton do 
place A(j) in its correct position in the sorted set A(l:j -
1) 
repeat 
Though all of the elements in A(l:j -
1) may have to be moved to accom-
modate A(j), for small values of n the algorithm will work well. A com-
pletely stated procedure is given in Algorithm 3.9. 
procedureINSERTIONSORT(A, n) 
I I sort the values in A(l :n) into nondecreasing order, n ~ 11 I 
A(O) -
- oo 
I I create a dummy value at the beginning/ I 
forj -
2 ton do I !A(l;j -
1) is sorted// 
item - A(j); i - j -
1 
while item < A(i) do //0 s i < jl / 
A(i + 1) - A(i); i - i - 1 
repeat 
A(i + 1) -
item 
repeat 
end INSERTIONSORT 
Algorithm 3.9 lnsertionsort 

Further Refinements 
119 
The statements within the while loop may be executed zero up to a max-
imum of j times. Since j goes from 2 to n the worst case time of this pro-
cedure is bounded by 
E j = (n(n + 1)/2) -
1 = O(n 2) 
2'5.j'S.n 
Its best case computing time is O(n) under the assumption that the body of 
the while loop is never entered. This will be true when the data is already 
in sorted order. 
We are now ready to present the revised version of mergesort with the 
inclusion of insertionsort and the links. 
procedure MERGESORTl(low, high, p) 
I /The global array A(low :high) is sorted into nondecreasing/ I 
I /order using the auxiliary array LINK(low, high). The values in/ I 
I /LINK will represent a list of the indices low through high/ I 
I I giving A in sorted order. p is set to point to the I I 
I /beginning of the list. 
I I 
globalA(low:high), LINK(low:high) 
if. high - low + 1 < 16 
then call/NSERTIONSORT(A, LINK, low, high, p) 
else mid -
L(low + high)l2j 
endif. 
call MERGESORTl(low, mid, q) //return list qi/ 
call MERGESORTl(mid + 1, high, r) //return list rl / 
call MERGEl(q, r, p) //merge lists q and r to pl/ 
end MERGESORTl 
Algorithm 3.10 Mergesort using links 
Procedure MERGESORTl is initially invoked by placing the keys of the 
records to be sorted in A(l:n) and setting LINK(l:n) to zero. Then one 
says call MERGESORTl(l, n, p) and p is returned as a pointer to a list 
of indices which give the elements of A in sorted order. INSERTIONSORT 
is used whenever the number of items to be sorted is less than 16. The 
version of INSERTIONSORT as given by Algorithm 3.9 needs to be altered 
so that it sortsA(low:high) into a linked list beginning atp. 
Now we present the revised merging procedure. 

*' 
120 
Divide-and-Conquer 
procedureMERGEl(q, r,p) 
I I q and r are pointers to lists contained in the global array I I 
I /LINK (O:n). LINK(O) is introduced only for convenience and// 
//need not be initialized. The lists pointed at by q and r can be/ I 
//used to obtain sorted subsets/ I 
I /LINK(l: n ). These lists can be used to obtain sorted subsets/ I 
I !of elements in the global array A(l:n). After execution, a new// 
I /list has been formed, pointed at by p, which can be used to obtain/ I 
I I a sorted list of the elements in A in nondecreasing order .I I 
I I Assume that zero terminates a list./ I 
I /The lists pointed at by q and r are destroyed./ I 
global n, A(l:n), LINK(O:n) 
local integer i, j, k 
i - q;j - r; k -
0 I /the new list starts at LINK(O)/ I 
while i ¢ 0 andj ¢ 0 do I /while both lists are nonempty do/ I 
if A(i) s A(j) I /find the smaller key/ I 
thenLINK(k) - i; k -
i; i - LINK(i) //add a new key to the list// 
else LINK(k) - j; k - j;j - LINK(j) 
end if 
repeat 
if i = 0 thenLINK(k) - j 
else LINK(k) -
i 
endif 
p -LINK(O) 
endMERGE1 
Algorithm 3.11 
Merging linked lists of sorted elements 
As an aid to understanding this new version of mergesort, suppose we 
simulate the algorithm as it sorts the eight element sequence (SO, 10, 
25, 30, 15, 70, 35, 55). We will ignore the fact that less than 16 elements 
would normally be sorted using INSERTIONSORT. The LINK array is 
initialized to zero. Table 3.3 shows how the LINK array changes after each 
call of MERGESORTl completes. On each row the value of p points to 
the list in LINK which was created by the last completion of MERG El. 
To the right are the subsets of sorted elements which are represented by 
these lists. For example in the last row p = 2 which begins the list of links 
2, 5, 3, 4, 7, 1, 8, 6 which implies A(2) s A(S) s A(3) s A(4) s A(7) s 
A(l) s A(8) s A(6). 

Quicksort 
121 
(0) 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
A: 
-
SO, 
10, 2S, 30, lS, 70, 3S, SS, 
LINK 0, 
0, 
0, 
0, 
0, 
0, 
0, 
0, 
0, 
qrp 
122 2 
0 
1 
0 
0 
0 
0 
0 
0 (10, SO) 
343 3 
0 
1 
4 
0 
0 
0 
0 
0 (10, 50), (25, 30) 
232 2 
0 
3 
4 
0 
0 
0 
0 (10, 2S, 30, SO) 
S6S s 
0 
3 
4 
6 
0 
0 
0 (10, 2S, 30, SO), (lS, 70) 
787 7 
0 
3 
4 
6 
0 
8 
0 (10, 25, 30, SO), (lS, 70), (3S, SS) 
S7S s 
0 
3 
4 
1 
7 
0 
8 
6 (10, 2S, 30, SO) (lS, 70) 
2S2 2 
8 
s 
4 
7 
3 
0 
6 (10, lS, 2S, 30, 3S, SO, SS, 70) 
Table 3.3 Example of how the LINK array changes when MERGESORTl 
is applied toA(1:8) =(SO, 10, 25, 30, 15, 70, 35, 55). 
3.5 QUICKSORT 
The divide-and-conquer approach may be used to arrive at an efficient 
sorting method different from mergesort. In mergesort, the file A(l:n) was 
divided at its midpoint into subfiles which were independently sorted and 
later merged. In quicksort, the division into two subfiles is made such that 
the sorted subfiles do not need to be later merged. This is accomplished 
by rearranging the elements in A(l :n) such that A(i) ::5 A(j) for all i between 
1 and m and allj between m + 1 and n for some m, 1 ::5 m ::5 n. Thus, 
the elements inA(l:m) andA(m + l:n) may be independently sorted. No 
merge is needed. The rearrangement of the elements is accomplished by 
picking some element of A, say t = A(s), and then reordering the other 
elements so that all elements appearing before t in A(l:n) are less than or 
equal to t and all elements appearing after t are greater than or equal to 
t. This rearranging is referred to as partitioning. 
Procedure PARTITION of Algorithm 3.12 (due to C. A. R. Hoare) 
accomplishes an in-place partitioning of the elements of A(m:p -
1). It 
is assumed that A(p) ~ A(m) and that A(m) is the partitioning element. 
If m = 1 and p -
1 = n then A(n + 1) must be defined and must be 
greater than or equal to those elements in A(l:n). The assumption that 
A(m) is the partition element is merely for convenience and we shall see 
that other choices for the partitioning element than the first item in the set 
will be better in practice. The procedure INTERCHANGE(x, y) performs 
the assignments: temp - x; x - y; y -
temp. 

., 
122 
Divide-and-Conquer 
procedure PARTITION(m, p) 
//Within A(m), A(m + 1), ... , A(p -
1) the elements are// 
//rearranged in such a way that if initially t = A(m),11 
I /then after completion A(q) = t, for some q between m and p - 1,/ I 
/IA(k) ::5 t form ::5 k < q andA(k) ~ t forq < k < p.11 
I /The final value of p is q I I 
integer m, p, i; global A(m: p) 
v - A(m); i - m 
I IA(m) is the partition element// 
loop 
loop i -
i + 1 until A(i) ~ v repeat I Ii moves left to right// 
loop p - p -
1 until A(p) ::5 v repeat I Ip moves right to left/ I 
if i < p 
then callINTERCHANGE(A(i),A(p)) //exchangeA(i) andA(p)// 
else exit 
end if 
repeat 
A(m)-A(p);A(p)- v 
end PARTITION 
I /the partition element belongs at position p I I 
Algorithm 3.12 
Partition the set A(m:p - 1) about A(m) 
As an example of how PARTITION works consider the following array 
of9 elements. The procedure is initially invoked as call PARTITION(l, 10). 
The vertical bars connected by a horizontal line indicate those elements 
which were interchanged to produce the next row. A(l) = 65 is the parti-
tioning element and it is eventually (in the sixth row) determined to be 
the 5th smallest element of the set. Notice that the remaining elements are 
unsorted but they are partitioned about A(S) = 65. 
(1) 
(2) 
(3) 
(4) (5) 
(6) 
(7) (8) 
(9) (10) 
p 
65 
70 
75 
80 
85 
60 
55 
50 
45 
+oo 
2 
9 
I·················································· I 
65 
45 
75 
80 
85 
60 
55 
50 
70 
+oo 
3 
8 
I ····································I 
65 
45 
50 
80 
85 
60 
55 
75 
70 
+oo 
4 
7 
I ····················I 
65 
45 
50 
55 
85 
60 
80 
75 
70 
+oo 
5 
6 
I······ I 
65 
45 
50 
55 
60 
85 
80 
75 
70 
+oo 
6 
5 
I··························· I 
60 
45 
50 
55 
65 
85 
80 
75 
70 
+oo 

Analysis of Quicksort 
123 
Using Hoare's clever method of partitioning a set of elements about a 
chosen element we can directly devise a divide-and-conquer method for 
completely sorting n elements. Following a call to procedure PARTITION 
two sets S1 and S 2 are produced. All elements in S1 are less than or equal 
to the elements in S2. Hence S1 and S2 may be sorted independently. Each 
set will be sorted by reusing procedure PARTITION. Algorithm 3.13 
describes the complete process as a program. 
procedure QUICKSORT(p, q) 
I I sorts the elements A (p ), ... , A (q) which reside/ I 
I /in the global array A (1 :n) into ascending order;/ I 
I I A (n + 1) is considered to be defined/ I 
//and must be ~ all elements in A (p :q ); A (n + 1) = + oo// 
integer p, q; global n, A (l:n) 
ifp < q 
thenj -
q + 1 
call PARTITION(p, j) 
call QUICKSORT(p, j -
1) I lj is the position of the partitioning/ I 
I I element/ I 
call QUICKSORT(j + 1, q) 
endif 
end QUICKSORT 
Algorithm 3.13 Sorting by partitioning 
Analysis of Quicksort 
In analyzing QUICKSORT, we shall count only the number of element 
comparisons C(n). It is easy to see that the frequency count of other opera-
tions is of the same order as C(n). We make the following assumptions: 
(i) the n elements to be sorted are distinct; 
(ii) the partitioning element v in PARTITION is chosen using a ran-
dom selection process. 
If RANDOM(i,j) is a function that generates a random integer in the in-
terval [i, j], then the selection element is chosen by replacing the state-
ments v -A(m); i - min PARTITION by i -
RANDOM(m,p - 1); v -
A(i); A(i) - A(m); i -
m. 
First, let us obtain the worst case value Cw(n) of C(n). The number of 
element comparisons in each call of PARTITION is at most p -
m + 1. 

124 
Divide-and-Conquer 
(Note that if the elements are not distinct then at most p -
m + 2 com-
parisons may be made.) Let r be the total number of elements in all of the 
calls to PARTITION at any level of recursion. At level one only one call, 
PARTITION(!, n + 1) is made and r = n; at level two at most two calls 
are made and r = n -
1; etc. At each level of recursion, O(r) element 
comparisons are made by PARTITION. At each level r is at least one less 
than the r at the previous level as the partitioning elements of the previous 
level are eliminated. Hence Cw (n) is the sum on r as r varies from 2 to n or 
0(n 2). An exercise'examines input data on which QUICKSORT uses O(n 2) 
comparisons. 
The average value CA (n) of C(n) is much less than Cw(n ). Under the 
assumptions made earlier, the partitioning element v in the call to PARTI-
TION(m, p) has an equal probability of being the ith smallest element 
1 ::5 i ::5 p -
m, inA(m:p -
1). Hence the two subfiles remaining to be 
sorted will beA(m:j) and AU+ 1:p -
1) with probability 1/(p -
m), 
m ::5 j < p. From this we obtain the recurrence 
CA(n) = n + 1 + _!_ E (CA(k -
1) + CA(n -
k)) 
(3.4) 
n l:s;ksn 
n + 1 is the number of element comparisons required by PARTITION on 
its first call. Note that C A(O) = C A(l) = 0. Multiplying both sides of (3.4) 
by n we obtain 
nCA(n) = n(n + 1) + 2(CA(O) + CA(l) + · · · + CA(n -
1)) 
(3.5) 
Replacing n by n -
1 in (3.5) gives 
(n -
1)CA(n -
1) = n(n -
1) + 2(CA(O) + · · · + CA(n -
2)) 
Subtracting this from (3.5) we get 
nCA(n) -
(n -
1)CA(n -
1) = 2n + 2CA(n -
1) 
or 
CA(n)l(n + 1) = CA(n -
1)/n + 2/(n + 1) 

Analysis of Quicksort 
125 
Repeatedly using this equation to substitute for CA (n -
1), CA (n -
2), 
... we get 
2 
2 
CA(n)l(n + 1) = CA(n -
2)/(n -
1) + -
+ --
n 
n + 1 
2 
2 
2 
= CA (n -
3)/(n -
2) + -- + -
+ --
n -
1 
n 
n+1 
= CA(l)/2 + 2 
E 
Jsksn+l 1/k 
= 2 
E 
1/k 
(3.6) 
Jsksn+I 
Since 
~(n) 
E 
Ilk ::5 
Jsksn+I 
r 
n + I 
L 1/xdx = log,(n + 1)- log,2 
(3.6) yields 
CA (n) ~ 2(n + I) [log,(n + 2) - log,2] = 0 (n log n) 
Even though the worst case time is O(n 2 ) the average is only O(n logn). 
Let us now look at the stack space needed by the recursion. In the worst 
case the maximum depth of recursion may be n -
1. This happens for 
example when the partition element on each call to PARTITION is the 
smallest value in A (m: p -
1). The amount of stack space needed may be 
reduced to 0 (log n) by using an iterative version of quicksort in which 
the smaller of the two subfiles A (p: j -
1) and A (j + 1: q) is always 
sorted first. Also, the second recursive call may be replaced by some as-
signment statements and a jump to the beginning of the algorithm. Incor-
porating these changes QUICKSORT takes the form of Algorithm 3.14. 

•· 
126 
DiYide-and-Conquer 
procedure QUICKSORT2(p, q) 
int.egerSTACK(l: max), top //max= 2 Llog2nJ // 
global A(l:n); local integerj; integer p. q 
top -
0 
loop 
whllep < q do 
j -
q + 1 
call PARTITION(p, j) 
if.j - p < q - j thenSTACK(top + 1) -j + 1 
ST ACK(top + 2) -
q 
endif. 
top -
top + 2 
q -j -
1 
else STACK(top + 1) - p 
STACK(top + 2) - j -
1 
p -j + 1 
repeat //sort the smaller subfile// 
if. top = 0 then return endlf 
q - STACK (top); p - STACK (top -
1) 
top -
top - 2 
repeat 
end QUICKSORT2 
Algorithm 3.14 Iterative version of QUICKSORT 
We may now verify that the maximum stack space needed is 0 (log n ). 
Let S(n) be the maximum stack space needed. Then it follows that 
S(n) ~ {~ + S( L<n - 1)/2j ), : ~ ! 
which is less than 2 log n. 
As remarked in Section 3.4 INSERTIONSORT is exceedingly fast for n 
less than about 16. Hence QUICKSORT2 may be speeded up by using 
INSERTIONSORT whenever q -
p < 16. The exercises explore various 
possibilities for selection of the partition element. 
Testing 
The QUICKSORT and MERGESORT procedures were tested on an 
IBM 370/158. In both cases the recursive versions were used and pro-
grammed in PL/I. For QUICKSORT the PARTITION procedure was 

Selection 
127 
altered to carry out the median of three rule (i.e. the partitioning element 
was the median of A(m), A((m + p -
1)/2) and A(p -
1)). The data set 
consisted of random integers in the range (0,1000). Table 3.4 records the 
actual average computing times in milliseconds. 
n 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
4500 
MERGESORT 
500 
750 
1050 
1400 
1650 
2000 
2250 
2650 
QUICKSORT 
400 
600 
850 
1050 
1300 
1550 
1800 
2050 
n 
5000 
5500 
6000 
6500 
7000 
7500 
8000 
8500 
MERGESORT 
2900 
3450 
3500 
3850 
4250 
4550 
4950 
5200 
QUICKSORT 
2300 
2650 
2800 
3000 
3350 
3700 
3900 
4100 
Table 3.4 Average computing times for two sorting algorithms 
Scanning the table we immediately see that QUICKSORT is faster than 
MERGESORT for all values. Also we observe that with each increment of 
500, the time for QUICKSORT roughly increases by 250 milliseconds. The 
behavior of MERGESORT is somewhat more erratic, increasing by roughly 
350 milliseconds, on the average, for each increase of 500. Of course this 
is only an approximation since both algorithms require O(n log n) time on 
the average. The exercises discuss other tests which would make useful com-
parisons. 
3.6 SELECTION 
The PARTITION algorithm of the previous section may also be used to 
obtain an efficient solution to the selection problem. In this problem, we 
are given n elements A (1 :n) and are required to determine the kth smallest 
element. If the partitioning element v is positioned at A(j), then j -
1 
elements are less than or equal to A(j) and n -
j elements are greater 
than or equal to A(j). Hence if k < j then the kth smallest element is in 
A(l;j -
1); if k = j then A(j) is the kth smallest element; if k > j then 
the kth smallest element is the (k - j)th smallest element in A(j + l:n). 
The resulting algorithm is procedure SELECT (Algorithm 3.15). This 
procedure places the kth smallest element into position A(k) and partitions 
the remaining elements such thatA(i) ::5 A(k), 1 ::5 i < k andA(i) ~ A(k), 
k < i ::5 n. 

. 
, 
128 
Divide-and-Conquer 
procedure SELECT(/!, n, k) 
//Within the array A(l), ... , A(n) the kth smallest// 
I I element s is found and placed at position k .I I 
I /It is assumed that 1 ::5 k ::5 n .I I 
I /The remaining elements are rearranged in such a/ I 
//manner that A(k) = t, A(m) ::5 t for 1 ::5 m < k, and// 
/ IA(m) ~ t fork < m ::5 n. A(n + 1) = + oo./ I 
integer n, k, m, r,j; 
m -
l; r - n + l; A(n + 1) -
+ oo; 
loop I /each time the loop is entered, 1 ::5 m ::5 k ::5 r ::5 n + 111 
j -
r 
I I setj to the high index + 1 of the remaining items/ I 
call PARTITION(m,j) //j returns such that A(j) is thejth smallest// 
//value// 
case 
:k = j: return 
:k < j: r - j 
//j is the new upper limit// 
:else: m - j + 1 I /j + 1 is the new lower limit/ I 
endcase 
repeat 
end SELECT 
Algorithm 3.15 Finding the kth smallest element 
Let us simulate SELECT as it operates on the same array used to test 
PARTITION in section 3.5. If k = 5 then the first call of PARTITION 
will be sufficient since 65 is placed into A(5). Instead lets assume that we 
are looking for the seventh smallest element of A, i.e. k = 7. The next in-
vocation of PARTITION is call PARTITION(6,10). 
A: 
(5) 
(6) 
(7) 
(8) 
(9) 
(10) 
p 
65 
85 
80 
75 
70 
+oo 
10 
9 
I························· I 
65 
70 
80 
75 
85 
+oo 
This last call of PARTITION has uncovered the 9th smallest element of A. 
The next invocation is call PARTITION(6,9) . 
A: 
(5) 
(6) 
(7) 
(8) 
(9) 
(10) 
p 
65 
70 
80 
75 
85 
+oo 
7 
6 
I· I 
65 
70 
80 
75 
85 
+oo 

Analysis of Select 
129 
This time, the sixth element has been found. Since k ¢ j is still true in 
SELECT, another call to PARTITION is made, call PARTITION(7,9). 
A: 
(5) 
65 
65 
(6) 
70 
70 
(7) 
(8) 
80 
75 
1-------1 
75 
80 
(9) 
(10) 
85 
+oo 
85 
+oo 
9 
p 
8 
Now 80 is the partition value and that is correctly placed at A(8). However, 
SELECT has still not found the 7th smallest element. It needs one more 
call to PARTITION, which is call PARTITION(7, 8). This performs only 
an interchange between A(7) and A(7) and then returns having found the 
correct value. 
ANALYSIS OF SELECT 
In analyzing SELECT we shall make the same assumptions that were 
made for QUICKSORT viz.: 
i) the n elements are distinct and 
ii) the partitioning element is chosen at random so that each element in 
A(m:p) has an equal probability of being the partitioning element. 
PARTITION requires O(p -
m) time. On each successive call to PAR-
TITION, either m increases by at least one or j decreases by at least one. 
Initially m = 1 andj = n + 1. Hence, at most n calls to PARTITION 
may be made. Thus, the worst case complexity of SELECT is at most O(n 2). 
O(n 2) behavior occurs, for example, when the input A(l:n) is such that 
the partitioning element on the ith call to PARTITION is the ith smallest 
element and k = n. In this case, m increases by one following each call to 
PARTITION and j remains unchanged. Hence, n calls are made for a 
total cost of od!; i) = O(n 2). The average computing time of SELECT is 
l 
however only O(n). Before proving this fact, we shall specify more pre-
cisely what we mean by the average time. 
Let TA k(n) be the average time to find the kth smallest element in A (1 :n). 
This average is taken over all n I different permutations of n distinct elements. 
Now, define TA(n) and R(n) as follows: 
TA (n) = _..!_ 
E T~(n) 
n 
l:sksn 

130 
Divide-and-Conquer 
and 
R(n) = max{ TAk(n)} 
k 
T A(n) is the average computing time of SELECT. It is easy to see that 
TA(n) ::5 R(n). We are now ready to show that T A(n) = O(n). 
Theorem 3.3: The average computing time, T A.(n), of SELECT is O(n). 
Proof: 
On the first call to PARTITION, the partitioning element v is 
the ith smallest element with probability 1/n, 1 ::5 i ::5 n (this follows from 
the random selection of v). The time required by PARTITION and the 
case statement in SELECT is O(n). Hence, there is a constant c, c > 0 
such that: 
So, 
1 
R(n) ::5 en +-max{ E R(n -
i) + 
E R(i -
1)} 
n 
k 
l:Si<k 
k<i:sn 
1 
n-1 
n-1 
=en + -
max{ E R(i) + E R(i)}, 
n ~ 2 
(3.7) 
n 
k 
n-k+t 
k 
We shall assume that c is chosen such that R(l) ::5 c and show, by induction 
on n, that R(n) ::5 4 en. 
Induction Base: For n = 2, (3. 7) gives: 
1 
R(n) ::5 2 c + 2 max {R(l), R(l)} 
::5 2.Sc < 4 en. 
Induction Hypothesis: Assume R(n) ::5 4 en for all n, 2 ::5 n < m. 
Induction Step: For n = m, (3.7) gives: 
R(m) ::5 cm + ~ax { mt
1 
R(i) + "'t
1R(i)} 
m 
k 
m-k+l 
k 
Since we know that R(n) is a nondecreasing function of n, it follows that 

m-1 
m-1 
E R(i) + E R(i) 
m-k+I 
k 
is maximized if k = m/2 when m is even and k 
odd. Thus, if mis even we obtain 
If m is odd then 
2 m-1 
R(m) ::5 cm + -
E R(i) 
m m/2 
8cm-1 
::5 cm + -
Ei 
m m/2 
< 4cm 
2 m-1 
R(m) ::5 cm + -
E R(i) 
m(m+l)/2 
8c m-1 
::5 cm + -
Ei 
m (m+ 1)/2 
< 4cm 
Analysis of Select 
131 
(m + 1)/2 when m is 
Since, TA (n) ::5 R(n), it follows that TA (n) ::5 4cn and so TA (n) is O(n). 
D 
The space needed by select is 0(1). 
By choosing the partitioning element v more carefully, we can obtain a 
selection algorithm with worst case complexity O(n). In order to obtain 
such an algorithm, v must be chosen such that at least some fraction of 
the elements will be smaller than v and at least some (other) fraction of 
elements will be greater than v. Such a selection of v may be made using 
the median of medians (mm) rule. In this rule then elements are divided 
into L nlr J groups of r elements each (for some r, r > 1). The remaining 
n - r Lnlr J elements are not used. The median m; of each of these Lnlr J 
groups is found. Then, the median mm of the m;'s, 1 ::5 i ::5 L nlr J is 
found. mm is used as the partitioning element. Figure 3.5 illustrates the 
m;'s and mm when n = 35 and r = 7. B;, I ::5 i ::5 5 are the five groups 
of elements. The seven elements in each group have been arranged into non-

•· 
132 
Divide-and-Conquer 
decreasing order down the column. The middle elements are the m;'s. The 
columns have been arranged in nondecreasing order of m;. Hence, them; 
corresponding to column 3 is mm. 
elements s mm 
,------1 
I • 
• 
• I • 
• 
I 
• I • 
• 
,--!-----, 
medians--.! 
• 
• 
I m•m I 
• 
• 
1 
1-----1--~ 
I 
• 
• 
I 
• 
• 
• 
I 
I 
• 
• 
• 
I 
I 
I 
I • 
• 
• 
• 
• 
• I • 
• 
• I 
I 
I 
• 
• I • 
• 
• I 
L ______ _J 
elements ~ mm 
nondecreasing 
order 
Figure 3.5 The median of medians when r = 7, n = 35 
Since the median of r elements is the I r/21 smallest element, it follows 
(see Figure 3.5) that at least I Ln/r J 121 of the m;'s are less than or equal 
to mm and at least Lnlr J 
I Ln/r J 121 + 1 ~ I Ln/r J 121 m;'s 
---
~~--~ 

Analysis of Select 
133 
are greater than or equal to mm. Hence, at least jr/21 I Ln/r J 121 
elements are less than or equal to (or greater than or equal to) mm. When 
r = 5, this quantity is at least 1.5 L n/5 J . Thus, if we use the median of 
medians rule with r = 5 to select v = mm, we are assured that at least 
1.5 Ln/5j elements will be greater than or equal to v. This in turn implies 
that at most n -
1.5 Ln/5j ~ .7n + 1.2 elements are less than v. Also, 
at most . 7 n + 1.2 elements are greater than v. Thus, the median of 
medians rule satisfies our earlier requirement on v. 
procedure SELECT2 (A, k, n) 
I /find the kth smallest element in set Al I 
0. 
if n ~ r then sort A and return the kth element endif 
1. 
divide A into L nlr J subsets of size r each, ignore excess elements 
2. let M = {mi, m 2, .•• , m Lnlr J } be the set of medians of the above 
L nlr J subsets. 
3. 
v - SELECT2 (M. r Lnlr J 121' Lnlr J) 
4. 
use PARTITION to partition A using v as the partitioning element 
5. assume vis at position j. 
6. 
case 
:k = j: return (v) 
: k < j: let S be the set of elements A(l :j -
1) 
return (SELEC'n (S, k,j -
1)) 
:else: let R be the set of elements A(j + 1: n) 
return (SELEC'n (R, k - j, n - j -
1)) 
endcase 
endSELEC'n 
Algorithm 3.16 High level description of selection algorithm using 
median of medians rule 
The algorithm to select the kth smallest element uses the median of 
medians rule to determine a partitioning element. This element is computed 
by a recursive application of the selection algorithm. A high level description 
of the new selection algorithm appears as procedure SELECT2 (Algorithm 
3.16). Procedure SELECT2 may now be analyzed for any given r. First, 
let us consider the case when r = 5 and all elements in A are distinct. Then, 
ISi and IRI (line 6) are at most .7n + 1.2 which is no more than 3n/4 for 
n ~ 24. Let T(n) be the worst case time requirement of SELECT2. Steps 

.. 
134 
Divide-and-Conquer 
1, 2, 4 and 5 require at most O(n) time (note that since r = 5 is fixed, 
each m; (step 2) may be found in 0(1) time). The time for step 3 is T(n!S) 
and that for step 6 is at most T(3n/4) when n ~ 24. Hence, for n ~ 24 
we obtain: 
T(n) ::s; T(n!S) + T(3n/4) + en 
(3.8) 
where e is chosen sufficiently large so that 
T(n) ::s; en for 
n ::s; 24 
A proof by induction easily establishes that T(n) ::s; 20 en for n ~ 1. 
Procedure SELECT2 with r = 5 is a linear time algorithm for the selection 
problem on distinct elements! The exercises examine other values of r that 
also yield this behavior. Let us now see what happens when the elements 
of A are not all distinct. In this case, following a use of PARTITION (step 
4) the size of Sor R may be more than .7n + l.2 as some elements equal 
to v may appear in both S and R. One way to handle the situation is to 
partition A into three sets U, S and R such that U contains all elements 
equal to v, S has all elements smaller than v and R has the remainder. 
Steps 4 to 6 become: 
Partition A into U, S, and R as above. 
case 
:ISi ~ k: return (SELECT2 (S, k, ISi) 
: ISi + I UI ~ k: return (v) 
:else: return (SELECT2 (R, k - IS I - I U I , IR I)) 
end case 
When this is done, the recurrence (3.8) is still valid as ISi and IR I ::s; 
. 7n + 1.2. Hence, the new SELECT2 will be of linear complexity even 
when elements are not distinct. 
Another way to handle the case of nondistinct elements is to use a dif· 
ferent r. To see why a different r is needed, let us analyze SELECT2 with 
r = 5 and nondistinct elements. Consider the case when . 7n + 1.2 ele-
ments are less than v and the remaining elements are equal to v. An ex-
amination of PARTITION reveals that at most half the remaining elements 
may be in S. We may verify that this is the worst case. Hence, ISi ::s; .7n 
+ 1.2 + (.3n -
1.2)/2 = .85n + .6. Similarly, IRI ~ .85n + .6. 

Analysis of Select 
135 
Since, the total number of elements involved in the two recursive calls (step 
3 and 6) is now 1.05n + .6 ~ n, the complexity of SELECT2 is not O(n). 
If we try r = 9 then, at least 2.5 L n/9 J elements will be less than or equal 
to v and at least this many will be greater than or equal to v. Hence, the 
size of S and R will be at most n - 2.5 L n/9 J + 1/2 (2.5 L n/9 J ) = n -
1.25 L n/9 J ::5 31/36n + 1.25 ::5 63n/72 for n ~ 90. Hence, we obtain 
the recurrence: 
{
T(n/9) + T(63n/72) + c 1n, 
T(n) ::5 
c 1n, 
n < 90 
where c 1 is a suitable constant. 
n ~ 90 
An inductive argument shows that T(n) ::5 72c in, n ~ 1. Other suit-
able values of r are obtained in the exercises. 
As far as the additional space needed by SELECT2 is concerned, we 
see that space is needed for the recursion stack. The recursive call from 
step 6 is easily eliminated as this call is the last statement executed in 
SELECT2. Hence, stack space is needed only for the recursion from step 3. 
The maximum depth of recursion is log n. Hence, the recursion stack 
should be capable of handling this depth. As we shall see, in addition to 
this stack space, space is needed only for some simple variables. 
Implementation of SELECT 2 
Before attempting to write a SPARKS algorithm implementing procedure 
SELECT2, we need to decide (i) how the median of a set of size r is to be 
found and (ii) where we are going to store the L nlr J medians of step 2. 
Since, we expect to be using a small r (say r = 5 or 9) an efficient way to 
find the median of r elements is to sort them using INSERTIONSORT 
(A, i, j). This algorithm is a modification of Algorithm 3.9 to sort A(i;i). 
The median is now the middle element in A(i;j). A convenient place to 
store these medians is at the front of the array. Thus, if we are finding the 
kth smallest element in A (m :p) then the elements may be rearranged 
such that the medians are A(m), A(m + 1), A(m + 2), etc. This makes 
it easy to implement step 3 as a selection on consecutive elements of A. 
Procedure SEL (Algorithm 3.17) results from the above discussion and 
the replacement of the recursive calls of step 6 by equivalent code to restart 
the algorithm (i.e. the calls are replaced by the loop-repeat of lines 2 and 18 
and by code in lines 13-17). INTERCHANGE (X, Y) just interchanges 
the values of X and Y. 

136 
Divide-and-Conquer 
line 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
procedure SEL (A, m, p, k) 
//return i such that i E [m, p] and A(i) isl I 
//thekth smallest number inA(m:p). r is a global// 
I /variable as described in the text/ I 
global r; integer n, i,j 
loop 
n -
p - m + 1 
//number of elements// 
if n ~ r then call INSERTIONSORT (A, m, p) 
return (m + k -- 1) 
endif 
for i -
1 to L nlr J do I I compute medians/ I 
callINSERTIONSORT (A, m + (i - l)*r, m + i*r - 1) 
I I collect medians in front part of A (m :p )I I 
call INTERCHANGE (A(m + i -
1), A(m + (i -
l)*r 
+ lr/21 -
1)) 
repeat 
j-SEL(A,m,m + Ln/rj -1, 
call INTERCHANGE(A (m), A(j)) 
j-p+l 
call PARTITION (m,j) 
case 
;j -
m + 1 = k: return (j) 
;j -
m + 1 > k: p - j - 1 
I Ln/r J 121) /Imm// 
//set up partitioning// 
//element// 
:else: k - k - (.i - m + I); m - .i + I, 
endcase 
repeat 
endSEL 
Algorithm 3.17 SPARKS version of SELECT2 
An alternative to moving the medians to the front of the array A(m:p) 
(as in line 7) is to delete line 7 and use the fact that the medians are located 
at m + (i -
1) r + jr/21 -
1, 1 ::5 i ::5 Ln/r J. Hence, SEL, PARTI-
TION and INSERTIONSORT need to be rewritten to work on arrays for 
which the inter-element distance is b, b ~ I. At the start of the algorithm 
all elements are a distance of one apart i.e. A(l), A(2), ... , A(n). On the 
first call from line 9 we wish to use only elements which are r apart starting 
with A ( Ir /2 l ) . At the next level of recursion, the elements will be r 2 apart 
and so on. This idea is developed further in the exercises. We shall refer to 
arrays with an inter-element distance of b as a b-spaced array. 

Strassen's Matrix Multiplication 
137 
3.7 STRASSEN'S MATRIX MULTIPLICATION 
Let A and B be two n x n matrices. The product matrix C = AB is 
also an n x n matrix whose i,jth element is formed by talcing the elements 
in the ith row of A and the jth column of B and multiplying them to give 
C(i,j) = 
1: A(i, k) B(k,j) 
(3.9) 
l:sk:sn 
for all i andj between 1 and n. To compute C(i,j) using this formula, we 
need n multiplications. As the matrix C has n 2 elements, the time for the 
resulting matrix multiplication algorithm, which we shall refer to as the 
"conventional" method is 9(n 3). 
The divide-and-conquer strategy suggests another way to compute the 
product of two n x n matrices. For simplicity we will assume that n is a 
power of 2, i.e. that there exists a nonnegative integer k such that n = 2k. 
In case n is not a power of two then enough rows and columns of zeros 
may be added to both A and B so that the resulting dimensions are a power 
of two (see the exercises for more on this subject). Imagine that A and B 
are each partitioned into four square submatrices, each submatrix having 
dimensions n/2 x n/2. Then the product AB can be computed by using 
the above formula for the product of 2 x 2 matrices, namely if AB is 
then 
[
A 11 A 
12] [B 11 B 
12] 
= [C 11 C 
12]. 
A21 A22 
B21 
B22 
C21 
C22 
C 11 = A 11B 11 + A 12B 21 
C12 = AllB12 + A12B22 
C21 = A21B11 + A22B21 
C22 = A21B12 + A22B22 
(3.10) 
(3.11) 
If n = 2 then the above formulas are computed using a multiplication 
operation for the elements of A and B. These elements are typically floating 
point numbers. For n > 2 the elements of C can be computed using matrix 
multiplication and addition operations applied to matrices of size n/2 x 
n/2. Since n is a power of 2, these matrix products can be recursively com-
puted by the same algorithm we are using for the n x n case. This algo-

138 
Divide-and-Conquer 
rithm will continue applying itself to smaller size submatrices until n be· 
comes suitably small (n = 2) so that the product is computed directly. 
In order to compute AB using (3.11), we need to perform eight mul· 
tiplications of n/2 x n/2 matrices and four additions of n/2 x n/2 ma· 
trices. Since two n/2 x n/2 matrices may be added in time en 2 for some 
constant c, the overall computing time, T(n) of the resulting divide-and-
conquer algorithm is given by the recurrence 
T(n) = { 
b, 
n :s; 2 
8T(nl2) + cn2, 
n > 2 
where b and c are constants. 
This recurrence may be solved in the same way as earlier recurrences 
to obtain T(n) = O(n 3). Hence no improvement over the conventional 
method has been made. Since matrix multiplications are more expensive 
than matrix additions (O(n 3) vs. O(n 2)) one may attempt to reformulate 
the equations for C ij so as to have fewer multiplications and possibly 
more additions. Volker Strassen has discovered a way to compute the 
Cijs of (3.11) using only 7 multiplications and 18 additions or subtrac-
tions. His method involves first computing the seven n/2 x n/2 matrices 
P, Q, R, S, T, U, V as in (3.12). Then the C ijS are computed using the 
formulas in (3.13). As can be seen, P, Q, R, S, T, U, V may be com-
puted using 7 matrix multiplications and 10 matrix additions or subtractions. 
The C,jS require an additional 8 additions or subtractions. 
P =(Au+ A2i) (Bu+ B2i) 
Q = (A21 + A2i)B11 
R = A 11 (B 12 - B 22) 
S = A 22 (B 21 - B 11) 
T = (A 11 + A 12) B 22 
U = (A 21 - A 11) (B 11 + B d 
V = (A 12 - A 2i) (B 21 + B 2i> 
C 11 =P+S-T+V 
C12 = R + T 
C21 = Q + S 
C22 = P + R - Q + U 
The resulting recurrence relation for T(n) is 
(3.12) 
(3.13) 

Strassen's Matrix Multiplication 
139 
{ 
b, 
T(n) = 
7T(n/2) + an 2, 
where a and b are constants. 
Working with this formula we get 
n :$; 2 
n>2 
T(n) = an 2(1 + 714 + (7/4) 2 + ... + (7/4)k- 1) + 7k T(l) 
:$; en 2 (7 I 4) log, n + 7 iog, n, 
e a constant 
= en log, 4 + log, 7 - log, 4 + n log, 7 
= O(n log, 7) = O(n 2.81) 
(3.14) 
K. Glover has shown that there are exactly 36 different ways to compute 
the C ijS of (3.11). All of these use 7 multiplications. The bound of O(n 2·81) 
may be further reduced if we could find a way to multiply two 2 x 2 ma-
trices using less than 7 multiplications. But Hopcroft and Kerr have shown 
that 7 multiplications are necessary (see Chapter 10). Thus any further 
improvement can come only by considering higher dimensions such as 
3 x 3 or 4 x 4 and using the recursive divide-and-conquer approach or 
by a totally different method. A new method by Victor Pan has improved 
the time to O(n2·681). 
Strassen's matrix multiplication algorithm has been programmed by 
Cohen and Roth in Algol and run on a PDP/10 computer. They have de-
termined that Strassen's method is slightly faster than the conventional 
method when n is greater than about 40. However the difference in com-
puting time remains small for values of n as large as l20. Another con-
sideration is the space needed by the two methods. The conventional 
method needs only constant space in addition to that needed for A, B and 
C. In the divide-and-conquer approach space is needed for P, Q, R, S, T, 
U and V at each level of recursion. The total space needed is 7n 2(1/ 4 + 
1/16 + 1/64 + ... ) :$; (7n 2/4)(4/3) = 7n 2/3. By carefully reusing space 
which is no longer needed, some of this additional space may be saved. 
Another question which researchers have investigated is how to store 
large matrices in a paging environment so that during the Strassen algo-
rithm page fetches are minimized. Fischer and Probert give a "conversion" 
algorithm which permutes the elements of the two matrices in such a way 
that no more than O(n 2·81) page fetches are required. 

140 
Divide-and-Conquer 
At this point one may wonder why all this interest in matrix multiplication. 
As it turns out we can show that more typical matrix operations such as 
inverting a matrix and finding its determinant are directly related to matrix 
multiplication, in the sense that an efficient algorithm for one of these 
operations will immediately yield a similarly efficient algorithm for the 
other operations. Bunch and Hopcroft have shown that these other opera-
tions can be accomplished using no more than O(n 2•81) operations. 
REFERENCES AND SELECTED READINGS 
For a complete discussion of the maxmin problem see 
"A sorting problem and its complexity", by I. Pohl, CACM, 15:6, 462-463. 
For a more comprehensive discussion of mergesort and quicksort see 
The art of computer programming: sorting and searching, Volume 3, by D. E. 
Knuth Sections 5.2.2 and 5.2.4 
For more on FIND, SELECT, and QUICKSORT see 
"Partition (Algorithm 63), Quicksort (Algorithm 64), and Find (Algorithm 65)'', 
C. A. R. Hoare, CACM, vol. 4, no. 7, July 1961, 321-322. 
Algorithm 489 (SELECT) by R. Floyd and R. Rivest, CACM, 18, (1975), 173. 
3. 
Quicksort, by R. Sedgewick, Computer science dept. Stanford technical report 
STAN-CS-75-492, May 1975 
"Quicksort with equal keys'', by R. Sedgewick, SIAM J. Computing, vol. 6 no. 2, 
June 1977, 240-267. 
"Time bounds for selection", by M. Blum, R. Floyd, V. Pratt, R. Rivest and R. 
Tarjan, J. CSS, 7:4, (1972) pp. 448-461. 
For an interesting way to merge files of unequal sizes see 
"A simple algorithm for merging two disjoint linearly ordered sets'', by F. K. Hwang 
and S. Lin, SIAM J. Computing, 1, (1972), 31-39. 
For more information on the matrix multiplication problem see the following papers. 
Strassen's original method was given in 
"Gaussian elimination is not optimal'', by Volker Strassen Numerische Mathematik, 
13, 354-356. 
"On minimizing the number of multiplications necessary for matrix multiplication", 
by .I E. Hopcrvft and L. R. Kerr, SIAM J. App. Math., vol. 20, no. 1, Jan. 1971, 
30-36. 
"A note on Strassen's matrix multiplication method'', by Keith Glover, Unpublished 
manuscript, Oxford University, England 

Exercises 
141 
"On obtaining upper bounds on the complexity of matrix multiplication", by 
Charles M. Fiduccia, Proc. IBM Symposium on Complexity of Computations, 
March 1972 
"Fast matrix multiplication", by Charles M. Fiduccia Proc. 3rd Annual ACM 
Symposium on Theory of Computing 45-49, 1971 
"On the implementation of Strassen's fast multiplication algorithm", by Jacques 
Cohen and Martin Roth, Acta Informatica, 6, 1976, 341-355. 
"A note on matrix multiplication in a paging environment" by P. C. Fischer and 
R. L. Rivest, Proc. ACM Annual Conj. Oct. 1976, 17-21 
"On the additive complexity of matrix multiplication" by R. L. Probert, SIAM J. 
Computing, (5, 2), June 1976, 187-203 
"Further schemes for combining matrix algorithms" by P. C. Fischer, Automata 
Languages and Programming -2nd Colloquium ed. J. Loeckx, Springer Verlag, 
Berlin, 1974, 428-436. 
"Triangular factorization and inversion by fast matrix multiplication", by James 
Bunch and John E. Hopcroft, Math Comp., 28:125, 231-236. 
"Duality in determining the complexity on noncommutative matrix multiplication", 
by John E. Hopcroft and Jean Musinski, Proc. 5th Annual ACM Symposium on the 
Theory of Computing, 73-87. 
Also see 
"Divide-and-conquer in multidimensional space", by John L. Bentley and Michael 
I. Shamos, Proc. 8th Symposium on Theory of Computing, ACM, May 1976, 220-
230. 
"On some generalizations.of binary search" by David Dobkin and R. J. Lipton, 
Proc. 6th Symposium on the Theory of Computing, ACM, April, 1974, 310-316. 
"Strassen's algorithm is not optimal" by Viktor Pan, Proc. 19th Annual Symposium 
on the Foundations of Computer Science, 1978. 
"Field extension and trilinear aggregating, uniting and canceling for the acceleration 
of matrix multiplications," by V. Ya. Pan, Proc. 20th Annual Symposium on the 
Foundations of Computer Science, 1979. 
EXERCISES 
1. Solve the recurrence relation of formula (3.1) when 
(i) g(n) = 0(1) and/(n) = O(n); 
(ii) g(n) = 0(1) andfin) = 0(1). 

142 
Divide-and~Conquer 
2. Given the strategy for binary search as outlined in the beginning of section 3.2, 
write a recursive binary search program. 
3. Using the result of exercise 2, run the recursive and iterative versions and 
compare the times. For appropriate sizes of n have each algorithm find every 
element in the set. Then try all n + 1 possible unsuccessful searches. 
4. Devise a "binary" search algorithm which splits the set not into 2 sets of (al-
most) equal sizes, but into 2 sets of sizes one third and two thirds. How does 
this algorithm compare with binary search? 
5. Devise a "ternary" search algorithm which first tests the element at position 
n/3 for equality with some value x and then possibly checks the element at 2n/3 
either discovering x or reducing the set size to one third of the original. Com-
pare this with binary search. 
6. (a) Prove that BINSRCHl works correctly. 
(b) Verify that the following program segment correctly functions according 
to the specifications of binary search. Discuss its computing time. 
low -
1; high - n 
loop 
mid -
(low + high)/2 
ifx ~ A(mid) 
then low - mid 
else high - mid 
endif 
until low + 1 = high repeat 
Algorithm 3.19 A program segment 
7. Using the transformations for removing recursion show the resulting iterative 
program that can be formed by starting with the result of exercise 2. 
8. Prove the relationship E = I + 2n for a binary tree with n internal nodes. 
E and I are the external and internal path length respectively. 
9. 
Translate procedure MAXMIN into a computationally equivalent procedure 
which uses no recursion. 
10. Test your iterative version of MAXMIN derived above against procedure 
STRAITMAXMIN. Count all comparisons. 
11. There is an iterative program for finding the maximum and minimum which, 
----- ----

Exercises 
143 
though not a divide-and-conquer based algorithm is probably more efficient 
than MAXMIN. It works by comparing consecutive pairs of elements and 
then comparing the larger one with the current maximum and the smaller 
one with the current minimum. Write out the algorithm completely and 
analyze the number of comparisons it requires. 
12. Why is it necessary to have the auxiliary array B(low:high) in procedure 
MERGE. Give an example which shows why in-place merging is inefficient. 
13. The worst case time of procedure MERGESORT is O(n log n). What is its 
time in the best case? Can we say that the time for mergesort is 9(n log n)? 
14. A sorting method is said to be stable if at the end of the method identical 
elements occur in the same order as in the original unsorted set. Is mergesort 
a stable sorting method? 
15. QUICKSORT is not a stable sorting algorithm. However if the key in A(i) is 
changed to A(i) * n + i -
1 then the new keys are all distinct. After sorting, 
what transformation will restore the keys back to their original values? 
16. 
In procedure PARTITION, Algorithm 3.12, discuss the merits or demerits 
of altering the statement "if i < p" to "if i s p". Simulate both algorithms 
on the data set (5, 4, 3, 2, 5, 8, 9) to see how they work differently. 
17. 
Procedure QUICKSORT uses the output of procedure PARTITION, which 
gives the position where the partition element is placed. If equal keys are pre-
sent then two elements may be properly placed instead of one. Show how you 
might change the output parameters of PARTITION so that QUICKSORT 
can take advantage of this situation. 
18. Show how procedure QUICKSORT sorts the following sets of keys: (1, 1, 1, 1, 
1, 1, 1) and (5, 5, 8, 3, 4, 3, 2). 
19. There are many other ways to partition a set than procedure PARTITION. 
Consider modifying PARTITION so thati is incremented untilA(i) > v instead 
of A(i) 2:: v. Rewrite PARTITION making all of the necessary changes to it 
and then compare it with PARTITION. 
20. 
Compare the sorting methods MERGESORTl with QUICKSORT2. Devise 
data sets which compare both the average and worst case times for these two 
algorithms. 
21. 
Suppose A(l:m) and B(l:n) both contain sorted elements in nondecreasing 

144 
Divide-and-Conquer 
order. Write an algorithm which merges these items into C(l:m + n). Your 
algorithm should be shorter than Algorithm 3.8 (MERGE) since you can now 
place a large value in A(m + 1) and B(n + 1). 
22. 
Given a file of n records which are partially sorted as x 1 :s; x 2 s . . . :s; x m 
and x m + 1 s . . . s x n is it possible to sort the entire file in time O(n) using 
only a small fixed amount of additional storage? 
23. 
Another way to sort a file of n records is to scan the file first merging con-
secutive pairs of size one, then merging pairs of size two, etc. Write a program 
which carries out this process. Show how your algorithm works on the data 
set keys (100, 300, 150, 450, 250, 350, 200, 400, 500). 
24. 
(i) 
On what input data does QUICKSORT exhibit its worst case behavior? 
(ii) 
Answer (i) for the case when the partitioning element is selected according 
to the median of three rule. 
25. 
With MERGESORT we included insertion sorting to eliminate the bookkeeping 
for small merges. How would you use this trick to improve QUICKSORT? 
26. 
Take the iterative versions of MERGESORT and QUICKSORT and compare 
them for the same size data sets as was used in section 3.5. 
27. 
A version of insertionsort is used by Algorithm 3.10 to sort small subfiles. 
However its parameters and intent are slightly different than the procedure 
INSERTIONSORT of Algorithm 3.9. Write a version of insertionsort which 
will work as Algorithm 3.10 expects. 
28. Let u and v be two n bit numbers where for simplicity n is a power of 2. The 
traditional multiplication algorithm requires O(n 2) operations. A divide-and-
conquer based algorithm splits the numbers into two equal parts, computing 
the product as 
29. 
UV = (a2n 12 + b) (e2n/2 + d) 
(3.16) 
= ae2n + (ad + be)2n12 + bd 
The multiplications ae, ad, be, and bd are done using this algorithm recur-
sively. 
a) Determine the computing time of the above algorithm. 
b) What is the computing time if ad + be is computed as (a + b) (e + 
d) -ae -bd? 
If k is a nonnegative constant then the solution to the recurrence 

Exercises 
145 
T(n) 
{ 
k 
n = 1 
3~n/2) +kn, 
(3.17) 
n > 1 
for n a power of 2 is 
T(n) = 3kn log, 3 -
2kn 
(3.18) 
Prove this statement. 
30. 
(1) 
Assume that SELECT2 is to be used only when all elements in A are 
distinct. Which of the following values of r guarantee O(n) worst case 
performance? Prove your answers. r = 3, 5, 7, 9, 11. 
(ii) Do you expect the computing time of SELECT2 to increase or decrease 
if a larger (but still eligible) choice for r is made? Why? 
31. Do exercise 30 for the-ease when A is not restricted to distinct elements. An-
swer (i) for r = 7, 9, 11, 13, 15. Also answer (ii). 
32. 
Rewrite SEL, PARTITION, and INSERTIONSORT using the idea of b spaced 
arrays. 
33. What test data would you use to determine worst case and average times for 
SELECT4? 
34. Program SELECT! and SELECT3. Determine when SELECT! becomes 
better than SELECT3 on the average and also when SEL is better than 
SELECT3 for worst case performance. 
35. Program SEL and determine optimal r values for worst case and average 
performance. 
36. Section 3.6 describes an alternative way to handle the situation when A is not 
restricted to distinct elements. Using the partitioning element v, A is divided 
into three subsets. Write algorithms corresponding to SELECT! and SELECT2 
using this idea. Using your new version of SELECT2 show that the worst case 
computing time is O(n) even when r = 5. 
37. [Project] Program the algorithms of exercise 36 as well as SELECT3 and 
SELECT4. Carry out a complete test along the lines discussed in section 3.6. 
Write a detailed report together with graphs explaining the data sets, test 
strategies and determination of c1, .. ., c4. Write the final composite algo-
rithms and give tables of computing times for these algorithms. 

• 
1 .. , 
146 
Divide-and-Conquer 
38. Write a SPARKS algorithm which multiplies two n x n matrices using O(n 3) 
operations. Determine the precise number of multiplications, additions, and 
array element accesses. 
39. Give a proof which shows that the recurrence relation T(n) = mT(n/2) + an 2 
is satisfied by T(n) = O(n log m). 
40. 
Verify by hand that equations (3.12) and (3.13) actually yield the correct values 
for Cu, C12, C21 and C22. 
41. It is possible to consider the product of matrices of size n x n where n is a 
power of 3. Using divide-and-conquer the problem can be reduced to the 
multiplication of 3 x 3 matrices. The conventional method requires 27 multi-
plications. In how many multiplications must one be able to multiply 3 x 3 
matrices so that the resultant computing time is smaller than O(n 2.81)? Do the 
same for 4 x 4 matrix multiplication. 
42. 
For any even integer n it is always possible to find integers m and k such that 
n = m2k. To find the product of two n x n matrices Strassen suggests par-
titioning them into lk x 2k submatrices each having m x m elements. One 
then starts with Strassen's method to multiply the original matrices and uses 
the standard method for multiplying the required pairs of submatrices. Write 
a multiplication procedure for general n. 
43. (Winograd) Let n = 2p, V = (vi. ... , v,, ), W = (w 1, ••• , Wn ). Then we can 
compute the vector product VW by the formula. 
E (v21-1 + wu) (vu+ wu-i) -
l:Si:Sp 
E VU-I vu - I: WU-I WU 
l:si:sp 
l:si:sp 
(3.19) 
which requires Jn/2 multiplications. Show how to use this formula for the 
multiplication of two n x n matrices giving a method which requires n 3/2 + 
n 2 multiplications rather than the usual n 3 multiplications. 
44. 
(Shamos) Let X(l:n) and Y(l:n) contain two sets of integers, each sorted in 
nondecreasing order. Write an algorithm which finds the median of the 2n 
combined elements. (Hint: use binary search) 
45. 
Given two vectors X = (x i, ••• , x n), Y = ( y i, ••• , y n), then X < Y if there 
exists an i, 1 :s; i s n such that Xj = yj for 1 :s; j < i and x; < y;. Given m 
vectors each of size n, write an algorithm which determines the minimum 
vector. Analyze the time of your algorithm . 
46. 
[Fiduccia] The product of two 2 x 2 matrices can be rewritten as the matrix-
vector product: 

l
a 11a12 0 0 ] 
a21a22 0 0 
0 0 
a 11 a 12 
0 0 
a 21a22 l
b11] 
b21 
b12 
b22 
Exercises 
147 
the above matrix can be further decomposed into a product of three matrices: 
1 
1 
0 
0 0 0 
0 
0 
-1 
1 
0 0 1 
1 
-1 
0 
0 
-1 1 0 
-1 
0 
0 
-1 
1 0 0 
0 
a - b 
0 
0 
0 
0 
0 
0 
1 0 0 
0 
0 
b 
0 
0 
0 
0 
0 
1 1 0 
0 
0 
0 c -
d 
0 
0 
0 
0 
0 0 0 
1 
0 
0 
0 
c 
0 
0 
0 
0 0 1 
1 
0 
0 
0 
0 a + c 
0 
0 
1 0 
1 
0 
0 
0 
0 
0 
0 
b + d 
0 
0 1 0 
1 
0 
0 
0 
0 
0 
0 
b + c 
1 0 0 -1 
Resolve the seven multiplication scheme implied by this matrix decomposition. 
Is it different from the one given in section 3. 7? 
47. 
Testing 
In addition to SELECT! and SEL, we can think of at least two more selec-
tion algorithms. The first of these is very straightforward and appears as 
Algorithm 3.18 (procedure SELECT3). The time complexity of SELECT3 is 
O(n * min{k, n - k + l}) 
Hence, it is very fast for values of k close to 1 or close to n. In the worst case, 
it complexity is O(n 2). Its average complexity is also O(n 2). 

148 
Divide-and-Conquer 
Une 
procedwe SELECT3 (A, n, k) 
//return index i such thatA(i) is the kth smallest// 
//element in A(l:n)// 
1 
integeri,j, I, min, max 
2 
case 
3 
:k s n/2: for i - 1 to k do I /find ith smallest element/ I 
4 
I - i; min - A(i) 
S 
fotj-i+lton~ 
6 
if A(j) < min then I - j; min - A(j) 
7 
endif 
8 
repeat 
9 
call INTERCHANGE (A(l), A(i)) 
10 
repeat 
11 
:else: for i -
n to k by -
1 do I /find ith largest element/ I 
12 
I - i; max - A(i) 
13 
forj - i - 1 to 1 by -
1 do 
14 
ifA(j) >max then I -j; max -A(j) 
15 
endif 
16 
repeat 
17 
call INTERCHANGE (A(l), A(i) 
18 
repeat 
19 
endcase 
20 
end SELECT3 
Algorithm 3.18 Straightforward selection algorithm 
Another selection algorithm proceeds by first sorting the n elements into 
nondecreasing order and then picking out the kth element. A complete sort 
can be avoided by using a min-heap. Now, only k elements need to be removed 
from the h~ap. The time to set up the heap is O(n). An additional O(k log n) 
time is needed to make k deletions. The total complexity is O(n + k log n). 
This basic algorithm can be improved further by using a max-heap when k > 
n/2 and deleting n - k + 1 elements. The complexity is now O(n + log n • 
min{k, n - k + 1}). Call the resulting algorithm SELECT4. 
Now that we have four plausible selection algorithms, we would like to 
know which is best. Based upon the asymptotic analyses of the four selection 
algorithms, we can make the following qualitative statements about our ex· 
pectations on the relative performance of the four algorithms. 
i) 
Because of overheads involved in SELECT!, SEL and SELECT4 and 
the relative simplicity of SELECTJ, SELECTJ will be fastest both on 
the average and in the worst case for "small" values of n. It will also 
be fastest for large n and very small or very large keg: k = 1, 2, n, n - 1. 
ii) 
For larger values of n, SELECT! will have best behavior on the average. 

Exercises 
149 
iii) 
As far as worst case behavior is concerned, SEL will outperform the 
others when n is suitably large. However, there will probably be a range 
of n for which SELECT4 will be faster than both SEL and SELECT3. 
We except this because of the relatively large overhead in SEL (i.e. the 
constant term in O(n) is relatively large). 
iv) 
As a result of (i)-(iii) it will be desirable to obtain composite algorithms 
for good average and worst case performance. The composite algorithm 
for good worst case performance will have the form of procedure SEL 
but will include line 4.1 as below 
4.1 
case 
:n < c 1: return (SELECT3 (A, m, p, k)) 
:n < c2: return (SELECT4 (A, m, p, k)) 
endcase 
Since the overhead in SELECT! and SELECT4 is about the same, the 
constants associated with the average computing times will be about the same. 
Hence, SELECT! may always be better than SELECT4 or there may be a 
small c J such that SELECT4 is better than SELECT! for n < c J. In any 
case, we expect there is a c 4, c 4 > 0 such that SELECT3 is faster than 
SELECT! on the average for n < c 4. 
In order to verify the preceding statements and determine c 1, c 2, c 3 and c 4, 
it is necessary to program the four algorithms in some programming language 
and run the four corresponding programs on a computer. Once the programs 
have been written, test data is needed to determine average and worst case 
computing times. So, let us now say something about the data needed to ob-
tain computing times from which c ;, 1 s i s 4 may be determined. Since, we 
would also like information regarding the average and worst case computing 
times of the resulting composite algorithms we need test data for this too. 
We shall limit out testing to the case of distinct elements. 
To obtain worst case computing times for SELECT!, we shall change the 
algorithm slightly. This change will not affect its worst case computing time 
but will enable us to use a rather simple data set to determine this time for 
various values of n. We shall dispense with the RANDOM selection rule for 
PARTITION and instead use A(m) as the partitioning element. It is easy 
to see that the worst case time is obtained with A(i) = i, 1 s i :S n and k = n. 
As far as the average time for any given n is concerned, it is not easy to arrive 
at one data set and a k which exhibits this time. On the other hand, trying 
out all n! different input permutations and k = 1, 2, ... , n for each of these 
is not a feasible way to find the average. An approximation to the average 
computing time may be obtained by trying out a few (say 10) random per-
mutations of the numbers { 1, 2, ... , n} and for each of these use a few (say 5) 
random values of k. The average of the times obtained may be used as an 
approximation to the average computing time. Of course, using more permuta-

150 
*I 
Divide-and-Conquer 
tions and more k values with result in a better approximation. However, the 
number of permutations and k values we can use is limited by the amount of 
computational resources (in terms of time) we have available. 
For SEL, the average time may be obtained in the same way as for SELECT!. 
For the worst case time we can either try and figure out an input permuta-
tion for which the number of elements less than the median of medians is 
always as large as possible and then use k = 1. A simpler approach is to 
just find an approximation to the worst case time. This can be obtained by 
taking the max of the computing times for all the tests done to obtain the 
average computing time. Since, the computing times for SEL vary with r, it 
will first be necessary to determine an r which yields optimum behavior. Note 
that the r's for optimum average and worst case behaviors may be different. 
One may verify that the worst case data for SELECT3 is A(i) = n + 1 -
i, 1 s i s n and k = n/2. The computing time for SELECT3 is relatively 
insensitive to the input permutation. This permutation affects only the num-
ber of times the 'then' clause of lines 6 (Algorithm 3.18) is executed. On the 
average, this will be done about half the time. This can be achieved by using 
A(i) = n + 1 -
i, 1 s i s n!2 and A(i) = n + 1, n/2 < i s n. The k 
value needed to obtain the average computing time is readily seen to be n/4. 
An exercise examines how to obtain worst case and average times for 
SELECT4. 
Computer Times 
To verify the above qualitative statements, the four selection algorithms 
were programmed in FORTRAN by Elaine Frankowski and Warren Cartwright. 
The programs were run on a Cyber 74 computer and average and worst case 
times determined as described above. In programming algorithm SEL, b 
spaced arrays were used (thus eliminating line 7 of the algorithm). The value 
of r used was 5. In order to obtain accurate worst case times, the algorithms 
were made to perform the same selection on the same input sequence many 
times. The total time spent was divided by the number of times the selection 
was performed to obtain the time taken to solve the given problem instance. 
For average times (SELECT! and SEL) for any fixed n many different input 
sequences and k were used. The total time spent was divided by the number 
of problem instances generated to obtain the average time. This approach was 
necessitated by the fact that the clock accuracy on the Cyber 74 is much higher 
than the time to solve one problem instance for small n. In all tests only 
distinct elements were used. 
Table 3.5 gives the computing times obtained. There appear to be some 
"apparent" inconsistencies in the table. For example the worst case time for 
algorithm SEL with n = 23 is less than that when n = 20. These inconsistencies 
are easily explained by the fact that the worst case times are only the maximum 
time taken on any of the generated instances. This is only an approximation 
(hopefully a good one) to the actual worst case times. As can be seen, for worst 

n 
Exercises 
151 
case performance, SELECT3 is best for n :S 21. For n 2: 21, SEL is fastest. 
SEL becomes faster than SELECT3 before SELECT4 does. So, c 1 = 21 and 
c 2 = 0. For average behavior, SELECT3 is fastest for n :S 11 while SELECT! 
is fastest when n 2: 11. Hence, c4 = 11. SELECT4 is never faster than 
SELECT!. So, c J = 0. 
One should remember that the values of c 1 -
c 4 will in general be different 
if a different programming language or computer were used. The above values 
do however give "ball park" figures. 
SELECT! 
SEL 
SELECTJ 
SELECT4 
Average 
Worse case 
Average 
Worst case 
Average 
Worst case 
Average 
Worst case 
5 
.2 
.3 
.3 
.37 
.12 
.2 
.3 
.3 
10 
.3 
.8 
.64 
.86 
.32 
.4 
.8 
.8 
13 
.4 
1.0 
.6 
1.19 
.so 
.8 
.9 
1.1 
15 
.4 
1.2 
.9 
1.28 
.60 
1.0 
I.I 
1.3 
17 
.5 
l.5 
l.O 
1.36 
.75 
l.l 
1.2 
l.4 
20 
.5 
1.9 
1.27 
1.77 
I.I 
1.6 
1.6 
l.9 
23 
.6 
2.2 
1.2 
1.69 
1.3 
2.2 
1.8 
2.2 
25 
.6 
2.6 
1.5 
2.01 
1.5 
2.4 
1.9 
2.4 
50 
1.3 
7.6 
3.1 
4.32 
5.5 
9.7 
4.0 
5.5 
75 
1.5 
15.0 
4.9 
5.86 
11.0 
19.0 
7.0 
8.5 
100 
2.0 
26.0 
6.5 
12.0 
17.0 
31.0 
9.0 
12.0 
500 
10.0 
53.9 
34.0 
41.0 
422.0 
765.0 
45.0 
66.0 
I 000 
19.0 
2185.0 
69.0 
77.0 
96.0 
140.0 
5 000 
89.0 
52000.02 
356.0 
375.0 
557.0 
843.0 
10,000 
175.0 
> 2 minutes 
717.0 
759.0 
1160.0 
1745.0 
Times in milliseconds 
Table 3.5 Computing times for selection algorithms. 
(Table prepared by Elaine Frankowski) 
Repeat the experiment just described and obtain a table corresponding to Table 3.5 

Chapter 4 
THE GREEDY METHOD 
4.1 THE GENERAL METHOD 
The greedy method is perhaps the most straightforward design technique 
we shall be considering in this text, and what's more it can be applied to a 
wide variety of problems. Most, though not all, of these problems have n 
inputs and require us to obtain a subset that satisfies some constraints. 
Any subset that satisfies these constraints is called a feasible solution. We 
are required to find a feasible solution that either maximizes or minimizes 
a given objective function. A feasible solution that does this is called an 
optimal solution. There is usually an obvious way to determine a feasible 
solution, but not necessarily an optimal solution. 
The greedy method suggests that one can devise an algorithm which 
works in stages, considering one input at a time. At each stage, a decision 
is made regarding whether or not a particular input is in an optimal solu-
tion. This is done by considering the inputs in an order determined by 
some selection procedure. If the inclusion of the next input into the par-
tially constructed optimal solution will result in an infeasible solution, then 
this input is not added to the partial solution. The selection procedure 
itself is based on some optimization measure. This measure may or may 
not be the objective function. In fact, several different optimization mea-
sures may be plausible for a given problem. Most of these, however, will 
result in algorithms that generate suboptimal solutions. 
We can describe the greedy method abstractly, but more precisely than 
above, by considering the following control abstraction. 
152 

Optimal Storage on Tapes 
153 
procedure GREEDY(A.n) 
I IA(l:n) contains then inputs// 
solution -
¢ 
I /initialize the solution to empty I I 
for i -
1 to n do 
x -
SELECT(A) 
if FEASIBLE(solution,x) 
then solution -
UNION(solution,x) 
endif 
repeat 
return (solution) 
end GREEDY 
Algorithm 4.1 
Greedy method control abstraction 
The function SELECT selects an input from A, removes it and assigns 
its value to x. FEASIBLE is a Boolean-valued function which determines if 
x can be included into the solution vector. UNION actually combines x 
with solution and updates the objective function. Procedure GREEDY des-
cribes the essential way that a greedy based algorithm will look, once a 
particular problem is chosen and the procedures SELECT, FEASIBLE and 
UNION are properly implemented. 
4.2 OPTIMAL STORAGE ON TAPES 
There are n programs that are to be stored on a computer tape of length 
L. Associated with each program i is a length l;, 1 :5 i :5 n. Clearly, all 
programs can be stored on the tape if and only if the sum of the lengths of 
the programs is at most L. We shall assume that whenever a program is to 
be retrieved from this tape, the tape is initially positioned at the front. 
Hence' if the programs are stored in the order I = i I ' i 2 ' ••• ' in ' the time 
tj needed to retrieve program ij is proportional to EisA:sj l;A:. If all programs 
are retrieved equally often then the expected or mean retrieval time (MRT) 
is (1/n)E1sjsn tj. In the optimal storage on tape problem, we are required 
to find a permutation for the n programs so that when they are stored on 
the tape in this order the MRT is minimized. Minimizing the MRT is 
equivalent to minimizing D(J) = E 1sjsn E 1sA:sj l;A:. 

154 
The Greedy Method 
Example 4.1 Let n = 3 and (/1 , Ii, 13 ) = (5, 10, 3). There are n! 
possible orderings. These orderings and their respective D values are: 
ordering I 
D(I) 
1,2,3 
5 + 5+ 10 + 5+ 10 + 3 = 38 
1,3,2 
5 +5 + 3 + 5+ 3 + 10 = 31 
2,1,3 
10 + 10 + 5 + 10 + 5 + 3 = 43 
2,3,1 
10 + 10 + 3 + 10 + 3 + 5 = 41 
3,1,2 
3 + 3 + 5 + 3+ 5+ 10 = 29 
3,2,1 
3 + 3+ 10 + 3+ 10 + 5 = 34 
The optimal ordering is 3,1,2. 
D 
6 
A greedy approach to building the required permutation would choose 
the next program based upon some optimization measure. One possible 
measure would be the D value of the permutation constructed so far. The 
next program to be stored on the tape would be one which minimizes the 
increase in D. Ifwe have already constructed the permutation i1, ii, ... , i,, 
then appending program j gives the permutation i 1, ii, ... , in i r +I = j. 
This increases the D value by EI sA:sr I ile + I j· Since E 1 s1e"' I ik is fixed 
and independent of j, we trivially observe that the increase in D is mini-
mized if the next program chosen is the one with the least length from 
among the remaining programs. 
The greedy algorithm resulting from the above discussion is so simple 
that we won't bother to write it out. The greedy method simply requires us 
to store the programs in nondecreasing order of their lengths. This order-
ing can be carried out in O(n log n) time using an efficient sorting algorithm 
(e.g. heap sort from Chapter 2). Theorem 4.1 shows that the MRT is mini-
mized when programs are stored in this order. 
Theorem 4.1 
If / 1 s Ii s · · · s Zn then the ordering ij = j, 1 s j s n 
minimizes 
n 
le 
E EI; 
le=I j=I 
1 
over all possible permutations of the ij. 
Proof: Let I = i1, ii, ... , in be any permutation of the index set { 1, 2, 
... , n}. Then 

Optimal Storage on Tapes 
155 
n 
k 
D(I) = E E /;j = 
E 
(n -
k + l)l;k. 
k~lj~I 
lsksn 
If there exist a, b such that a < b and I;. > l;b then interchanging i. and 
i b results in a permutation I' with 
D(I') = ( E (n - k + 1) lik) + (n -
a + 1) lib + (n - b + 1) Zia· 
k 
1 ... 
k .. b 
Subtracting D(J') from D(J) we obtain: 
D(I) - D(I') = (n -
a + 1) (I;. -
l;b) + (n -
b + 1) (l;b -
I;.) 
> 0. 
Hence, no permutation which is not in nondecreasing order of the //s can 
have minimum D. It is easy to see that all permutations in nondecreasing 
order of the I ;'s have the same D value. Hence, the ordering defined by 
i1 = j, 1 s j s n minimizes the D value. 
D 
The tape storage problem can be extended to several tapes. If there are 
m > 1 tapes, To, ... , Tm -1 , then the programs are to be distributed over 
these tapes. For each tape a storage permutation is to be provided. If lj 
is the storage permutation for the subset of programs on tape j then D(Ij) 
is as defined earlier. The total retrieval time (TD) is Eo,.Jsm-I D(I). The 
objective is to store the programs in such a way as to minimize TD. 
The obvious generalization of the solution for the one tape case would 
be to consider the programs in nondecreasing order of //s. The program 
currently being considered is placed on the tape which results in the mini-
mum increase in TD. This tape will be the one with the least amount of 
tape used so far. If there is more than one tape with this property then the 
one with smallest index can be used. If the jobs are initially ordered such 
that 11 s 12 s · · · s Zn then the first m programs will be assigned to 
tapes To, ... , Tm -1 respectively. The next m programs will be assigned to 
tapes To, ... ' Tm - I respectively. The general rule is that program i is 
stored on tape T; mod m· On any given tape the programs are stored in non-
decreasing order of their lengths. Algorithm 4.2 presents this rule as a 
SPARKS program. It assumes that the programs are ordered as above. It 

156 
The Greedy Method 
has a computing time of O(n) and does not need to know the actual pro-
gram lengths. Theorem 4.2 proves that the resulting storage pattern is 
optimal. 
procedure STORE(n, m) 
I In is the number of programs and m the number of tapes/ I 
integer m, n, j 
j -
0 
I !next tape to store on/ I 
fori -
1 ton do 
print ('append program', i, 'to permutation for tape', j) 
j - (j + 1) mod m 
repeat 
end STORE 
Algorithm 4.2 Assigning programs to tapes 
Theorem 4.2 If Z1 :5 Z2 :5 · · • :5 Zn then Algorithm 4.2 generates an 
optimal storage pattern form tapes. 
Proof: 
In any storage pattern for m tapes, let r; be one greater than the 
number of programs following program i on its tape. Then the total re-
trieval time TD is given by 
n 
TD 
E r;/;. 
izl 
In any given storage pattern, for any given n, there can be at most m pro-
grams for which r; = j. From Theorem 4.1 it follows that TD is minimized 
if them longest programs haver; = 1, the next m longest programs have 
r; = 2 and so on. When programs are ordered by length, i.e., Z1 :5 Z2 :5 
· · · :5 Zn, then this minimization criteria is satisfied ifr; = j(n - i + Olm l . 
It is easy to see that Algorithm 4.2 results in a storage pattern with these 
r;'s. 
D 
The above proof shows that there are actually many storage patterns 
that minimize TD. If we computer; = 
j(n -
i + l)!m l for each pro-
gram i, then so long as all programs with the same r; are stored on dif-
ferent tapes and haver; -
1 programs following them, the TD is the same. 
If n is a multiple of m then there are at least (m !)nlm storage patterns that 
minimize TD. Algorithm 4.2 produces one of these. 

Knapsack Problem 
157 
4.3 KNAPSACK PROBLEM 
Now, let us try to apply the greedy method to solve a more complex prob-
lem. This problem is the knapsack problem. We are given n objects and a 
knapsack. Object i has a weight w; and the knapsack has a capacity M. If 
a fraction x;, 0 :5 x; :5 1, of object i is placed into the knapsack then a 
profit of PiXi is earned. The objective is to obtain a filling of the knapsack 
that maximizes the total profit earned. Since the knapsack capacity is M, 
we require the total weight of all chosen objects to be at most M. Formally, 
the problem may be stated as: 
maximize E p;x; 
l:Si:Sn 
(4.1) 
subject to E w;x; :5 M 
t:si:sn 
(4.2) 
and 0 :5 x; :5 1, 1 :5 i :5 n 
(4.3) 
The profits and weights are positive numbers. 
A feasible solution (or filling) is any set (x1, ... , Xn) satisfying (4.2) and 
(4.3) above. An optimal solution is a feasible solution for which (4.1) is 
maximum. 
Example 4.2 Consider the following instance of the knapsack problem: 
n = 3, M = 20, (p1, pi, p3) = (25, 24, 15) and (w1, w2, w3) = (18, 15, 10). 
Four feasible solutions are: 
(X1,X2,X3) 
Ew;X; 
Ep;X; 
i) (112, 113, 114) 
16.5 
24.25 
ii) (1, 2/15, 0) 
20 
28.2 
iii) (0, 213, 1) 
20 
31 
iv) (0, 1, 112) 
20 
31.5 
Of these four feasible solutions, solution (iv) yields the maximum profit. 
As we shall soon see, this solution is optimal for the given problem in-
stance. 
D 
In case the sum of all the weights is :5M, then clearly x; = 1, 1 :5 i :5 n 
is an optimal solution. So, let us assume the sum of weights exceeds M. 
Now all the x;'s cannot be 1. Another observation to make is that all 
optimal solutions will fill the knapsack exactly. This is true because we can 
always increase by a fractional amount the contribution of some object i 
until the total weight is exactly M. 

158 
The Greedy Method 
Several simple greedy strategies to obtain feasible solutions whose sum is 
identically M suggest themselves. First, we may try to fill the knapsack by 
including next the object with largest profit. If an object under considera-
tion doesn't fit then a fraction of it is included to fill the knapsack. Thus 
each time an object is included (except possibly when the last object is 
included) into the knapsack we obtain the largest possible increase in profit 
value. Note that if only a fraction of the last object is included then it 
may be possible to get a bigger increase by using a different object. For 
example, if we have two units of space left and two objects with (p; = 4, 
Wi = 4) and (pj = 3, Wj = 2) remaining then usingj is better than using 
half of i. Let us use this selection strategy on the data of Example 4.2. 
Object one has the largest profit value (p1 = 25). So, it is first placed 
into the knapsack. x1 = 1 and a profit of 25 is earned. Only 2 units of 
knapsack capacity are left. Object two has the next largest profit (p 2 = 24). 
However, w2 = 15 and it doesn't fit into the knapsack. Using x 2 = 2/15 
fills the knapsack exactly with part of object 2 and the value of the result-
ing solution is 28.2. This is solution (ii) and it is readily seen to be sub-
optimal. The method used to obtain this solution is termed a "greedy 
method" because at each step (except possibly the last one) we chose to 
introduce that object which would increase the objective function value the 
most. However, this greedy method did not yield an optimal solution. Note 
that even if we change the above strategy so that in the last step the ob-
jective function increases by as much as possible, an optimal solution is 
not obtained for the instance of Example 4.2. 
We can formulate at least two other greedy approaches attempting to 
obtain optimal solutions. From the preceding example we note that con-
sidering objects in order of nonincreasing profit values does not yield an 
optimal solution because even though the objective function value took on 
large increases at each step, the number of steps was few as the knapsack 
capacity was used up at a rapid rate. So, let us try to be greedy with capacity 
and use it up as slowly as possible. This would require us to consider the 
objects in order of nondecreasing weights w ;. Using Example 4.2, solution 
number (iii) is the resulting solution. This too is suboptimal. This time 
even though capacity was used slowly, profits weren't coming in rapidly 
enough. Thus, our next attempt will be an algorithm that strives to achieve 
a balance between the rate at which profit increases and the rate at which 
capacity is used. At each step we shall include that object which has the 
maximum profit per unit of capacity used. This means that objects will be 
considered in order of the ratio p;/w;. Solution (iv) of Example 4.2 will be 
produced by this strategy. If the objects have already been sorted into 
nonincreasing order of p;/w; then procedure GREEDY_KNAPSACK 
----- ---

Knapsack Problem 
159 
(Algorithm 4.3) obtains solutions corresponding to this strategy. Note 
that solutions corresponding to the first two strategies can be obtained 
using this algorithm if the objects are initially in the appropriate order. 
Disregarding the time to initially sort the objects, each of the three strategies 
outlined above requires only O(n) time. 
procedure GREEDY _KNAPSACK(P, W. M, X, n) 
I I P(l :n) and W(l :n) contain the profits and weights respectively of then/ I 
I /objects ordered so that P(i)IW(i) ~ P(i + l)!W(i + 1). Mis the// 
I /knapsack size and X(l :n) is the solution vector/ I 
real P(l:n), W(l:n), X(l:n), M. cu; 
integer i, n ; 
X -
0 I /initialize solution to zero/ I 
cu -
M 
I !cu = remaining knapsack capacity// 
fori -
1 ton do 
if W(i) > cu then exit endif 
X(i) -
1 
cu -
cu -
W(i) 
repeat 
if i :5 n then X(i) -
cu/W(i) endif 
end GREEDY _KNAPSACK 
Algorithm 4.3 Algorithm for greedy strategies for the knapsack problem 
We have seen that when one applies the greedy method to the solution of 
the knapsack problem there are at least three different measures one can 
attempt to optimize when determining which object to include next. These 
measures are total profit, capacity used and the ratio of accumulated profit 
divided by capacity used. Once an optimization measure has been chosen, 
the greedy method suggests choosing objects for inclusion into the solu-
tion in such a way that each choice optimizes the measure at that time. 
Thus a greedy method using profit as its measure will at each step choose 
an object that increases the profit the most. If the capacity measure is used, 
the next object included will increase this the least. While greedy based 
algorithms using the first two measures do not guarantee optimal solu-
tions for the knapsack problem, Theorem 4.3 shows that a greedy algorithm 
using the third strategy always obtains an optimal solution. This theorem 
is proved by comparing the greedy solution to any optimal solution. If the 
two solutions differ, then we find the first x; at which they differ. Next, it 

160 
The Greedy Method 
is shown how to make the x; in the optimal solution equal to that in the 
greedy solution without any loss in total value. Repeated use of this trans-
formation shows that the greedy solution is optimal. This technique of 
proving solutions optimal will be used often in this text. Hence, you should 
master it at this time. 
Theorem4.3 Ifp1/w1 ~p 2 1w2 ~ ... ~p,,/w,,thenalgorithmGREEDY_ 
KNAPSACK generates an optimal solution to the given instance of the 
knapsack problem. 
Proof: Let X = (x1, ... , Xn) be the solution generated by GREEDY_ 
KNAPSACK. If all the x; equal one then clearly the solution is optimal. 
So, let j be the least index such that Xj ¢. 1. From the algorithm it fol-
lows that X; = l for l ~ i < j, X; = 0 for j < i ~ n and 0 ~ xj < l. Let Y = (y 1, 
... , y) be an optimal solution. Without loss of generality we may assume 
that E W;Y; = M. Let k be the least index such that y k ~ xk' Clearly, such a k 
must exist. It also follows that y k < x k' To see this, consider the three 
possibilities: k < j, k = j or k > j. 
(i) If k < j then XA: = 1. But, YA: ¢. XA: and so YA: < XA:. 
(ii) If k = j then since E w;x; = M and y; = x; for 1 :5 i < j, it follows 
that either YA: < XA: or E w;y; > M. 
(iii) If k > j then E w ;y; > M which is not possible. 
Now suppose we increase YA: to XA: and decrease as many of (yA:+1 •••• , Yn) 
as is necessary so that the total capacity used is still M. This results in a 
new solution Z = (z1, ... , Zn) with Z; = X;, 1 :5 i :5 k and EA:<i:sn w;(y; 
- z;) = WA:(ZA: -
YA:). Then, for Z we have 
= 
E 
p;y; 
1~1Sn 
If E p;z; > E p;y; then Y could not have been an optimal solution. If 
these sums are equal then either Z = X and X is optimal or Z ¢. X. In 
this latter case, repeated use of the above argument will either show that Y 
is not optimal or will transform Y into X, showing that X too is optimal. D 

Job Sequencing with Deadlines 
161 
4.4 JOB SEQUENCING WITH DEADLINES 
We are given a set of n jobs. Associated with job i is an integer deadline 
d; ~ 0 and a profit p; ~ 0. For any job i the profit p; is earned iff the job 
is completed by its deadline. In order to complete a job one has to process 
the job on a machine for one unit of time. Only one machine is available for 
processing jobs. A feasible solution for this problem is a subset, J, of jobs 
such that each job in this subset can be completed by its deadline. The 
value of a feasible solution J is the sum of the profits of the jobs in J or 
E ;" p ;. An optimal solution is a feasible solution with maximum value. 
Example 4.3 Let n = 4, (p1, pi, p3, p4) = (100, 10, 15, 27) and (d1, di, 
dJ, d4) = (2, 1, 2, 1). The feasible solutions and their values are: 
feasible 
processing 
solution 
sequence 
value 
(i) 
(1, 2) 
2, 1 
110 
(ii) 
(1, 3) 
l,3or3,l 
115 
(iii) 
(1, 4) 
4, 1 
127 
(iv) 
(2, 3) 
2,3 
25 
(v) 
(3, 4) 
4,3 
42 
(vi) 
(1) 
1 
100 
(vii) 
(2) 
2 
10 
(viii) 
(3) 
3 
15 
(ix) 
(4) 
4 
27 
Solution (iii) is optimal. In this solution only jobs 1 and 4 are processed 
and the value is 127. These jobs must be processed in the order: job 4 fol-
lowed by job 1. Thus the processing of job 4 begins at time zero and that 
of job 1 is completed at time 2. 
D 
In order to formulate a greedy algorithm to obtain an optimal solution 
we must formulate an optimization measure to determine how the next job 
will be chosen. As a first attempt we can choose the objective function Ew 
p; as our optimization measure. Using this measure, the next job to include 
will be the one that increases E;" p; the most subject to the constraint 
that the resulting J is a feasible solution. This requires us to consider jobs 
in nonincreasing order of the p ;s. Let us apply this criterion to the data of 
Example 4.3. We begin with J = ¢and E;" p; = 0. Job 1 is added to J 

*· 
162 
The Greedy Method 
as it has the largest profit and J = { 1} is a feasible solution. Next, job 4 
is considered. J = { 1, 4} is also feasible. Next, job 3 is considered and 
discarded as J = { 1, 3, 4} is not feasible. Finally, job 2 is considered for 
inclusion into J. It is discarded as J = { 1, 2, 4} is not feasible. Hence, we 
are left with the solution J = { 1, 4} with value 127. This is the optimal 
solution for the given problem instance. Theorem 4.5 proves that the greedy 
algorithm just described always obtains an optimal solution to this se-
quencing problem. 
Before attempting the proof let us first see how we may determine whether 
or not a given J is a feasible solution. One obvious way would be to try out 
all possible permutations of the jobs in J and check if the jobs in J can 
be processed in any one of these permutations (sequences) without violating 
the deadlines. For a given permutation a = i1 ii iJ · · · i1c this is easy to 
do, as the earliest time job i1, 1 :5 j :5 k will be completed isj. Ifj > di1 
then using a, at least job i1 will not be completed by its deadline. However, 
if Ill = i this requires checking i! permutations. Actually, the feasibility 
of a set J can be determined by checking only one permutation of the jobs 
in J. This permutation is any one of the permutations in which jobs are 
ordered in nondecreasing order of deadlines. 
Theorem 4.4 Let J be a set of k jobs and a = i1, ii, ••• , i1c a permuta-
tion of jobs in J such that di, :5 di, :5 • • • :5 d;1c. J is a feasible solution 
iff the jobs in J can be processed in the order a without violating any dead-
line. 
Proof: 
Clearly, if the jobs in J can be processed in the order a without 
violating any deadline then J is a feasible solution. So, we have only to show 
that if J is feasible then a represents a possible order in which the jobs may 
be processed. If J is feasible then there exists a' = r1, ri, ••• , r1c such 
that d,1 ~ j, 1 :5 j :5 k. Assume a' ¢. a. Then let a be the least index 
such that r. ¢. i •. Let r b = i •. Clearly, b > a. In a' we can interchange 
r. and r b· Since d '• ~ d rb, the resulting permutation a" = s i. s i, ••• , s k 
represents an order in which the jobs may be processed without violating 
a deadline. Continuing in this way, a' can be transformed into a without 
violating any deadline. Hence, the theorem is proved. 
D 
The above theorem is true even if the jobs have different processing 
times ti ~ 0 (see the exercises). 

Job Sequencing with Deadlines 
163 
Theorem 4.5 The greedy method described above always obtains an op-
timal solution to the job sequencing problem. 
Proof: 
Let (p;, d;), 1 :5 i :5 n define any instance of the job sequencing 
problem. Let I be the set of jobs selected by the greedy method. Let J be the 
set of jobs in an optimal solution. We shall show that both I and J have the 
same profit values and so I is also optimal. We may assume I ¢. J as other-
wise we have nothing to prove. Note that if J C I then J cannot be optimal. 
Also, the case I C J is ruled out by the nature of the greedy method. So, there 
exist jobs a and b such that a E /, a E J, b E J and b E /. Let a be a highest 
profit job such that a E I and a E J. It follows from the greedy method that 
Pu ~ Ph for all jobs b which are in J but not in I. To see this, note that if Ph > 
Pu then the greedy method would consider job b before job a and include it 
into I. 
Now, consider feasible schedules Sr and S1 for I and J respectively. Let i be 
a job such that i E I and i E J. Let i be scheduled from t tot + 1 in Sr and t' to 
t' + 1 in Sr If t < t' then we may interchange the job (if any) scheduled in 
[t', t' + 1] in Sr with i. If no job is scheduled in [t', t' + 1] in I then i is 
moved to [t', t' + 1]. The resulting schedule is also feasible. If t' < t then a 
similar transformation may be made in S1 • In this way, we can obtain 
schedules Sr' and S1' with the property that all jobs common to I and J are 
scheduled at the same time. Consider the interval [t.,, tu + 1] in S,' in which 
the job a (defined above)is scheduled.Let b be the job( if any)scheduled in S1' 
in this interval. From the choice ofa, Pa ~Ph· Scheduling a from ta to ta + 1 
in S/ and discarding job b gives us a feasible schedule for job setJ' = J -
{ b} U {a}. Clearly, J' has a profit value no less than that of J and differs 
from I in one less job than does J. 
By repeatedly using the transformation just described, J can be trans-
formed into I with no decrease in profit value. Hence, I must also be op-
timal. 
0 
A high level description of the greedy algorithm just described appears 
as Algorithm 4.4. This algorithm constructs an optimal selection of jobs, J, 
to be processed by their due times. The selected jobs may be processed in 
the order given by Theorem 4.4. 

*' 
--
164 
The Greedy Method 
line procedure GREEDY _JOB(D, J, n) 
I /J is an output variable. It is the set of jobs to be completed by// 
I /their deadlines/ I 
1 
j-{1} 
2 
for i -
2 to n do 
3 
if all jobs in J U { i} can be completed by their deadlines 
then] - JU {i} 
4 
end if 
5 
repeat 
6 
end GREEDY _JOB 
Algorithm 4.4 High level description of job sequencing algorithm 
Now, let us see how to represent the set J and how to carry out the test 
of line 3. Theorem 4.4 tells us how to determine if all jobs in J U {i} can 
be completed by their deadlines. We can avoid sorting the jobs in J each 
time by keeping the jobs in J ordered by deadlines. J itself may be repre-
sented by a one dimensional arrayJ(l:k) such thatJ(r), 1 s r s k are the 
jobs in J and D(J(l)) s D(J(2)) s · · · s D(J(k)). To test if J U {i} is 
feasible, we have just to insert i into J preserving the deadline ordering 
and then verify that D(J(r)) s r, 1 s r s k + 1. The insertion of i into J 
is simplified by the use of a fictitious job 0 with D(O) = 0 and J(O) = 0. 
Note also that if job i is to be inserted at position I then only the position 
of jobs J(l), J(l + 1), ... , J(k) is changed after the insertion. Hence, it is 
necessary to verify only that these jobs (and also job i) do not violate their 
deadlines following the insertion. The algorithm which results from this 
discussion is procedure JS (Algorithm 4.5). The algorithm assumes that 
the jobs are already sorted such that p 1 :2: p 2 :2: • • • :2: p n· Further it 
assumes n :2: 1 and that the deadline D(i) of job i is at least 1. Note that 
no job with D(i) < 1 can ever be finished by its deadline. Theorem 4.6 
proves that JS is a correct implementation of the greedy strategy. 

Job Sequencing with Deadlines 
165 
line procedureJS(D. J. n, k) 
I I D(i) :2: 1, 1 s i s n are the deadlines, n :2: 1. The jobs are/ I 
I I ordered such that p 1 :2: p 2 :2: . . • :2: p ,.. J(i) is the ith job in/ I 
I /the optimal solution, 1 s i s k. Also, at termination D(J(i))l I 
I Is D(J(i + 1)), 1 s i < k.I I 
1 
integer D(O:n), J(O;n), i, k, n, r 
2 
D(O) - J(O) - 0 I /initialize/ I 
3 
k -
1; J(l) -
1 I /include job 1/ I 
4 
for i -
2 to n do I I consider jobs in nonincreasing order of p ;/I 
I /Find position for i and check feasibility of insertion/ I 
5 
r -
k 
6 
while D(J(r)) > D(i) and D(J(r)) ¢. r do 
7 
r-r-1 
8 
repeat 
9 
if D(J(r)) s D(i) and D(i) > r then 
I /insert i into JI I 
10 
for I -
k to r + 1 by -
1 do 
11 
J(l + 1) +- J(l) 
12 
repeat 
13 
J(r + 1) -
i; k -
k + 1 
14 
endif 
15 
repeat 
16 
endJS 
Algorithm 4.5 Greedy algorithm for sequencing unit time jobs with deadlines and 
profits 
Theorem 4.6 Procedure JS is a correct implementation of the greedy based 
method described above. 
Proof: 
Since D(i) :2: 1, the job with largest p; will always be in the greedy 
solution. As the jobs are in nondecreasing order of the p;'s, line 3 includes 
the job with largest p ;. The loop of lines 4-15 considers the remaining jobs 
in the order required by the greedy method described earlier. At all times, 
the set of jobs already included in the solution is maintained in J. If J(i), 
1 s i s k is the set already included then Jis such that D(J(i)) s D(J(i + 1)), 
1 s i < k. This allows for easy application of the feasibility test of Theorem 
4.4. When job i is being considered, the loop of lines 6-8 determines where 

*' 
166 
The Greedy Method 
in J this job will have to be inserted. The use of a fictitious job 0 (line 2) 
allows easy insertion into position 1. Let q be such that D(J(q)) :s D(i) 
and D(J(l)) > D(i), q < I :s k. If job i is included into J then jobs J(l), 
q < I :s k will have to be moved one position up in J (lines 10-12). From 
Theorem 4.4, it follows that such a move will retain feasibility of Jiff D(J(l)) 
¢. I, q < I :s k. This condition is verified in line 6. In addition, i may be 
inserted at position q + 1 iff D(i) > q. This is verified in line 9 (note r = q 
upon exit from the while loop if D(J(l)) ¢. I, q < I :s k). The correctness 
of JS follows from these observations. 
D 
Complexity Analysis of Algorithm JS 
For JS there are two possible parameters in terms of which its complexity 
may be measured. We can use n, the number of jobs ands, the number of 
jobs included in the solution J. The loop of lines 6-8 is iterated at most k 
times. Each iteration takes 0(1) time. If the conditional of line 9 is true 
then lines 10-13 are executed. These lines require O(k -
r) time to insert 
job i. Hence, the total time for each iteration of the loop of lines 4-15 is 
O(k). This loop is iterated n -
1 times. If s is the final value of k i.e., 
s is the number of jobs in the final solution, then the total time needed by 
algorithm JS is O(sn). Since s :5 n, the worst case time, as a function of n 
alone is O(n 2). If we consider the job set p; = d; = n - i + 1, 1 :5 i :s n 
then algorithm JS takes O(n 2) time to determine J. Hence, the worst case 
computing time for JS is O(n 2). In addition to the space needed for D, JS 
needs O(s) amount of space for J. Note that the profit values are not needed 
by JS. It is sufficient to know that p; :2: p ;+ i, 1 s i < n. 
D 
A Faster Implementation 
The computing time of JS can be reduced from O(n 2) to nearly O(n) by 
using the disjoint set UNION and FIND algorithms (see Section 2.4) and 
using a different method to determine the feasibility of a partial solution. 
If J is a feasible subset of jobs then we can determine the processing times 
for each of the jobs using the rule: if job i hasn't been assigned a process-
ing time then assign it to the slot [ex -
1, ex] where ex is the largest integer 
r such that 1 s r s di and the slot [ex -
1, ex] is free. This rule simply 
delays the processing of job i as much as possible. Consequently, when J is 
being built up job by job, jobs already in J do not have to be moved 
from their assigned slots in order to accommodate the new job. If for the 

Job Sequencing with Deadlines 
167 
new job being considered there is no ex as defined above then it cannot be 
included in J. The proof of the validity of this statement is left as an exer-
cise. 
Example 4.4 Let n = 5, (p 1, ••• , p s) = (20, 15, 10, 5, 1) and (d 1, 
ds) = (2, 2, 1, 3, 3). Using the above feasibility rule we have: 
J 
assigned slots 
job being considered 
action 
<P 
none 
1 
assign to [l, 2] 
{ 1} 
[l, 2] 
2 
assign to [O, 1] 
... , 
{ 1, 2} 
[O, 1], [1, 2] 
3 
cannot fit; reject 
{ 1, 2} 
[O, 1], [1, 2] 
4 
assign to [2, 3] 
{ 1, 2, 4} [O, 1], [1, 2], [2, 3] 
5 
reject. 
The optimal solution is J = {l, 2, 4 }. 
D 
Since there are only n jobs and each job takes one unit of time, it is nec-
essary only to consider the time slots [i -
1, i], 1 ~ i ~ b such that b = 
min{ n, max{ d;}}. One way to implement the above scheduling rule is to 
partition the time slots [i -
1, i], 1 s i s b into sets. We shall use i to 
represent the time slot [i -
l, i]. For any slot i let n; be the largest integer 
such that ni ~ i and slot n; is free. To avoid end conditions, we introduce 
a fictitious slot [ -1, O] which is always free. Two slots i andj are in the 
same set iff ni = nj. Clearly, if i andj, i < j, are in the same set then 
i, i + l, i + 2, ... , j are in the same set. Associated with each set, k, of 
slots is a value F(k). F(k) = n; for all slots i in set k. Using the set repre-
sentation of Section 2.4, each set will be represented as a tree. The root 
node will identify the set. F will be defined only for root nodes. Initially, 
all slots are free and we have b + 1 sets corresponding to the b + 1 slots 
[i -
1, i], 0 s i s b. At this time F(i) = i, 0 s i s b. We shall use P(i) 
to link slot i into its set tree. Using the conventions for the UNION and 
FIND algorithms of Section 2.4, P(i) = -1, 0 s i ~ b initially. If a job 
with deadline d is to be scheduled then we need to find the root of the tree 
containing the slot min{ n, d}. If this root is j then F(j) is the nearest free 
slot proviJed F(j) ¢. 0. Having used this slot, the set with rootj should be 
combined with the set containing slot F(j) -
1. 
Example 4.5 
Using the problem instance of Example 4.4, the trees defined 
by the P(i)'s for the first three iterations, are: 

-
168 
The Greedy Method 
job 
J 
trees 
considered 
action 
'5 
F 
0 
I 
2 
<:D 
4 
~ 
ED 
ED 
e 
ED 
l,d,=2 
O. 2] is free 
P(O) 
P(I) 
P(2) 
P(3) 
P(4) 
P(5) 
/ 1 f 
F 
0 
I 
3 
4 
5 
0 
@ P(I) 
0 
0 
0 
2,d 2=2 
F(I) ~I, 
P(O) 
0 
(3) 
(4) 
(5) 
[o, a free 
P(2) 
) 1.2 \ 
F 
(1)=0 
F(3)=3 
F(4)=4 
Fa=5 
F(I) = 0 
~(I) 
0 
0 
P(3) 
P(4) 
P(5) 
reject 
Figure 4.1 Fast job scheduling 
The faster algorithm appears as FJS. Its computing time is readily ob-
served to be O(n a(2n, n)) (recall that a(2n, n) is the inverse of Ackermann's 
function defined in Section 2.4). It needs an additional 2n words of space 
for F and P. 
line procedure FJS(D, n, b, J, k) 
//find an optimal solution J = J(l), ... , J(k)I I 
I lit is assumed that pi 2:: pi 2:: ... 2:: Pm and that b 
min{n, 
max{D(i)} }// 
1 
integer b, D(n), J(n), F(O:b ), P(O:b) 
2 
for i -
0 to b do I /initialize trees/ I 
3 
F(i) -
i; P(i) -
- 1/ I 
4 
//repeat 
5 
k -
0 
I /initialize JI I 
6 
for i -
1 ton do I /use greedy rule/ I 
7 
j -
FIND(min(n, D(i)) 
8 
ifF(j) ~ Othenk -
k + l;J(k)- i 
//select job ii/ 
9 
I -
FIND(F(j) -
1); call UNION(!, j) 
10 
F(j) -
F(l) 
I lj may be new root/ I 
11 
endif 
12 
repeat 
13 
endFJS 
Algorithm 4.6 Faster algorithm for job sequencing 

Optimal Merge Patterns 
169 
4.5 OPTIMAL MERGE PATTERNS 
In Section 3.4 we saw that two sorted files containing n and m records 
respectively could be merged together to obtain one sorted file in time 
O(n + m). When more than two sorted flies are to be merged together 
the merge can be accomplished by repeatedly merging sorted files in pairs. 
Thus, if files Xl, X2, X3 and X4 are to be merged we could first merge 
Xl and X2 to get a file Yl. Then we could merge Yl and X3 to get Y2. 
Finally, Y2 and X4 could be merged to obtain the desired sorted file. 
Alternatively, we could first merge Xl and X2 getting Yl, then merge 
X3 and X 4 getting Y2 and finally Y1 and Y2 getting the desired sorted 
file. Given n sorted files there are many ways in which to pairwise merge 
them into a single sorted file. Different pairings require differing amounts 
of computing time. The problem we shall address ourselves to now is that 
of determining an optimal (i.e. one requiring the fewest comparisons) way 
to pairwise merge n sorted files together. 
Example 4.6 Xl, X2 and X3 are three sorted files of length 30, 20 and 10 
records each. Merging Xl and X2 requires 50 record moves. Merging the 
result with X3 requires another 60 moves. The total number of record 
moves required to merge the three files this way is 110. If instead, we first 
merge X2 and X3 (taking 30 moves) and then Xl (taking 60 moves), the 
total record moves made is only 90. Hence, the second merge pattern is 
faster than the first. 
D 
A greedy attempt to obtain an optimal merge pattern is easy to formu-
late. Since merging an n record file and an m record file requires possibly 
n + m records moves, the obvious choice for a selection criterion is: at each 
step merge the two smallest size files together. Thus, if we have five files 
(F 1 , ••• , F 5) with sizes (20, 30, 10, 5, 30) our greedy rule would generate 
the following merge pattern: merge F 4 and F Jto get Z 1 ( / Z 1 / = 15 ); merge 
Z1 and F1 to get Z2 (IZ2 I = 35); merge Fi and Fs to get ZJ (IZJ I = 60); 
merge Z2 and ZJ to get the answer Z 4 • The total number of record moves 
is 205. One can verify that this is an optimal merge pattern for the given 
problem instance. 
The merge pattern such as the one just described will be referred to as a 
2-way merge pattern (each merge step involves the merging of two files). 
2-way merge patterns may be represented by binary merge trees. Figure 4.2 
shows a binary merge tree representing the optimal merge pattern obtained 
for the above five files. The leaf nodes are drawn as squares and represent 

170 
The Greedy Method 
the given five files. These nodes will be called external nodes. The remain-
ing nodes are drawn circular and are called internal nodes. Each internal 
node has exactly two children and it represents the file obtained by merging 
the files represented by its two children. The number in each node is the 
length (i.e., the number of records) of the file represented by that node. 
5 
F4 
F3 
Figure 4.2 Binary merge tree representing a merge pattern 
The external node F4 is at a distance of 3 from the root node Z4 (a node 
at level i is at a distance of i -
1 from the root). Hence, the records of 
file F4 will be moved three times, once to get Z1, once again to get Z2 and 
finally one more time to get Z 4 • If d; is the distance from the root to the 
external node for file F; and q; the length of F; then the total number of 
record moves for this binary merge tree is 
n 
E drq;. 
i-1 
This sum is called the weighted external path length of the tree. 
An optimal 2-way merge pattern corresponds to a binary merge tree with 
minimum weighted external path length. The procedure TREE of Algo-
rithm 4. 7 uses the greedy rule stated earlier to obtain a 2-way merge tree 
for n files. The algorithm has as input a list L of n trees. Each node in a 
tree has three fields, LCHILD, RCHILD and WEIGHT. Initially, each tree 
in L has exactly one node. This node is an external node and has LCHILD 
and RCHILD fields zero while the WEIGHT is the length of one of the n 
files to be merged. During the course of the algorithm, for any tree in L 
with root node T, WEIGHT(T) is the length of the merged file it repre-
sents (WEIGHT(T) equals the sum of the lengths of the external nodes in 
tree T). Procedure TREE uses three subalgorithms, GETNODE(T), 

Optimal Merge Patterns 
171 
LEAST(L) and INSERT(L, T). GETNODE(T) provides a new node for use 
in building the tree. LEAST(L) finds a tree in L whose root has least 
WEIGHT. This tree is removed from L. INSERT(L, T) inserts the tree with 
root T into the list L. Theorem 4. 7 below will show that the greedy pro-
cedure TREE (Algorithm 4. 7) generates an optimal 2-way merge tree. 
line procedure TREE(L. n) 
I IL is a list of n single node binary trees as described above/ I 
1 
for i -
1 to n - 1 do 
2 
call GETNODE(T) I /merge two trees with/ I 
3 
LCHILD(T) - LEAST(L) //smallest lengths// 
4 
RCHILD(T) - LEAST(L) 
5 
WEIGHT(T)- WEIGHT(LCHILD(T)) + WEIGHT(RCHILD(T)) 
6 
call INSER T(L, T) 
7 
repeat 
8 
return (LEAST(L)) 
I !tree left in L is the merge tree/ I 
9 
end TREE 
Algorithm 4. 7 Algorithm to generate a 2-way merge tree 
Example 4. 7 Let us see how algorithm TREE works when L initially repre-
sents 6 files with lengths (2, 3, 5, 7, 9, 13). Figure 4.3 shows the list L at 
the end of each iteration of the for loop. The binary merge tree which 
results at the end of the algorithm can be used to determine which files are 
merged. Merging is performed on those files which are "lowest" (have the 
greatest depth) in the tree. 
Analysis of Algorithm 4. 7 
The main loop is executed n -
1 times. If L is kept in nondecreasing 
order according to the WEIGHT value in the roots, then LEAST(L) re-
quires only 0(1) time and INSERT(L, T) can be done in O(n) time. Hence 
the total time taken is O(n 2). In case L is represented as a min-heap where 
the root value is s the values of its children (Section 2.3), then LEAST(L) 
and INSERT(L, T) can be done in O(log n) time. In this case the computing 
time for TREE is O(n log n). Some speed-up may be obtained by combining 
the INSERT of line 6 with the LEAST of line 4. 
Theorem 4. 7 If L initially contains n ~ 1 single node trees with WEIGHT 
values (q1, qi, ... , qn) then algorithm TREE generates an optimal 2-way 
merge tree for n files with these lengths. 

• 
• 
172 
The Greedy Method 
ofter iteration 
L 
initial 
(g] CT.] [fil[z] (2) 
!@] 
~ 
[fil [I] 
3 
ffi] 
[@) 
2 
~ 
!@] 
3 
l1J ffi] 
3 
~ 
hr@ 
9 
3 
4 
R h 9 
2 
5 
5 
Figure 4.3 Trees in list L of procedure TREE for Example 4. 7 
Proof: The proof is by induction on n. For n = 1, a tree with no internal 
nodes is returned and this tree is clearly optimal. For the induction hy-
pothesis, assume the algorithm generates an optimal 2-way merge tree for 
all (q1, qi, ... , qm), 1 s m < n. We will show that the algorithm also 
generates optimal trees for all (q1, qi, ... , qn). Without loss of generality, 
we may assume q1 s qi $ 
• · • s qn and that q1 and qi are the values 
of the WEIGHT fields of the trees found by algorithm LEAST in lines 3 
and 4 during the first iteration of the for loop. Now, the subtree T of 
Figure 4.4 is created. Let T' be an optimal 2-way merge tree for (qi. qi, 
... , qn). Let P be an internal node of maximum distance from the root. If 
the children of Pare not q1 and qi then we may interchange the present 
children with q1 and qi without increasing the weighted external path length 
of T'. Hence, T is also a subtree in an optimal merge tree. Now in T' if we 
replace T by an external node with weight q 1 + qi then the resulting tree 
T" is an optimal merge tree for (q 1 + qi, q 3, ••• q n).From the induction 
hypothesis procedure TREE, after replacing T by the external node with 
------------ ---- -

Optimal Merge Patterns 
173 
value q 1 + q 2, proceeds to find an optimal merge tree for (q 1 + q 2, q 3, 
... , q n). Hence, TREE generates an optimal mergetreefor(qi, q 2, ••• , q n). D 
T 
Figure 4.4 The simplest binary merge tree 
The greedy method to generate merge trees also works for the case of k-ary 
merging. In this case the corresponding merge tree is a k-ary tree. Since 
all internal nodes must have degree k, for certain values of n there does not 
correspond a k-ary merge tree. For example when k = 3 there is no k-ary 
merge tree with n = 2 external nodes. Hence, it is necessary to introduce 
a certain number of "dummy" external nodes. Each dummy node is assigned 
a q; of zero. This dummy value does not affect the weighted external path 
length of the resulting k-ary tree. Exercise 13 shows that a k-ary tree with 
all internal nodes having degree k exists only when the number of external 
nodes n satisfies the equality n mod(k -
1) = 1. Hence, at most k -
2 
dummy nodes have to be added. The greedy rule to generate optimal 
merge trees is: at each step choose k subtrees with least length for merging. 
Exercise 14 proves the optimality of this rule. 
Huffman Codes 
Another application of binary trees with minimal weighted external path 
length is to obtain an optimal set of codes for messages M 1 , ••• , Mn+ 1• 
Each code is a binary string which will be used for transmission of the cor-
responding message. At the receiving end the code will be decoded using a 
decode tree. A decode tree is a binary tree in which external nodes repre-
sent messages. The binary bits in the code word for a message determine 
the branching needed at each level of the decode tree to reach the correct 
external node. For example, if we interpret a zero as a left branch and a 
one as a right branch, then the decode tree of Figure 4.5 corresponds to 
codes 000, 001, 01, and 1 for messages M 1, M 2, M 3 and M 4 respectively. 
These codes are called Huffman codes. The cost of decoding a code word 
is proportional to the number of bits in the code. This number is equal 
to the distance of the corresponding external node from the root node. If 
q; is the relative frequency with which message M; will be transmitted, then 

174 
The Greedy Method 
the expected decode time is I: 1:si:sn+ 1 q,.d,. where d,. is the distance of the 
external node for message M; from the root node. The expected decode time 
is minimized by choosing code words resulting in a decode tree with mini-
mal weighted external path length! Note that I: t:si:sn+t q;d; is also the ex-
pected length of a transmitted message. Hence the code which minimizes 
expected decode time also minimizes the expected length of a message. 
Figure 4.5 Huffman codes 
4.6 MINIMUM SPANNING TREES 
Definitlon Let G = 
( V, E) be an undirected connected graph. A sub-
graph T = ( V, E ') of G is a spanning tree of G iff T is a tree. 
Example 4.8 Figure 4.6 shows the complete graph on 4 nodes together 
with three of its spanning trees. 
D 
Figure 4.6 An undirected graph and three of its spanning trees 
Spanning trees can be used to obtain an independent set of circuit equa-
tions for an electrical network. First, a spanning tree for the electrical 
network is obtained. Let B be the set of network edges not in the spanning 
tree. Adding an edge from B to the spanning tree creates a cycle. Different 
edges from B result in different cycles. Kirchoff's second law is used on 

Minimum Spanning Trees 
175 
each cycle to obtain a circuit equation. The cycles obtained in this way are 
independent (i.e., none of these cycles can be obtained by taking a linear 
combination of the remaining cycles) as each contains an edge from B 
which is not contained in any other cycle. Hence, the circuit equations 
so obtained are also independent. In fact, it may be shown that the cycles 
obtained by introducing the edges of B one at a time into the resulting 
spanning tree form a cycle basis and so all other cycles in the graph can 
be constructed by taking a linear combination of the cycles in the basis (see 
Harary in the references for further details). 
It is not difficult to imagine other applications for spanning trees. One 
that is of interest arises from the property that a spanning tree is a mini-
mal subgraph G' of G such that V(G ') = V(G) and G' is connected 
(by a minimal subgraph, we mean one with the fewest number of edges). 
Any connected graph with n vertices must have at least n -
1 edges and 
all connected graphs with n -
1 edges are trees. If the nodes of G rep-
resent cities and the edges represent possible communication links con-
necting 2 cities, then the minimum number of links needed to connect the 
n cities is n - 1. The spanning trees of G will represent all feasible choices. 
In any practical situation, however, the edges will have weights assigned 
to them. These weights might represent the cost of construction, the length 
of the link, etc. Given such a weighted graph one would then wish to select 
for construction a set of communication links that would connect all the 
cities and have minimum total cost or be of minimum total length. In 
either case the links selected will have to form a tree (assuming all weights 
are positive). In case this is not so, then the selection of links contains 
a cycle. Removal of any one of the links on this cycle will result in a link 
selection of less cost connecting all cities. We are therefore interested in 
finding a spanning tree of G with minimum cost. (The cost of a spanning 
16 
16 
21 
II 
5 
II 
5 
19 
6 
3 
6 
3 
3 
4 
10 
5 
18 
5 
18 
Figure 4.7 A graph and one of its minimum costs spanning trees 

176 
The Greedy Method 
tree is the sum of the costs of the edges in that tree.) Figure 4. 7 shows a 
graph and one of its minimum cost spanning trees. 
A greedy method to obtain a minimum cost spanning tree would build 
this tree edge by edge. The next edge to include is chosen according to 
some optimization criterion. The simplest such criterion would be to choose 
an edge that results in a minimum increase in the sum of the costs of the 
edges so far included. There are two possible ways to interpret this criterion. 
In the first, the set of edges so far selected form a tree. Thus, if A is the set 
of edges selected so far, then A forms a tree. The next edge (u, v) to be 
included in A is a minimum cost edge not in A with the property that 
A U { (u, v)} is also a tree. Exercise 17 shows that this selection criterion 
results in a minimum cost spanning tree. The corresponding algorithm is 
known as Prim's algorithm. 
Example 4.9 Figure 4.8(b) shows the working of Prim's method on the 
graph of Figure 4.8(a). The spanning tree obtained has a cost of 105. 
0 
Having seen how Prim's method works, let us obtain a SPARKS algorithm 
to find a minimum spanning tree using this method. The algorithm will 
start with a tree that includes only a minimum cost edge of G. Then, edges 
will be added to this tree one by one. The next edge (i, j) to be added is 
such that i is a vertex already included in the tree, j is a vertex not yet 
included and the cost of (i, j), COST(i, j) is minimum among all edges 
(k, I) such that vertex k is in the tree and vertex I not in the tree. In order 
to determine this edge (i, j) efficiently, we shall associate with each vertexj 
not yet included in the tree a value NEARU). NEARU) is a vertex in the 
tree such that COST(j, NEAR(j)) is minimum among all choices for 
NEAR(j). We shall define NEAR(j) = 0 for all verticesj that are already 
in the tree. The next edge to include is defined by the vertex j such that 
NEAR(j) ~ 0 (j not already in the tree) and COST(j, NEAR(j)) is 
minimum. 
In procedure PRIM (Algorithm 4.8), line 3 selects a minimum cost edge. 
Lines 4-10 initialize the variables so as to represent a tree comprising only 
the edge (k, I). In the loop of lines ll-21 the remainder of the spanning 
tree is built up edge by edge. Line 12 selects (j, NEAR(j)) as the next edge 
to include. Lines 16-20 update NEAR( ). 

Minimum Spanning Trees 
177 
Figure 4.8(a) Graph for Examples 4.9 and 4.10 
Edge 
Cost 
Spanning tree 
(1,2) 
10 
©--@ 
(2,6) 
25 
ef® 
(3.,6) 
15 
~ 
(6,4) 
20 
~ 
6 
3 
(1,4) 
reject 
(3,5) 
35 
Figure 4.8(b) 
Stages in Prim's Algorithm 
The time required by procedure PRIM is readily seen to be 8(n 2 ) where 
n is the number of vertices in the graph G. To see this note that line 3 
takes fJ(e) (e = IEI) time and line 4 takes 8(1) time. The loop of lines 6-9 

178 
The Greedy Method 
line procedure PRIM(E, COST, n, T, mincost) 
I IE is the set of edges in GI I 
I ICOST(n, n) is the cost adjacency matrix of an n vertex graph/ I 
//such that COST(i,j) is either a positive real number or + oo if// 
I !no edge (i,j) exists. A minimum spanning tree is computed and// 
//stored as a set of edges in the array T(l:n -
1, 2). (T(i, 1),11 
I IT(i, 2)) is an edge in the min-cost spanning tree. The final cost/ I 
I !is assigned to mincost/ I 
1 
real COST(n, n), mincost; 
2 
integer NEAR (n ), n, i, j, k, I, T (1 :n - 1, 2); 
3 
(k, I) -
edge with minimum cost 
4 
mincost -
COST (k, I) 
5 
(T(l, 1), T(l, 2)) -
(k, I) 
6 
for i -
1 to n do I /initialize NEAR/ I 
7 
if COST(i, I) < COST(i, k) then NEAR (i) -
I 
8 
else NEAR (i) -
k endif 
9 
repeat 
10 
NEAR (k) -
NEAR (l) -
0 
11 
for i -
2 to n -
1 do 
I !find n -
2 additional edges for TI I 
12 
let j be an index such that NEAR (j) ~ 0 and COST(j, NEAR (j)) 
is minimum 
13 
(T(i, 1), T(i, 2)) -
(j, NEAR (j)) 
14 
mincost -
mincost + COST(.i, NEAR (.i)) 
15 
NEAR (j) -
0 
16 
fork -
1 ton do I /update NEAR/ I 
17 
if NEAR (k) ~ 0 and COST(k, NEAR(k)) > COST(k,j) 
18 
then NEAR (k) - j 
19 
endif 
20 
repeat 
21 
repeat 
22 
if mincost ~ oo then print ('no spanning tree') endif 
23 
end PRIM 
Algorithm 4.8 Prim's minimum spanning tree algorithm 
takes fJ(n) time. Line 12 and the loop of lines 16-20 require fJ(n) time. So, 
each iteration of the loop of lines 11-21 takes fJ(n) time. The total time for 
the loop of therefore fJ(n 2). Hence, procedure PRIM has a time complexity 
that is (J(n 2). 
The algorithm may be speeded a bit by making the observation that a 
minimum spanning tree includes for each vertex v a minimum cost edge 

Minimum Spanning Trees 
179 
incident to v. To see this, suppose T is a minimum cost spanning tree for 
G = ( V, E ). Let v be any vertex in T. Let ( v, w) be an edge with mini-
mum cost among all edges incident to v. Assume (v, w) EE (T) and that 
COST( v, w) < COST( v, x) for all edges ( v, x) E E ( T ). The inclusion 
of (v, w) into T creates a unique cycle. This cycle must include an edge 
(v. x), x ~ w. Removing (v, x) from E(T) U {(v. w)} breaks this cycle 
without disconnecting the graph ( V, E ( T) U { ( v, w)} ). Hence, ( V, E ( T) U 
{(v, w} -
{(v, x)}) is also a spanning tree. Since COST(v, w) < COST(v, 
x) this spanning tree has lesser cost than T. This contradicts the assump-
tion that T is a minimum cost spanning tree of G. So, T includes mini-
mum cost edges as stated above. 
From this observation it follows that we can actually start the algorithm 
with a tree consisting of any arbitrary vertex and no edge. Then edges 
may be added one by one. The changes needed are to lines 3-11. These 
lines may be replaced by the lines 
3 ' 
mincost -
0 
4 ' 
for i -
2 to n do 
I /vertex 1 is initially in TI I 
5' 
NEAR (i) -
1 
6' 
repeat 
7 ' 
NEAR (1) -
0 
8 '-11 ' for i -
1 to n - 1 do I /find n - 1 edges for TI I 
The overall complexity remains 8(n 2). 
There is a second possible interpretation of the optimization criteria 
mentioned earlier where the edges of the graph are considered in nonde-
creasing order of cost. This interpretation is that the set T of edges so far 
selected for the spanning tree be such that it is possible to complete T into 
a tree. Thus T may not be a tree at all stages in the algorithm. In fact, it 
will generally only be a forest since the set of edges T can be completed 
into a tree iff there are no cycles in T. We shall show in Theorem 4. 7 that 
this interpretation of the greedy method also results in a minimum cost 
spanning tree. This method is due to Kruskal. 
Example 4.10 Consider the graph of Figure 4.8(a). Using Kruskal's 
method the edges of this graph are considered for inclusion in the mini-
mum cost spanning tree in the order (1, 2), (3, 6), (4, 6), (2, 6), (1, 4), 
(3, 5), (2, 5), (1, 5), (2, 3), and (5, 6). This corresponds to the cost se-
quence 10, 15, 20, 25, 30, 35, 40, 45, 50, 55. The first four edges are 
included in T. The next edge to be considered is (I, 4). This edge con-
nects two vertices already connected in T and so it is rejected. Next, the 
edge (3, 5) is selected and that completes the spanning tree. Figure 4.9 

• 
• 
•, 
180 
The Greedy Method 
shows the forest represented by T during the various stages of this com-
putation. The spanning tree obtained ha& a cost of 105. 
D 
~ Cast 
S~nning Forest 
CD®®©®® 
(1,2) 
10 
©-®@©®® 
(3,6) 
15 
©-®I©® 
(4,6) 
20 
CD-®J ® 
4 
6 
(2,6) 
25 
~® 
3 
6 
( 1,4) 
30 
(reject l 
(3,5) 
35 
~ 
3 
Figure 4.9 Stages in Kruskal's algorithm 
For clarity, Kruskal's algorithm is written out more formally in 
Algorithm 4.9. Initially Eis the set of all edges in G. The only functions we 
wish to perform on this set are: (i) determine an edge with minimum cost 
(line 3), and (ii) delete this edge (line 4). Both these functions can be 
performed efficiently if the edges in E are maintained as a sorted sequential 
list. Actually, it is not essential to sort all the edges so long as the next 
edge for line 3 can be determined easily. If the edges are maintained as a 
m!n-heap then the next edge to consider can be obtained in O(log e) time 
if G has e edges. The construction of the heap itself takes O(e) time. 
- -------
-----~--~ 

Minimum Spanning Trees 
181 
1 
T-
</> 
2 
while T contains fewer than n - 1 edges and E ~ cJ> do 
3 
choose an edge (v, w) from E of lowest cost 
4 
delete (v, w) from E 
5 
if (v, w) does not create a cycle in T 
6 
then add (v, w) to T 
7 
else discard ( v, w) 
8 
endif 
9 
repeat 
Algorithm 4.9 Early form of minimum spanning tree algorithm due to Kruskal 
In order to be able to perform steps 5 and 6 efficiently, the vertices in 
G should be grouped together in such a way that one may easily determine 
if the vertices v and w are already connected by the earlier selection of 
edges. In case they are, then the edge (v, w) is to be discarded. If they 
are not, then ( v, w) is to be added to T. One possible grouping is to place 
all vertices in the same connected component of Tinto a set (all connected 
components of T will also be trees). Then, two vertices v, w are con-
nected in T iff they are in the same set. For example, when the edge (2, 6) 
is to be considered, the sets would be { 1, 2 }, { 3, 4, 6 }, and { 5 }. Vertices 
2 and 6 are in different sets so these sets are combined to give { 1, 2, 3, 4, 6} 
and { 5}. The next edge to be considered is (1, 4). Since vertices 1 and 4 
are in the same set, the edge is rejected. The edge (3, 5) connects vertices 
in different sets and results in the final spanning tree. Using the set repre-
sentation of Section 2.4 and the UNION and FIND algorithm of that 
section we can obtain an efficient (almost linear) implementation of lines 
5 and 6. The computing time is, therefore, determined by the time for 
lines 3 and 4 which in the worst case is O(e loge). 
If the representations discussed above are used then the procedure of 
Algorithm 4.10 results. In line 3 an initial heap of edges is constructed. In 
line 4 each vertex is assigned to a distinct set (and hence to a distinct tree). 
T is the set of edges to be included in t!le minimum cost spanning tree 
while i is the number of edges in T. T itself may be represented as a se-
quential list using a two dimensional array T(l:n -
1, 2). Edge (u, v) 
may be added to T by the assignments T(i, 1) -
u and T(i, 2) -
v. In the 
loop of lines 6-14 edges are removed from the heap one by one in non-
decreasing order of cost. Line 8 determines the sets containing u and v. 

182 
The Greedy Method 
Ifj ~ k then vertices u and v are in different sets (and so in different trees) 
and edge (u, v) is included into T. The sets containing u and v are com-
bined (line 12). If u = v the edge (u, v) is discarded as its inclusion into T 
will create a cycle. Line 15 determines whether a spanning tree was found. 
It follows that i ~ n -
1 iff the graph G is not connected. One may verify 
that the computing time is O(e log e) where e is the number of edges in 
G (e = IEI). 
line procedure.KR USKAL (E, COST, n, T, mincost) 
I IE is the set of edges in G. G has n vertices. COST (u, v) is the// 
/I cost of edge (u, v). Tis the set of edges in the minimum span-/ I 
I I ning tree and mincost is its cost/ I 
1 
real mincost, COST (l:n, l:n) 
2 
Integer PARENT(l:n), T(l:n - 1, 2), n 
3 
construct a heap out of the edge costs using HEAP/FY 
4 
PARENT -
- 1 I I each vertex is in a different set/ I 
5 
i -
mincost -
0 
6 
while i < n - 1 and heap not empty do 
7 
delete a minimum cost edge (u, v) from the heap and reheapify 
using ADJUST 
8 
j -
FIND(u); k -
FIND(v) 
9 
if.j ~ k then i - i + 1 
10 
T(i, 1) -
u; T(i, 2) -
v 
11 
mincost -
mincost + COST(u, v) 
12 
call UNION(j, k) 
13 
endif. 
14 
repeat 
15 
if. i ~ n -
1 then print ('no spanning tree') endif. 
16 
return 
17 
end KR USKAL 
Algorithm 4.10 Kruskal's Algorithm 
Theorem 4.8 Kruskal's algorithm generates a minimum cost spanning 
tree for every connected undirected graph G. 
Proof: Let G be any undirected connected graph. Let T be the spanning 
tree for G generated by Kruskal's algorithm. Let T' be a minimum cost 
spanning tree for G. We shall show that both T and T' have the same cost. 
Let E ( T) and E ( T') respectively be the edges in T and T'. If n is the 
number of vertices in G then both T and T' have n -
1 edges. If E ( T) = 

Single Source Shortest Paths 
183 
E ( T') then T is clearly of minimum cost. If E ( T) ~ E ( T') then let 
e be a minimum cost edge such that e E E(T) and e E E(T'). Clearly, 
such an e must exist. The inclusion of e into T' creates a unique cycle 
(Exercise 20). Let e, e i. e 2, ••• , e k be this unique cycle. At least one of the 
e;'s, 1 ::::; i ::::; k is not in E(T) as otherwise T will also contain the cycle 
e, e i. e 2, ••• , e k· Let e 1 be an edge on this cycle such that e 1 E E(T). If e 1 
is of lesser cost thane then Kruskal's algorithm would consider e1 before e 
and include e1 into T. To see this note that all edges in E(T) of cost less 
than the cost of e are also in E(T') and do not form a cycle with e1• So 
c(ej) ~ c(e) (c( ·)is the edge-cost function). 
Now, reconsider the graph with edge set E(T') U { e }. Removal of any 
edge on the cycle e, e 1, e 2, ••• , e k will leave behind a tree T" (Exercise 20). 
In particular, if we delete the edge ei then the resulting tree T" will have 
a cost no more than the cost of T' (as c(eJ) ~ c(e)). Hence, T" is also a 
minimum cost tree. 
By repeatedly using the transformation described above, tree T' can 
be transformed into the spanning tree T without any increase in cost. 
Hence, T is a minimum cost spanning tree. 
D 
4.7 SINGLE SOURCE SHORTEST PATHS 
Graphs may be used to represent the highway structure of a state or 
country with vertices representing cities and edges representing sections of 
highway. The edges may then be assigned weights which might be either 
the distance between the two cities connected by the edge or the average 
time to drive along that section of highway. A motorist wishing to drive 
from city A to city B would be interested in answers to the following ques-
tions: 
(i) Is there a path from A to B? 
(ii) If there is more than one path from A to B, which is the shortest 
path? 
The problems defined by (i) and (ii) above are special cases of the path 
problem we shall be studying in this section. The length of a path is now 
defined to be the sum of the weights of the edges on that path. The starting 
vertex of the path will be referred to as the source and the last vertex the 
destination. The graphs will be digraphs to allow for one way streets. In 
the problem we shall consider, we are given a directed graph G = ( V, E ), 
a weighting function c(e) for the edges of G and a source vertex vo. The 

184 
The Greedy Method 
problem is to determine the shortest paths from v 0 to all the remaining 
vertices of G. It is assumed that all the weights are positive. 
Example 4.11 
Consider the directed graph of Figure 4.lO(a). The num-
bers on the edges are the weights. If vo is the source vertex, then the 
shortest path from vo to v1 is vo v2 VJ v1. The length of this path is 10 + 
15 + 20 = 45. Even though there are three edges on this path, it is 
shorter than the path vov1 which is of length 50. There is no path from 
vo to vs. Figure 4. lO(b) lists the shortest paths from vo to v1, v2, v3 and V4. 
The paths have been listed in nondecreasing order of path length. 
D 
45 
Path 
Length 
I) 
Vo Vz 
10 
2) 
Vo Vi V3 
25 
3) 
VoVzV3V1 
45 
4) 
Vo V4 
45 
(a) 
(b) 
Figure 4.10 Graph and shortest paths from v o to all destinations 
In order to formulate a greedy based algorithm to generate the shortest 
paths, we must conceive of a multistage solution to the problem and also 
conceive of an optimization measure. One possibility is to build the 
shortest paths one by one. As an optimization measure we can use the sum 
of the lengths of all paths so far generated. In order for this measure to 
be minimized, each individual path must be of minimum length. Using 
this optimization measure, if we have already constructed i shortest paths 
then the next path to be constructed should be the next shortest minimum 
length path. The greedy way (and also a systematic way) to generate the 
shortest paths from vo to the remaining vertices would be to generate these 
paths in nondecreasing order of path length. First, a shortest path to the 
nearest vertex is generated. Then a shortest path to the second nearest 
vertex is generated and so on. For the graph of Figure 4.lO(a) the nearest 
vertex to v 0 is v 2 (c(v 0, vi) = 10). The path v 0v 2 will be the first path 
generated. The second nearest vertex to vo is VJ and the distance between 
vo andv3 is 25. The path vov2v3 will be the next path generated. In order 
to generate the shortest paths in this order, we need to be able to deter-

Single Source Shortest Paths 
185 
mine (i) the next vertex to which a shortest path must be generated and (ii) 
a shortest path to this vertex. Let S denote the set of vertices (including 
vo) to which the shortest paths have already been generated. For w not in 
S, let DIST( w) be the length of the shortest path starting from v0 going 
through only those vertices which are in S and ending at w. We observe 
that: 
(i) If the next shortest path is to vertex u, then the path begins at vo, 
ends at u and goes through only those vertices which are in S. To 
prove this we must show that all of the intermediate vertices on the 
shortest path to u must be in S. Assume there is a vertex w on this 
path that is not in S. Then, the vo to u path also contains a path 
from vo tow which is of length less than the vo to u path. By assump-
tion the shortest paths are being generated in nondecreasing order of 
path length, and so the shorter path vo to w must already have been 
generated. Hence, there can be no intermediate vertex which is not 
in S. 
(ii) The destination of the next path generated must be that vertex u 
which has the minimum distance, DIST(u ), among all vertices not in 
S. This follows from the definition of DIST and observation (i). In 
case there are several vertices not in S with the same DIST, then any 
of these may be selected. 
(iii) Having selected a vertex u as in (ii) and generated the shortest vo to 
u path, vertex u becomes a member of S. At this point the length of 
the shortest paths starting at vo, going through vertices only in S and 
ending at a vertex w not in Smay decrease. I.e., the value of DIST(w) 
may change. If it does change, then it must be due to a shorter path 
starting at vo going to u and then to w. The intermediate vertices 
on the vo to u path and the u tow path must all be in S. Further, the 
vo to u path must be the shortest such path, otherwise DIST(w) is 
not defined properly. Also, the u tow path can be chosen so as to not 
contain any intermediate vertices. Therefore, we may conclude that if 
DIST( w) is to change (i.e., decrease), then it is because of a path 
from vo to u to w where the path from vo to u is the shortest such 
path and the path from u to w is the edge (u, w ). The length of this 
path is DIST(u) + c(u, w). 
The above observations lead to a simple algorithm (Algorithm 4.11) 
for the single source shortest path problem. This algorithm (known as 
Dijkstra's algorithm) actually only determines the lengths of the shortest 
paths from vo to all other vertices in G. The actual generation of the paths 

., 
#I 
•I 
• 
186 
The Greedy Method 
requires a minor extension to this algorithm and is left as an exercise. In 
procedure SHORTEST_PATHS (Algorithm 4.11) it is assumed that then 
vertices of G are numbered 1 through n. The set S is maintained as a bit 
array with S (i) = 0 if vertex i is not in S and S (i) = 1 if it is. It is assumed 
that the graph itself is represented by its cost adjacency matrix with 
COST(i, j) being the weight of the edge (i, j). COST(i, j) will be set to 
some large number, + oo, in case the edge (i, j) is not in E(G ). For i = j, 
COST(i, j) may be set to any nonnegative number without affecting the 
outcome of the algorithm. 
procedure SHORTEST-PATHS(v, COST, DIST, n) 
I /DIST(j), 1 ::5 j ::5 n is set to the length of the shortest path/ I 
I !from vertex v to vertexj in a digraph G with n vertices./ I 
I IDIST(v) is set to zero. G is represented by its cost adjacency/ I 
//matrix, COST(n, n)l I 
boolean S(l:n); real COST(l:n, l:n), DIST(l:n) 
integer u, v, n, num, i, w 
1 
for i -
1 ton do 
//initialize set S to empty// 
2 
S(i) -
O; DIST(i) -
COST(v, i) 
3 
repeat 
4 
S(v) -
1; DIST(v) -
0 
//put vertex v in set SI/ 
5 
for num - 2 to n - 1 do 
I I determine n - 1 paths from vertex v I I 
6 
choose u such that DIST(u) = min{ DIST(w)} 
S(w) = 0 
7 
S(u) -
1 I /put vertex u in set SI I 
8 
for all w with S(w) = 0 do //update distances/ I 
9 
DIST(w) -
min(DJST(w), DIST(u) + COST(u, w)) 
10 
repeat 
11 
repeat 
12 
end SHORTEST-PATHS 
Algorithm 4.11 
Greedy algorithm to generate shortest paths 
Analysis of Algorithm SHORTEST-PATHS 
From our earlier discussion, it is easy to see that the algorithm is correct. 
The time taken by the algorithm on a graph with n vertices is O(n 2). To see 
this note that the for loop of line 1 takes 8(n) time. The for loop of line 5 is 
executed n -
2 times. Each execution of this loop requires O(n) time at 

Single Source Shortest Paths 
187 
line 6 to select the next vertex and again at lines 8-10 to update DIST. So 
the total time for this loop is O(n 2). In case a list T of vertices currently not 
in S is maintained, then the number of nodes on this list would at any time 
be n -
num. This would speed up lines 6 and 8-10, but the asymptotic 
time would remain O(n 2). This and other variations of the algorithm are 
explored in the exercises. 
Any shortest path algorithm must examine each edge in the graph at 
least once since any of the edges could be in a shortest path. Hence, the 
minimum possible time for such an algorithm would be O(e). Since cost 
adjacency matrices were used to represent the graph, it takes O(n 2) time just 
to determine which edges are in G and so any shortest path algorithm using 
this representation must take O(n 2). For this representation then, algorithm 
SHORTEST-PATHS is optimal to within a constant factor. Even if a change 
to adjacency lists is made, only the overall time for the for loop of lines 
8-10 can be brought down to O(e) (since the DIST can change only for 
vertices adjacent from u). The total time for line 6 remains O(n 2). 
Example 4.12 Consider the 8 vertex digraph of Figure 4.ll(a) with cost 
adjacency matrix as in 4.ll(b). The values of DIST and the vertices selected 
at each iteration of the while loop of line 5 for finding all the shortest paths 
from Boston are shown in Figure 4.12. Note that the algorithm terminates 
when only seven of the eight vertices are in S. By the definition of DIST, 
the distance of the last vertex, in this case Los Angeles, is correct as the 
shortest path from Boston to Los Angeles can go through only the remain-
ing six vertices. 
D 
Chicago 
1500 
Boston 
San . 
120Q~O 
-?2550 
Fro~~ 
--.·-·---~w 
. 
.· 
· Denver 
• · · New 
300 
/ 
1000 
1400. 
. 
I 
York 
0 
-
-:------..J_700 
1900 
Los Angeles 
~~ 
...... 
/ 
I 
New Orleans 
Miami 
Figure 4.11 (a) 
Directed graph from a map 

• 
• 
.. 
. 
' 
188 
The Greedy Method 
2 
3 
4 
5 
6 
7 
8 
I 
0 
l 
2 
300 
0 
3 
1000 
800 
0 
4 
1200 
0 
5 
1500 
0 
250 
6 
1000 
0 
900 
1400 
7 
0 
1000 
8 
1700 
0 
Figure 4.ll(b) Cost adjacency matrix for Figure 4.ll(a). All entries not shown 
are + oo 
Vertex 
LA 
SF 
D 
c 
B 
NY 
M 
NO 
Iteration 
s 
Selected DI ST (I) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
Initial 
I 
5 
6 
+x +x 
+x 
1500 0 
250 
+x 
+x 
2 
5,6 
7 
+x +x 
+x 
1250 0 
250 
1150 
1650 
3 
5,6,7 
4 
+x +x 
+x 
1250 0 
250 
1150 
1650 
4 
5,6,7,4 
8 
+x +x 
2450 
1250 0 
250 
1150 
1650 
5 
5,6,7,4,8 
3 
3350 +x 
2450 
1250 0 
250 
1150 
1650 
6 
5,6,7,4,8,3 
2 
3350 3250 2450 
1250 0 
250 
1150 
1650 
5,6,7,4,8,3,2 
3350 3250 2450 
1250 0 
250 
1150 
1650 
Figure 4.12 Action of SHORTEST_PATHS 
One may easily verify that the edges on the shortest paths from a vertex 
v to all remaining vertices in a connected undirected graph G form a 
spanning tree of G. This spanning tree is called a shortest path spanning 
tree. Clearly, this spanning tree may be different for different root vertices 
v. Figure 4.13 shows a graph G, its minimum cost spanning tree and a 
shortest path spanning tree from vertex 1. 
REFERENCES AND SELECTED READINGS 
Further work concerning the application of the greedy method to knapsack type 
problems can be found in: 
"When the greedy solution solves a class of knapsack problems," by M. Magazine, 
G. Nemhauser and L. Trotter, Operations Research, 23(2), pp. 207-217 (1975) . 

References and Selected Readings 
189 
2 
5 
10 
6 
(a) A graph 
(b) Minimum cost spanning tree 
5 
45 
2 
5 
6 
(c) Shortest path spanning tree from vertex 1. 
Figure 4.13 Graphs and spanning trees 
"Canonical coin changing and greedy solutions," by L. Chang and J. Korsh, JACM, 
23(3), pp. 412-422 (1976). 
The greedy method developed here to optimally store programs on tapes was 
first devised for a machine scheduling problem. In this problem n jobs have to be 
scheduled on m processors. Job i talces t; amount of time. The time at which a job 
finishes is the sum of the job times for all jobs preceding and including job i. The 
average finish time corresponds to the mean access time for programs on tapes. 
The (m!)nlm schedules referred to in Theorem 4.2 are known as SPT schedules 
(shortest processing time). The rule to generate SPT schedules as well as the rule of 
exercise 9 first appeared in: 
"Various optimizers for single-state production," by W. E. Smith, Nav. Res. Log. 
Quart., 3(1), March (1956). 
The greedy algorithm for generating optimal merge trees is due to D. Huffman 
and first appeared in: 

. 
, 
190 
The Greedy Method 
"A method for the construction of minimum-redundancy codes," Proc. IRE 40, pp. 
1098-1101 (1952). 
For a given set { q 1 , ••• , q n} there are many sets of Huffman codes minimizing 
E q ;d;. From amongst these code sets there is one that has minimum E d; and 
minimum max{d;}. An algorithm to obtain this code set is given in: 
"An optimal encoding with minimum longest code and total number of digits, by E. 
S. Schwartz, Info. and Contr., 7, pp. 37-44 (1964). 
The two greedy methods for obtaining minimum cost spanning trees first ap-
peared in: 
"On the shortest spanning subtree of a graph and the traveling salesman problem," 
by J. B. Kruskal, Jr., Proc. Amer. Math. Soc., 7(1), pp. 48-50 (1956). 
"Shortest connection networks and some generalizations," by R. C. Prim, Bell Sys-
tem Technical Jr., pp. 1389-1401 (1957). 
For a graph with e edges and v vertices D. B. Johnson has described a way to 
implement Prim's method so that when e = O(v I+<), for some fixed E, the compu-
tation time is O(e). His implementation uses priority queues with updates. It 
appears in: 
"Priority queues with update and finding minimum spanning trees," by D. B. John-
son, lnfor. Proc. Let., 4(3), pp. 53-57 (1975). 
An O(e loglog v) spanning tree algorithm is given in: 
"An 0(\EI log log I YI> algorithm for finding minimum spanning trees," by A. C. 
Yao, lnfor. Proc. Letters, 4(1), pp. 21-23 (1975). 
A study of several spanning tree algorithms appears in: 
"Finding Minimum Spanning Trees," by D. Cheriton and R. Tarjan, SIAM Jr. on 
Computing, 5(4), pp. 724-742 (1976). 
The shortest path algorithm of the text is from: 
"A note on two problems in connexion with graphs," by E.W. Dijkstra, Numerische 
Mathematik, 1, pp. 269-271 (1959). 
A better algorithm for graphs having few edges appears in: 
"Efficient algorithms for shortest paths in sparse networks," by D. B. Johnson, 
J. ACM, 24(1), pp. 1-13 (1977) . 

Exercllea 
191 
Algorithms to update a minimum spanning tree or shortest path following either 
the addition or deletion of a vertex or edge can be found in: 
"On finding and updating spanning trees and shortest paths," by P. M. Spira and 
A. Pan, SIAM Jr. on Computing, 4(3), pp. 375-380 (1975). 
The relationship between greedy methods and matroids is discussed in: 
Combinatorial optimization by E. Lawler, Holt, Reinhart and Winston, 1976. 
"Matroids and the greedy algorithm," by I. Edmonds, Math. Prag., 1, pp. 127-136 
(1971). 
"A greedy algorithm for solving a certain class of linear programmes," by F. D. J. 
Dunstan and D. J. A. Welsh, Math. Prag., 5, pp. 338-353 (1973). 
"Optimal assignments in an ordered set: an application of Matroid Theory," by D. 
Gale, Jr. of Combin. Theo., 4, pp. 176-180 (1968). 
EXERCISES 
1. 
a) Find an optimal solution to the knapsack instance n = 7, M = 15, (Pt, 
p2, ... ,p7) = (10, 5, 15, 7, 6, 18, 3) and (wt, w2, ... , w7) = (2, 3, 5, 7, 
1, 4, 1)? 
b) LetF(J) be the value of the solution generated by GREEDY_KNAPSACK 
on problem instance I when the objects are input in nonincreasing order 
of the p;'s. Let F*(J) be the value of an optimal solution for this instance. 
How large can the ratio F*(J)IF(J) get? 
c) 
Answer b) for the case when the input is in nondecreasing order of the w;'s. 
2. [Coin changing] Let An = {at , a 2, ... , an} be a fmite set of distinct coin 
types (e.g., at = 50¢, a2 = 25¢, a 3 = 10¢ etc.) We may assume each a; is an 
integer and that at > ai > · · · > an. Each type is available in unlimited 
quantity. The coin changing problem is to make up an exact amount C using 
a minimum total number of coins. C is an integer > 0. 
a) Show that if an ¢ 1 then there exists a finite set of coin types and a C for 
which there is no solution to the coin changing problem. 
b) Show that there is always a solution when an = 1. 
c) When an = 1 a greedy solution to the problem will make change by using 
the coin types in the order a1, a2 , •• ., an. When coin type a; is being 
considered, as many coins of this type as possible will be given. Write an 
algorithm based on this strategy. Show that this algorithm doesn't nec-
essarily generate solutions that use the minimum total number of coins. 
d) Show that ifAn = {kn-I, kn- 2, ••• , k 0 } for some k > 1 then the greedy 

., 
192 
The Greedy Method 
method of c) above always yields solutions with a minimum number of 
coins. 
3. Let P1, P 2 , ••• , Pn be a set of n programs that are to be stored on a tape of 
length L. Program P; requires a; amount of tape. If I:a; s L then clearly all 
the programs can be stored on the tape.So, assume I:a; > L. The problem is 
to select a maximum subset Q of the programs for storage on the tape. A maxi-
mum subset is one with the maximum number of programs in it. A greedy 
algorithm for this problem would build the subset Q by including programs in 
nondecreasing order of a;. 
a) Assume the P; are ordered such that a1 s a2 s · · · s an. Write a 
SPARKS algorithm for the above strategy. Your algorithm should output 
an array S(l:n) such that S(i) = 1 if P; is in Q and S(i) = 0 otherwise. 
b) 
Show that this strategy always finds a maximum subset Q such that 
EP;EQ a; s L. 
c) Let Q be the subset obtained using the above greedy strategy. How small 
can the tape utilization ratio (E PiEQa ;)IL get? 
, 
d) 
Suppose the objective now is to determine a subset of programs that maxi-
mizes the tape utilization ratio. A greedy approach now would be to con-
sider programs in nonincreasing order of a;. If there is enough space 
left on the tape for P; then it is included in Q. Assume the programs are 
ordered such that a 1 ~ a2 ~ · · · ~ an. Write a SPARKS algorithm in-
corporating this strategy. What is its time and space complexity? 
e) Show that the strategy of (c) doesn't necessarily yield a subset that maxi-
mizes (EPiEQa;)IL. How small can this ratio get? Prove your bound. 
4. 
(0/1 Knapsack] 
Consider the knapsack problem discussed in Section 4.3. We 
add the requirement that x; = 1 or x; = 0, 1 s i s n. I.e. an object is either 
included or not included into the knapsack. We wish to solve the problem: 
n 
max Ep;x; 
I 
n 
subject to E w;x; s M 
I 
x; = 0 or 1, 
1 sis n 
One greedy strategy is: consider the objects in order of nonincreasing density 
p;lw;; add the object into the knapsack if it fits. Show that this strategy doesn't 
necessarily yield optimal solutions. 

Exercises 
193 
5. 
[Set Cover] 
You are given a family S of m sets S;, 1 s i s m. Denote by IA I 
the size of set A. L~t I S;I = j;, i.e. S; = {s1, s2, ... , s;1}. A subset T = 
{T1, T2, ... , Tt} of Sis a family of sets such that for each i, 1 sis k, 
T; == Sr for some r, 1 s r s m. T is a cover of S iff U T; = US;. The size of 
T, I Tl' is the number of sets in T. A minimum cover of sis a cover of smallest 
size. Consider the following greedy strategy: build T iteratively; at the k'th 
iteration T = {Ti, ... , T k - i}; now add to T a set Si from S that contains the 
largest number of elements not already in T; stop when U T; = US;. 
a) Assume that US1 = { 1, 2, ... , n} and that m < n. Using the strategy 
outlined above write an algorithm to obtain set covers. How much time 
and space does your algorithm require? 
b) Show that the greedy strategy above doesn't necessarily obtain a minimum 
set cover. 
c) 
Suppose now that a minimum cover is defined to be one for which E 7~ 1 
I T;I is minimum. Does the above strategy always find a minimum cover? 
6. 
[Node Cover] Let G = (V, E) be an undirected graph. A node cover of G is 
a subset U of the vertex set V such that every edge in E is incident to at least 
one vertex in U. A minimum node cover is one with the fewest number of ver-
tices. Consider the following greedy algorithm for this problem: 
procedure COVER(V. E) 
U-</> 
loop 
let vt V be a vertex of maximum degree 
U-UU{v};V-V-{v} 
E-E - {(u,w)suchthatu = vorw = v} 
until E = </> repeat 
return (U) 
end COVER 
Does this algorithm always generate a minimum node cover? 
7. 
You are given a set of n jobs. Associated with each job, i, is a processing time 
t; and a deadlined; by which it must be completed. A feasible schedule is a 
permutation of the jubs such that if the jobs are processed in that order then 
each job finishes by its deadline. Define a greedy schedule to be one in which 
the jobs are processed in nondecreasing order of deadlines. Show that if there 
exists a feasible schedule then all greedy schedules are feasible. 
8. [Optimal Assignment] Assume there are n workers and n jobs. Let Vii be the 

194 
The Greedy Method 
value of assigning worker i to job j. An assignment of workers to jobs cor-
responds to the assignment of 0 or 1 to the variables X;j, 1 $ i, j $ n. xii = 
1 means worker i is assigned to jobj; Xij = 0 means that worker i is not assigned 
to jobj. A valid assignment is one in which each worker is assigned to exactly 
one job and exactly one worker is assigned to any one job. The value of an 
assignment is E; Ei v;i · Xij. An optimal assignment is a valid assignment of 
maximum value. Write algorithms for two different greedy assignment schemes. 
One of these assigns a worker to the best possible job. The other assigns to a 
job the best possible worker. Show that neither of these schemes is guaranteed 
to yield optimal assignments. Is either scheme always better than the other? 
Assume Vij > 0. 
9. Assume n programs of lengths l 1 , l 2, ••• , l n are to be stored on a tape. Pro-
gram i is to be retrieved with frequency f;. If the programs are stored in the 
order i 1 , i 2, ... , in , the expected retrieval time (ERT) is 
a) Show that storing the programs in nondecreasing order of l; does not nec-
essarily minimize the ERT. 
b) Show that storing the programs in nonincreasing order of/; does not nec-
essarily minimize the ERT. 
c) Show that the ERT is minimized when the programs are stored in nonin-
creasing order of f;ll;. 
10. Consider the tape storage problem of Section 4.2. Assume that 2 tapes Tl and 
12, are available and we wish to distribute n given programs of lengths l 1, l 2, 
... , l,, onto these two tapes in such a manner that the maximum retrieval 
time is minimized. I.e. if A and B are the sets of programs on the tapes Tl 
and 12 respectively then we wish to choose A and B such that max{ E iEA ! ,, 
E;EB /;} is minimized. A possible greedy approach to obtaining A and B would 
be to start with A and B initially empty. Then consider the programs one at a 
time. The program currently being considered is assigned to set A if E;EA/; = 
min{ E iEA l ;, E iEB l;}. otherise it is assigned to B. Show that this does not 
guarantee optimal solutions even if /1 s 12 s · · · s ln. Show that the same 
is true if we require 11 
<:?: 12 <:?: • • • <:?: ln. 
11. a) What is the solution generated by Algorithm 4.5 when n = 7, (pi, p 2, ••• , 
p1) = (3, 5, 20, 18, 1, 6, 30) and (d 1, di, ... , d1) = (1, 3, 4, 3, 2, l, 2)? 
b) Show that Theorem 4.4 is true even if jobs have different processing re-
quirements. Associated with job i is a profit p; > 0, a time requirement 
ti > 0 and a deadlined; <:?: t;. 

Exercises 
195 
c) Show that for the situation of a), the greedy method of Section 4.4 doesn't 
necessarily yield an optimal solution. 
12. a) For the job sequencing problem of Section 4.4 show that the subset J rep-
resents a feasible solution iff the jobs in J can be processed according to 
the rule: if job i in J hasn't been assigned a processing time then assign it 
to the slot [a -
l, a] where a is the least integer r such that 1 s r s d; 
and the slot [a -
1, a] is free. 
b) For the problem instance of Exercise ll(a) draw the trees and give the 
values of F(i), 0 s i s n after each iteration of the loop of lines 6-13 of 
Algorithm 4.6. 
13. a) 
Show that if all internal nodes in a tree have degree k then the number of 
external nodes n is such that n mod (k -
1) = 1. 
b) Show that for every n such that n mod (k -
1) = 1 there exists a k-ary 
tree T with n external nodes (in a k-ary tree all nodes have degree at most 
k). Moreover, all internal nodes of T have degree k. 
14. a) Show that if n mod (k -
1) = 1 then the greedy rule described following 
Theorem 4.7 generates an optimal k-ary merge tree for all (q1, qi, ... , 
qn). 
b) Draw the optimal 3-way merge tree obtained using this rule when (qi. q 2, 
... ' q 11) = (3, 7, 8, 9, 15, 16, 18, 20, 23, 25, 28). 
15. Obtain a set of optimal Huffman codes for the seven messages (M 1, ... , M1) 
with relative frequencies (q1, ... q1) = (4, 5, 7, 8, 10, 12, 20). Draw the 
decode tree for this set of codes. 
16. Let T be a decode tree. An optimal decode tree mm1m1zes Eq ;d ;. For a 
given set of q's let D denote all the optimal decode trees. For any tree T E D 
let L(T) = max{ d;} and let SL(T) = E d;. Schwartz has shown that there 
exists a tree T* ED such that L(T*) = minrED{L(T)} and SL(T*) = minrED 
{SL(T)}. 
a) For (q 1, ••• , q 8) = (1, 1, 2, 2, 4, 4, 4, 4) obtain trees Tl and T2 ED 
such that L(Tl) > L(T2). 
b) Using the data of a) obtain Tl and T2 E D such that L(Tl) = L(T2) but 
SL(Tl) > SL(T2). 
c) 
Show that if the subalgorithm LEAST used in algorithm TREE is such 
that in case of a tie it returns the tree with least depth, then TREE gen-
erates a tree with the properties of T*. 
17. Prove that Prim's method of Section 4.6 generates minimum cost spanning trees. 

196 
The Greedy Method 
18. a) Rewrite Prim's algorithm under the assumption that the graphs are repre-
sented by adjacency lists. 
b) Program and run the above version of Prim's algorithm against Algorithm 
4.9. Compare the two on a representative set of graphs. 
c) Analyze precisely the computing time and space requirements of your new 
version of Prim's algorithm using adjacency lists. 
19. Program and run Kruskal's algorithm as described in Algorithm 4.11. You 
will have to modify procedures HEAPIFY and ADJUST of Chapter 2. Use the 
same test data you devised to test Prim's algorithm in Exercise 18. 
20. 
a) Show that if T is a spanning tree for the undirected graph G, then the 
addition of an edge e, e ~ E(T) and e EE(G), to T creates a unique cycle. 
b) Show that if any one of the edges on this unique cycle is deleted from 
E(T) U { e} then the remaining edges form a spanning tree of G. 
21. 
By considering the complete graph with n vertices, show that the number of 
spanning trees in an n vertex graph can be greater than 2n - 1 -
2. 
22. 
Use algorithm SHORTEST-PATHS to obtain in nondecreasing order the 
lengths of the shortest paths from vertex 1 to all remaining vertices in the 
digraph of Figure 4.14. 
Figure 4.14 Directed graph 
23. Using the directed graph of Figure 4.15 explain why SHORTEST-PATHS will 
not work properly. What is the shortest path between vertices v 1 and v 1? 
24. 
Rewrite algorithm SHORTEST-PATHS under the following assumptions: 
(i) G is represented by its adjacency lists. The head nodes are HEAD(l), · · · 
HEAD(n) and each list node has three fields: VERTEX, COST, and 
LINK. COST is the length of the corresponding edge and n the number 
of vertices in G. 
(ii) Instead of representing S, the set of vertices to which the shortest paths 

Exercises 
197 
5 
2 
4 
2 
4 
3 
5 
2 
Figure 4.15 Directed graph 
have already been found, the set T = V(G) - S is represented using a 
a linked list. 
What can you say about the computing time of your new algorithm rela-
tive to that of SHORTEST-PATHS? 
25. 
Modify algorithm SHORTEST-PATHS so that it obtains the shortest paths in 
addition to the lengths of these paths. What is the computing time of your al-
gorithm? 
26. 
[Traveling Salesperson] Let G be a complete directed graph with n vertices. 
Let length (( u, v)) be the length of the edge ( u, v). A path starting at a given 
vertex v 0, going through every other vertex exactly once and finally returning 
to vo will be called a tour. The length of a tour is the sum of lengths of the 
edges on the path defining the tour. We are concerned with finding a tour of 
minimum length. A greedy way to construct such a tour would be: let (P, v) 
represent the path so far constructed, it starts at v0 and ends at v. Initially P 
is empty and v = vo; if all vertices in Gare on P then include the edge (v, vo) 
and stop otherwise include an edge ( v, w) of minimum length among all edges 
from v to a vertex w not on P. Show that this greedy method doesn't nec-
essarily generate a minimum length tour. 

Chapter 5 
DYNAMIC PROGRAMMING 
5.1 THE GENERAL METHOD 
Dynamic Programming is an algorithm design method that can be used 
when the solution to a problem may be viewed as the result of a sequence 
of decisions. In earlier chapters we have seen many problems that can be 
viewed this way. Some examples are: 
Example 5.1 
[Knapsack] The solution to the knapsack problem (Section 
4.3) may be viewed as the result of a sequence of decisions. We have to 
decide the values of Xi, 1 s i s n. First we may make a decision on x 1 , 
then on x2, then on XJ etc. An optimal sequence of decisions will maximize 
the objective function E p;x;. (It will also satisfy the constraints E w;x; s 
Mand 0 s x; s 1.) 
0 
Example 5.2 [Optimal Merge Patterns] This problem was discussed in 
Section 4.4. An optimal merge pattern tells us which pair of files should be 
merged at each step. As a decision sequence, the problem calls for us to 
decide which pair of files should be merged first; which pair second; which 
pair third, etc. An optimal sequence of decisions is a least cost 
sequence. 
D 
Example 5.3 [Shortest Path] One way to find a shortest path from vertex 
i to vertex j in a directed graph G is to decide which vertex should be the 
second vertex, which the third, which the fourth; etc. until vertex j is 
reached. An optimal sequence of decisions is one which results in a path of 
least length. 
D 
For some of the problems that may be viewed in this way, an optimal se-
quence of decisions may be found by making the decisions one at a time 
and never making an erroneous decision. This is true for all problems solv-
198 

The General Method 
199 
able by the greedy method. For many other problems, it is not possible to 
make stepwise decisions (based only on local information) in such a man-
ner that the sequence of decisions made is optimal. 
Example 5.4 [Shortest Path] Suppose we wish to find a shortest path 
from vertex i to vertexj. Let A; be the vertices adjacent from vertex i. Which 
of the vertices in A, should be the second vertex on the path? There is no 
way to make a decision at this time and guarantee that future decisions 
may be made leading to an optimal sequence. If on the other hand we wish 
to find a shortest path from vertex i to all other vertices in G then at each 
step, a correct decision can be made (see Section 4. 7). 
D 
One way to solve problems for which it is not possible to make a se-
quence of stepwise decisions leading to an optimal decision sequence is to 
try out all possible decision sequences. We could enumerate all decision 
sequences and then pick out the best. Dynamic programming often dras-
tically reduces the amount of enumeration by avoiding the enumeration of 
some decision sequences that cannot possibly be optimal. In dynamic pro-
gramming an optimal sequence of decisions is arrived at by making explicit 
appeal to the Principle of Optimality. This principle states that an optimal 
sequence of decisions has the property that whatever the initial state and 
decision are, the remaining decisions must constitute an optimal decision 
sequence with regard to the state resulting from the first decision. Thus, 
the essential difference between the greedy method and dynamic pro-
gramming is that in the greedy method only one decision sequence is ever 
generated. In dynamic programming, many decision sequences may be gen-
erated. However, sequences containing suboptimal subsequences cannot be 
optimal (if the principal of optimality holds) and so will not (as far as pos-
sible) be generated. 
Example 5.5 [Shortest Path] Consider the shortest path problem of Ex-
ample 5.3. Assume that i, i1, i2, ... , ik, j is a shortest path from i to j. 
Starting with the initial vertex i, a decision has been made to go to vertex 
i1. Following this decision, the problem state is defined by vertex i1 and 
we need to find a path from i1 toj. It is clear that the sequence i 1 , i2, ... , 
ib j must constitute a shortest i1 toj path. If not, let i1, r1, r2, ... , r9 ,j 
be a shorter i1 toj path. Then i, i1, r1, ... , r9 , j is an i toj path which is 
shorter than the path i, i1, i2, ... , ib j. Therefore the principle of opti-
mality applies for this problem. 
D 
Example 5.6 [O/l Knapsack] The 0/1 knapsack problem is similar to 

200 
Dynamic Programming 
the knapsack problem of Section 4.3 except that the x/s are restricted to 
have a value either 0 or 1. Using KNAP(/, j, Y) to represent the problem 
maximize E p;x; 
l:sl:sj 
subject to E w;x; ::5 Y 
/:sl:sj 
x; = 0 or 1, 
I ::5 i ::5 j 
(5.1) 
the 0/1 knapsack problem is KNAP(l, n, M). Let y1 , y2, ... , Yn be an 
optimal sequence of 0/1 values for x1, x2, ... , Xn respectively. If y 1 = 0 
then y2 , YJ , ... , Yn must constitute an optimal sequence for the problem 
KNAP(2, n, M). If it does not then y1 , y2, ... , Yn is not an optimal se-
quence for KNAP(l, n, M). If y1 = 1 then y2, ... , Yn must be an optimal 
sequence for the problem KNAP(2, n, M -
w1 ). If it isn't, then there is 
another 0/1 sequence z2, ZJ, ••• , Zn such that E w;z; ::5 M -
w1 and 
2:si:sn 
~ p;z; > 
~ p;y;. Hence, the sequencey1, z2, z3, ... , Zn is a sequence 
2:s1 :sn 
2:s1:sn 
for (5.1) with greater value. Again the principle of optimality applies. 
D 
Let So be the initial problem state. Assume that n decisions d;, 1 ::5 i ::5 n 
have to be made. Let D 1 = { r1, r2, ... , rj} be the set of possible decision 
values for d1. Let S; be the problem state following the choice of decision 
r;, 1 ::5 i ::5 j. Let f; be an optimal sequence of decisions with respect to 
the problem state S ;. Then, when the principle of optimality holds, an 
optimal sequence of decisions with respect to S 0 is the best of the decision 
sequences r i r ,., 1 ::5 i ::5 j. 
Example 5. 7 [Shortest Path] Let A,. be the set of vertices adjacent from 
vertex i. For each vertex k EA,. let rk be a shortest path from k toj. Then, 
a shortest i to j path is the shortest of the paths { i, r k I k E A,.}. 
D 
Example 5.8 [0/1 Knapsack] Let gj (y) be the value of an optimal solu-
tion to KNAP(j + l, n, y). Oearly, g 0 (M) is the value of an optimal solu-
tion to KNAP(l, n, M). The possible decisions for x 1 are 0 and 1 (D1 
{ 0, 1 }). From the principle of optimality it follows that: 
go(M} = max{gi(M), gi(M -
wi) + p1} 
(5.2) 
D 
While the principle of optimality has been stated only with respect to the 

The General Method 
201 
initial state and decision, it may be applied equally well to intermediate 
states and decisions. The next two examples show how this can be done. 
Example 5.9 [Shortest Path] Let k be an intermediate vertex on a short-
est i to j path i, i 1 , ii , ... , k, p 1 , pi · · · j. The paths i, i 1 , ... , k and 
k, Pi, ... ,j must respectively be shortest i to k and k toj paths. 
D 
Example 5.10 [Oil Knapsack] Lety1, yi, ... , Yn be an optimal solution 
to KNAP(l, n, M). Then, for eachj, 1 5 j 5 n, y1, ... , yj andyj+1, ... , 
Yn must be optimal solutions to the problems KNAP(l,j, Eisisj w;y;) and 
KNAP(j + 1, n, M - E1sisj w;y;) respectively. This observation allows us 
to generalize (5.2) to: 
g;(y) = max{g;+1(y),g;+1(y - W;+1) + p;+1} 
(5.3) 
D 
The recursive application of the optimality principle results in a recur-
rence relation of the type (5.3). Dynamic programming algorithms solve 
this recurrence to obtain a solution to the given problem instance. The 
recurrence (5.3) may be solved using the knowledge gn (y) = 0 for ally. 
From gn (y) one may obtain gn-1 (y) using (5.3) with i = n -
1. Then, 
using gn-1 (y) one may obtain gn-i (y). Repeating in this way, one can 
determineg1 (y) and finally g 0 (M) using (5.3) with i = 0.. 
D 
In formulating the dynamic programming recurrence relation(s) that has 
(have) to be solved, one may use one of two different approaches: forward, 
or backward. Let x1, xi, ... , Xn be the variables for which a sequence of 
decisions has to be made. In theforward approach. the formulation for de-
cision x; is made in terms of optimal decision sequences for x; + 1 , ... , Xn • 
In the backward approach the formulation for decision x; is in terms of 
optimal decision sequences for x1, ... , x;_ 1. Thus, in the forward approach 
formulation we "look" ahead on the decision sequence xi, xi, ... , Xn. 
In the backward formulation we "look" backwards on the decision sequence 
xi , xi, ... , Xn. Both examples 5.8 and 5.9 correspond to the forward ap-
proach. Examples 5.11 and 5.12 correspond to the backward approach. 
Example 5.11 
[Shortest Path] Let Pj be the set of vertices adjacent to 
vertex j (i.e. k E Pj iff < k. j > E E(G)). For each k E Pj let rk be a 
shortest i to k path. The principle of optimality holds and a shortest i to j 
path is the shortest of the paths { r k.j I k E pj }. 
To obtain this formulation, we started at vertexj and looked at the last 

• 
I 
202 
Dynmlc Programming 
decision made. The last decision was to use one of the edges (k,j), k E Pj. 
In a sense, we are looking backwards on the i toj path. 
D 
Example 5.12 [0/1 Knapsack) Looking backwards on the sequence of 
decisions xi, xi, ... , Xn we see that:. 
(5.4) 
wherejj(y) is the value of an optimal solution to KNAP(l,j, y). 
The value of an optimal solution to KNAP(l, n, M) isfn(M). (5.4) may 
be solved by beginning with fo (y) = 0 for ally, y ~ 0 and fo (y) 
- oo, 
y < 0. From this,/1 ,/2, ... ,Jn may be successively obtained. 
D 
While at this point one may be skeptical about the virtue of the back-
ward approach vis a vis the forward approach, future examples will show 
that, in many instances, it is easier to obtain the recurrence relations using 
the backward approach. It is also worth noting that if the recurrence rela-
tions are formulated using the forward approach then the relations are 
solved backwards (i.e. beginning with the last decision). On the other hand 
if the relations are formulated using the backward approach they are 
solved forwards. This is illustrated in Examples 5.10 and 5.12. 
The solution method outlined in Examples 5.10 and 5.12 may indicate 
that one has to look at all possible decision sequences in order to obtain an 
optimal decision sequence using dynamic programming. Actually, this is 
not the case. Because of the use of the principle of optimality, decision 
sequences containing subsequences that are suboptimal are not considered . 
While the total number of different decision sequences is exponential in 
the number of decisions (if there are d choices for each of the n decisions 
to be made then there are dn possible decision sequences), dynamic pro-
graming algorithms often have a polynomial complexity. 
Another important feature of the dynamic programming approach is 
that optimal solutions to subproblems are retained so as to avoid recom-
puting their values. The use of these tabulated values makes it natural to 
recast the recursive equations into an iterative program. Most of the dy-
namic programming algorithms in this chapter will be expressed in this 
way. 
Remaining sections of this chapter apply dynamic programming to a var-
iety of problems. These examples should help you understand the method 
better and also realize the advantage of dynamic programming over 
explicitly enumerating all decision sequences. 

Multistage Graphs 
203 
5.2 MULTISTAGE GRAPHS 
A multistage graph G = (V, E) is a directed graph in which the vertices 
are partitioned into k ~ 2 disjoint sets V;, 1 ::5 i ::5 k. In addition, if (u, v) 
is an edge in Ethen u E V; and v E V;+ 1 for some i, 1 ::5 i < k. The sets 
V1 and Vi are such that I Vt I = I Vi I = 1. Let s and t respectively be the 
vertex in V1 and Vi . s is the source and t the sink. Let c(i, j) be the cost 
of edge (i,j). The cost of a path from s tot is the sum of the costs of the 
edges on the path. The multistage graph problem is to find a minimum 
cost path from s to t. Each set V; defines a stage in the graph. Because of 
the constraints on E, every path from s tot starts in stage 1, goes to stage 
2, then to stage 3, then to stage 4 etc. and eventually terminates in stage k. 
Figure 5.1 shows a 5 stage graph. A minimum costs tot path is indicated 
by the dark edges. 
Figure 5.1 
A 5 stage graph 
Many problems can be formulated as multistage graph problems. We 
shall give only one example. Consider a resource allocation problem in 
which n units of resource are to be allocated tor projects. Ifj, 0 ::5 j ::5 n 
units of the resource are allocated to project i then the resulting net profit 
is N(i, j). The problem is to allocate the resource to the r projects in such 
a way as to maximize total net profit. This problem may be formulated as 
an r + 1 stage graph problem as follows. Stage i, 1 ::5 i ::5 r represents 
project i. There are n + 1 vertices V(i, j), 0 ::5 j ::5 n associated with stage 
i, 2 ::5 i ::5 r. Stages 1 and r + 1 each have one vertex V(l, 0) = s and 
V(r + 1, n) = t respectively. Vertex V(i, j), 2 ::5 i ::5 r represents the state 
in which a total ofj units of resource have been allocated to projects 1, 2, 

204 
Dynamic Programming 
... , i -
1. The edges in G are of the form < V(i, j), V(i + 1, /)) for all 
j ::5 I and 1 ::5 i < r. The edge (V(i,j), V(i + 1, l)),j ::5 I is assigned a 
weight or cost of N(i, I - j) and corresponds to allocating I - j units of re-
source to project i, 1 ::5 i < r. In addition, G has edges of the type < V(r, j), 
V(r + 1, n)). Each such edge is assigned a weight of maxosp sn -j{ N(r, p)}. 
The resulting graph for a three project problem with n = 4 is shown in 
Figure 5.2. It should be easy to see that an optimal allocation of resources 
is defined by a maximum cost s to t path. This is easily converted into a 
minimum cost problem by changing the sign of all the edge costs. 
Figure 5.2 4 stage graph corresponding to a 3 project problem 
A dynamic programming formulation for a k stage graph problem is ob-
tained by first noticing that every s to t path is a result of a sequence of 
---
--
-
--- --

Multistage Graphs 
205 
k - 2 decisions. The ith decision involves determining which vertex in V; + 1, 
1 ~ i ~ k - 2, is to be on the path. It is easy to see that the principle of 
optimality holds. Let P(i, j) be a minimum cost path from vertex j in V; 
to vertex t. Let COST(i, j) be the cost of this path. Then, using the for-
ward approach, we obtain: 
COST(i. j) = min { c(j, /) + COST(i + 1, /)} 
(5.5) 
IEV;+ 1 
(j.i)EE 
Since, COST(k - l,j) = c(j, t) if (j, t) EE and COST(k -
l,j) = oo 
if (j, t) E E, (5.5) may be solved for COST(l, s) by first computing 
COST(k -
2, j) for all j E Vk-2, then COST(k -
3, j) for all j E Vk- 3 , 
etc., and finally COST(l, s). Trying this out on the graph of Figure 5.1, we 
obtain the following values: 
COST(3, 6) = min{ 6 + COST(4, 9), 5 + COST(4, 10} 
= 7 
COST(3, 7) = min{ 4 + COST( 4, 9), 3 + COST( 4, 10)} 
= 5 
COST(3, 8) = 7 
COST(2, 2) = min{ 4 + COST(3, 6), 2 + COST(3, 7), 1 + COST(3, 8)} 
= 7 
COST(2, 3) = 9 
COST(2, 4) = 18 
COST(2, 5) = 15 
COST(l, 1) = min{ 9 + COST(2, 2), 7 + COST(2, 3), 3 + COST(2, 4 ), 
2 + COST(2, 5)} 
= 16 
Thus, a minimum costs to t path has a cost of 16. This path can be de-
termined easily if we record the decision made at each state (vertex). Let 
D(i, j) be the value of I which minimizes c(j, /) + COST(i + 1, /) (see 
Eq. (5.5)). For Figure 5.1 we obtain. 
D(3, 6) = 10; D(3, 7) = 10; D{3, 8) = 10; 
D(2, 2) = 7; D(2, 3) = 6; D(2, 4) = 8; D(2, 5) = 8; 
D(l, 1) = 2; 
Let the minimum cost path bes = 1, v2, VJ, ••• , Vk-t, t. It is easy to 
see that v2 = D(l, 1) = 2; v3 = D(2, D(l, 1)) = 7 and v4 = D(3, D(2, 
D(l, 1))) = D(3, 7) = 10. 

• 
. 
' 
206 
Dynamic Programming 
Before writing an algorithm to solve (5.5) for a general k stage graph, let 
us impose an ordering on the vertices in V. This ordering will make it 
easier to write the algorithm. We shall require that then vertices in V are 
indexed 1 through n. Indices are assigned in order of stages. First, s is 
assigned index 1, then vertices in V 2 are assigned indices, then vertices 
from VJ and so on. t has index n. Hence, indices assigned to vertices in 
V;+ 1 are bigger than those assigned to vertices in V; (see Figure 5.1). As a 
result of this indexing scheme, COST and D may be computed in the order 
n -
1, n - 2, ... , 1. The first subscript in COST, P and D only identifies 
the stage number and is omitted in the algorithm. The resulting algorithm 
is procedure FGRAPH. 
line 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
procedure FGRAPH(E, k, n, P) 
I /The input is a k stage graph with n vertices indexed in order/ I 
I !of stages.Eis a set of edges and c(i, j) is the cost of (i, j) .I I 
//P(l:k) is a minimum cost path// 
real COST(n), integer D(n -
1), P(k), r,j, k, n 
COST(n) -
0 
forj -
n - 1to1 by -
1 do //compute COST(j)l I 
let r be a vertex such that (j, r) E E and c(j, r) + COST(r) is 
minimum 
COST(j) -
c(j, r) + COST(r) 
D(j) -
r 
repeat 
I /find a minimum cost path/ I 
P(l) -
1; P(k) -
n 
forj -
2 to k -
1 do //findjth vertex on path// 
P(j) -
D(P(j -
1)) 
repeat 
endFGRAPH 
Algorithm 5.1 
Multistage graph algorithm corresponding to forward approach 
The complexity analysis of procedure FGRAPH is fairly straightforward. 
If G is represented by its adjacency lists, then r in line 4 may be found in 
time proportional to the degree of vertex j. Hence, if G has e edges then 
the time for the for loop of lines 3 to 7 is {)(n + e). The time for the for loop 
of lines 9 to 11 is {)(k ). Hence, the total time in {)(n + e ). In addition to the 
space needed for the input, space is needed for COST, D and P. 
The multistage graph problem can also be solved using the backward 
approach. Let BP(i, j) be a minimum cost path from vertex s to a vertexj 
in V;. Let BCOST(i, j) be the cost of BP(i, j). From the backward approach 
we obtain: 

Multistage Graphs 
207 
BCOST(i, j) = min { BCOST(i -
1, /) + c(l, j)} 
(5.6) 
/EY;- t 
(/,j)E£ 
Since BCOST(2, j) = c(l, j) if (1, j) E E and BCOST(2, j) = oo if 
< 1, j) E E, BCOST(i, j) may be computed using (5.6) by first computing 
BCOST for i = 3, then for i = 4 etc. For the graph of Figure (5.1) we ob-
tain 
BCOST(3, 6) = min{ BCOST(2, 2) + 4, BCOST(2, 3) + 2} 
= 9 
BCOST(3, 7) = 11 
BCOST(3, 8) = 10 
BCOST(4, 9) = 15 
BCOST(4, 10) = 14 
BCOST(4, 11) = 16 
BCOST(5, 12) = 16 
The corresponding algorithm to obtain a minimum cost s -
t path is 
procedure BGRAPH. The first subscript on BCOST, P and D are omitted 
for the same reasons as before. This algorithm has the same complexity as 
FGRAPH provided G is now represented by its inverse adjacency lists (i.e. 
for each vertex v we have a list of vertices w such that < w, v > E E). 
procedureBGRAPH(E, k, n, P) 
//same function as FGRAPH// 
realBCOST(n); integerD(n -
1), P(k), r,j k, n 
BCOST(l) - 0 
forj -
2 ton do //compute BCOST(j)// 
let r be a vertex such that (r,j) EE and BCOST(r) + c(r,j) is min-
imum 
BCOST(j) -
BCOST(r) + c(r,j) 
D(j) -
r 
repeat 
I /find a minimum cost path/ I 
P(l) -
1; P(k) -
n 
forj -
k -
1 to 2 by -
1 do 
I /findjth vertex on path/ I 
P(j) -
D(P(j + 1)) 
repeat 
endBGRAPH 
Algorithm 5.2 Multistage graph algorithm corresponding to backward approach 

. 
' 
208 
Dynamic Programming 
It should be easy to see that both FGRAPH and BGRAPH work cor-
rectly even on a more generalized version of multistage graphs. In this gen-
eralization, the graph is permitted to have edges (u, v) such that u E V ;, 
v E V; and i < j. 
5.3 ALL PAIRS SHORTEST PATHS 
Let G = (V, E) be a directed graph with n vertices. Let C be a cost adja-
cency matrix for G such that C(i, i) = 0, 1 ::5 i ::5 n, C(i, j) is the length 
(or cost) of edge (i,j) if (i,j) E E(G) and C(i,j) = oo if i ,&. j and (i,j) E 
E(G). The all pairs shortest path problem is to determine a matrix A such 
that A (i, j) is the length of a shortest path from i to j. The matrix A may 
be obtained by solving n single source problems using the procedure 
SHORTEST_PATHS of Section 4. 7. Since each application of this pro-
cedure requires O(n 2 ) time, the matrix A may be obtained in 0(n 3 ) time. 
We shall obtain an alternate O(n 3) solution to this problem using the prin-
ciple of optimality. Our alternate solution will require a weaker restriction 
on edge costs than required by SHORTEST _PATHS. Rather than require all 
C(i, j) ;;:::: 0, we shall only require that G have no cycles with negative length. 
Note that if we allow G to contain a cycle of negative length then the 
shortest path between any two vertices on this cycle will have length -
oo. 
Let us examine a shortest i to j path in G, i ,&. j. This path originates at 
vertex i and goes through some intermediate vertices (possibly none) and 
terminates at vertex j. We may assume that this path contains no cycles 
for if there is a cycle then this may be deleted without increasing the path 
length (no cycle has negative length). If k is an intermediate vertex on this 
shortest path then the subpaths from i to k and from k toj must be short-
est paths from i to k and k to j respectively. Otherwise, the i to j path is 
not of minimum length. So, the principle of optimality holds. This alerts 
us to the prospect of using dynamic programming. If k is the intermediate 
vertex with highest index then the i to k path is a shortest i to k path in G 
going through no vertex with index greater than k -
1. Similarly the k to 
j path is a shortest k to j path in G going through no vertex of index 
greater than k -
1. We may regard the construction of a shortest i to j 
path as first requiring a decision as to which is the highest indexed inter-
mediate vertex k. Once this decision has been made, we need to find two 
shortest paths. One from i to k and the other from k toj. Neither of these 
may go through a vertex with index greater thank -
1. Using Ak(i, J) to 
represent the length of a shortest path from i toj going through no vertex 
of index greater than k, we obtain 
---------------------

All Pain Shortest Paths 
209 
A(i,j) = min [min {Ak- 1(i, k) + Ak- 1(k,j)}, C(i,j)J 
(5.7) 
lsksn 
Clearly, A 0(i, j) = C(i, j), 1 ::5 i ::5 n, 1 ::5 j ::5 n. We can obtain a 
recurrence for A k (i, j) using an argument similar to that used before. A 
shortest path from i to j going through no vertex higher than k either goes 
through vertex k or it does not. If it does, A k (i, j) = A k- 1 (i, k) + A k- 1 (k, j). 
If it does not then no intermediate vertex has index greater than k -
1. 
Hence A k (i, j) = A k- 1 (i. j). Combining, we get 
Ak(i,j) = min{Ak- 1(i,j),Ak- 1(i, k) + Ak- 1(k.j)}, k ~ 1 
(5.8) 
The following example shows that (5.8) is not true for graphs with cycles 
of negative length. 
Example 5.13 Figure 5.3 shows a digraph together with its matrix A 0• 
For this graph A 2(1, 3) ,e. min{A 1(1, 3), A 1(1, 2) + A 1(2, 3)} = 2. In-
stead we see that A 2(1, 3) = - oo as the length of the path 
1, 2, 1, 2, 1, 2, ... ' 1, 2, 3 
can be made arbitrarily small. This is so because of the presence of the 
cycle 1 2 1 which has a length of - 1. 
D 
-2 
~ 
[ 
0. 
I, x] 
-2, 
0, 
I 
x, 
x. 
() 
Figure 5.3 Graph with Negative Cycle 
Recurrence (5.8) may be solved for An by first computing A 1, then A 2, 
then A 3 , etc. Since there is no vertex in G with index greater then n, A(i, j) 
= An (i, j). Procedure ALL_PATHS computes A n(i, j). The computation 
is done in-place so the superscript on A is not needed. The reason this 
computation can be carried out in-place is that A k(i, k) = A k- 1(i, k) and 
A k(k, j) = A k- 1(k, j). Hence, when A k is formed, the kth column and row 
do not change. Consequently, when A k(i, j) is computed in line 9, A(i, k) = 
Ak-1(i, k) = Ak(i, k) andA(k,j) = Ak- 1(k,j) = Ak(k,j). So, the old 
values upon which the new values are based do not change on this iteration. 

210 
Dynamic Programming 
procedure ALLJATHS(COST, A, n) 
//COST(n, n) is the cost adjacency matrix of a graph with n ver-11 
//tices; A(i,j) is the cost of a shortest path from v; to v/ I 
! /COST(i,i) = 0, 1 :5 i :5 nl I 
integer i. j, k. n; real COST(n, n), A(n, n) 
1 
for i -
1 to n do 
2 
for J -
1 to n do 
3 
A(i. j) -
COST(i, j) //copy COST into A! I 
4 
repeat 
5 
repeat 
6 
fork -
1 ton do 
I !for a path with highest vertex index kl I 
7 
for i -
1 to n do 
I !for all possible pairs of vertices/ I 
8 
for J -
1 to n do 
9 
A(i,j) -
min{A(i,j), A(i, k) + A(k,j)} 
10 
repeat 
11 
repeat 
12 
repeat 
13 
endALL_FATHS 
Algorithm 5.3 Procedure to compute lengths of shortest paths 
Example 5.14 The graph of Figure 5.4(a) has the cost matrix of Figure 
5.4(b ). The initial A matrix, A <0> plus its values after 3 iterations Am, A (2), 
A <3> are given in Figure 5.5. 
D 
2 
3 
0 
6 
3 
2 
4 
0 
x 
II 
2 
0 
(a) G 
(b) Cost Matrix for G 
Figure 5.4 Directed graph and its cost matrix 
Let M = max{COST(i, j)I (i, j) E E(G)}. It is easy to see that An(i, j) 
::5 (n -
l)*M. From the working of procedure ALL_PATHS, it is clear 
that if (i, j) E E(G) and i ,&. j then we may initialize COST(i, j) to any 
number greater than (n -
l)*M (rather than oo). If at termination A(i, j) 
> ( n -
1)* M then there is no directed pa th from i to j in G. 

Optimal Binary Search Trees 
211 
A<o> 
2 
3 
A<" 
2 
3 
0 
4 
II 
0 
4 
II 
2 
6 
() 
2 
2 
6 
() 
2 
3 
3 
0 
3 
3 
7 
0 
A("' 
2 
3 
Am 
2 
3 
() 
4 
6 
0 
4 
6 
2 
6 
() 
2 
2 
5 
0 
2 
3 
3 
7 
() 
3 
3 
7 
() 
Figure 5.5 
Matrices A k produced by ALL_PATHS for the digraph of Figure 5.4 
The time needed by procedure ALL_P A THS is especially easy to deter-
mine because the looping is independent of the data in the matrix A. Line 
9 is iterated n 3 times and so the time for procedure ALL_PATHS is fJ(n 3). 
An exercise examines the extensions needed to actually obtain the i to .i 
paths with these lengths. Some speed-up can be obtained by noticing that 
the innermost for loop need be executed only when A(i, k) and A(k, j) are 
not equal to oo. 
5.4 OPTIMAL BINARY SEARCH TREES 
Definition A binary search tree T is a binary tree; either it is empty or 
each node in the tree contains an identifier and 
(i) all identifiers in the left subtree of T are less (numerically or alpha-
betically) than the identifier in the root node T; 
(ii) all identifiers in the right subtree are greater than the identifier in the 
root node T; 
(iii) the left and right subtrees of Tare also binary search trees. 
Note that the definition of a binary search tree requires that all identifiers 
in the tree be distinct. For a given set of identifiers, several different binary 
search trees are possible. Figure 5.6 shows two possible binary search trees 
for a subset of the reserved words of SPARKS. 
To determine whether an identifier X is present in a binary search tree, 
X is compared with the root. If X is less than the identifier in the root, 
then the search continues in the left subtree; if X equals the identifier in 
the root, the search terminates successfully; otherwise the search continues 
in the right subtree. This is formalized in procedure SEARCH. 

•I 
212 
Dynamic Programming 
(a) 
(b) 
Figure 5.6 Two possible binary search trees 
procedure SEARCH(T, X. i) 
I /Search the binary search tree T for X. Each node of the tree has/ I 
//fields LCHILD, IDENT, RCHILD. If Xis not in T then set i = 11 
I 10. Otherwise, set i such that IDENT(i) = X.I I 
1 
i - T 
2 
while i ,&. 0 do 
3 
4 
5 
6 
7 
8 
9 
case 
:X < IDENT(i): i - LCHILD(i) //search left subtree// 
:X = IDENT(i): retum 
:X > IDENT(i): i -
RCHILD(i) //search right subtree// 
end case 
repeat 
end SEARCH 
Algorithm 5.4 Searching a binary search tree 
Given a fixed set of identifiers, we wish to create a binary search tree 
organization. We may expect different binary search trees for the same 
identifier set to have different performance characteristics. The tree of 
Figure 5.6(a), in the worst case, requires four comparisons to find an iden-
--- ----~ 
-· -

Optimal Binary Search Trees 
213 
tifier, while the tree of 5.6(b) requires only three. On the average the two 
trees need 12/5 and 11/5 comparisons respectively. This calculation assumes 
that each identifier is searched for with equal probability and that no 
searches for an identifier not in Tare ever made. 
In a general situation, we may expect different identifiers to be searched 
for with different frequencies (or probabilities). In addition, we may expect 
unsuccessful searches (i.e. searches for identifiers not in the tree) also to be 
made. Let us assume that the given set of identifiers is {a 1, a 2, ••• , an} 
with a 1 < a 2 < · · · < an. Let P(i) be the probability with which we shall 
be searching for a;. Let Q(i) be the probability that the identifier X being 
searched for is such that a; < X < a;+ 1, 0 ::5 i ::5 n (assume a 0 = 
- oo 
and lln+t = + oo). Then, Eosisn Q(i) is the probability of an unsuccessful 
search. Clearly, Etsisn P(i) + Eosisn Q(i) = 1. Given this data, we wish 
to construct an optimal binary search tree for {a 1 , a 2 , ••• , an } . First, of 
course, we must be precise about what we mean by an optimal binary 
search tree. 
In obtaining a cost function for binary search trees, it is useful to add a 
fictitious node in place of every empty subtree in the search tree. Such 
nodes are called external nodes and are drawn square in Figure 5. 7. All 
other nodes are internal nodes. If a binary search tree represents n identi-
fiers then there will be exactly n internal nodes and n + 1 (fictitious) exter-
nal nodes. Every internal node represents a point where a successful search 
may terminate. Every external node represents a point where an unsuccess-
ful search may terminate. 
ci (a) 
(b) 
Figure 5. 7 Binary search trees of Figure 5.6 with external nodes added 

• I 
•I 
• I 
-
i 
214 
Dynamic Programming 
If a successful search terminates at an internal node at level l then l 
iterations of the loop of lines 2-7 (Algorithm 5.4) are needed. Hence, the 
expected cost contribution from the internal node for a; is P(i)*level (a;). 
Unsuccessful searches terminate with i = 0 (i.e. at an external node) in 
algorithm SEARCH. The identifiers not in the binary search tree may be 
partitioned into n + 1 equivalence classes E;, 0 ::5 i ::5 n. Eo contains all 
identifiers X such that X < a 1. E; contains all identifiers X such that a; < 
X < a;+1, 1 ::5 i < n. En contains all identifiers X, X > an. It is easy to 
see that for all identifiers in the same class E;, the search terminates at 
the same external node. For identifiers in different E; the search terminates 
at different external nodes. If the failure node for E; is at level l then only l 
-
1 iterations of the while loop are made. Hence, the cost contribution of 
this node is Q (i) * (level(E;) -
1). 
The preceding discussion leads to the following formula for the expected 
cost of a binary search tree: 
E P(i)*level(a;) + E Q(i)*(level(E;) -
1) 
(5.9) 
1 sisn 
O:si:sn 
We shall define an optimal binary search tree for the identifier set {a 1 , 
a 2, ••• , an} to be a binary search tree for which (5. 9) is minimum. 
Example 5.15 
The possible binary search trees for the identifier set (a1, 
a2, a3) = (do, if, stop) are: 
(a) 
(b) 
(c) 
--------- --- - --

Optimal Binary Search Trees 
215 
;\ 
I 
I 
do 
o 
and 
[J 
~ 
(d J 
( e) 
With equal probabilities P(i) = Q(j) = 117 for all i andj, we have 
cost(tree a) = 1517; cost(tree b) = 1317 
cost(tree c) = 1517; cost(tree d) = 1517 
cost(tree e) = 1517. 
As expected, tree b is optimal. With P(l) = .5, P(2) = .1, P(3) 
.05, 
Q(O) = .15, Q(l) = .1, Q(2) = .05 and Q(3) = .OS we have 
cost(tree a) = 2.65; cost(tree b) = 1.9 
cost( tree c) = 1.5; cost( treed) = 2.05 
cost(tree e) = 1.6 
Tree c is optimal with this assignment of Ps and Qs. 
D 
In order to apply dynamic programming to the problem of obtaining an 
optimal binary search tree we need to view the construction of such a tree 
as the result of a sequence of decisions and then observe that the principle 
of optimality holds when applied to the problem state resulting from a 
decision. A possible approach to this would be to make a decision as to 
which of the a/s be assigned to the root node of T. If we choose ak then it 
is clear that the internal nodes for a 1 , a 2 , • • • a k _ 1 as well as the external 
nodes for the classes E 0 , Ei, ... , Ek-1 will lie in the left subtree, L, of the 
root. The remaining nodes will be in the right subtree, R. Define 
COST(L) = 
E P(i)*level(a;) + E Q(i)*(level(E;) -
1) 
I :si<k 
O:si<k 

•I 
•I 
216 
Dynamic Programming 
and 
COST(R) = 
E P(i)*level(a;) + E Q(i)*(level(E;) -
1) 
lc<is,n 
lcsisn 
In both cases the level is measured by regarding the root of the respective 
subtree to be at level 1. 
Figure 5.8 An optimal binary search tree with root ak 
.i 
Using W(i, j) to represent the sum Q(i) + E (Q(l) + P(l)) we obtain 
/=i+I 
the following as the expected cost of-the search tree T (Figure 5.8) 
P(k) + COST(L) + COST(R) + W(O, k -
1) + W(k, n) 
(5.10) 
If Tis optimal then (5.10) must be minimum. Hence, COST(L) must be 
minimum over all binary search trees containing a1, a2, ••• , ak-1 and-Eo, 
Ei, ... , Ek- I· Similarly COST(R) must be minimum. If we use C(i, j) to 
represent the cost of an optimal binary search tree, T1;, containing a; - 1 , ... , 
a.; and £;, ... , E.; then for T to be optimal, we must have COST(L) = 
C(O, k -
1) and COST(R) = C(k, n). In addition, k must be chosen such 
that 
P(k) + C(O, k -
1) + C(k, n) + W(O, k -
1) + W(k, n) 
is minimum. Hence, for C(O, n) we obtain: 
C(O, n) = min { C(O, k -
1) + C(k, n) + P(k) + W(O, k -
1) + W(k, n)} 
l:sk:sn 
(5.11) 
We may generalize (5.11) to obtain for any C(i,j) 

Optimal Binary Search Trees 
217 
C(i,J) = min{C(i, k -
1) + C(k,J) + P(k) + W(l, k -
1) + W(k,J)} 
i<lcsj 
= min { C(i. k -
1) + C(k. /)} + W(i. /) 
i<lc::=j 
(5.12) 
Equation (5.12) may be solved for C(O, n) by first computing all C(i, j) 
such thatj -
i = 1 (note C(i, i) = 0 and W(i, i) = Q(i), 0 ::5 i ::5n).Next 
we can compute all C(i, /)such that/ -
i = 2, then all C(i. /)with/ -
i 
= 3 etc. If during this computation we record the root R(i, j) of each tree 
T1; then an optimal binary search tree may be constructed from these R(i. /). 
Note that R(i,j) is the value of k that minimizes (5.12). 
Example 5.16 Let n = 4 and (a1, a1, a3, a4) = (do, if, read, while). 
Let P(1:4) = (3, 3, 1, 1) and Q(0:4) = (2, 3, 1, 1, 1). The Ps and Qs have 
been multiplied by 16 for convenience. Initially, we have W(i, i) = Q(i), 
C(i, i) = 0 and R(i, i) = 0, 0 ::5 i ::5 4. Using eq (5.12) and the observa-
tion W(i,J) = P(j) + Q(j) + W(i,j -
1) we get: 
W(O, 1) = P(l) + Q(l) + W(O, 0) = 8 
C(O, 1) = W(O, 1) + min{ C(O, 0) + C(l, 1)} = 8 
R(O, 1) = 1 
W(l, 2) = P(2) + Q(2) + W(l, 1) = 7 
C(l,2) = W(l,2) + min{C(l, 1) + C(2,2)} = 7 
R(O, 2) = 2 
W(2, 3) = P(3) + Q(3) + W(2, 2) = 3 
C(2, 3) = W(2, 3) + min{ C(2, 2) + C(3, 3)} = 3 
R(2, 3) = 3 
W(3, 4) = P(4) + Q(4) + W(3, 3) = 3 
C(3, 4) = W(3, 4) + min{ C(3, 3) + C(4, 4)} = 3 
R(3, 4) = 4 
Knowing W(i, i + 1) and C(i, i + 1), 0 ::5 i < 4 we can again use equa-
tion (5.12) to compute W(i, i + 2), C(i, i + 2), R(i, i + 2), 0 ::5 i < 3. 
This process may be repeated until W(O, 4), C(O, 4) and R(O, 4) are ob-
tained. The table of Figure 5. 9 shows the results of this computation. The 
box in row i and columnj shows the values of W(J,J + i), C(j,j + i) and 
R (J, J + i) respectively. The computation is carried out row-wise from 
row 0 to row 4. Form the table we see that C(O, 4) = 32 is the minimum 
cost of a binary search tree for (a 1 , a 2 , a J , a 4 ). The root of tree T 04 is a 2. 
Hence, the left subtree is T 01 and the right subtree T 24. T 01 has root a 1 
and subtrees Too and T 11 • T 24 has root a3; its left subtree is therefore T22 

218 
Dynamic Programming 
and right subtree T34. Thus, with the data in the table it is possible to re-
construct T04. Figure 5.10 shows T04. 
D 
column -
row l 
0 
1 
2 
3 
4 
0 
2, 0, 0 
3, 0, 0 
1, 0, 0 
1, 0, 0 
l, 0, 0 
1 
8, 8, 1 
7, 7, 2 
3,3,3 
3, 3, 4 
2 
12, 19, 1 
9, 12, 2 
5, 8, 3 
3 
16, 25, 2 
11, 19, 2 
4 
16, 32, 2 
Figure 5.9 Computation of C(O, 4), W(O, 4) and R(O, 4) 
Figure 5.10 Optimal search tree for Example 5.16 
The above example illustrates how Equation (5.12) may be used to de-
termine the Cs and Rs and also how to reconstruct Ton knowing the Rs. Let 
us examine the complexity of this procedure to evaluate the Cs and Rs. 
The evaluation procedure described in the above example requires us to 
compute C(i,j) ior (j -
i) = 1, 2, ... , n in that order. Whenj - i = m 
there are n - m + 1 C(i,j)s to compute. The computation of each of these 
C(i, j)s requires us to find the minimum of m quantities (see Equation 
(5.12)). Hence, each such C(i,j) can be computed in time O(m). The total 
time for all C(i, j)s with j -
i = m is therefore O(nm -
m 2). The total 
time to evaluate all the C(i,j)s and R(i, j)s is therefore 
Actually we can do better than this using a result due to D. E. Knuth 
which shows that the optimal k in Equation (5.12) may be found by limiting 

Oil-Knapsack 
219 
the search to the range R(i, j -
1) ::5 k ::5 R(i + 1, j). In this case the 
computing time becomes O(n 2) (see exercises). Procedure OBST (Algorithm 
5.5) uses this result to obtain in O(n 2) time the values of W(i, j), R(i, j) 
and C(i, j), 0 ::5 i ::5 j ::5 n. The actual tree Ton may be constructed from 
the values of R(i, j) in O(n) time. The algorithm for this is left as an exer-
cise. 
procedure OBST(P. Q, n) 
I /Given n distinct identifiers a 1 < a 2 < ... < an and probabilities/ I 
I IP(i), 1 ::5 i ::5 n and Q(i), 0 ::5 i ::5 n this algorithm computes the cost/ I 
I I C(i, j) of optimal binary search trees T;; for identifiers a;+ 1 , ••• , a;.! I 
I !It also computes R(i, j), the root of T;1• W(i, j) is the weight of T;/ I 
real P(n), Q(O:n), C(O:n, O:n), W(O:n, O:n) 
integer R(O:n, O:n) 
for i -
0 to n -
1 do 
(W(i, i), R(i, i), C(i. i)) -
(Q(i), 0, 0) 
I /initialize/ I 
(W(i, i + 1), R(i, i + 1), C(i, i + 1)) -
(Q(i) + Q(i + 1) + P(i + 1), 
i + 1, Q(i) + Q(i + 1) + P(i + 1)) I I optimal trees with one node/ I 
repeat 
(W(n, n), R(n, n), C(n, n)) -
(Q(n), 0, 0) 
for m -
2 to n do 
I /find optimal trees with m nodes/ I 
for i -
0 to n - m do 
j -
i + m 
W(i,j) -
W(i,j -
1) + P(j) + Q(j) 
k -
a value of l in the range R(i, j -
1) ::5 l ::5 R(i + 1, j) that 
minimizes { C(i, I -
1) + C(l, .i)} 
I /solve (5.12) using Knuth's/ I 
I !result! I 
C(i. j) -
W(i. j) + C(i. k -
1) + C(k. j) 
R(i,j) -
k 
repeat 
repeat 
end OBST 
Algorithm 5.5 Finding a minimum cost binary search tree 
5.5 Oil-KNAPSACK 
The terminology and notation used in this section is the same as in section 
5.1. A solution to the knapsack problem may be obtained by making a se-
quence of decisions on the variables xi, x2, ... , Xn. A decision on variable 
x; involves deciding which of the values 0 or 1 is to be assigned to it. Let us 

220 
Dynamic Programming 
assume that decisions on the x; are made in the order x n, x n -1 , ••• , x 1• 
Following a decision on Xn we may be in one of two possible states: the 
capacity remaining in the knapsack is M and no profit has accrued or the 
capacity remaining is M -
Wn and a profit of Pn has accrued. It is clear 
that the remaining decisions Xn-1, ••• , xi must be optimal with respect to 
the problem state resulting from the decision on Xn. Otherwise, Xn, ••• , x 1 
will not be optimal. Hence, the principle of optimality holds. 
Letf;(X) be the value of an optimal solution to KNAP(l,j, X). Since the 
principle of optimality holds, we obtain 
(5.13) 
For arbitrary f;(X), i > 0, Equation (5.13) generalizes to 
f;(X) = max{fi-1(X).f;-1(X - w;) + p;} 
(5.14) 
Equation (5.14) may be solved forfn(M) by beginning with the knowledge 
fo(X) = 0 for all X andf;(x) = 
- oo, x < 0. /1, f2, ... , fn may be suc-
cessively computed using (5.14). 
Example 5.17 
Consider the knapsack instance n = 3, (w1, w2, WJ) = 
(2, 3, 4), (p1, p2, p3) = (1, 2, 5) and M = 6. Figure 5.11 graphically 
displays/1 ,/2 andf3. The first column of graphs gives the functionf;-1(X 
-
w;) + p ;. It is obtained by shifting fi-1 (X) w; units right on the X axis 
and then adding p, to it. The second column gives the functions f;(X) ob-
tained by using Equation (5.14)./3(6) = 6. 
0 
From Figure 5.11 one sees that each/; is completely specified by the pairs 
(Pj, Wj) where Wj is a value of X at which/; takes a jump. Pj = f;(Wj). 
If there are r jumps then we need to know r pairs (Pj. Wj), 1 ::5 j ::5 r. For 
convenience we introduce the pair (Po, W0) = (0, 0). If we assume Wj < 
Wj+i, 0 ::5 j < r then from (5.14) it follows that Pj < Pj+l· Further, 
f;(X) = f;( W;) for all X such that Wj ::5 X < Wj + 1, 0 ::5 j < r. f;(X) = 
f;(W,) for all X. X ~ W,. If Si-I is the set of all pairs forf;-1 (including(O, 
0)) then the set S'1 of all pairs for g;(X) = /;-1(X -
w;) + p; is obtained 
by adding to each pair in Si- I the pair (p;, W;). 
(5.15) 
S; may now be obtained by merging together Si-I and s;. This merge 
corresponds to taking the maximum of the two functions f;- 1 (X) and f; - 1 (X 

0 
2 ~l-----r~-
--~' 
5 
3 
8~-- -
- --
7 ______ _ 
6---- -
I 
Sf----
I 
I 
i 
I 
I 
I 
14 6 7 
9 
I 
0/l~:Knapsack 
221 
f1 (X) 
f01 f0(X)=O 
2 3 5 
6 7 
9 
x --------
Figure 5.11 Knapsack values 
-
w;) + p; in Equation (5.14). Thus, if one of Si-I and s; has a pair 
(P1. W1) and the other has a pair (Pk, Wk) and P; ::5 Pk while W1 ~ Wk 
then the pair (Ph W;) is discarded. This is required by (5.14). f;(W1) 
max{P1, Pk} = Pk. 
Example 5.18 For the data of Example 5.17 we have 
s 0 = {(O,O)};SI = {(1,2)} 
s 1 = {(0,0),(1,2)};ST = {(2,3),(3,5)} 
S 2 = {(O, 0), (1, 2), (2, 3), (3, 5)}; S1 = {(5, 4), (6, 6), (7, 7), (8, 9)} 
SJ = {(O, 0), (1, 2), (2, 3), (5, 4), (6, 6), (7, 7), (8, 9)}. 
Note that the pair (3, 5) has been eliminated from SJ as a result of the 
purging rule stated above. 
D 
The above computation procedure for Si, 0 ::5 i ::5 n may also be arrived 
at using a different line of reasoning. Suppose we attempt to solve the knap-
sack problem by explicitly enumerating all 2n possibilities for x 1 , x 2, ••• , 
Xn. Then each Si represents the possible states resulting from the 2; 

•1 
-
I 
222 
Dynamic Propammlng 
decision sequences for x 1 , ••• , x;. Each state is defined by the pair (Pj, 
Wj) as above. To obtain Si+i, we note that the possibilities for X;+ 1 are 
Xi+1 = 0 or Xi+I = 1. When Xi+1 = 0, the resulting states are the same 
as for Si. When Xi+I = 1, the resulting states are obtained by adding (,pi+i, 
w i+i) to each state in Si. This addition just yields SL (Eq. 5.15). Now, Si+I 
may be computed by merging the states in Si and s; together. Note that if 
Si+! contains two tuples (Pj. Wj) and (P1r., W1r.) with the property that 
Pj ::5 P 1r. and Wj ~ W 1r. then the tuple (Pj, W;) may be discarded. This is 
so because for any decision sequence X;+2, ••• , Xn with the property W1 + 
E7+2 W1X1 ::5 M, it is the case that W1r. + E7+2 W1X1 ::5 Mand P1r. + E7+2 
p1x1 ~ P1 + E7+2 p1x1. Hence, (P1, W;) cannot lead to a solution better 
than the best obtainable from (P1r., W1r.). This discarding rule is identical 
to the purging rule stated above. Discarding or purging rules are also known 
as dominance rules. Dominated tuples get purged. In the above, (P1r., W1r.) 
dominates (P;, W;). 
When generating the S's we may also purge all pairs (P, W) with W > M 
as these pairs determine the value of/n(X) only for X > M. Since the knap-
sack capacity is M, we are not interested in the behavior offn for X > M. 
When all pairs (P,;, ~;) with ~; > M are purged from the Sis, fn(M) is 
given by the P value of the last pair in sn (note that the Sis are ordered 
sets). Note also that by computing Sn, we can find the solutions to all the 
knapsack problems KNAP(l, n, X), 0 ::5 X ::5 Mand not just KNAP(l, n, M). 
Since, we want only a solution to KNAP(l, n, M), we may dispense with 
the computation of Sn altogether. The solution to KNAP(l, n, M) is given 
by the last tuple (P. W) in Sn and only this has to be computed. The last 
tuple in sn is either the last tuple in sn-I or it is (P1 + Pn, Wj + Wn) where 
(Pj. W1) E sn- 1 and W1 is the largest jump point in sn- 1 such that W1 + 
Wn SM. 
If (Pl, Wl) is the last tuple in Sn, a set of 0/1 values for the xis such 
that E PiXi = Pl and E w;x; = Wl may be determined by carrying out a 
search through the Sis. We may set Xn = 0 if (Pl, Wl) E sn - 1• If (Pl, Wl) 
~ sn-I then (Pl - Pn, Wl -
Wn) E sn-I and we may set Xn = 1. This 
leaves us to determine how either (Pl, Wl) or (Pl - Pn. Wl -
Wn) was 
obtained in Sn - 1• This may be done by using the argument used to deter-
mine xn. 
Example 5.19 With M = 6, the value of/J(6) is given by the tuple (6, 6) 
in S 3 (Example 5.18). (6, 6) ~ S 2 and so we must setx 3 = 1. The pair (6, 6) 
came from the pair (6 - p 3 , 6 -
w 3) = (1, 2). Hence (1, 2) E S2. (1, 2) E 

O/l~:Knapsack 
223 
Si and so we may set x2 = 0. Since (1, 2) !!: s0 , we obtain xi = 1. Hence 
an optimal solution is (xi, x2, XJ) = (1, 0, 1). 
D 
We may sum up all we have said so far in the form of an informal algo-
rithm procedure DKP (Algorithm 5.6). In order to be able to evaluate the 
complexity of the algorithm we need to specify how the sets Si and S'; are 
to be represented. An algorithm to merge Si and S~ is needed. This algo-
rithm must purge pairs as needed. In addition, we need to specify an algo-
rithm which Will trace through Sn- i, ... , Si and determine a set Of 0/1 
values forxn • ... , xi. 
line procedure DKP(p, w, n, M) 
1 
s0 -
{ (0, 0)} 
2 
for i -
1 to n -
1 do 
3 
Sii -
{(Pl, Wl) I (Pl - Pi, Wl -
Wi) E Si - i and Wl ::5 M} 
4 
Si -
MERGE_PURGE(Si-1, SD 
5 
repeat 
6 
(PX, WX) -
last tuple in Sn - i 
7 
(PY, WY) -
(Pl + Pn• Wl + Wn) where Wl is the largest Win 
any tuple in sn-i such that w + Wn ::5 M 
I I trace back for x n, x II -
(, ••• ' x i I I 
8 
if PX> PYthenxn -
0 
9 
elsexn -
1 
10 
endif 
11 
trace back for X n - i, ••• , X i 
12 
endDKP 
Algorithm 5.6 Informal knapsack algorithm 
Implementation of DKP 
We can use two one dimensional arrays P and W to represent all the pairs 
(Pl, Wl). The Pl values will be stored in P and the Wl values in W. Sets 
so' s i, ... ' sn - i may be stored adjacent to each other. This will require 
the use of pointers F(i), 0 ::5 i ::5 n with F(i) being the location of the first 
element in Si, 0 ::5 i < n and F(n) being one more than the location of the 
last element in sn - i. 

-
•I 
I 
224 
Dynamic Programming 
Example 5.20 
Using the representation above, the sets s 0, S 1 and S 2 of 
Example 5.18 will appear as: 
1 
2 
3 
4 
5 
6 
7 
p 
0 
0 
1 
0 
1 
2 
3 
w 
0 
0 
2 
0 
2 
3 
5 
I 
I 
I 
I 
F(O) 
F(l) 
F(2) 
F(3) 
D 
The merging and purging of Si - 1 and S 1i may be carried out at the same 
time that s Ii is generated. Since the pairs in Si- I are in increasing order of 
P and W, the pairs for Si will be generated in this order. If the next pair 
generated for S 1i is (PQ, WQ) then we may merge into Si all pairs from 
Si- I with W value ::5 WQ. The purging rule may be used to decide whether 
any pairs get purged. Hence, no additional space is needed in which to 
explicitly store S 1i. 
Procedure DKNAP generates Si from Si- 1 in this way. The Sis are gen-
erated in the loop of lines 4-29. At the start of each iteration l = F(i -
1) 
and h is the index of the last pair in Si- 1• Hence h = next -
1. k points 
to the next tu pie in Si - 1 that has to be merged into Si. Line 6 sets u such 
that for all wj, h ~ j > u, wj + Wi > M. Thus these pairs are not even 
generated in S 1;. The pairs for S 1i are therefore all pairs (P(j) + pi, W(j) 
+ wi), 1 ::5 j 
::5 u. The loop of lines 7-22 generates these pairs. Each 
time a pair (pp, ww) is generated, all pairs (p, w) in Si- I with w < ww 
not yet purged or merged into Si are merged into Si. Note that none of 
these may be purged. Lines 13-14 handle the case when the next pair in 
Si-I has a w value equal to ww. In this case the pair with lesser p value 
gets purged. In case pp > P(next -
1) then the pair (pp, ww) gets purged. 
Otherwise, (pp, ww) is added to Si. Lines 19-21 purge all unmerged pairs 
in Si - 1 that can be purged at this time. Finally, following the merging of 
S 1i, into S; there may be pairs remaining in S i- 1 to be merged into Si. 
This is taken care of in lines 23-26. Note that because of lines 19-21, none 
of these pairs can be purged. Procedure PARTS (line 29) implements 
lines 8-9 of procedure DKP (Algorithm 5.6). This is left as an exercise. 

line procedure DKNAP(p, w, n, M, m) 
realp(n), w(n), P(m), W(m),pp, ww, M 
integer F(O:n), /, h, u, i, j, p, next 
1 
F(O) -
1; P(l) -
W(l) -0 I /SO// 
2 
I -
h -
1 //start and end of s 0; I 
Oil-Knapsack 
225 
3 
F(l) -
next -
2 I !next free spot in P and WI I 
4 
fori - 1 ton - 1 do //generate Si// 
5 
k -
l 
6 
u -
largest k, l ::5 k ::5 h, such that W(k) + w; ::5 M 
7 
forj -
l to u do 
//generate S1; and merge// 
8 
(pp, ww) -
(P(j) + p;, W(j) + w;) 
I !next element in S 1i// 
9 
while k ::5 hand W(k) ::5 WW do //merge in from s- 111 
10 
P(next) - P(k); W(next) -
W(k) 
11 
next -
next + 1; k -
k + 1 
12 
repeat 
13 
if k ::5 h and W(k) = ww then pp -
max(pp, P(k)) 
14 
k -
k + 1 
15 
endif 
16 
if pp > P(next -
1) then (P(next), W(next)) -
(pp, ww) 
17 
next -
next + 1 
18 
endif 
19 
while k ::5 hand P(k) ::5 P(next - 1) do 
I !purge! I 
20 
k - k + 1 
21 
repeat 
22 
repeat 
I !merge in remaining terms from Si- I I I 
23 
while k ::5 h do 
24 
(P(next), W(next)) -
(P(k), W(k)) 
25 
next -
next + 1; k -
k + 1 
26 
repeat 
I /initialize for Si+ 1 I I 
27 
I -
h + 1; h -
next -
l; F(i + 1) -
next 
28 
repeat 
29 
call PARTS 
30 
endDKNAP 
Algorithm S. 7 Algorithm for 0/ 1 knapsack problem 

226 
Dynamic Programming 
Analysis of Procedure DKNAP 
If J Si J is the number of pairs in Si then the arrays P and W should have a 
minimum dimension of m = E osisn IS; I. Since it is not possible to pre-
dict the exact space needed, it will be necessary to test for next > m each 
time next is incremented. Since each Si, i > 0, is obtained by merging si-l 
and S1i and IS1;I ::5 IS;- 11, it follows that IS;I ::5 21Si-ll. In the worst 
case no pairs will get purged and 
E 
ISil= 
E 
2;=2n-1. 
O:si:sn -1 
O:si!:::n-1 
The time needed to generate S; from s- 1 is B(IS- 11). Hence, the time 
needed to compute all the Sis, 0 ::5 i < n is B(EISi-11). Since IS;I ::5 2 1, 
the time needed to compute all the Sis is 0(2n). If the Pis are integer 
then each pair (P, W) in Si has integer P and P ::5 Eisjsi Pj· Similarly, if 
the WjS are integer, each Wis integer and W ::5 M. In any S; the pairs 
have distinct W values and also distinct P values. Hence, 
JS;I ::5 1 + 
E pj 
lsj:si 
when the pis are integer and 
IS; I ::5 1 + min{ E 
Wj. M} 
l:Sj:Si 
when the w jS are integer. When both the p jS and w jS are integer the time 
and space complexity of DKNAP (excluding the time for PARTS) is 
0 (min{ 2n, n E 1 sisn p;, nM} ). In this bound E 1 sisn p; may be replaced by 
kisisn p;/gcd(p1, .. ., Pn) and M by gcd (w1, W2, .. ., Wn, M) (see ex-
ercises). The exercises indicate how PARTS may be implemented so as to 
have a space complexity 0 (1) and a time complexity 0 (n 2 ). 
While the above analysis may seem to indicate that DKNAP requires too 
much computational resource to be practical for large n, in practice many 
instances of this problem can in fact be solved in a "reasonable" amount of 
time. This happens because usually, all the ps and ws are integer and 
M is much smaller than 2n. The purging rule is effective in purging most 
of the tuples that would otherwise remain in the Sis. 
Procedure DKNAP may be speeded by the use of heuristics. Let L be an 
estimate on the value of an optimal solution such that fn (M) ~ L. Let 
PLEFT(i) = E,<jsn Pj· If Si contains a tuple (P, W) such that P + 
PLEFT(i) < L then (P. W) may be purged from S. To see this, observe 
that (P, W) can contribute at best the pair (P + ki<jsn Ph W + k;<jsn w ) 
to Sin. Since P + Ei<jsn pj = P + PLEFT(i) < L, it follows that this 
pair cannot lead to a pair with value at least L and so cannot determine an 

Oil-Knapsack 
227 
optimal solution. A simple way to estimate L such that L ::5 fn ( M) is to 
consider the last pair (P, W) in Si. Then, P ::5 fn(M). A better estimate 
is obtained by adding to (P. W) some of the remaining objects. Example 
5.21 illustrates this. Heuristics for the knapsack problem will be discussed 
in greater detail in the chapter on branch-and-bound. The exercises ex-
plore a divide and conquer approach to speed DKNAP so that the worst 
case time is 0 (2n12 ). 
Example 5.21 
Consider the following instance of the knapsack problem: 
n = 6; (p1,p2,pJ, p4,ps,p6) = (w1, W2, WJ, W4, Ws, W6) = (100, 50, 20, 
10, 7, 3) and M = 165. Attempting to fill the knapsack using objects in 
the order 1, 2, 3, 4, 5 and 6, we see that objects 1, 2, 4 and 6 fit in yielding 
a profit of 163 and a capacity utilization of 163. We may thus begin with 
L = 163 as a value with the property L ::5 fn(M). Since Pi = Wi, every 
pair (P, W) E Si, 0 ::5 i ::5 6 has P = W. Hence, each pair may be re-
placed by the singleton P or W. PLEFT(O) 
== 190; PLEFT(l) = 90; 
PLEFT(2) = 40; PLEFT(3) = 20; PLEFT(4) : 10; PLEFT(5) = 3 and 
PLEFT(6) = 0. Eliminating from each Si any singleton P such that P + 
PLEFT(i) < L we obtain: 
s0 = {O};S11 = {100} 
S1 = { 100}; S12 = { 150} 
S 2 = {150};Sr1 = 
</> 
SJ = { 150}; S14 = { 160} 
S4 = {160}; S15 = </> 
S 5 = {160} 
The singleton 0 is deleted from S 1 as 0 + PLEFT(l) < 163. S 1J does not 
contain the singleton 150 + 20 = 170 as M < 170. SJ does not contain the 
100 or the 120 as each is less than L -
PLEFT(3) etc. F6 (165) may be 
determined from S 5• In this example, the value of L did not change. In 
general, L will change if a better estimate is obtained as a result of the 
computation of some Si. If the heuristic wasn't used then the computation 
would have proceeded as: 
S0 = {O} 
S1 = {O, 100} 
S2 = { 0, 50, 100, 150} 
SJ = {O, 20, 50, 70, 100, 120, 150} 
S4 = { 0, 10, 20, 30, 50, 60, 70, 80, 100, 110, 120, 130, 150, 160} 
S 5 = {O, 7, 10, 17,20, 27, 30,37, 50, 57, 60,67, 70, 77, 80, 87, 100, 
107, 110, 117, 120, 127, 130, 137, 150, 157, 160} 

228 
Dynamic Programming 
/6 (165) may now be determined from S5 using the knowledge (p6, W6) 
(3, 3). 
D 
5.6 RELIABILITY DESIGN 
In this section we look at an example of how to use dynamic programming 
to solve a problem with a multiplicative optimization function. The prob-
lem is to design a system which is composed of several devices connected 
in series (Figure 5.12). Let r; be the reliability of device D; (i.e. r; is the 
probability that device i will function properly). Then, the reliability of 
the entire system is Ilr;. Even if the individual devices are very reliable 
(the r;'s are very close to one), the reliability of the system may not be very 
good. For example, if n = 10 and r; = .99, 1 ~ i ~ 10 then Ilr; = .904. 
Hence, it is desirable to duplicate devices. Multiple copies of the same 
device type are connected in parallel (Figure 5.13) through the use of 
switching circuits. The switching circuits determine which devices in any 
given group are functioning properly. They then make use of one such 
device at each stage. 
--~ 
[5]-[§J-- ..... - ~ 
Figure 5.12 n devices D;, 1 s; i s; n connected in ·series 
Stage I 
Stage 2 
Stage3 
Stage n 
D1 
D2 
D3 
Dn 
-
-
-
D3 -···-
Dn -
DI 
D2 
D3 
DI 
D3 
Dn 
Figure 5.13 Multiple devices connected in parallel in each stage 
If stage i contains m; copies of device D; then the probability that all 
m; have a malfunction is (1 - r;r· Hence the reliability of stage i becomes 
1 -
(1 -
r; )m;. Thus, if r; = . 99 and m; = 2 the stage reliability becomes 
.9999. In any practical situation, the stage reliability will be a little less 

Reliability Design 
229 
than 1 -
(1 -
r; )m; because the switching circuits themselves are not fully 
reliable. Also, failures of copies of the same device may not be fully in-
dependent (e.g. if failure is due to design defect). Let us assume that the 
reliability of stage i is actually given by a function <f>;(m,), 1 ::5 i ::5 n. (It is 
quite conceivable that <f> ;(m ;) may decrease after a certain value of m ;). The 
reliability of the system of stages is II 1 s;s,. </> ;(m ;). 
Our problem is to use device duplication to maximize reliability. This 
maximization is to be carried out under a cost constraint. Let c; be the 
cost of each unit of device i and let c be the maximum allowable cost of 
the system being designed. We wish to solve the following maximization 
problem: 
maximize II 
c/>; (m,) 
lsisn 
subject to 
E 
c;m; ::5 c 
l:Si:Sm 
(5.16) 
m; ;::::: 1 and integer, 1 ::5 i ::5 n 
A dynamic programming solution may be obtained in a manner similar 
to that used for the knapsack problem. Since, we may assume each c; > 0, 
each m; must be in the range 1 ::5 m; ::5 u; where 
n 
u; = 
L(c + c; -
E Cj)/c;J 
1 
The upper bound u; follows from the observation that mj ;::::: 1. An optimal 
solution m 1 , m 2, ••• , m. is the result of a sequence of decisions, one deci-
sion for each m;. Letf;(x) represent the maximum value of II . c/>(mj) sub-
t:S)sr 
ject to the constraints E1s;s; c;m; ::5 x and 1 ::5 m; ::5 u;, 1 ::5 j ::5 i. Then, 
the value of an optimal solution is f,.(c). The last decision made requires 
one to choose m. from one of { l, 2, 3, ... , u.}. Once a value for m. has 
been chosen, the remaining decisions must be such as to use the remaining 
funds c -
c.m. in an optimal way. The principal of optimality holds and 
f.(c) = 
max { c/>. (m n)fn-1 (c - c.m.)} 
l:Smn :Sun 
(5.17) 
For any f;(x ), i ;::::: 1, this equation generalizes to 
f;(x) = 
max {c/>;(m;)f;-1(c - c;m;)} 
lsm;su; 
(5.18) 

• 
• 
• 
• 
230 
Dynamic Programming 
Clearly, fo(x) = 1 for all x, 0 5 x 5 c. Hence, (5.18) may be solved 
using an approach similar to that used for the knapsack problem. Let Si 
consist of tuples of the form (f, x) where f = f;(x). There is at most one 
tuple for each different x that results from a sequence of decisions on 
m 1, m 2, ... , mi. The dominance rule (f1, xi) dominates (f2, x2) iff f 1 ~ Ji 
and x1 5 x2 holds for this problem too. Hence, dominated tuples may be 
discarded from Si. 
Example 5.23 We are to design a three stage system with device types D 1 , 
D2 and DJ. The costs are $30, $15 and $20 respectively. The cost of the 
system is to be no more than $105. The reliability of each device type is 
.9, .8 and .5 respectively. We shall assume that if stage i has mi devices 
of type i in parallel then c/>;(mi) = 1 -
(1 -
ri )mi. In terms of the notation 
used earlier, c1 = 30; c2 = 15; CJ = 20; c = 105; r1 = .9; ri = .8; TJ = 
.5; U1 = 2; U2 = 3 and UJ = 3. 
We shall use S to represent the set of all undominated tuples (f, x) 
that may result from the various decision sequences for m 1, m 2, ... , mi. 
Hence, f(x) = f;(x). Beginning with s0 = { (1, 0)} we may obtain each Si 
from s- 1 by trying out all possible values for mi and combining the result-
ing tuples together. Using Si to represent all tuples of obtainable from Si-t 
by choosing mi= .i we obtain: Sl = {(.9, 30)} andS1 = {(.9, 30), (.99, 60)}. 
S~ = {(.72, 45), (.792, 75)}; S~ = {(.864, 60)}. Note that the tuple 
(.9504, 90) which comes from (.99, 60) has been eliminated from S~ as this 
leaves only $10. This is not enough to allow mJ = 1. S~ = { (.8928, 75)}. 
Combining, we get S2 = { (.72, 45), (.864, 60), (.8928, 75)} as the tuple 
(.792, 75) is dominated by (.864, 60). s1 = {(.36, 65), (.432, 80), (.4464, 
95)}; S~ = {(.54, 85), (.648, 100)}; S~ = {(.63, 105)}. Combining, we 
get SJ = { (.36, 65); (.432, 80); (.54, 85); (.648, 100)}. 
The best design has a reliability of .648 and a cost of 100. Tracing back 
through the Sis we determine that m 1 = 1, m2 = 2 and mJ = 2. 
D 
As in the case of the knapsack problem, a complete dynamic programming 
algorithm for the reliability problem will use heuristics to reduce the size 
of the Sis. As noted in Example 5.23 there is no need to retain any tuple 
(f, x) in Si with x value greater that c -
EisJsn c1 as such a tuple will 
not leave adequate funds to complete the system. In addition, we may 
devise a simple heuristic to determine the best reliability obtainable by 
completing a tuple (f, x) in Si. If this is less than a heuristically determined 
lower bound on the optimal system reliability then (f, x) may be eliminated 
from S. 

The Traveling Salesperson Problem 
231 
5. 7 THE TRAVELING SALESPERSON PROBLEM 
We have seen how to apply dynamic programming to a subset selection 
problem (0/1 knapsack). Now we turn our attention to a permutation 
problem. Note that permutation problems will usually be much harder to 
solve than subset problems as there are n ! different permutations of n 
objects while there are only 2" different subsets of n objects (n ! > 0 (2• )). 
Let G = ( V, E) be a directed graph with edge costs Cij. Cij is defined such 
that Cy > 0 for all i andj and Cy = co if< i,j > ~ E. Let I VI = n and 
assume n > 1. A tour of G is a directed cycle that includes every vertex 
in V. The cost of a tour is the sum of the cost of the edges on the tour. 
The traveling salesperson problem is to find a tour of minimum cost. 
The traveling salesperson problem finds application in a variety of 
situations. Suppose we have to route a postal van to pick up mail from 
mail boxes located at n different sites. An n + 1 vertex graph may be 
used to represent the situation. One vertex represents the post office from 
which the postal van starts and to which it must return. Edge < i, j > is 
assigned a cost equal to the distance from site i to site j. The route taken 
by the postal van is a tour and we are interested in finding a tour of mini-
mum length. 
As a second example, suppose we wish to use a robot arm to tighten the 
nuts on some piece of machinery on an assembly line. The arm will start 
from its initial position (which is over the first nut to be tightened), suc-
cessively move to each of the remaining nuts and return to the initial posi-
tion. The path of the arm is clearly a tour on a graph in which vertices 
represent the nuts. A minimum cost tour will minimize the time needed for 
the arm to complete its task (note that only the total arm movement time 
is variable; the nut tightening time is independent of the tour). 
Our final example is from a production environment in which several 
commodities are manufactured on the same set of machines. The manu-
facture proceeds in cycles. In each production cycle, n different com-
modities are produced. When the machines are changed from production 
of commodity i to commodity j, a change over cost Cij is incurred. It is 
desired to find a sequence in which to manufacture these commodities. 
This sequence should minimize the sum of change over costs (the remain-
ing production costs are sequence independent). Since the manufacture 
proceeds cyclically, it is necessary to include the cost of starting the next 
cycle. This is just the change over cost from the last to the first commodity. 
Hence, this problem may be regarded as a traveling salesperson problem 
on an n vertex graph with edge cost Cij being the changeover cost from 
commodity i to commodity j. 

• 
• 
• 
• 
232 
Dynamic Programming 
In the following discussion we shall, without loss of generality, regard 
a tour to be a simple path that starts and ends at vertex 1. Every tour 
consists of an edge < 1, k > for some k E V - { 1} and a path from vertex 
k to vertex 1. The path from vertex k to vertex 1 goes through each vertex 
in V - { 1, k} exactly once. It is easy to see that if the tour is optimal then 
the path from k to 1 must be a shortest k to 1 path going through all 
vertices in V -
{ 1, k }. Hence, the principle of optimality holds. Let g(i, 
S) be the length of a shortest path starting at vertex i, going through all 
vertices in S and terminating at vertex 1. g ( 1, V - { 1 } ) is the length of an 
optimal salesperson tour. From the principal of optimality it follows that: 
g(l, V- {1}) = 
min {c1k + g(k, V- {l,k})} 
2:Sk:Sn 
(5.19) 
Generalizing (5.19) we obtain (for i E S) 
g(i, S) = min {cii + g(j, S - {j})} 
JES 
(5.20) 
(5.19) may be solved for g(l, V - { 1}) if we know g(k, V - { 1, k }) for 
all choices of k. The g values may be obtained by using (5.20). Clearly, 
g(i, cf>) = C;,1, 1 ::5 i ::5 n. Hence, we may use (5.20) to obtain g(i, S) for 
all S of size I. Then we can obtain g(i, S) for S with ISi = 2 etc. When 
IS I < n -
1, the values of i and S for which g (i, S) is needed are such 
that i ~ 1; 1 E S and i E S. 
Example 5.23 Consider the directed graph of Figure 5.14(a). The edge 
lengths are given by the matrix c of Figure 5.14(b) . 
0 
10 
15 
20 
5 
0 
9 
10 
6 
13 
0 
12 
8 
8 
9 
0 
(b) 
Figure 5.14 Directed graph and edge length matrix c 
g(2, cf>)= c21 = 5;g(3, cf>)= C31 = 6 andg(4, cf>)= C41 = 8 . 
--------

The Traveling Salesperson Problem 
233 
Using (5.20) we obtain 
g(2, { 3}) = C23 + g(3, cf>) = 15; 
g(3, {2}) = 18; 
g(4, {2}) = 13; 
g(2, {4}) = 18 
g(3,{4}) = 20 
g(4, {3}) = 15 
Next, we computeg(i, S) with ISi = 2, i -;t. 1, 1 ~Sandi~ S. 
g(2, {3, 4}) = min{c23 + g(3, {4}),c24 + g(4, {3})} = 25 
g(3, {2, 4}) = min{c32 + g(2, { 4}), C34 + g(4, {2})} = 25 
g(4, {2, 3}) = min{c42 + g(2, {3}), C43 + g(3, {2})} = 23 
Finally, from (5.19) we obtain 
g(l,{2,3,4}) = min{c12 +g(2,{3,4}),c13 +g(3,{2,4}),c14 +g(4,{2,3})} 
= min{ 35, 40, 43} 
= 35 
An optimal tour of the graph of Figure 5.14(a) has length 35. A tour of 
this length may be constructed if we retain with each g(i, S) the value ofJ 
that minimizes the right hand side of (5.20). Let l(i, S) be this value. Then, 
1(1, {2, 3, 4}) = 2. Thus the tour starts from 1 and goes to 2. The remain-
ing tour may be obtained from g(2, {3, 4}). 1(2, {3, 4}) = 4. Thus the 
next edge is (2, 4). The remaining tour is for g(4, {3}). 1(4, {3}) = 3. 
The optimal tour is 1, 2, 4, 3, 1. 
D 
Let N be the number of g(i, S)s that have to be computed before (5.19) 
may be used to compute g(l, V -
{l} ). For each value of IS I there are 
n -
1 choices for i. The number of distinct sets S of size k not including 1 
and i is (" : 2J. 
Hence 
n -2 
J 
2) 
N = 1c~0
(n - 1'\n; 
= (n -
1)2•- 2 • 
An algorithm that proceeds to find an optimal tour by making use of (5.19) 
and (5.20) will require O(n 22") tiine as the computation of g(i, S) with ISi 
= k requires k -
1 comparisons when solving (5.20). This is better than 

• 
• 
• 
• 
• 
234 
Dynamic Programming 
enumerating all n! different tours to find the best one. The most serious 
drawback of this dynamic programming solution is the space needed. The 
space needed is O(n2"). This is too large even for modest values of n. 
5.8 FLOW SHOP SCHEDULING 
Often, the processing of a job requires the performance of several distinct 
tasks. Computer programs run in a multiprogramming environment are in-
put, then executed. Following the execution, the job is queued for output 
and the output eventually printed. In a general flow shop we may have n 
jobs each requiring m tasks T1;, Tu, ... , T mi, 1 ::5 i ::5 n to be performed. 
Task Tj; is to be performed on processor Pj, 1 ::5 j 
::5 m. The time re-
quired to complete task Tj; is tj;. A schedule for the n jobs is an assign-
ment of tasks to time intervals on the processors. Task Tj; must be assigned 
to processor Pj. No processor may have more than one task assigned to it 
in any time interval. Additionally, for any job i the processing of task Tj;, 
j > 1 cannot be started until task Tj-1,; has been completed. 
Example 5.24 Two jobs have to be scheduled on three processors. The 
task times are given by the matrix 3: 
Two possible schedules for the jobs are shown in Figure 5.15. 
D 
0 
2 3 
5 
6 
II 
~ 
{a) 
{b) 
Figure 5.15 Two possible schedules for Example 5.24 
A non-preemptive schedule is a schedule in which the processing of a task 
on any processor is not terminated until the task is complete. A schedule 
for which this need not be true is called preemptive. The schedule of Figure 
5.15(a) is a preemptive schedule. Figure 5.15(b) shows a nonpreemptive 

Flow Shop Scheduling 
235 
schedule. The finish time, f;(S), of job i is the time at which all tasks of 
job i have been completed in schedule S. In Figure 5.15(a),f1(S) = 10 and 
f2(S) = 12. In Figure 5.15(b),f1(S) = 11 andfi(S) = 5. The finish time, 
F(S), of a schedule Sis given by 
F(S) = max {f; (S)} 
1 sisn 
(5.21) 
The meanflow time, MFT(S), is defined to be 
MFT(S) = _!._ E f; (S) 
n lsisn 
(5.22) 
An optimal finish time (OFT) schedule for a given set of jobs is a non-
preemptive schedule S for which F(S) is minimum over all nonpreemptive 
schedules S. A preemptive optimal finish time (POFT) schedule, optimal 
mean finish time schedule (OMFT) and preemptive optimal mean finish 
(POMFT) schedules are defined in the obvious way. 
While the general problem of obtaining OFT and POFT schedules for 
m > 2 and of obtaining OMFT schedules is computationally difficult (see 
chapter 11), dynamic programming leads to an efficient algorithm to ob-
tain OFT schedules for the case m = 2. In this section we consider this 
special case. 
For convenience, we shall use a; to represent t1;, and b; to represent t2i. 
For the two processor case one may readily verify that nothing is to be 
gained by using different processing orders on the two processors (this is 
not true for m > 2). Hence, a schedule is completely specified by providing 
a permutation of the jobs. Jobs will be executed on each processor in this 
order. Each task will be started at the earliest possible time. The schedule 
of Figure 5.16 is completely specified by the permutation (5, 1, 3, 2, 4). We 
shall make the simplifying assumption that a; ~ 0, 1 ~ i ~ n. Note that if 
jobs with a; = 0 are allowed then an optimal schedule may be constructed 
by first finding an optimal permutation for all jobs with a; ~ 0 and then 
adding all jobs with a; = 0 (in any order) in front of this permutation (see 
the exercises). 
Figure 5.16 A schedule 

• 
• 
• 
• 
236 
Dynamic Programming 
It is easy to see that an optimal permutation (schedule) has the property 
that given the first job in the permutation, the remaining permutation is 
optimal with respect to the state the two processors are in following the 
completion of the first job. Let u i. u 2, ••• , u k be a permutation prefix de-
fining a schedule for jobs Ti. T 2, ••• , T k· For this schedule let f 1 and Ji be 
the time at which the processing of jobs Ti. Ti. ... , T k is completed on pro-
cessors Pi and P2 respectively. Lett =Ji - f 1• The state of the processors 
following the sequet).ce of decisions Ti, T 2, ••• , T k is completely character-
ized by t. Let g(S, t) be the length of an optimal schedule for the subset 
of jobs S under the assumption that processor 2 is not available until time 
t. The length of an optimal schedule for the job set { 1, 2, ... , n} is 
g({l, 2, ... , n }, 0). 
Since the principle of optimality holds, we obtain 
g({l, 2, ... , n}, 0) = m_in{a; + g({l, 2, ... , n} - {i}, b;)}(5.23) 
l:s1sn 
Equation (5.23) generalizes to (5.24) for arbitrary S and t. This gen-
eralization requires that g(c/>, t) = max{t, O} and that a; ~ 0, 1 :S i :S n. 
g(S, t) = min{ a; + g(S - {i}, b; + max{ t -
a;, 0 }) 
iES 
(5.24) 
The term max{t -
a;, O} comes into (5.24) as task T2; cannot start until 
max{ a;, t} (P2 is not available until time t). Hence Ji - f1 = b; + max{ a;, t} 
- a; = b; + max{t - a;, O}. We may solve for g(S, t) using an approach 
similar to that used to solve (5.20). However, it turns out that (5.24) may 
be solved algebraically obtaining a very simple rule to generate an optimal 
schedule. 
Consider any schedule R for a subset of jobs S. Assume that P2 is not 
available until time t. Let i and j be the first two jobs in this schedule. 
Then, from (5.24) we obtain 
g(S, t) = a; + g(S - {i}, b; + max{t -
a;, O}) 
= a; + aj + g(S - {i, j}, bj + max{b; + max{t - a;, O} -
aj, O} 
(5.25) 
(5.25) may be simplified using the following result 

Flow Shop Scheduling 
237 
tii = bi + max{b; + max{t - a;, O} - ai, O} 
= bi + b; - ai + max{max{t - a;, 0}, ai - b;} 
= bi + b; - ai + max{t - a;, ai - b;, O} 
= bi + b; - ai - a; + max{ t, a; + ai - b;, a;} 
(5.26) 
If jobs i andj are interchanged in R then the finish time g '(S, t) will be 
g '(S, t) = a; + ai + g(S - {i, j}, tji) 
where 
tii = bi + b; - ai - a; + max{ t, a; + ai - bj, ai} 
Comparing g(s, t) and g '(s, t) we see that if (5.27) below hold then g(s, t) 
~ g '(s, t). 
max{ t, a; + ai - b;, a;} s max{ t, a; + ai - bi, ai} 
(5.27) 
In order for (5.27) to hold for all values oft, we need 
max{ a;+ ai - b;, a;}~ max{ a;+ aj - bj. ai} 
or 
a;+ ai + max{ -b;, -ai} ~a;+ ai + max{ -bi, -a;} 
or 
(5.28) 
From (5.28) we can conclude that there exists an optimal schedule in 
which for every pair (i, j) of adjacent jobs, min{ b;, ai} ;;:::: min{ b;, a;}. 
Exercise 26 shows that all schedules with this property have the same length. 
Hence, it suffices to generate any schedule for which (5.28) holds for every 
pair of adjacent jobs. We can obtain a schedule with this property by 
making the following observations resulting from (5.28). If min{ a 1, a2, ... , 
an, bi, b 2, ••• , b n} is a; then job i should be the first job in an optimal 

• 
• 
• 
• 
• 
238 
Dynamic Programming 
schedule. If min{ a1, a2, ... , an. b 1, h2, ... , bn} is bi then job j should be 
the last job in an optimal schedule. This enables us to make a decision as 
to the positioning of one of then jobs. (5.28) may now be used on the re-
maining n -
1 jobs to correctly position another job etc. The scheduling 
rule resulting from (5.28) is therefore 
i) sort all the a; sand b; s into nondecreasing order. 
ii) consider this sequence in this order. If the next number in the se-
quence is a j and job j hasn't yet been scheduled, schedule job j at the 
left most available spot. If the next number is bj and job j hasn't yet 
been scheduled, schedule job j at the right most available spot. If j 
has already been scheduled go to the next number in the sequence. 
Note that the above rule also correctly positions jobs with a; = 0. Hence 
these jobs need not be considered separately . 
Example 5.25 Letn = 4, (ai, a2, a3, a4) = (3, 4, 8, 10) and (bi, h2, b3, b4) 
== (6, 2, 9, 15). The sorted sequence of a's and h's is (b 2, a1, a2, hi, a3, b3, 
a4, b4) = (2, 3, 4, 6, 8, 9, 10, 15). Let <Ti, <T2, <TJ, <T4, be the optimal 
schedule. Since, the smallest number is h2, we set <T4 = 2. The next num-
ber is a1 and we set <T1 
== a1. The next smallest number is a2. Job 2 has 
already been scheduled. The next number is b 1• Job 1 has already been 
scheduled. a3 is the next and so we set u2 = 3. This leaves <TJ free and job 
4 unscheduled. Thus, <TJ = 4. 
D 
The scheduling rule above may be implemented to run in time O(n log n). 
(see exercises). Solving (5.23) and (5.24) directly for g({ 1, 2, ... , n }, 0) 
for the optimal schedule will take at least 0(2") time as there are this 
many different S's for which g(S, t) will be computed. 
REFERENCES AND SELECTED READINGS 
Two classic references on dynamic programming are: 
Introduction to Dynamic Programming by G. Nemhauser, John Wiley and Sons, 
Inc., 1966 
Applied Dynamic Programming by R. E. Bellman and S. E. Dreyfus, Princeton 
University Press, 1962. 
The dynamic programming formulation for the shortest paths problem appears in: 
"Algorithm 97: shortest path," by R. Floyd, C.ACM, 5(6), p. 345, 1962 . 

References and Selected Readings 
239 
An all pairs shortest path algorithm with average behavior O(n 2 log n) appears in: 
"A new algorithm for finding all shortest paths in a graph of positive arcs in aver-
age time O(n 2 log n)," by P. Spira, SIAM Jr. on Computing, 2, pp. 28-32, 1973. 
The construction of optimal binary search trees using dynamic programming is 
described in: 
The Art of Programming: Sorting and Searching, Vol. 3, by D. E. Knuth, Addison 
Wesley, 1973. 
"Optimum binary search trees," by D. E. Knuth, Acta informatica, 1, pp. 14-25, 
1971. 
A fast heuristic to generate nearly optimal binary search trees appears in: 
"Nearly optimal binary search trees," by K. Melhorn, Acta Informatica, 5, pp. 
287-295, 1975. 
The set generation approach to solving the 0/1 Knapsack problem may be found in 
the papers: 
"Discrete dynamic programming and capital allocation", by G. Nemhauser and Z. 
Ullman, Management Science, 15(9), pp. 494-505 (1969). 
"Computing partitions with applications to the knapsack problem", by E. Horowitz 
and S. Sahni,!. ACM, 21, pp. 277-292 (1974). 
The paper by Horowitz and Sahni extends this approach to include a divide and 
conquer scheme so that the recurrence (5.15) may be solvedintime0(2"12)(exercise 
13). Extensive experimentally observed computing times comparing various algo-
rithms for the knapsack problem are also presented. 
Dynamic programming recurrences similar to (5.15) can be obtained for many 
kinds of scheduling problems. Some references are: 
"A functional equation and its application to resource allocation and sequencing 
problems," by E. Lawler and J. Moore, Management Science, 16(1), pp. 85-103 
(1969). 
"Algorithms for scheduling independent tasks," by S. Sahni, J. ACM, 23(1), pp. 
114-127 (1976). 
"Exact and approximate algorithms for scheduling nonidentical processors," by 
E. Horowitz and S. Sahni, J. ACM. 23(2), pp. 317-327 (1976). 
Solutions to many of the exercises may be found in the above three papers. Our 
discussion on reliability design and flow shop scheduling is from the book by Bell-

• 
• 
• 
• 
• 
240 
Dynamic Programming 
man and Dreyfus. The rule of exercise 19 was derived by Bellman and Dreyfus and 
may be found in their book. Many other interesting examples are contained here . 
The flow shop scheduling rule was originally obtained by S. Johnson using a non 
dynamic programming approach. His original derivation may be found in: 
"Optimal two- and three-stage production schedules with set-up times included," 
by S. Johnson, Nav. Res. Log. Quat., 1, pp. 61-68 (1954). 
The dynamic programming formulation for the traveling salesperson problem is due 
to M. Held and R. Karp and may be found in: 
"A dynamic programming approach to sequencing problems," by M. Held and R. 
Karp, J. Soc. Ind. and Appl. Math., 10(2), 1962 . 
R. Bellman obtained a similar solution to the traveling salesperson problem. His 
work appears in: 
"Dynamic programming treatment of the traveling salesman problem," J. ACM, 9, 
pp. 61-63 (1962). 
The dynamic programming solution to the matrix product chain problem (exercises 
7 and 8) is due to S. Godbole and appears in: 
"On efficient computation of matrix chain products," by S. Godbole, IEEE Trans. 
on Computers, C-22(9), pp. 864-866, 1973. 
EXERCISES 
1. 
i) Does the recurrence (5.8) hold for the following graph? Why? 
ii) Why does eq. (5.8) not hold for graphs with cycles of negative length? 
2. 
Modify procedure ALL-PATHS so that a shortest path is output for each pair 
of vertices (i, j). What are the time and space complexities of the new algo-
rithm? 
3. Let A be the adjacency matrix of a directed graph G. Define the transitive 
closure, A +, of A to be a matrix with the property A + (i, j) = 1 iff G has a 

Exercises 
241 
directed path, containing at least one edge, from vertex i to vertexj. A+ (i, j) 
= 0 otherwise. The reflexive transitive closure, A•, is a matrix with the prop-
erty A *(i, j) = 1 iff G has a path, containing zero or more edges, from i to j. 
A *(i, j) = 0 otherwise. 
i) Obtain A + and A* for the following directed graph: 
4 
3 
ii) Let A le(i, j) = 1 iff there is a path with zero or more edges from i to j go-
ing through no vertex of index greater than k. Define A 0 in terms of the 
adjacency matrix A. 
iii) Obtain a recurrence between A 1e and A le- I similar to (5. 9). Use the logi-
cal operators or and and rather than min and + . 
iv) 
Write an algorithm, using the recurrence of (iii), to find A*· Your algo-
rithm can use only O(n 2) space. What is its time complexity? 
v) 
Show that A + = A x A* where matrix multiplication is defined as 
A+ (i, j) = VL 1 (A(i, k) /\ A *(k. j)). V is the logical or operation and 
A the logical and operation. Hence A + may be computed from A*. 
4. 
Using algorithm OBST compute W(i,j), R(i,j) and C(i,j), 0 s i < j s 4 for 
the identifier set (a 1, a 2, a 3, a4) = (end, goto, print, stop) with P(l) = 1/20, 
P(2) = 1/5, P(3) = 1/10, P(4) = 1/20, Q(O) = 1/5, Q(l) = 1/10, Q(2) = 
1/5, Q(3) = 1/20, Q(4) = 1/20. Using the R(i, j)s construct the optimal 
binary search tree. 
S. 
(a) Show that the computing time of algorithm OBST is O(n 2). 
(b) Write an algorithm to construct the optimal binary search tree T given 
the roots R (i, j), 0 s. i < j s n. Show that this can be done in time O(n). 
6. 
Since, often only the approximate values of the Ps and Qs are known, it is per-
haps just as meaningful to find a binary search tree that is nearly optimal i.e. 
its cost, eq. 5.9, is almost minimal for the given Ps and Qs. This exercise ex-
plores an O(n log n) algorithm that results in nearly optimal binary search trees. 
The search tree heuristic we shall study is: 
Choose the root k such that I W(O, k -
1) -
W(k, n)I 

I 
I 
~I 
•! 
242 
7. 
Dynamic Programming 
is as small as possible. Repeat this procedure to find the left and right sub-
trees of the root. 
(a) Using this heuristic obtain the resulting ·binary search tree for the data of 
exercise 4. What is its cost? 
(b) Write a SPARKS algorithm implementing the above heuristic. Your algo-
rithm should have a time complexity of at most O(n log n). 
An analysis of the performance of this heuristic may be found in the paper 
by Melhorn. 
[Matrix Product Chains] 
Let A, B and C be three matrices such that C = A 
x B. Let the dimensions of A, B and C respectively be m x n, n x p and 
m x p. From the definition of matrix multiplication, 
ti 
C(i, j) = E A(i, k)*B(k, j). 
k=I 
a) Write an algorithm to compute C directly using the above formula. Show 
that the number of multiplications needed by your algorithm is mnp. 
b) Let M 1 x Mi x · · · x Mr be a chain of matrix products. This chain 
may be evaluated in several different ways. Two possibilities are ( .. . ((M 1 
x Mi) x M 3) x M 4) x · · ·) x Mr and (M 1 x (Mi x ( · · · x (M r-1 x 
Mr) · · · ). The cost of any computation of M 1 x Mi x · · · x Mr is the 
number of multiplications used. Consider the case r = 4 and matrices M 1 
through M 4 with dimensions 100 x 1, 1 x 100, 100 x 1 and 1 x 100 
respectively. What is the cost of each of the five ways to compute M 1 x 
Mi x M 3 x M 4? Show that the optimal way has a cost of 10,200 while 
the worst way has a cost of 1,020,000. Assume that all matrix products 
are computed using the algorithm of (a). 
c) Let M,1 denote the matrix product M; x M;+ 1 x · · · x Mi. Thus, M;; 
= M;, 1 s i s r. S = Pt, pi, ... , Pr-I is a product sequence com-
puting Mir iff each product p1c is of the form MiJ x Mj+l,q where MiJ 
and Mi+ 1. q have been computed either by an earlier product p 1. I < k or 
represent an input matrix Mu. Note that MiJ x Mj+t,q = M;q. Also note 
that every valid computation of M tr using only pairwise matrix products at 
each step is defined by a product sequence. Two product sequences S 1 = 
Pt• pi, ... , Pr-I and Si = u 1, ui, ... , Ur-I are different if p; ;t!: u; for 
some i. Show that the number of different product sequences is (r -
1)! 
d) While there are (r -
1)! different product sequences, many of these are 
essentially the same in the sense that the same pairs of matrices are multi-
plied. For example, the sequences S 1 = (M 1 x Mi), (M 3 x M 4), (M 12 
x M 34) and Si = (MJ x M4), (M 1 x Mi), (M12 x MJ4) are different 
under the definition of c). However, the same pairs of matrices are multi-
plied in both S 1 and Si. Show that if we consider only those product se-
---··----
-- ---
. 

Exercises 
243 
quences that differ from each other in at least one matrix product then 
the number of different sequences is equal to the number of different 
binary trees having exactly r - 1 nodes. 
e) 
Show that the number of different binary trees with n nodes is 
_J____j2n) 
n + 1 \ n 
8. [Matrix Product Chains] 
In the preceding exercise it was established that the 
number of different ways to evaluate a matrix product chain is very large even 
when r is relatively small (say 10 or 20). In this exercise we shall develop an 
O(r3) algorithm to find an optimal product sequence (i.e. one of minimum 
cost). Let D(i), 0 :s; i :s; r represent the dimensions of the matrices, i.e. Mi 
has D(i -
1) rows and D(i) columns. Let C(i, j) be the cost of computing M ii 
using an optimal product sequence for M ii· Observe that C(i, i) = 0, 1 s i s r 
and that C(i, i + 1) = D(i -
l)*D(i)*D(i + 1), 1 s. i < r. 
a) Obtain a recurrence relation for C(i, j),j > i. This recurrence relation will 
be similar to Equation (5.13). 
b) Write an algorithm to solve the recurrence relation of a) for C(l, r). Your 
algorithm should be of complexity O(r 3). 
c) 
What changes are needed in the algorithm of b) to determine an optimal 
product sequence. Write an algorithm to determine such a sequence. Show 
that the overall complexity of your algorithm remains O(r 3). 
d) Work through your algorithm (by hand) for the product chain of part (b) 
of the previous exercise. What are the values of C(i, j), 1 s i s r and 
j ~ i? What is an optimal way to compute M 14? 
9. 
Generate the sets Si of jump points in fi(x), 0 s i s 4 (eq. 5.15) when 
(w1, wi, W3, w4) = (10, 15, 6, 9) and (p1,pi,p3,p4) = (2, 5, 8, 1). 
10. 
Write an algorithm, PARTS to determine an optimal solution xi, xi, ... , x. 
to the knapsack problem. Assume that Si, 0 s i < n have already been com-
puted as in procedure DKNAP. Knowing F(i) and F(i + 1) one can use a 
binary search to determine if (p ', w ') E S 1". Hence, the time complexity of 
your algorithm should be no more than O(n max{ log I Si I } ) s O(n i). 
I 
11. Give an example of a set of knapsack instances for which I Si I = 2i, 0 s i s n. 
Your set should include one instance for each n. 
12. 
(i) Show that if the p/s are integer then the size of each Si, I Si I, in the 
knapsack problem is no more than 1 + E1sisj pJgcd(p1, pi, ... , p.) 
where gcd(p 1, pi, ... , p.) is the greatest common divisor of the p;'s. 
(ii) Show that when the w/s are integer then I Si I :s; 1 + min{ l: 1 sjsi Wj. M}/ 
gcd (w1, w2, ... , w., M). 

244 
Dynamic Programming 
13. 
Using a divide-and-conquer approach coupled with the set generation ap-
proach of the text, show how to obtain an 0(2"12) algorithm for the 0/1 knap-
sack problem. 
14. Write an algorithm similar to DKNAP to solve the recurrence 5.18. What are 
the time and space requirements of your algorithm? 
15. 
a) Obtain a data representation for the values g(i, S) of the traveling sales-
person problem. Your representation should allow for easy access to the 
value of g(i, S) given iand S. (i) How much space does your representa-
tion need for an n vertex graph? (ii) How much time is needed to retrieve 
or update the value of g(i, S)? 
b) 
Using the representation of a) write a SPARKS algorithm corresponding 
to the dynamic programming solution of the traveling salesperson problem. 
16. 
[W. Miller] Show that BGRAPHl computes shortest paths for directed, acyclic 
graphs represented by adjacency lists (instead of inverse adjacency lists used 
by BGRAPH). 
procedure BGRAPHl (E, n) 
real BCOST(n ); integer}, n 
BCOST(l) -
0 
for} -
2 ton do BCOST(j) -
oo repeat 
for} -
1 ton -
1 do 
for all (j, r) EE do 
BCOST(r) -
min(BCOST(r), BCOST(j) + c(j, r)) 
repeat 
repeat 
endBGRAPHJ 
17. 
Consider the integer knapsack problem obtained by replacing the 0/1 con-
straint in (5.1) by x; ~ 0 and integer. Generalizef;(x) to this problem in the 
obvious way. 
i) Obtain the dynamic programming recurrence relation corresponding to 
(5.14). 
ii) 
Show how to transform this problem into a 0/1 knapsack problem. 
(Hint: introduce new 0/1 variables for each x;. If 0 s x; < 2i then intro-
duce j variables, one for each bit in the binary representation of x;). 
18. 
There are two warehouses W 1 and W2 from which supplies are to be shipped 
to destinations D;, 1 s i s n. Let d; be the demand at D; and let r; be the in-
ventory at W;. Assume r1 + r 2 = l: d;. Let cii(xii) be the cost of shipping Xii 
units from warehouse W; to destination Dj. The warehouse problem is to find 
nonnegative integers xii• 1 s i s 2 and 1 s j s n such that x tj + x 2j = dj, 
----
--~~---
-
-

Exercises 
245 
1 s j s n and l:,.; c1;(x1;) is minimized. Let g 1(x) be the cost incurred when 
W 1 has an inventory of x and supplies are sent to D;, 1 s j s. i, in an opti-
mal manner (the inventory at W 2 is l: 1 sJsi d1 -
x). The cost of an optimal 
solution to the warehouse problem is g.(r1 ). 
i) Use the optimality principle to obtain a recurrence relation for g;(x). 
ii) Write an algorithm to solve this recurrence and obtain an optimal se-
quence of values for xij, 1 s is 2, 1 s j s n. 
19. 
We are given a warehouse with a storage capacity of B units and an initial 
stock of v units. Let y; be the quantity sold in each month i, 1 s i s n. P; is 
the per unit selling price in month i. Let x; be the quantity purchased in 
month i. The buying price is c; per unit. At the end of each month, the stock 
in hand must be no more than B. i.e. 
v + I; (x · - y) s B, 
1 s 1· s n 
l!: ~j 
I 
l 
The amount sold in each month cannot be more than the stock at the end 
of the previous month (new stock arrives only at the end of a month) i.e. 
y s v + 
l: (x · -
y ), 
1 s is n 
1 
IS j< i 
J 
J 
Also, we require x; and Yi to be non-negative integers. The total profit 
derived is 
The problem is to determine XJ. Yi such that P. is maximized. Letf;(vi) 
represent the maximum profit that can be earned in months i + 1, i + 2, 
... , n starting with v; units of stock at the end of month i. Thenfo(v) is the 
maximum value of P n. 
i) 
Obtain the dynamic programming recurrence forfi(v;) in terms off;+ 1(vi). 
ii) What isf.(vi)? 
iii) Solve (i) analytically to obtain the formula 
fi(vi) = aiXi + biVi 
for some constants a; and b; 
iv) 
Show that an optimal P n is obtained by using the following strategy: 
1. 
if p; ~ c i and 
(a) hi+t ~Ci thenyi =Vi andx; = B 
(b) h;+1 s c;theny; =Vi and xi= 0 

246 
Dynamic Programming 
2. 
ifc; ~ p; and 
• 
(a) 
h;+ 1
~c;theny;=Oandx;=B-v; 
(b) h;+ 1 s p; theny; = v; andx; = 0 
(c) p; s h;+1 s c;theny; = Oandx; = 0 
v) 
Use the followingp;, c; and obtain an optimal decision sequence from (iv). 
p; 
C; 
1 
2 
8 
8 
3 
6 
3 
4 
2 
3 
7 
1 
5 
4 
4 
6 
3 
5 
7 
2 
1 
8 
5 
3 
Assume the warehouse capacity to be 100 and the initial stock to be 60. 
vi) 
From (iv) conclude that an optimal set of values for x; and y; will always 
lead to the following policy: Do no buying or selling for the first k months 
(k may be zero) and then oscillate between a full and an empty ware-
house for the remaining months. 
20. 
Assume that n programs are to be stored on two tapes. Let/; be the length of 
tape needed to store the ith program. Assume that l: I; s L where L is the 
length of each tape. A program may be stored on either of the two tapes. If Sl 
is the set of programs on tape 1 then the worst case access time for a program 
is proportional to max{l:;Es, /;, l:;fs, /;}.An optimal assignment of programs 
to tapes minimizes the worst case access times. Formulate a dynamic pro-
gramming approach to determine the worst case access time of an optimal 
assignment. Write an algorithm to determine this time. What is the complexity 
of your algorithm? 
21. 
Redo problem 20 making the assumption that programs will be stored on tape 
2 using a different tape density than used on tape 1. If/; is the tape length 
needed by program i when stored on tape 1 then a· I; is the tape length needed 
on tape 2. 
22. N jobs are to be processed. Two machines A and B are available. If job i is 
processed on machine A then a; units of processing time are needed. If it is 
processed on machine B then b; units of processing time are needed. Because 
of the peculiarities of the jobs and the machines, it is quite possible that a; ~ b; 
for some i while aj < bj for some}, j ?! i. Obtain a dynamic programming 
formulati~n to determine the minimum time needed to process all the jobs. 
Note that jobs cannot be split between machines. Indicate how you would go 
about solving the recurrence relation obtained. Do this on an example of your 
choice. Also indicate how you would determine an optimal assignment of jobs 
to machines. 

Exercises 
247 
23. N jobs have to be scheduled for processing on one machine. Associated with 
job i is a 3-tuple (p;, t;, d; ). t; is the processing time needed to complete job 
i. If job i is completed by its deadlined; then a profit p; is earned. If not then 
nothing is earned. From chapter 4 we know that J is a subset of jobs that can 
all be completed by their deadlines iff the jobs in J can be processed in non-
decreasing order of deadlines without violating any deadline. Assumed; :5 d; + 1, 
1 :5 i < n. Letf;(x) be the maximum profit that can be earned from a subset 
J of jobs when n = i. fn(dn) is the value of an optimal selection of jobs J. 
fo(x) = 0. Showthatforx st;, 
f;(x) = max{f;-1(x), f;-1(x - t;) + p;} 
24. 
Let I be any instance of the 2 processor flow shop problem. 
(a) Show that the length of every POFT schedule for I is the same as the 
length of every OFT schedule for I. Hence, the algorithm of section 5.8 
also generates a POFT schedule. 
(b) Show that there exists an OFT schedule for I in which jobs are processed 
in the same order on both processors. 
(c) Show that there exists an OFT schedule for I defined by some permuta-
tion u of the jobs (see (b)) such that all jobs with a; = 0 are at the front 
of this permutation. Further, show that the order in which these jobs ap-
pear at the front of the permutation is not important. 
25. Let I be any instance of the two processor flow shop problem. Let u = u 1 u2 
• • • Un be a permutation defining an OFT schedule for I. 
(a) 
Use (5.28) to argue that there exists and OFT u such that min{b;, a,;} ~ 
min{b,;, a;} for every i andj such that i = Uk andj = Uk+ 1 (i.e. i andj 
are adjacent). 
(b) For au satisfying the conditions of a) show that min{ b ;, a,;} ~ min{ b,;, a;} 
for every i andj such that i = Uk andj = u,, k < r. 
(c) Show that all schedules corresponding to u's satisfying the conditions of 
a) have the same finish time. (Hint: use b) to transform one of two dif-
ferent schedules satisfying a) into the other without increasing the finish 
time.) 
26. 
The principle of optimality does not hold for every problem whose solution 
may be viewed as the result of a sequence of decisions. Find two problems for 
which the principle does not hold. Explain why the principle does not hold for 
these problems. 

Chapter 6 
BASIC SEARCH AND TRAVERSAL 
TECHNIQUES 
6.1 THE TECHNIQUES 
The solution to many problems involves the manipulation of binary trees, 
trees or graphs. Often, this manipulation requires us to determine a vertex 
(node) or a subset of vertices in the given data object that satisfies a given 
property. For example, we may wish to find all vertices in a binary tree 
with a data value less than X or we may wish to find all vertices in a given 
graph G that can be reached from another given vertex v. The determina-
tion of this subset of vertices satisfying a given property can be carried out 
by systematically examining the vertices of the given data object. This often 
takes the form of a search in the data object. When the search necessarily 
involves the examination of every vertex in the object being searched, it is 
called a traversal. 
We have already seen an example of a problem whose solution required 
a search of a binary tree. In Section 5.4 we presented an algorithm to search 
a binary search tree for an identifier X. This algorithm is not a traversal 
algorithm as it does not examine every vertex in the search tree. Sometimes, 
we may wish to traverse a binary search tree (e.g. when we wish to list out 
all the identifiers in the tree). Algorithms for this will be studied in this 
chapter. 
The techniques to be discussed in this section are divided into three 
categories. The first two categories include techniques applicable only to 
binary trees and trees respectively. As described, these techniques will in-
volve examining every node in the given data object instance. Hence, these 
techniques are referred to as traversal methods. The third category includes 
techniques applicable to graphs (and hence also to trees and binary trees). 
These search strategies may not examine all vertices and so are referred 
to only as search methods. During a search (or traversal) the fields of a 
248 
----- ---- -- - - --

The Techniques 
249 
node may be made use of several times. It may be necessary to distinguish 
certain uses of the fields of a node. During these uses, the node is said to 
be visited. Visiting a node may involve printing out its data field, evaluating 
the operation specified by the node in case of a binary tree representing 
an expression, setting a mark bit to one or zero, etc. Since we are describing 
search and traversals of trees and graphs independent of the application, 
we use the term visited rather than state the specific function performed 
on the node at this time. 
6.1.1. BINARY TREE TRAVERSAL 
There are many operations that we often want to perform on binary trees. 
One notion that arises frequently is the idea of traversing a tree or visiting 
each node in the tree exactly once. A full traversal produces a linear order 
for the information in a tree. This linear order may be familiar and useful. 
When traversing a binary tree we want to treat each node and its subtrees 
in the same fashion. If we let L, D, R stand for moving left, printing the 
data, and moving right when at a node then there are six possible combina-
tions of traversal: LDR, LRD, DLR, DRL, RDL, andRLD. Ifwe adopt the 
convention that we traverse left before right then only three traversals re-
main: LDR, LRD and DLR. To these we assign the names inorder, post-
order and preorder. We will define these three traversals and show how 
they work on the binary tree of Figure 6.1. 
Inorder Traversal: 
informally this calls for moving down the tree towards 
the left until you can go no farther. Then you "visit" the node, move one 
F 
H 
Figure 6.1 
A binary tree 

# 
# 
250 
Buie Search and Travenal Techniques 
node to the right and continue again. If you cannot move to the right, go 
back one more node. A precise and elegant way to describe this traversal 
is to write it as a recursive procedure. Algorithm 6.1 is the result. In sub-
algorithm VISIT, we perform whatever function needs to be performed at 
the time a node is visited. 
procedure INORDER(T) 
I IT is a binary tree. Each node of T has three fields// 
//LCHILD, DATA, RCHILD// 
if T ~ 0 then call INORDER(LCHILD(T)) 
call VISIT(T) 
end.if 
end/NORDER 
call (INORDER(RCHILD(T)) 
Algorithm 6.1 
Recursive formulation of inorder traversal 
Figure 6.2 traces how !NORDER works on the binary tree of Figure 6.1. 
This trace assumes that visiting a node requires only the printing of its 
DATA field. The output resulting from this traversal is FDHGIBEAC. 
Call of 
value 
/NORDER 
in root 
Action 
MAIN 
A 
1 
B 
2 
D 
3 
F 
4 
print ('F') 
4 
print ('D') 
3 
G 
4 
H 
5 
print ('H') 
5 
print ('G') 
4 
I 
5 
print ('I') 
5 
print ('B') 
2 
E 
3 
print ('E') 
3 
print ('A') 
1 
c 
2 
print ('C') 
2 
Figure 6.2 Inorder traversal of binary tree of Figure 6.1 with call VISIT(T) 
replaced by print (DATA(T)) 

The Techniques 
251 
The recursive procedures corresponding to preorder and postorder appear 
in Algorithms 6.2 and 6.3. 
procedure PREORDER (T) 
I IT is a binary tree. Each node in T has three fields LCHILD, DATA,/ I 
//RCHILD// 
If T ~ 0 then call VJSJT(T) 
end.if 
call PREORDER(LCHJLD(T)) 
call PREORDER(RCHILD(T)) 
end PREORDER 
Algorithm 6.2 Preorder traversal 
procedure POSTORDER(T) 
! IT is a binary tree. Each node in T has three fields LCHILD, DATA,// 
//RCHILD// 
if T ~ 0 then call POSTORDER(LCHILD(T)) 
call POSTORDER (RCHILD(T)) 
call VISIT(T) 
end.if 
end POSTORDER 
Algorithm 6.3 Postorder traversal 
With call VISIT(T) replaced by print(DA T A(T)) the application of Algo-
rithms 6.2 and 6.3 to the binary tree of Figure 6.1 results in the outputs 
ABDFGHIEC and FHIGDEBCA respectively. 
Theorem 6.1 
Let t(n) and s(n) respectively represent the time and space 
needed by any one of the traversal algorithms when the input tree T has 
n ~ 0 nodes. If the time and space needed to visit a node is 8(1) then 
t(n) = 8(n) and s(n) = O(n). 
Proof: 
The work done by each traversal algorithm is made up of two 
components: (i) work done on this level of recursion and (ii) work done due 
to recursive invocation of the algorithm from this level. The time required 
for the first of these is bounded by a constant c 1• If the number of nodes 
in the left subtree of Tis n 1 then t(n) is given by the recursion: 
t(n) :5 maxn 1 {t(nt) + t(n -
n 1 -
1) +ct}, 
n ~ 1. 
Note that t(O) :s; c 1 • A proof by induction establishes that t(n) :5 c 2n + 

252 
Buie Search and Travenal Techniques 
ct where c 2 is a constant such that c 2 ~ 2c t· This inequality clearly holds 
when n = 0. Assume it holds for all n, 0 :$ n < m. We shall show it is 
true when n = m. Let T be an m node tree. Let n t be the number of nodes 
in the left subtree of T. Then 
t(m) s max{t(ni) + t(n - nt -
1) + ct} 
S max{cint + Ct + ci(n - nt -
1) + Ct + ct} 
= cin + 3ct -
c2 
s cin + Ct 
It is easy to see that there exist c t ' and c 2' such thatt(n) ~ c 2 'n + c t '. 
Hence, t(n) = 9(n). The only additional space needed is for saving the 
values of local variables on recursive calls. If T has depth d then this space 
is clearly O(d). For an n node binary treed s n and so s(n) = O(n). 
D 
While the recursive traversal algorithms can be used directly, the over-
head of recursion may make it desirable to recode the algorithms first into 
nonrecursive versions. Standard rules for obtaining a nonrecursive equivalent 
of a recursive algorithm were given in Chapter 1. These rules generally re-
sult in inelegant algorithms. However, using the standard translation rules 
has the virture that given a correct recursive algorithm the nonrecursive 
version is guaranteed to be correct. Let us attempt to directly write a non-
recursive algorithm for inorder traversal. If T is the root of a binary tree 
then its left subtree (if nonempty) must be traversed before T can be visited. 
Thus, we may put T on a stack and proceed to traverse its left subtree. The 
stack will be maintained such that when the left subtree has been traversed, 
Tis at the top of the stack. 
Consider the binary tree of Figure 6.3. Node A has a left subtree B and 
so it is stacked. We then proceed to traverse B. Node B has a left subtree 
D so B gets stacked and we traverse D. D's left subtree is empty and so 
node D may be visited. Now, we have to traverse D's right subtree. This 
requires us to visit node G. At this time we have completed traversing the 
left subtree of B. Node B is on the top of the stack. B is removed and 
visited. We now continue with B's right subtree. Since B has an empty 
right subtree, the traversal of the subtree B which is the left subtree of A 
has been completed. A is at the top of the stack. In general, the stack will 
contain only those nodes whose left subtrees haven't yet been traversed. 
Whenever the traversal of a subtree which is a left subtree of some node 

The Techniques 
253 
Q is completed, Q will be at the top of the stack. Thus, when the subtree 
with root D has been traversed, B will be the topmost node on the stack; 
when the traversal of tree B is completed A will be on top; when tree A has 
been traversed, the stack will be empty. 
The formal algorithm is procedure INORDERl (Algorithm 6.4.). The 
variable P traverses the binary tree T and at the start of the loop of lines 
4-19, P points to the root of a subtree to be traversed. In lines 5-11 the 
roots of all left subtrees starting from P get stacked. On exit from this 
loop, P points to a node with empty left subtree and so P is now to be 
visited. At the start of the loop of lines 12-18, P points to a node that is 
now to be visited (i.e. its left subtree, if nonempty, has been traversed). 
Following the visiting of node P, its right subtree, if nonempty, is to be 
traversed. In case P has an empty right subtree then we have completed the 
traversal of a left subtree and we must now move to the parent of this com-
pleted left subtree. The parent is the topmost node on the stack (lines 16-
17). It is easy to see that if Q is the root of the left subtree of R then when 
the traversal of Q is completed R is the node at the top of the stack. When-
ever a node is visited it is removed from the stack. All nodes in Q must be 
visited before the traversal of Q is complete. Hence all nodes stacked after 
R is stacked must be deleted before the traversal of Q is complete. 
A 
D 
E 
F 
G 
Figure 6.3 A binary tree 

254 
Basic Search and Travenal Techniques 
procedure!NORDER1 (T) 
I I a nonrecursive version using a stack of size ml I 
1 
integer STACK(m), i, m 
2 
If T = 0 then return I IT empty/ I 
3 
P -
T; i -
0 I IP traverses T; i is top of stack/ I 
4 
loop 
5 
whileLCHILD(P) ~ 0 do //traverse left subtree// 
6 
i-i+l 
7 
if i > m then print ('stack overflow') 
8 
sfup 
9 
endlf 
10 
STACK(i) -
P; P - LCHILD(P) 
11 
repeat 
12 
loop 
13 
call VISIT(P) 
I /left subtree of P has been traversed/ I 
14 
P - RCHILD(P) 
15 
if P ~ 0 then exit I /traverse right subtree/ I 
16 
if i = 0 then return 
17 
P - STACK(i); i -
i -
1 
18 
repeat //visit a parent node/ I 
19 
repeat 
end INORDER1 
Algorithm 6.4 Nonrecursive algorithm for inorder traversal 
Analysis of INORDERl 
We shall analyze the computing time of INORDERl in terms of the 
number of nodes, n, in the binary tree T. On each iteration of the while 
loop of lines 5-11, a node gets stacked (line 10). Every node that gets stacked 
gets visited (line 13). Since no node is visited more than once, the loop of 
lines 5-11 cannot be iterated more than n times in the entire execution of 
the algorithm. Actually, at most n -
1 nodes can get stacked as leaf nodes 
don't get stacked (line 5) and every tree with n ~ 1 has at least one leaf 
node. The total time for lines 5-11 is therefore O(n). On each iteration 
of the loop of lines 12-18 a node gets visited. Since each node in T is visited 
exactly once and nodes are not visited anywhere else in the algorithm, this 
loop is iterated a total of n times in the algorithm. The total time needed 
for this loop is therefore 9(n). Hence, the time complexity of INORDERl 
is e(n). 

The Teclmiqam 
255 
As far as the stack space is concerned, we see that only nodes with a 
nonempty left subtree can be stacked. The worst case occurs when Tis a 
left skewed binary tree (Figure 6.4(b)). In a left skewed binary tree every 
node except the leaf has a nonempty left subtree and an empty right sub-
tree. In this case, a stack of size n -
1 is needed. The best case is when 
every node has an empty left subtree and all nodes other than the leaf have 
a nonempty right subtree. Such a binary tree is a right skewed binary tree 
(Figure 6.4(a)). In this case, no nodes get stacked. A more useful statement 
of the stack space needed is in terms of the depth of T. One may verify 
that if T has depth d then the stack space needed is O(d). 
(a) Right Skewed 
/~T 
• 
• 
• 
cf 
(b) Left Skewed 
Figure 6.4 Skewed binary trees 
At this point we may ask the question: can we do any better? Clearly, 
all traversal algorithms must visit each node and so the computing time 
must be at least 9(n). The only improvement we can expect then is a re-
duction in the additional space required (i.e., the stack space). Is it possible 
to traverse binary trees in 9(n) time and 9(1) space? 
Binary Tree Traversal in 9(n) time and 9( I) space 
If each node has a PARENT field linking to its parent then, Exercises 10 to 
12 examine how traversals may be accomplished in 9(n) time and 9( l) space. 
We will address ourselves here to the problem of obtaining a similarly 
behaved algorithm for the case when no PARENT fields exist. The presence 
of parent fields allows one to go from any node P to the root node. In 
obtaining a 9(1) space algorithm we will achieve this effect by reversing the 
direction of links from the root node to the node currently being examined. 
Thus, if P points to the node in tree T that is currently being examined 

• 
• 
• 
• 
• 
256 
Buie Search and Travenal Techniques 
and Q points to its parent then we will maintain a path from Q to the root 
T. This path will be called the Q-T path and will be built by linking together 
all nodes on the path from T to Q. If U, V, and Ware three nodes on 
this path such that U is the parent of V and V the parent of W, then V 
will be linked to U through its RCHILD field if W is the RCHILD of V. 
Otherwise, V will be linked to U through its LCHILD field. 
Let us see how this works on the tree T of Figure 6.3. Initially, P is at 
the root A and Q is also atA indicating an empty Q-T path. Next, P moves 
to node Band the Q-T path contains only the root node A. LCHILD(A) is 
set to T since P is LCHILD(A) and this field is to be used to link A into 
the empty Q-T path list. We shall use LCHILD(A) = A rather than 
LCHILD(A) = 0 to indicate the end of the Q-T path. As we shall see, 
this will simplify the resulting traversal algorithms. P next moves to node D. 
The resulting Q-T path is shown in Figure 6.S(a). Node B is linked into 
this path through its LCHILD field. This fact is easy to determine since 
RCHILD(B) = 0. Node D is now visited as LCHILD(D) = 0. P next 
advances to its right subtree, i.e. to node G. The resulting Q-T path list 
is shown in Figure 6.S(b). Again, since LCHILD(Q) = 0 and Q is not the 
last node on the Q-T list, Q must be linked via its RCHILD field. Node 
G can now be visited. Since G is a leaf, it is now necessary to back up 
along the Q-T path list until we reach a node with a nonempty right sub-
tree. From Git is easy to back up to D resetting RCHILD(D) to point to G. 
This results in the situation of Figure 6.S(a). From D we back up to B 
resetting LCHILD(B) to D. Since we have returned from B's left subtree, 
it is time to visit B. B's right subtree is empty and we must back up to 
node A. At this time it is necessary to be able to determine whether B was 
the left or right child of A. Since neither LCHILD(A) = 0 nor RCHILD(A) 
= 0, the test used at nodes B and D cannot be used here. However, since 
LCHILD(A) = A, we know that B must be the left subtree of A. So, 
LCHILD(A) is set to Band node A visited. Now, we move P to node C and 
then to E getting the configuration of Figure 6.S(c). Backing up from E 
to C, we are faced with the problem of determining whether E is C's left 
or right child. Neither LCHILD(C) = 0 nor RCHILD(C) = 0. With the 
information we have at present, there is no way to determine whether E is 
C's left or right child. More information is needed. A TAG field in each 
node would be useful (see exercise). However, such a field is not available 
for use. We shall explicitly keep track of the last node R such that 
LCHILD(R) ¥- 0 and RCHILD(R) -:;e 0 and a move to R's right subtree 
was made. This will be done through variable LR. Initially, LR = 0. During 
P's traversal in A's left subtree LR remains zero. As a result, when we wish 
to back up from B to A, since LR -:;e A it follows that B was A's left sub-

The Techniques 
257 
tree. When P moves to C, LR is updated to A. When backing up from E, 
LR = A -:;e C and so E was C's left child. Moving from C to F requires 
updating LR to C. Since the old value of LR will be needed once we have 
finished traversing the subtree C, it is necessary to save the values of LR 
on a stack. This stack can also be built in place making use of leaf nodes 
in T. Since both LCHILD and RCHILD fields of leaf nodes are zero, 
LCHILD can be used to retain the value of LR and RCHILD to link to the 
remainder of the stack. This is shown in Figure 6.S(d). The remaining 
details of the algorithm are spelled out in INORDER2 (Algorithm 6.5). 
TOP 
LR= A 
(a) 
(c) 
0 
D 0 
DO 
a 
~G 
p 
(b) 
(d) 
Figure 6.5 Traversing a binary tree in 8(1) space 

• 
• 
• 
• 
• 
line procedure INORDER2(T) 
I /inorder traversal of binary tree Tusing a fixed amount of additional/ I 
//space// 
1 
if T = 0 then return endif //empty binary tree// 
2 
TOP - LR -
O; Q - P -
T 
I /initialize/ I 
3 
loop 
4 
loop //move down as far as possible// 
5 
case 
6 
"LCHILD(P) = 0 and RCHILD(P) = 0: 
I I can't move down/ I 
7 
call VISIT(P); exit 
8 
:LCHILD(P) = 0: 
//move to RCHILD(P)// 
9 
call VISIT(P) 
10 
R -
RCHJLD(P); RCHILD(P) -
Q 
Q - P;P -
R 
11 
:else: 
//move to LCHILD(P)// 
12 
R - LCHILD(P); LCHILD(P) -
Q; Q -
P; 
P-R 
13 
endcase 
14 
repeat 
/IP is a leaf node, move upwards to a node whose right subtree// 
//hasn't yet been examined// 
15 
AV -
P 
//leaf node to be used in stack// 
16 
loop 
//move up from P 11 
17 
case 
18 
:P = T: return //can't move up from root/ I 
19 
:LCHILD(Q) = 0: 
I IQ is linked via RCHILD// 
20 
R -
RCHILD(Q);RCHILD(Q) - P;P- Q; Q -
R 
21 
:RCHILD(Q) = 0: 
! IQ is linked via LCHILD// 
22 
R - LCHILD(Q); LCHILD(Q) -
P; P -
Q; Q - R; 
call VISIT(P) 
23 
:else: 
I I check if P is RCHILD of QI I 
24 
if Q = LR then //Pis RCHILD of QI I 
25 
R -
TOP; LR - LC HILD (R) 
I /update LR I I 
26 
TOP -
RCHJLD(R) //unstack// 
27 
LCHILD(R) - RCHILD(R) -
0 
//reset leafnode links// 
28 
R - RCHILD(Q);RCHILD(Q) - P;P- Q; Q -
R 
29 
else 
I IP is LC HILD of QI I 
30 
call VISJT(Q) 
31 
LCHJLD(A V) - LR; RCHILD(A V) -
TOP 
32 
TOP - AV; LR -
Q 
33 
R - LCHILD(Q); LCHILD(Q) - P 
I /restore link toP I I 
34 
P - RCHILD(Q); RCHILD(Q) - R; exit 
//move right/ I 
35 
endif 
36 
endcase 
37 
repeat 
38 
repeat 
39 
end !NORDER 2 
Algorithm 6.5 
Procedure to traverse a binary tree in 8(n) time and 8(1) space 

The Teclmiq-
259 
Analysis of INORDER2 
Let no, n 1 and n 2 be the number of nodes of degree 0, 1 and 2 respectively. 
Let n = no + n 1 + n 2• It is clear that P points to a node of degree zero 
exactly once, i.e. when the node is reached during a downward move in 
the loop of lines 4-14. P will reach a node with one child exactly two times, 
once during a downward move and once again during an upward move 
from its child (lines 16-37). A node with two children will be reached by P 
exactly three times, once during a downward move (lines 4-14) and twice 
during upward moves from its two children (lines 16-37). Hence, the total 
number of changes in P's value is no + 2n 1 + 3n 2. In every iteration of the 
loop of lines 4-14 P's value changes if P is not a leaf. If P is a leaf then an 
exit is made and P's value changes in the loop of lines 16-37. Each iteration 
of this loop necessarily changes P's value. Hence, the total number of 
iterations of the loops of lines 4-14 and 16-37 together is 2n o + 2n 1 + 3n 2• 
An iteration of either of these loops takes 9(1) time. The total time for 
the loops Of lines 3-38 is therefore 9(2n O + 2n I + 3n 2) = 9(n). 
Lines 1 and 2 contribute 9(1) and so the total time taken is 9(n). The 
additional space needed is 9(1) as this space is needed only for simple 
variables such as P, Q, AV, LR, TOP, R andLRl. One may readily verify 
that the algorithm on termination leaves the tree Tin its original form. 
D 
There are several other traversal algorithms for binary trees. Some of 
these are examined in the exercises. 
Empirical Comparison of lnorder Algorithms 
We have seen three different algorithms for inorder traversal. These are 
!NORDER, INORDERl and INORDER2. We shall abbreviate these names 
to IN, INl and IN2 respectively. When traversing an n node binary tree T 
of depth d, each of these algorithms takes 9(n) time. IN and INl require 
O(d) additional space while IN2 requires only 9(1) additional space. Since 
it is pretty clear that both IN and INl will run faster than IN2, IN2 is to be 
used only when O(d) space is not available for IN and INl to operate in. 
When space is not at a premium, the choice is reduced to one between IN 
and INl. We would expect INl to run faster than IN (because of the over-
head of recursion). However, we do not know by how much INl will be 
faster than IN. This will depend on the programming language used. When 
the programming language does not support recursion (e.g. FORTRAN), 
only INl can be used. 
To get a feel for the "cost" of recursion, IN and INl were programmed 

• 
• 
• 
• 
260 
Buie Search and Travenal Techniques 
in PASCAL. It is easy to see that the time needed by IN to traverse an n 
node binary tree is relatively insensitive to the shape of the tree. For each 
node, two recursive calls are made. Hence, a total of 2n recursive calls 
will be made while traversing T. The time needed by INl, however, depends 
on the shape of the n node binary tree being traversed. To see this, note 
that only nodes that have a left child get stacked (lines 5-11). Hence if no 
node in the tree being traversed has a left child then no nodes will get 
stacked. Hence, none will get unstacked either. INl will work fast on such 
a tree. If every node in the binary tree has a left child and no right child 
then n -
1 nodes will get stacked (and also unstacked). INl will take 
maximum time when T is a left skewed tree. On the average, half the nodes 
will have a left child and the other half will not. This case is represented 
by a full binary tree. 
Since, INl is only an iterative version of IN, we programmed another 
inorder algorithm IN3 (Algorithm 6.6) which is obtained from IN by re-
moving only the second recursive call. The three algorithms IN, INl and 
IN3 were programmed in PASCAL and run on a CDC Cyber 74 computer. 
The observed computing times are shown in Table 6.1. For comparison 
purposes, IN2 was also programmed in PASCAL and run. In addition to 
carrying out the comparison tests just described, INl and IN2 were also 
programmed in FORTRAN and run on the same computer. The computing 
times for the FORTRAN programs are also given in Table 6.1. 
procedure IN3(T) 
while T -:;e 0 do 
call IN3(LCHILD(T)) 
I /recursively traverse left subtree/ I 
call VISIT(T) 
T -
RCHILD(T) I /traverse right subtree/ I 
repeat 
end!N3 
Algorithm 6.6 Another inorder algorithm 
The data of Table 6.1 indicates that in PASCAL recursion does not have 
an excessive overhead. In fact, algorithm IN takes less time than INl on 
left skewed binary trees. Algorithm IN3 took less time than IN on all 
data sets. It was faster than INl on left skewed and full binary trees. It 
is slower than INl only on right skewed binary trees. By comparison, IN2 
takes between 1.5 to 3 times as much time as INl. For PASCAL, IN3 is 

The Techniques 
261 
PASCAL 
FORTRAN 
n 
IN 
INl 
IN2 
IN3 
INl 
IN2 
31 
l.15 
0.75 
l.85 
0.85 
0.3 
1.2 
63 
2.15 
1.25 
4.0 
1.7 
0.5 
2.2 
127 
4.3 
2.55 
7.5 
3.5 
1.0 
4.45 
225 
8.75 
5.05 
15.3 
7.05 
2.25 
8.8 
511 
17.75 
10.4 
30.85 
14.3 
4.35 
17.3 
1023 
34.3 
20.6 
61.25 
28.05 
8.55 
36.15 
2047 
70.7 
40.65 
124.85 
55.2 
17.5 
70.45 
4095 
138.8 
81.75 
242.6 
112.25 
34.45 
139.85 
(a) Right skewed binary tree 
PASCAL 
FORTRAN 
n 
IN 
INl 
IN2 
IN3 
INl 
IN2 
31 
l.15 
1.3 
2.1 
0.9 
0.75 
1.15 
63 
2.15 
2.5 
4.15 
1.8 
1.6 
2.25 
127 
4.35 
5.1 
8.4 
3.65 
2.85 
4.85 
255 
8.8 
10.15 
16.6 
7.35 
6.3 
9.5 
511 
17.85 
20.6 
33.35 
13.85 
12.05 
19.2 
1023 
34.95 
41.2 
66.3 
28.15 
23.95 
38.1 
2047 
69.5 
82.15 
133.65 
56.95 
48.2 
75.5 
4095 
139.3 
162.4 
204.6 
111.9 
%.1 
152.1 
(b) Left skewed binary tree 
PASCAL 
FORTRAN 
n 
IN 
INl 
IN2 
IN3 
INl 
IN2 
31 
1.05 
0.95 
2.5 
0.9 
0.45 
1.35 
63 
2.2 
1.85 
5.0 
1.85 
1.0 
2.85 
127 
4.3 
4.0 
10.05 
3.7 
2.05 
5.65 
255 
8.8 
7.85 
20.1 
6.9 
3.95 
10.65 
511 
17.15 
15.7 
41.1 
13.85 
7.95 
21.6 
1023 
34.7 
30.3 
80.8 
27.35 
16.35 
44.3 
2047 
70.25 
61.55 
162. 75 
55.4 
32.55 
89.45 
4095 
139.8 
122.35 
327.2 
112.15 
65.25 
175.1 
(c) Full binary tree 
Table 6.1 
Computing times for IN, INl, IN2 and IN3. All times are in milliseconds. 
(Tables prepared by N. R. Venkatesh) 

• 
• 
• 
• 
262 
Basic Search and Travenal Techniques 
the best inorder algorithm (provided enough space is available). The 
FORTRAN versions of INl and IN2 took considerably less time than the 
corresponding PASCAL programs. 
6.1.2 TREE TRAVERSAL 
For trees we can define traversal methods analogous to the ones defined 
for binary trees. While the subtrees of a tree are not ordered, our traversal 
methods will assume that some ordering exists for the subtrees. This makes 
it meaningful to talk of the first, second, third subtrees, etc., of a node. 
Since a tree is just a forest with one tree and the removal of the root from 
a tree creates a forest, it is convenient to define tree traversals recursively 
in terms of forest traversal. The names of the traversal methods for trees 
have been chosen so that they correspond to those for binary trees. F is a 
forest. The traversal methods are: 
Tree Preorder (F) 
(i) 
if Fis empty then return; 
(ii) visit the root of the first tree of F; 
(iii) traverse the subtrees of the first tree of F in tree preorder; 
(iv) traverse the remaining trees of Fin tree preorder. 
Tree lnorder (F) 
(i) 
if F is empty then return; 
(ii) traverse the subtrees of the first tree of Fin tree inorder; 
(iii) visit the root of the first tree of F; 
(iv) traverse the remaining trees of Fin tree inorder. 
and Tree Postorder (F) 
(i) 
if F is empty then return; 
(ii) traverse the subtrees of the first tree of F in tree postorder; 
(iii) traverse the remaining trees of F in tree postorder; 
(iv) 
visit the root of the first tree of F. 
Since trees are usually represented by their corresponding (or associated) 
binary trees, we shall not attempt to write detailed traversal algorithms for 
trees. In later sections, we shall see examples of the use of postorder tra-
versal of a tree. In these examples however, the tree will be generated as 
needed. The whole tree being traversed will not reside in memory at any 
one given time. This situation is typical of most tree applications in which 
the corresponding binary tree is not used (see Chapters 7 and 8) . 

The Techniques 
263 
In chapter 2, we defined the corresponding binary tree T of a forest F. 
Preorder and inorder traversals of the corresponding binary tree T of a 
forest F have a natural correspondence with traversals on F. Preorder tra-
versal of T is equivalent to visiting the nodes of F in tree preorder. Inorder 
traversal of T is equivalent to visiting the nodes of F in tree inorder. There 
is no natural analog for postorder traversal of the corresponding binary 
tree of a forest. 
6.1.3 SEARCH AND TRAVERSAL TECHNIQUES FOR GRAPHS 
A fundamental problem concerning graphs is the path problem. In its 
simplest form it requires us to determine whether or not there exists a path 
in the given graph G = (V, E) such that this path starts at vertex v and 
ends at u. A more general form would be to determine for a given starting 
vertex v E V all vertices u such that there is a path from v to u. This latter 
problem can be solved by starting at vertex v and systematically searching 
the graph G for vertices that can be reached from v. We shall describe two 
search methods for this. 
Breadth First Search and Traversal 
In breadth first search we start at a vertex v and mark it as having been 
reached (visited). The vertex v will at this time be said to be unexplored. 
A vertex will be said to have been explored by an algorithm when the algo-
rithm has visited all vertices adjacent from it. All unvisited vertices adjacent 
from v are visited next. These are new unexplored vertices. Vertex v has 
now been explored. The newly visited vertices haven't been explored and 
are put onto the end of a list of unexplored vertices. The first vertex on 
this list is the next to be explored. Exploration continues until no unexplored 
vertex is left. The list of unexplored vertices operates as a queue and may 
be represented using any of the standard queue representations. Procedure 
BFS (Algorithm 6. 7) describes the details of the search. It makes use of 
two algorithms DELETEQ(v, Q) which deletes a vertex from the queue Q 
and returns, in v, the index and the vertex deleted and ADDQ(v, Q) which 
adds vertex v to the rear of queue Q. 
Let us try out the algorithm on the undirected graph of Figure 6.6(a). If 
the graph is represented by its adjacency lists as in Figure 6.6(b) then the 
vertices get visited in the order 1, 2, 3, 4, 5, 6, 7, 8. A breadth first search 
of the directed graph of Figure 6.6(c) starting at vertex 1 will result in only 
the vertices 1, 2 and 3 being visited. Vertex 4 cannot be reached from 1. 

• 
• 
• 
• 
• 
Buie Search and Travenal Techniques 
line procedureBFS(v) 
11 A breadth first search of G is carried out beginning at vertex v.I I 
I I All vertices visited are marked as VISITED(i) = 1. The graph/ I 
//G and array VISITED are global and VISITED is initialized to// 
I !zero.I I 
1 
VISITED(v) -
1; u -
v 
2 
initialize Q to be an empty queue I IQ is a queue of unex-// 
//plored vertices/ I 
3 
4 
5 
6 
loop 
for all vertices w adjacent from u do 
if VISITED(w) = 0 then callADDQ(w, Q) 
//plored// 
VISITED(w) -
1 
7 
endif 
8 
repeat 
I lw is unex-/ I 
9 
if Q is empty then return endif I /no unexplored vertex/ I 
10 
call DELETEQ(u, Q) I I get first unexplored vertex/ I 
11 
repeat 
12 endBFS 
Algorithm 6. 7 Algorithm for breadth first search 
Theorem 6.2 Algorithm BFS visits all vertices reachable from v. 
Proof: 
Let G = (V, E) be a graph (directed or undirected) and let v E V. 
We shall prove the theorem by induction on the length of the shortest paths 
from v to all reachable vertices w E V. The length (i.e. number of edges) 
of the shortest path from v to a reachable vertex w will be denoted by 
d(v, w). Clearly, all vertices w with d(v, w) :::;; 1 get visited. Now assume 
that all vertices w with d(v, w) :::;; r get visited. We will show that all vertices 
w with d( v, w) = r + 1 also get visited. Let w be a vertex in V such that 
d(v, w) = r + 1. Let u be a vertex that immediately precedes w on a 
shortest v tow path. Then d(v, u) = r and so u gets visited by BFS. We 
may assume u -:;e v and r 2!:: 1. Hence, immediately before u gets visited, 
it is placed on the queue Q of unexplored vertices. The algorithm doesn't 
terminate until Q becomes empty. Hence, u is removed from Q at some 
time and all unvisited vertices adjacent from it get visited in the loop 
of lines 4-8. Hence, w gets visited. 
D 

vertex 
I 
2 
3 
4 
5 
6 
7 
8 
The Techniques 
265 
(al Undirected graph G 
(cl Directed Graph 
(bl Adjacency list for G 
Figure 6.6 Example graphs and adjacency lists 
Theorem 6.3 Let t(n, e) and s(n, e) be the maximum time and maximum 
additional space taken by algorithm BFS on any graph G with n vertices 
and e edges. t(n, e) = 9(n + e) and s(n, e) = e(n) if G is represented by 
its adjacency lists. If G is represented by its adjacency matrix then t(n, e) = 
e(n 2) and s(n, e) = e(n). 
Proof: 
Vertices get added to the queue only in line 5. A vertex w can get 
onto the queue only ifVISITED(w) = 0. Immediately following w's addition 
to the queue VISITED(w) is set to 1 (line 6). Hence, each vertex can get 
onto the queue at most once. Vertex v never gets onto the queue and so 
at most n -
1 additions are made. The queue space needed is at most 
n -
1. The remaining variables take 0(1) space. Hence s(n, e) = O(n). If 
G is an n vertex graph with v connected to the remaining n -
1 vertices 
then all n -
1 vertices adjacent from v will be on the queue at the same 
time. Furthermore, 9(n) space is needed for the array VISITED. Hence 
s(n, e) = e(n). This result is independent of whether adjacency matrices 
or lists are used. 

• 
• 
• 
• 
266 
Buie Search and Travenal Techniques 
If adjacency lists are used then all vertices adjacent from u can be deter-
mined in timed (u) where d (u) is the degree of u if G is undirected and d (u) 
is the out-degree of u if G is directed. Hence, when vertex u is being ex-
plored, the time for the loop of lines 4-8 is 8(d(u)). Since each vertex in G 
can be explored at most once, the total time for the loop of lines 3-11 is at 
most O(E d(u)) = O(e). VISITED(i) has to be initialized to 0, 1 :::;; i :::;; n. 
This takes O(n) time. The total time is therefore O(n + e). If adjacency 
matrices are used then it takes 8(n) time to determine all vertices adjacent 
from u and the time becomes O(n 2). If G is a graph such that all vertices 
are reachable from v then all vertices get explored and the time is at least 
O(n + e) and O(n 2) respectively. Hence, t(n, e) = 8(n + e) when adjacency 
lists are used and t(n, e) = 8(n 2) when adjacency matrices are used. 
D 
If BFS is used on a connected undirected graph G then all vertices in G 
get visited and the graph is traversed. However, if G is not connected then 
at least one vertex of G is not visited. A complete traversal of the graph 
can be made by repeatedly calling BFS each time with a new unvisited 
starting vertex. The resulting traversal algorithm is known as breadth 
first traversal (BFT) (see Algorithm 6.8). The proof of Theorem 6.3 can be 
used for BFT too to show that the time and additional space required by 
BFT on an n vertex e edge graph are 8(n + e) and 8(n) respectively if 
adjacency lists are used. If adjacency matrices are used then the bounds 
are 8(n 2) and 8(n) respectively. 
procedure BFT( G, n) 
I /breadth first traversal of GI I 
declare VISITED(n) 
for i -
1 to n do I I mark all vertices unvisited/ I 
VISITED(i) -
0 
repeat 
for i -
1 ton do I /repeatedly call BFS/ I 
if VISITED(i) = 0 then call BFS(i) endif 
repeat 
endBFT 
Algorithm 6.8 Breadth first graph traversal 
If G is a connected undirected graph then all vertices of G will get visited 
on the first call to BFS. If G is not connected then at least two calls to BFS 
will be needed. Hence, BFS can be used to determine whether or not G is 
connected. Furthermore, all newly visited vertices on a call to BFS from 

The Techniques 
267 
BFT represent the vertices in a connected component of G. Hence the con-
nected components of a graph can be obtained using BFT. For this, BFS 
can be modified so that all newly visited vertices are put onto a list. Then 
the subgraph formed by the vertices on this list together with their adjacency 
lists form a connected component. Hence, if adjacency lists are used, a 
breadth first traversal will obtain the connected components in 8(n + e) 
time. BFT can also be used to obtain the reflexive transitive closure matrix 
of an undirected graph G. If A* is this matrix then A *(i ,j) = 1 iff either 
i = j or i -:;t. j and i and j are in the same connected component. We can 
set up in O(n) time an array CONNEC{i) such that CONNEC(i) is the index 
of the connected component containing vertex i, 1 :s; i :s; n. Hence, we can 
determine whether A *(i,j), i -;t j is 1 or 0 by simply seeing if CONNEC(i) 
= CONNEC{j). The reflexive transitive closure matrix of an undirected 
graph G with n vertices and e edges can therefore be computed in 8(n 2) 
time and 8(n) space using either adjacency lists or matrices (the space 
count does not include the space needed for A* itself). 
As a final application of breadth first search, consider the problem of 
obtaining a spanning tree for an undirected graph G. G has a spanning 
tree iff G is connected. Hence, BFS easily determines the existence of a 
spanning tree. Furthermore, consider the set of edges (u, w) used in lines 
4-8 of algorithm BFS to reach unvisited vertices w. These edges are called 
forward edges. Let T denote this set of forward edges. We claim that if G is 
connected then Tis a spanning tree of G. For the graph of Figure 6.6(a) 
the set of edges Twill be all edges in G except (5, 8), (6, 8) and (7, 8) 
(see Figure 6. 7(a)). Spanning trees obtained using breadth first searches 
are called breadth first spanning trees. 
Figure 6.7 BFS and DFS spanning trees for graph of Figure 6.6(a) 
Theorem 6.4 Modify algorithm BFS by adding on the statements T -
</> 
and T -
T U { (u, w)} to lines 1 and 6 respectively. Call the resulting 
algorithm BFS*. If BFS* is called such that v is any vertex in a connected 

• 
• 
• 
• 
268 
Basic Search and Travenal Techniques 
undirected graph G, then on termination, the edges in T form a spanning 
tree ofG. 
Proof: 
We have already seen that if G is a connected graph on n vertices 
then all n vertices will get visited. Also, each of these, except the start 
vertex v, will get onto the queue once (line 5). Hence, Twill contain exactly 
n -
1 edges. All of these edges are distinct. The n -
1 edges in T will 
therefore define an undirected graph on n vertices. This graph is connected 
since it contains a path from the start vertex v to every other vertex (and 
so there is a path between every pair of vertices). A simple proof by induc-
tion shows that every connected graph on n vertices with exactly n -
1 
edges is a tree. Hence T is a spanning tree of G. 
D 
The exercises explore further applications of breadth first search. An 
important technique to solve optimization problems is based on breadth 
first search. This technique, called branch-and-bound, is the subject of 
Chapter 8 . 
Depth First Search and Traversal 
A depth first search of a graph differs from a breadth first search in that 
the exploration of a vertex v is suspended as soon as a new vertex is reached. 
At this time the exploration of the new vertex u begins. When this new 
vertex has been explored, we continue to explore v. The search terminates 
when all reached vertices have been fully explored. This search process is 
best described recursively as in Algorithm 6.9. 
line procedure DFS( v) 
1 
2 
3 
I /Given an undirected (directed) graph G = (V, E) with n ver-/ I 
I ltices and an array VISITED(n) initially set to zero, this algo-/ I 
I /rithm visits all vertices reachable from v. G and VISITED are/ I 
I I global.I I 
VISITED(v) -
1 
for each vertex w adjacent from v do 
if VISJTED(w) = 0 then call DFS(w) endif 
4 
repeat 
5 endDFS 
Algorithm 6.9 Depth first search of a graph 

The Techniques 
269 
A depth first search of the graph of Figure 6.6(a) starting at vertex 1 and 
using the adjacency lists of Figure 6.6(b) results in the vertices being visited 
in the order 1, 2, 4, 8, 5, 6, 3, 7. A nonrecursive algorithm for DPS would 
use a stack to keep track of all partially explored vertices. One can easily 
prove that DFS visits all vertices reachable from vertex v. If t(n, e) and 
s(n, e) represent the maximum lime and maximum additional space taken 
by DFS for an n vertex e edge graph then s(n, e) = 9(n) and t(n, e) = 
9(n + e) if adjacency lists are used and t(n, e) = e(n 2) if adjacency mat-
rices are used (see exercises). 
A depth first traversal of a graph is carried out by repeatedly calling 
DFS each time with a new unvisited starting vertex. The algorithm for this 
(DFT) differs from BFT only in that the call to BFS(i) is replaced by a 
call to DFS(i). As in the case of BFT, the connected components of a 
graph can be obtained using DFT. Similarly, the reflexive transitive closure 
matrix of an undirected graph can be found using DFT. If DFS is modified 
by adding T -
</> and T -
T U { ( v, w)} to lines 1 and the then clause of 
line 3 respectively then, when DFS terminates, the edges in T define a 
spanning tree for the undirected graph G if G is connected. A spanning 
tree obtained in this manner is called a depth first spanning tree. For the 
graph of Figure 6.6(a) the spanning tree obtained will include all edges in 
G except for (2, 5), (8, 7) and (1, 3) (see Figure 6. 7(b)). Hence, DFS and 
BPS are equally powerful for the search problems discussed so far. The 
exercises contain some problems that are solved best by BFS and others 
that are best solved by DFS. Later sections of this chapter also discuss 
graph problems solved best by DFS. 
Epilogue 
BFS and DFS are two fundamentally different search methods. In BFS a 
node is fully explored before the exploration of any other node begins. The 
next node to explore is the first unexplored node remaining. The exercises 
examine a search technique (D-search) that differs from BFS only in that 
the next node to explore is the most recently reached unexplored node. In 
DFS the exploration of a node is suspended as soon as a new unexplored 
node is reached. The exploration of this new node is immediately begun. 
While the implementation of both DFS and D-Search requires a stack 
mechanism, the two search methods are different. The search methods 
presented in this section may be used on a variety of problems. Some ap-
plications are explored in the remaining sections of this chapter. 

• 
• 
• 
• 
• 
270 
Basic Search and Travenal Techniques 
6.2 CODE OPTIMIZATION 
The function of a compiler is to translate programs written in some source 
language into an equivalent assembly language or machine language pro-
gram. Thus, the PASCAL compiler on the CDC Cyber 74 translates 
PASCAL programs into the machine language of this machine. We shall 
look at the problem of translating arithmetic expressions in a language 
such as PASCAL into assembly language code. The translation will clearly 
depend on the particular assembly language (and hence machine) being 
used. To begin, we will assume a very simple machine model. We shall call 
this model machine A. This machine has only one register called the accu-
mulator. All arithmetic has to be performed in this register. If 8 repre-
sents a binary operator such as + , - , *, I then the left operand of 8 
must be in the accumulator. For simplicity, we shall restrict ourselves to 
these four operators. The discussion will easily generalize to other operators. 
The relevent assembly language instructions are: 
LOAD X ... load accumulator with contents of memory location X 
STORE X . . . store contents of accumulator into memory location X 
OP X ... OP may be ADD, SUB, MPY or DIV 
The instruction OP X computes the operator OP using the contents of 
the accumulator as the left operand and that of memory location X as the 
right operand. As an example consider the arithmetic expression: (a + b )I 
(c + d). Two possible assembly language versions of this expression are 
given in Figure 6.8. Tl and n are temporary storage areas in memory. In 
both cases the result is left in the accumulator. Code (a) is two instructions 
longer than code (b). If each instruction takes the same amount of time 
then code (b) will take 25% less time than code(a). For the expression 
(a + b )I (c + d) and the given machine A, it is not too difficult to see that 
code(b) is optimal. 
LOAD 
a 
LOAD 
c 
ADD 
b 
ADD 
d 
STORE 
Tl 
STORE 
Tl 
LOAD 
c 
LOAD 
a 
ADD 
d 
ADD 
b 
STORE 
n 
DIV 
Tl 
LOAD 
Tl 
DIV 
n 
(a) 
(b) 
Figure 6.8 Two possible codes for (a + b )l(c + d) 

Code Optimization 
271 
Definition A translation of an expression E into the machine or assembly 
language of a given machine is optimal iff it has a minimum number of 
instructions. 
Let us look at three more examples. Consider the expression a + b *C. 
Figure 6.9 shows two possible translations. At first sight, code(b) may 
appear incorrect since we require the left operand of + to be in the accu-
mulator and the right operand in memory. However, x + y = y + x and 
so (b) is equivalent to (a). 
LOAD 
b 
LOAD 
b 
MPY 
c 
MPY 
c 
STORE 
Tl 
ADD 
a 
LOAD 
a 
ADD 
Tl 
(a) 
(b) 
Figure 6.9 Possible codes for a + b *C 
Definition a binary operator O is commutative in the domain D iff 
a 0 b = b 0 a for all a and b in D. 
The operators + and * are commutative over the integers and reals while 
-
and I are not. Using the commutative property of certain operators 
can result in shorter code. Next, consider the expression a*b + c*b. Figure 
6.10 shows two possible codes. Code(b) actually computes (a + c )b which 
isequivalenttoa*b + c*b. 
LOAD 
c 
LOAD 
a 
MPY 
b 
ADD 
c 
STORE 
Tl 
MPY 
b 
LOAD 
a 
MPY 
b 
ADD 
Tl 
(a) 
(b) 
Figure 6.10 Possible codes for a*b + c*b 
Definition A binary operator (!) is left distributive with respect to the 
binary operator Gover a domain D iff for every a, b, c in D, a G (b G 

• 
• 
• 
• 
• 
272 
BulcSearchudTravenalT~hnlques 
e) = (a 8 b) G (a 8 e). 8 is right distributive with respect to Giff for 
every a, b, e in D, (a G b) (Ve = (a (Ve) G (b ~e). 
Over the domain of real numbers, * is left and right distributive with 
respectto +and - asa*(b + e) = (a*b) + (a*e),a*(b - e) = (a*b) -
(a*e),(a + b)*e = (a*e) + (b*e)and(a - b)*e = (a*e) - (b*e). /is 
not left distributive with respect to + as al(b + e) -:;t. (alb) + (ale). How-
ever, I is right distributive over the reals. Note that I is not right distribu-
tive with respect to + over the domain of integers as (2 + 3)/5 = 1 while 
(2/5) + (3/5) = 0 (note that 2/3 = 0 and 3/5 = 0 in integer arithmetic). 
As a final example, consider the expression a*(b*C) + d*e. Figure 6.11 
presents two possible codes. The code of Figure 6.ll(b) uses the knowledge 
(a*b)*e = a*(b*e). 
LOAD 
b 
LOAD 
a 
MPY 
e 
MPY 
b 
STORE 
Tl 
ADD 
d 
LOAD 
a 
MPY 
e 
MPY 
Tl 
STORE 
Tl 
LOAD 
d 
MPY 
e 
STORE 
n 
LOAD 
Tl 
ADD 
n 
(a) 
(b) 
Figure 6.11 Two possible codes for a •(b •c) + d •c 
Definition A binary operator O is associative over the domain D iff 
a 0 (b 0 e) = (a Ob) O e for all a, band e in D. 
* is associative over the integers and reals but I is not . 
Using the associative, distributive and commutative properties of operators 
can result in shorter codes. Note, however, that even though (a + e)*b = 
(a *b) + (c *h> for real numbers, the codes of Figures 6.lO(a) and (b) may 
generate different answers. This comes about because of the finiteness of 
computer arithmetic that creates errors in computation. In our discussion, 
we shall ignore this factor and assume that the associative, commutative 
and distributive laws may be freely used when applicable . 

Code Optlml:zatlon 
273 
Having seen that different codes are possible for a given expression, we 
address ourselves to the problem of obtaining optimal code. Initially, we 
shall restrict ourselves to the simple machine A. Later, we shall look at a 
more general machine model. The form in which we have seen expressions 
up to now is known as infix form. The operators appear in between their 
operands. This is the way we normally write arithmetic expressions. In 
generating optimal code, it is convenient to represent arithmetic expressions 
as binary trees. Each nonleaf node in the binary tree will represent an 
operator. A nonleaf node will be called an internal node. The left subtree 
of an internal node P will represent the binary tree form of the left operand 
of the operator represented at P while the right subtree will represent the 
right operand. A leaf node represents either a variable or a constant. Figure 
6.12 shows the binary tree forms for several expressions. The exercises 
develop an algorithm to obtain the binary tree representation of an arith-
metic expression presented in infix form. We shall refer to a binary tree 
representing an arithmetic expression as an expression tree. 
In obtaining an algorithm to generate optimal code from an expression 
tree, we shall first assume that none of the operators are either commutative, 
distributive or associative. In addition, we shall not concern ourselves with 
the possibility of using algebraic transformations to simplify the expression. 
Thus, while a + b -
a -
b has value zero, under the above assumption 
the optimal code will be LOAD a; ADD b; SUB a; SUB b. We shall also 
not be concerned with handling common subexpressions. All subexpres-
sions will be assumed independent. Hence, the optimal code for a *b * 
(a*b - d)isthesameasthatfora*b*(c*e - d). Undertheseassumptions, 
it is easy to see that if an expression has n operators then its code will have 
exactly n instructions of the type ADD, SUB, MPY, DIV. Instructions 
of this type will be called operator instructions. Only the number of accu-
mulator loads and stores will vary. Thus, the codes of Figures 6.8(a) and (b) 
both have three operator instructions. Code(a) has three loads and two 
stores while code(b) has only two loads and one store. One may readily 
verify that in any code that has no redundant statements, each load in-
struction except the first must be preceded immediately by a store instruc-
tion. Hence, the number of loads is always one more than the number of 
stores. Consequently, it is sufficient to generate code that minimizes either 
the number of loads or the number of stores. 
Let P be an internal node of any expression tree. Let L and R be its left 
and right subtrees respectively. Let O be the operator at node P. Because 
of the assumptions on operators, the only way to compute L 0 R is to com-
pute L and R independently and then compute L 0 R. The codes for L 
and R must also be optimal. Once we have optimal codes for L and R, 

• 
• 
• 
• 
274 
Basic Search and Travenal Techniques 
~ 
a+b 
(i) 
~ 
c 
al( blc) 
(iv) 
a+( b+(c+d )) 
(vii) 
~ 
c k 
b 
a+b*C 
(a+b)*C 
(ii) 
(iii) 
~ M 
* 
d 
b 
(alb) le 
(a+b)l(C*d) 
(v) 
(vi) 
al(b+c)+d*(e+f) 
(viii) 
Figure 6.12 Binary tree forms for some infix expressions 
several possibilities exist for the code for L O R. Let CL and CR repre-
sent optimal codes for the expression trees L and R respectively. Table 6.2 
shows the different possibilities for the code for L OR. The "condition" 
column exhausts all possibilities for L and R and the order in which L 
and R are computed in case they are not leaves. In writing out the code, 
we have simply put 0 a to represent an operator instruction. If 0 is + 
then we mean ADD a. This change is easy to make. An examination of 
Table 6.2 reveals that in generating the code for L O R, we have a choice 
only when both L and R are internal nodes. When either is a leaf then 
(conditions (i), (ii) and (iii)) the code is unique (barring introduction of 
nonuseful statements). When L and R are internal nodes, the code for 
condition (v) is smaller than that for (iv) and so should be used. This leads 
to the observation that if R is an internal node then in the optimal code 
CR precedes CL; otherwise CL precedes CR· 

i) 
ii) 
iii) 
iv) 
v) 
condition 
both L and R are leaves; variables are a 
and b respectively 
L is a leaf with variable a; R is not a leaf 
R is a leaf with variable a; L is not a leaf 
neither L nor R are leaves. L is computed 
before R 
neither L nor R are leaves. R is computed 
before L 
Code Optimization 
275 
corresponding code 
LOAD a; ()b 
CR; STORE Tl; LOAD a; Q Tl 
CL; Qa 
CL; STORE Tl; CR; STORE n; 
LOADTl; QT2 
CR; STORE Tl; CL; 0 Tl 
Table 6.2 Possibilities for evaluating L QR 
Note the similarity between the preceding discussion and one resulting 
from either divide-and-conquer or dynamic programming. Using divide-
and-conquer we would obtain optimal code for L and R and then combine 
these optimal codes in some way to obtain optimal code for L OR. For 
dynamic programming, we could view code as the result of a sequence of 
decisions. At each step, a decision is made as to which subexpression is 
to be coded next. A subexpression L OR may be coded next only if the 
codes for L and R have already been generated. It is easy to verify that the 
principle of optimality holds. 
Table 6.2 leads to the recursive code generation procedure CODEl 
(Algorithm 6.11). The algorithm uses procedures TEMP(i) and RETEMP(i). 
TEMP(i) gets a memory space for temporary storage while RETEMP(i) 
frees the temporary storage location i. It is assumed that the expression 
tree has a root node pointed at by T and that each node has three fields 
LCHILD, RCHILD and DATA. The DATA field for an internal node is 
an operator. For a leaf this field is an operand address. Furthermore, the 
algorithm assumes that T -:;t. 0. Note that the algorithm essentially carries 
out a traversal of the binary tree T. The traversal method however is not 
any of the three methods discussed in Section 6.1.1. Only internal nodes 
get visited. When a node gets visited, code for that node is generated. 
A node is visited only after the code for its two subtrees has been generated. 
This is similar to postorder traversal. However, in algorithm CODEl, a 
nontrivial right subtree is traversed before the corresponding left subtree 
(a trivial subtree is one with only a root node). If temporary storage is 
handled as a stack with TEMP and RETEMP respectively corresponding 
to deletion from and addition to the stack then Figure 6.13 shows the codes 
generated by CODEl for some of the examples of Figure 6.12. From our 
earlier discussion it follows that the code generated by CODEl is optimal 
for machine A. A more rigorous proof will be given when we study a 
generalization of machine A. 

• 
• 
• 
• 
Z76 
Basic Search and Travenal Techniques 
procedure CODEl (T) 
I I code generation for tree T. Assume T -;e 01 I 
if Tis a leaf then print ("LOAD", DATA (T)) 
return 
endif 
F -
0 /IF is set to 1 if RCHILD(T) is not a leaf// 
if RCHILD(T) is not a leaf then 
call CODEl (RCHILD(T)) //generate CR! I 
call TEMP(i) 
print ("STORE", i) 
F-l 
endif 
call CODEl (LCHILD(T)) I I generate Cd I 
if F = 1 then print (DATA(T), i) 
call RETEMP(i) 
else print (DATA(T), DATA(RCHILD(T))) 
endif 
endCODEl 
LOAD 
ADD 
(i) 
LOAD 
ADD 
STORE 
LOAD 
ADD 
STORE 
LOAD 
ADD 
(iv) 
a 
b 
c 
d 
Tl 
b 
Tl 
Tl 
a 
Tl 
Algorithm 6.10 Algorithm to generate code 
LOAD 
b 
MPY 
c 
STORE 
Tl 
LOAD 
a 
ADD 
Tl 
(ii) 
LOAD 
e 
ADD 
f 
STORE 
Tl 
LOAD 
d 
MPY 
Tl 
STORE 
Tl 
LOAD 
b 
ADD 
c 
STORE 
n 
LOAD 
a 
DIV 
n 
ADD 
Tl 
(v) 
LOAD 
a 
ADD 
b 
MPY 
c 
(iii) 
Figure 6.13 Code generated by CODEl for some of the examples of Figure 6.12 
-------
-
----~. -----

Code Optimization 
277 
Theorem 6.5 The code generated by CODEl correctly evaluates the arith-
metic expression represented by the expression tree T. 
Proof: 
The proof is a simple induction on the depth of T and is left as 
and exercise. 
D 
If we are allowed to use the commutative property of operators then, 
CODEl does not generate optimal code for machine A. To see this, see 
example (ii) of Figure 6.12 and 6.13. The optimal code when + is com-
mutative is LOAD b, MPY c, ADD a. Note that again, all nonredundant 
codes for a given expression will have the same number of operator instruc-
tions and that the number of loads will be one more than the number of 
stores. Let P be an internal node in an expression tree. Let L, R, CL and CR 
be as before. It is clear that the optimal code for LOR will be made up of 
optimal codes for L and R. However, ifQ is commutative then the possi-
bilities for Table 6.2 increase. The modificiations needed to CODEl so 
that it will generate optimal code taking into account commutative operators 
is left as an exercise. 
We now generalize the machine A to another machine B. B has N 2:: 1 
registers in which arithmetic can be performed. There are four types of 
machine instructions for B: 
1. 
2. 
3. 
4. 
LOAD 
STORE 
OP 
OP 
M,R 
M,R 
Rl, M, R2 
Rl, R2, R3 
These four instruction types perform the following functions: 
1) LOAD M, R places the contents of memory location Minto register R, 
lsRsN 
2) 
STORE M, R stores the contents of register R, 1 s R ~ N, into 
memory location M. 
3) 
OP Rl, M, R2 computes contents(Rl) OP contents(M) and places 
the result in register R 2. OP is any binary operator (e.g., + , - , *· 
/), Rl and R2 are registers, Mis a memory location. Rl may equal R2. 
4) 
OP Rl, R2, R3 is similar to (3). Rl, R2 and R3 are registers. Some or 
all of these registers may be the same. 
In comparing the two machine models A and B, we note that when N = 
1, instructions of types (1), (2) and (3) for model B are the same as the 

• 
• 
• 
• 
278 
Basic Search and Travenal Techniques 
corresponding instructions for model A. Instructions of type (4) only allow 
trivial operations like a + a, a - a, a *a and al a to be performed without 
an additional memory access. This does not change the number of instruc-
tions in the optimal codes for A and B when N = 1. Hence, model A is in 
a sense identical to model B when N = 1. For model B, we see that the 
optimal code for a given expression E may be different for different values 
of N. Figure 6.14 shows the optimal code for expression (vi) of Figure 6.12. 
Two cases are considered, N = 1 and N = 2. Note that when N = 1, one 
store has to be made while when N = 2 no stores are needed. The registers 
are labeled Rl and R2. Tl is a temporary storage location in memory. 
Further note that the number of LOADs need no longer be exactly one 
more than the number of STOREs. Thus, it is no longer sufficient to 
optimize only either the number of LOADs or the number of STOREs. 
Their sum is to be minimized. To simplify the discussion, we begin by 
assuming that none of the operators are associative, commutative or dis-
tributive. Further, we assume that both the left and right operands of an 
operator have to be independently computed even if they are the same 
subexpressions. This restriction is extended to the case of expressions such 
as a OPa and we require that a reference to memory be made for both 
the left and right operands . 
LOAD 
MPY 
STORE 
LOAD 
ADD 
DIV 
c, Rl 
Rl, d, Rl 
Rl, Tl 
a, Rl 
Rl, b, Rl 
Rl, Tl, Rl 
(i) N = 1 
LOAD 
MPY 
LOAD 
ADD 
DIV 
c, Rl 
Rl, d, Rl 
a,R2 
R2, b,R2 
R2, Rl, Rl 
(ii) N = 2 
Figure 6.14 Optimal codes for N = 1 and N = 2 
Given an expression E, the first question we may ask is: can Ebe eval-
uated without any STOREs? A closely related question is: what is the 
minimum number of registers needed to evaluate E without any stores? We 
answer these questions under the assumptions made above. We shall 
assume that the value of E is to be left in one of the N registers. Let E be 
represented by an expression tree T. If T has only one node then this node 
must be a leaf and clearly all that has to be done is load the value of the 
corresponding variable or constant into a register. Only one register is 
needed for this. If expression E has only one operator then it is of the form 

Code Optimization 
279 
a 0 b. One register (Rl) is needed to load a into. Then we can use the 
instruction ORI, b, Rl. Hence, for this case exactly one register is needed 
(Figure 6.lS(ii)). When more than one operator is present then we have the 
situation of Figure 6. lS(iii). Let I, and/ 2 respectively be the minimum num-
ber of registers needed to independently evaluate the left (L) and right (R) 
operands of the root operator. Let I be the minimum number of registers 
needed to compute L OR. Since, under the assumptions we have made 
it is necessary that the values of both L and R be computed independently, 
it follows that I 2!:: max{/,, / 2}. If l 1 > / 2 then we can compute L first using 
11 registers. Then, leaving the register containing the value L untouched, 
we can compute R using the remaining 11 -
1 2!:: / 2 registers. Finally, with 
an instruction of type ( 4) we can compute L O R. Hence, when l 1 > / 2, 
I = !,. Similarly, when /1 < /2, I = /2. So, when /1 -;e /2 then I = max{/i. 
Ii}. When l 1 = 12, we have two cases. First, if R is a leaf then I = 11 as we 
just compute L using l 1 registers and then use an instruction of type (3) to 
compute L O R placing the answer in one of the I, registers. If R is not 
a leaf then I = 11 + 1 as no matter which of L and R is computed first, 
one register will have to be set aside to hold the value of the operand 
computed first and another l 1 registers needed to compute the second 
operand. The preceding discussion leads to the following theorem. 
Theorem 6.6 Let P be a node in an expression tree T of depth at least 2. 
Define the function MR(P) (Minimum Registers) as follows: 
MR(P) = 
0 
if P is a leaf and the right child of its parent 
1 
if P is a leaf and the left child of its parent 
max{/i. Ii} where Ii= MR(LCHILD(P));/ 2 = 
MR(RCHILD(P)) and l 1 -;e l 2 
if I" l 2 as above and I, = / 2 
MR(P) for P an internal node is the minimum number of registers needed 
to compute the expression subtree with root P if no STOREs are per-
mitted. 
D 
The above theorem is true only under the stated assumptions about 
operators. For any expression tree T, the MR values of all nodes can be 
computed by a postorder traversal of T. Figure 6.16 gives the MR values 

• 
• 
• 
• 
280 
Buie Search and Travenal Techniques 
tree T 
OQtirnal code 
i) no operators 
@ 
LOAD 
a,RI 
ii) only one 
J2@ 
LOAD a,RI 
operator 
0 
Rl,b,RI 
iii) more than 
~2 
one operator 
Figure 6.15 Figuring out the minimum number of registers 
for all nodes of some expression trees. If the number N of registers avail-
able is greater than or equal to the MR value of the root T of the expression 
tree T then T can be evaluated without any STOREs. In this case an op-
timal code has to only minimize the number of LOADs. Because of the 
assumptions made, the number of instructions of types (2) and (3) is equal 
to the number of internal nodes. When MR(T) > N then the code has to 
contain some STOREs and an optimal code will minimize the total number 
11'@0 ~ N 
I 
0 
a 
b 
.. 
c 
0 
I 
b 
(i) 
(ii) 
(iii) 
c 
(iv) 
(v) 
Numbers above nodes are MR values 
Figure 6.16 MR values for nodes 

Code Optimization 
281 
of type (1) and type (2) instructions. The proof of Theorem 6.6 suggests a 
code generation algorithm (Algorithm 6.11). We shall show that CODE2 
does in fact generate optimal code under the stated assumptions. First, 
let us make sure we understand the algorithm. 
The algorithm assumes that each node in the expression tree T has four 
fields: LCHILD, RCHILD, DATA and MR. The MR values have been 
computed as defined by Theorem 6.6. CODE2 uses a subroutine TEMP. 
This is identical to that for CODEl. To generate the code for an expression 
tree T, CODE2 is called as call CODE2(T, 1). The total number of registers 
N is a global variable. It is assumed that T -;e 0, i.e. the expression is not 
null. On a call to CODE2(T, i), code is generated for the expression T 
using only registers Ri. . .. , RN. The result is left in Ri. If T is a leaf and 
this is the initial call to CODE2 then only a load is to be performed. If T 
is a leaf and this is a recursive call from within lines 6-24 then T must be 
the left child of its parent (as lines 7-9 take care of a right child leaf) and 
again only a load is to be performed. When Tis an internal node, the case 
statement (lines 6-24) is entered. L and R point to the left and right children 
of T respectively. Let O be the operator at R. If R is a leaf then MR (R) = 0 
and under the assumption made earlier, the optimal code for L OR is the 
optimal code for L followed by the operation Q. This is generated in lines 
7-9. When both MR(L) and MR(R) 2::: N then at least one store has to be 
made (Theorem 6.6). The optimal code for L OR is now the optimal code 
for R followed by a store of the result from R, then the optimal code for 
L followed by the operation O (lines 10-15). Note that this case can occur 
only when MR(T) > N on the initial call. Since both the calls from lines 
10 and 13 allow CODE2 to use registers Ri, ... , RN, it follows that i = 1 
whenever MR(L) and MR(R) 2::: N. When at least one of MR(R) and 
MR(L) is less than N the code for T is generated in lines 16-23. Line 16 
takes care of the case MR(L) < MR(R). Since at least one of MR(L) and 
MR(R) is less than N, it follows that MR(L) < N. In this case, the optimal 
code is the optimal code for R using registers Ri, ... , RN followed by the 
optimal code for L using registers R(i + 1), ... , RN followed by the opera-
tion Q. Note that if MR(R) is not less than N then i = 1. Theorem 6. 7 
shows that whenever this section of the code is entered, MR(L) :s N - i. 
Since following the computation of R, N -
i registers are free R(i + 1), 
... , RN), R can be computed with no stores. When MR(L) ;:::: MR(R) 
(lines 20-23) then Theorem 6. 7 shows that MR(R) :s N -
i and so fol-
lowing the code generation for L, R can be computed with no stores using 
registers R(i + 1), ... , RN. Theorem 6.8 and 6.9 prove the correctness 
and optimality of the code generated by algorithm CODE2. If T has n 
nodes, then the time required by CODE2 is 8(n) (see exercises). 

282 
Basic Search and Travenal Techniques 
line procedure CODE2(T, i) 
I I generate code for machine B with N registers using registers/ I 
I I Ri, ... , RN only. Result is left in Ri. N is a global variable/ I 
1 
if T is a leaf then I !left child of parent/ I 
2 
print('LOAD',DATA(T), 'R',i) 
3 
re tum 
4 
endif 
I IT is an internal node/ I 
5 
L - LCHILD(T); R -
RCHILD(T) 
6 
case 
7 
:MR(R) = 0: 
I IR is a leaf// 
8 
call CODE2(L, i) 
9 
print (DATA(T), 'R ', i, ', ', DATA(R), ', R ', i) 
10 
:MR(L) 2:: N and MR(R) 2:: N: call CODE2(R, i) 
11 
call TEMP(S) 
12 
print ( 'STORE', 'R ', i, ', ', S) 
13 
call CODE2(L, i) 
14 
print(DATA(T), 'R',i, ', ',S, 
', R ', i) 
15 
call RETEMP(S) 
16 
:MR(L) < MR(R): 
I IMR(L) < N, evaluate R first// 
17 
call CODE2(R, i) 
18 
call CODE2(L, i + 1) 
19 
print (DATA(T), ', R ', i + 1, ', R ', i, ', R ', i) 
20 
:else: 
I I MR(L) 2:: MR(R) and MR(R) < N, evaluate L first/ I 
21 
call CODE2(L, i) 
22 
call CODE2(R, i + 1) 
23 
print(DATA(T), ', R ', i,', R ', i + 1, ', R ', i) 
24 
endcase 
25 
end CODE2 
Algorithm 6.11 
Code generator for machine B 
Theorem 6.7 The following are true for CODE2: 
(i) i = 1 whenever lines 10-15 are executed 
(ii) MR(L) :s N - i whenever lines 16-19 are executed 
(iii) MR(R) :s N - i whenever lines 20-23 are executed 
(iv) i = 1 whenever MR(T) 2:: N. 

Code Optimization 
283 
Proof: The proof is a simple induction on the depth of recursion and is left 
as an exercise. 
D 
Theorem 6.8 CODE2 generates correct code for every expression tree T. 
Proof: Simple induction on the depth of T. 
0 
Figure 6.17 shows the code generated by CODE2 for some of the expres-
sions of Figure 6.16. Rl, R2 and R3 are registers while Tl is a temporary 
storage location generated by TEMP( ). 
LOAD 
a, Rl 
LOAD 
b,Rl 
LOAD 
a,Rl 
ADD 
Rl, b, Rl 
MPY 
Rl, c, Rl 
LOAD 
b,R2 
STORE 
Rl, Tl 
MPY 
R2, c, R2 
LOAD 
Rl, a 
ADD 
Rl, R2, Rl 
ADD 
Rl, Tl, Rl 
N=l 
N = 1 
N = 2 
(i) 
(ii) (a) 
(ii) (b) 
LOAD 
d,Rl 
LOAD 
a, Rl 
LOAD 
e, R2 
LOAD 
b, R2 
ADD 
R2,J, R2 
ADD 
R2, c, R2 
MPY 
Rl, R2, Rl 
DIV 
Rl, R2, Rl 
STORE 
Rl, Tl 
LOAD 
d, R2 
LOAD 
a, Rl 
LOAD 
e,R3 
LOAD 
b,R2 
ADD 
R3,f, R3 
ADD 
R2, c, R2 
MPY 
R2, R3, R2 
DIV 
Rl, R2, Rl 
ADD 
Rl, R2, Rl 
ADD 
Rl, Tl, Rl 
N = 2 
N = 3 
(v) (a) 
(v) (b) 
Figure 6.17 Code generated by CODE2 for trees (i), (ii) and (v) of Figure 6.16 
We now proceed to show that CODE2 generates optimal code. It is 
necessary to distinguish two types of nodes in an expression tree. 
Definition: Given a number of registers N, a node is major iff both its 
children have an MR value at least N. A node is minor iff it is either a leaf 
with no parent or it is a leaf and the left child of its parent. 

284 
Basic Search and Travenal Techniques 
Lemma 6.1 Let n be the number of major nodes in an expression tree T. 
At least n STOREs are needed to evaluate T when the expression T has no 
commutative operators and when there are no relationships among opera-
tors and operands (this disallows associative and distributive operators as 
well as common subexpressions). 
Proof: Can be proved by induction on the number of nodes in T. 
D 
Lemma 6.2 For any expression tree T, the number of STOREs in the code 
generated by CODE2 is equal to the number of major nodes in an expres-
sion tree T. 
Proof: This follows from the observation that line 12 is the only place in 
CODE2 that a store is generated. Line 12 is executed exactly once for each 
major node in T. 
D 
Lemma 6.3 Let m be the number of minor nodes in T. Under the as-
sumptions of Lemma 6.1, every code to evaluate T must have at least m 
LOAD instructions. 
Proof: Can be proved by induction on the number of minor nodes in any 
expression tree T. 
D 
Lemma 6.4 For any expression tree T, the number of LOAD instructions 
in the code generated by CODE2 is equal to the number of minor nodes in 
T. 
Proof: Line 2 is the only line generating a LOAD. It is visited exactly once 
for each minor node in T. 
D 
Theorem 6.9 Under the conditions of Lemma 6.1, algorithm CODE2 
generates optimal code. 
Proof: Follows from Lemmas 6.1-6.4 and the observation that under the 
given assumptions the number of instructions of types (3) and (4) equals 
the number of internal nodes (or operators) in the expression tree Tin all 
valid codes for T. 
D 
If commutative and associative operators are allowed then several dif-
ferent expression trees may coll}pute the same expression. Figure 6.18(a) 
shows an expression tree equivalent to that of Figure 6.16 (ii) when + is 
commutative. Figure 6.18(b) shows one equivalent to that of Figure 6.16 (iv) 
when + is associative. Note that CODE2 generates codes for (a) and (b) 
using one register only while two registers were needed for the corresponding 
trees of Figure 6.16. Moreover, the codes have fewer instructions when N = 
1. Hence, if the assumption of noncommutativity and nonassociativity of 

Code Optimization 
285 
operators is removed, Theorem 6.9 is no longer true. However, if :J is the 
class of equivalent expression trees corresponding to an expression E that 
has commutative and associative operators then all trees in :J have the same 
number of internal (operator) nodes. This follows from the observation 
that neither commutative nor associative transformations reduce the num-
ber of operators in E. From Lemmas 6.1-6.4 it follows that the optimal 
code for E now corresponds to the code generated by CODE2 using as 
input a tree in :J for which the sum of major and minor nodes is minimum. 
When E has commutative operators but no associative operators then such 
a tree can be easily obtained from any expression tree T for E. Commuta-
tivity only allows one to exchange the left and right operands of a com-
mutative operator. The sum of major and minor nodes is minimized if 
every left child of a commutative operator is an internal node (unless both 
children are leaves). Thus given any tree T we can obtain an optimal tree 
T' for use in CODE2 by simply examining all internal nodes that have 
exactly one leaf child and making this child the right child in case the 
parent operator is commutative (see exercise). The exercises develop algo-
rithms for the case when E has both commutative and associative operators. 
+ 
0 
do 
a 
a 
(a) 
Figure 6.18 Equivalent trees for trees (ii) and (iv) of figure 6.16 
When the assumption that the left and right operands of an operator 
have to be computed independently is dropped, the "expression tree" 
corresponding to an expression becomes a graph (as the expression may 
have common subexpressions). Obtaining optimal code for expressions with 
common subexpressions is computationally very difficult. The problem is 

286 
Basic Search and Travenal Techniques 
NP-Hard (see chapter 11). In fact, even determining MR(E) is NP-Hard. 
This change in complexity from a tree to a graph is typical of many opti-
mization problems. Problems that are efficiently solvable on trees often 
become very hard on graphs. 
6.3 AND/OR GRAPHS 
Many complex problems can be broken down into a series of subproblems 
such that the solution of all or some of these results in the solution of the 
original problem. These subproblems may be broken down further into sub-
subproblems and so on until the only problems remaining are sufficiently 
primitive as to be trivially solvable. This breaking down of a complex 
problem into several subproblems can be represented by a directed graph 
like structure in which nodes represent problems and descendents of a 
node represent the subproblems associated with it. For example the graph 
of Figure 6. l 9(a) represents a problem A that can be solved by either solving 
both the subproblems B and C or the single subproblems Dor E. Groups 
of subproblems that must be solved in order to imply a solution to the 
parent node are joined together by an arc going across the respective edges 
(as the arc across the edges (A, B) and (A, C)). By introducing dummy 
nodes as in Figure 6.19(b) all nodes can be made to be such that their solu-
tion requires either all descendents to be solved or only one descendent to 
be solved. Nodes of the first type are called AND nodes and those of the 
latter type OR nodes. Nodes A and A" of Figure 6.19(b) are OR nodes 
while node A ' is an AND node. AND nodes will be drawn with an arc 
across all edges leaving the node. Nodes with no descendents are termed 
terminal. Terminal nodes represent primitive problems and are marked 
either solvable or not solvable. Solvable terminal nodes will be represented 
by rectangles. As an example, consider the problem of doing one's weekly 
laundry. Figure 6.20 shows a possible AND/OR graph, which is actually 
a tree, for this problem. The original problem is divided into five sub-
problems: collect clothes, wash clothes, dry, iron and fold and stack washed 
clothes. Each of these has to be done in order to complete the task. To 
wash the clothes we may either hand wash or wash by machine. The node 
representing hand wash has no descendents and is not a square node either. 
Hence, hand wash is not possible for this graph. While, to most minds, 
the laundry problem described above will be simple enough that a solution 
can be obtained without constructing the AND/OR tree of Figure 6.20, 
there are many other problems for which this is not the case. Breaking 
down a problem into several subproblems is known as problem reduction. 

And/Or Graphs 
287 
E 
(a) 
Figure 6.19 Graphs representing problems 
collect 
clothe 
do laundry 
get correct 
change 
load and 
start 
Figure 6.20 AND/OR graph corresponding to the laundry problem 
Problem reduction has been used on such problems as theorem proving, 
symbolic integration and analysis of industrial schedules. 
When problem reduction is used, two different problems may generate a 
common subproblem. In this case it may be desirable to have only one 
node representing this subproblem (this would imply that the subproblem 
is to be solved only once). Figure 6.21 shows an AND/OR graph for a case 
where this is done. Note that the graph is no longer a tree. Furthermore, 
such graphs may have directed cycles as in Figure 6.21(b). The presence of 
a directed cycle does not in itself imply the unsolvability of the problem. 
In fact, problem A of Figure 6.21(b) can be solved by solving the primitive 
problems G, Hand/. This leads to the solution of D and E and hence of 
B and C. A solution graph is a subgraph of solvable nodes that shows that 
the problem is solved. The solution graphs for the graphs of Figure 6.21 are 
shown by heavy edges. 

288 
Basic Search and Travenal Techniques 
First, let us see how to determine whether or not a given AND/OR tree 
represents a solvable problem (the extension to graphs is left as an exercise). 
Clearly, we can determine whether or not a problem is solvable. This calls 
for a postorder traversal of the AND/OR tree. The algorithm is a straight-
forward extension of that discussed in Section 6.1 and appears as Algorithm 
6.12. Rather than evaluate all the children of a node, the algorithm ter-
minates as soon as it discovers that a node is either unsolvable (line 6) or 
solvable (line 13). This reduces the amount of work being done by the 
algorithm without affecting the outcome. A similar modification to post-
order is made in Section 6.4 when implementing an alpha-beta search (to be 
defined in Section 6.4). Algorithm SOLVE can easily be modified so as to 
identify a solution subtree. 
line procedure SOL VE(n 
I IT is an AND/OR tree with root T. T ~ 0. Algorithm returns 11 I 
I !if problem is solvable and 0 otherwise/ I 
1 
case 
2 
: Tis a terminal node: if Tis solvable then return (1) 
3 
else return (0) 
4 
end if 
5 
: T is an AND node: 
for each child S of T do 
6 
if SOL VE(S) = 0 then return (0) 
7 
endif 
8 
repeat 
9 
return (1) 
10 
:else: for each child S of T do I !OR node/ I 
11 
if SOL VE(S) = 1 then return (1) endif 
12 
repeat 
13 
return (0) 
14 
endcase 
15 
endSOLVE 
Algorithm 6.12 Algorithm to determine if the AND/OR tree Tis solvable 
Often, the AND/OR tree corresponding to a given problem is available 
only implicitly. We are given a function F that generates all the children of 
a node already generated. In this case, given the root node we have to de-
termine a solution tree (if one exists) for the problem. The nodes of the 
tree can be generated either in breadth first or depth first order. Since it 
is possible for an AND/OR tree to have infinite depth, a depth first gen-
eration of the tree may start generating all the nodes on an infinite path 

And/Or Graphs 
289 
(a) 
(b) 
Figure 6.21 
Two AND/OR graphs that are not trees 
from the root and hence never determine a solution subtree (even when one 
exists). This can be overcome by restricting the depth first search to the 
generation of the AND/OR tree only to a certain depth d. Nonterminal 
nodes at depth d are labeled unsolvable. In this case a depth first search 
is guaranteed to find a solution subtree provided there is one of depth no 
more than d. Breadth first search (or generation) does not suffer from this 
drawback. Since each node can have only a finite number of children, no 
level in the AND/OR tree can have an infinite number of nodes. Hence, 
a breadth first generation of the AND/OR tree is guaranteed to find a 
solution subtree if one exists. Moreover, such a generation procedure would 
generate a solution subtree of minimum depth. 
Algorithm BFGEN(T, F) generates a breadth first solution tree (if one 
exists) of the AND/OR tree obtained by applying the child generation func-
tion F starting with node T. If no solution subtree exists then BFGEN may 
not terminate. This happens only if the AND/OR tree has infinite depth. 
The algorithm can be made to terminate by restricting the depth of the 
desired solution tree. BFGEN makes use of a subalgorithm ASOLVE(T). 
ASOLVE(T) is similar to algorithm SOLVE. It makes a postorder traversal 
of the partially generated AND/OR tree T and labels the nodes as either 
solved, unsolvable or maybe solvable. Since Tis not a complete AND/OR 
tree it has three kinds of leaf nodes. The first kind is a nonterminal leaf 
node. A nonterminal leaf node may or may not be solvable. It is an unex-
plored node. The other two kinds are terminal leaf nodes and have already 
been marked either solvable or unsolvable. If a nonleaf node is an AND 
node then it is unsolvable if any one of its children is unsolvable. A nonleaf 
node which is an OR node is solvable if it has at least one child which is 
marked solvable. Any nodes found to be unsolvable may be discarded from 
T (line 7). Further, there is no need to explore any descendent of an un-
solvable node P as even if this descendent is solvable, P cannot be solved. 

290 
Buie Search and Travenal Techniques 
Line 9 deletes all unexplored descendents of P from the queue. If a node is 
already solved, then again, its descendents need not be explored further 
(line 9). One may readily verify that if there is a solution tree com~sponding 
to (T, F) then BFGEN will find one such tree. Note that if such a tree is 
found then T points to its root and the tree may have some solvable leaf 
nodes that need not be solved in order to solve the whole problem. An 
additional pass over T can be made to remove these extraneous nodes. 
line procedure BFGEN(T, F) 
I IF generates the children of nodes in T; T is the root/ I 
I /node. At termination Tis the root of the solution/ I 
I /subtree if any.I I 
initialize Q to be an empty queue; V -
T 
loop 
1 
2 
3 
4 
use F to generate the children of V 
I /explore VI I 
if V has no children then label V unsolvable 
else (i) put all children of V that are not 
leaf nodes onto Q and label the 
leaf nodes solvable or unsolvable. 
(ii) 
add all the children of V to tree T 
5 
endif 
6 
call ASOLVE(T) I /label nodes solved, unsolvable or maybe/ I 
I I solvable/ I 
7 
delete from tree T all nodes labeled unsolvable 
8 
if the root node T is labeled solvable then return (T) endif 
9 
delete from Q all nodes that had or have an ancestor in T 
labeled either unsolvable or solved 
10 
if Q is empty then stop 
I !no solution// endif 
11 
delete first node on Q; let this node be V 
12 
repeat 
13 
endBFGEN 
The exercises further explore AND/OR trees and graphs. We shall see 
more of AND/OR trees and graphs in Chapter 11. 
Algorithm 6.13 Breadth first generation of a solution tree 
6.4 GAME TREES 
An interesting application of trees is the playing of games such as tic-
tac-toe, chess, nim, kalah, checkers, go, etc. As an example, let us con-
sider the game of nim. This game is played by two players A and B. The 

Game Trees 
291 
game itself is described by a board which initially contains a pile of n tooth· 
picks. The players A and B make moves alternately with A making the 
first move. A legal move consists of removing either 1, 2 or 3 of the tooth-
picks from the pile. However, a player cannot remove more toothpicks 
than there are on the pile. The player who removes the last toothpick loses 
the game and the other player wins. The board configuration at any time 
is completely specified by the number of toothpicks remaining in the pile. 
At any time the game status is determined by the board configuration to· 
gether with the player whose turn it is to make the next move. A terminal 
board configuration is one which represents either a win, lose or draw 
situation. All other configurations are nonterminal. In nim there is only one 
terminal configuration: there are no toothpicks in the pile. This configura-
tion is a win for player A if B made the last move, otherwise it is a win for 
B. The game of nim cannot end in a draw. 
A sequence Ci. ... , Cm of board configurations is said to be valid if: 
(i) C 1 is the starting configuration of the game; 
(ii) 
C ,, 0 < i < m, are nonterminal configurations; 
(iii) C; + 1 is obtained from C; by a legal move made by player A ifi is odd 
and by player B if i is even. It is assumed that there are only finitely 
many legal moves. 
A valid sequence C 1, ••• , Cm of board configurations with Cm a terminal 
configuration is an instance of the game. The length of the sequence Ci. 
C 2, ••• , Cm ism. A finite game is one in which there are no valid sequences 
of infinite length. All possible instances of a finite game may be represented 
by a game tree. The tree of Figure 6.22 is the game tree for nim with n = 
6. Each node of the tree represents a board configuration. The root node 
represents the starting configuration C 1. Transitions from one level to the 
next are made via a move of A or B. Transitions from an odd level repre· 
sent moves made by A. All other transitions are the result of moves made 
by B. Square nodes have been used in Figure 6.22 to represent board con· 
figurations when it was A's turn to move. Circular nodes have been used 
for other configurations. The edges from level 1 nodes to level 2 nodes and 
from leve~ 2 nodes to level 3 nodes have been labeled with the move made 
by A and B respectively (for example, an edge labeled 1 means 1 tooth· 
pick is to be removed). It is easy to figure out the labels for the remaining 
edges of the tree. Terminal configurations are represented by leaf nodes. 
Leaf nodes have been labeled by the name of the player who wins when 
that configuration is reached. By the nature of the game of nim player A 
can win only at leaf nodes on odd levels while B can win only at leaf nodes 

Basic Search and Travenal Techniques 
on even levels. The degree of any node in a game tree is at most equal to 
the number of distinct legal moves. In nim there are at most 3 legal moves 
from any configuration. By definition, the number of legal moves from 
any configuration is finite. The depth of a game tree is the length of a 
longest instance of the game. The depth of the nim tree of Figure 6.22 is 
7. Hence, from start to finish this game involves at most 6 moves. It is 
not difficult to see how similar game trees may be constructed for other 
finite games such as chess, tic-tac-toe, kalah, etc. (Strictly speaking, chess 
is not a finite game as it is possible to repeat board configurations in the 
game. We can view chess as a finite game by disallowing this possibility. 
We could, for instance, define the repetition of a board configuration as 
resulting in a draw.) 
Now that we have seen what a game tree is, the next question is "of what 
use are they?" Game trees are useful in determining the next move a player 
should make. Starting at the initial configuration represented by the root 
of Figure 6.22 player A is faced with the choice of making any one of three 
possible moves. Which one should he make? Assuming that player A wants 
to win the game, he should make the move that maximizes his chances of 
winning. For the simple tree of Figure 6.22 this move is not too difficult to 
determine. We can use an evaluation function E(X) which assigns a numeric 
value to the board configuration X. This function is a measure of the value 
or worth of configuration X to player A. So, E(X) is high for a configuration 
from which A has a good chance of winning and low for a configuration 
from which A has a good chance of losing. E(X) has its maximum value 
for configurations that are either winning terminal configurations for A or 
configurations from which A is guaranteed to win regardless of B's coun-
termoves. E(X) has its minimum value for configurations from which B is 
guaranteed to win. 
For a game such as nim with n = 6, whose game tree has very few nodes, 
it is sufficient to define E(X) only for terminal configurations. We could 
define E(X) as: 
if X is a winning configuration for A 
if X is a losing configuration for A 
Using this evaluation function we wish to determine which of the con-
figurations b, c, d player A should move the game into. Clearly, the choice 
is the one whose value is max { V(b ), V(c ), V(d)} where V(x) is the value 
of configuration x. For leaf nodes x, V(x) is taken to be E(x). For all other 

Game Trees 
293 
nodes x let d 2:: 1 be the degree of x and let c i. c2, ... , c d be the configura-
tions represented by the children of x. Then V(x) is defined by: 
{
~~~ { V(c ,) } if x is a square node 
V(x) = 
min { V(c.)} if x is a circular node 
l:si:sd 
(6.1) 
The justification for (6.1) is fairly simple. If x is a square node, then it 
is at an odd level and it will be A's turn to move from here if the game 
ever reaches this node. Since A wants to win he will move to a child node 
with maximum value. In case x is a circular node it must be on an even 
level and if the game ever reaches this node, then it will be B's turn to 
move. Since B is out to win the game for himself, he will (barring mistakes) 
make a move that will minimize A's chances of winning. In this case the 
next configuration will be minLsi"'d { V(e;)}. Equation (6.1) defines the 
minimax procedure to determine the value of configuration x. 
This is illustrated on the hypothetical game of Figure 6.23. P 11 represents 
an arbitrary board configuration from which A has to make a move. The 
values of the leaf nodes are obtained by evaluating the function E(x). The 
value of P 11 is obtained by starting at the nodes on level 4 and computing 
their values using eq. (6.1). Since level 4 is a level with circular nodes all 
unknown values on this level may be obtained by taking the minimum of 
the children values. Next, values on levels 3, 2 and 1 may be computed in 
that order. The resulting value for P 11 is 3. This means that starting from 
P 11 the best A can hope to do is reach a configuration of value 3. Even 
though some nodes have value greater than 3, these nodes will not be reached, 
as B's countermoves will prevent the game from reaching any such con-
figuration (assuming B's countermoves are optimal for B with respect to A's 
evaluation function). For example, if A made a move to P 2i. hoping to win 
the game at P 3i. A would indeed be surprised by B's countermove to P 32 
resulting in a loss to A. Given A's evaluation function and the game tree of 
Figure 6.23 the best move for A to make is to configuration P22. Having 
made this move, the game may still not reach configuration P s2 as B would, 
in general, be using a different evaluation function, which might give dif-
ferent values to various board configurations. In any case, the minimax 
procedure can be used to determine the best move a player can make given 
his evaluation function. Using the minimax procedure on the game tree 
for nim (Figure 6.22) we see that the value of the root node is V(a) = 1. 
Since E(X) for this game was defined to be 1 iff A was guaranteed to win, 

294 
Basic Search and Travenal Techniques 
D Player 
A to move 
Q 
Player 
B to move 
Figure 6.22 Complete game 
this means that if A makes the optimal move from node a then no matter 
what B's countermove is A will win. The optimal move is to node b. One 
may readily verify that from b, A can win the game independent of B's 
countermove! 
For games such as nim with n = 6, the game trees are sufficiently small 
that it is possible to generate the whole tree. Thus, it is a relatively simple 
matter to determine whether or not the game has a winning strategy. More-
over, for such games it is possible to make a decision on the next move by 
looking ahead all the way to terminal configurations. Games of this type 
are not very interesting since assuming no errors are made by either player, 
the outcome of the game is predetermined and both players should use 
similar evaluation functions i.e., EA (X) = 1 for X a winning configuration 
and EA(X) = -1 for X a losing configuration for A; EB(X) = -EA(X). 
Of greater interest are games such as chess where the game tree is too 
large to be generated in its entirety. It is estimated that the game tree for 
chess has more than 10 100 nodes. Even using a computer which is capable 
of generating 10 11 nodes a second, the complete generation of the game tree 
for chess would require more than 10 80 years. In games with large game trees 
the decision as to which move to make next can be made only by looking 
at the game tree for the next few levels. The evaluation function E(X) is 
used to get the values of the leaf nodes of the subtree generated and then 

a 
Game Trees 
295 
111111 
2 
3 
A's move 
3 
8
1 s countermove 
A 
B's countermove 
A's move 
8
1 s countermove 
tree for Nim with n 
6 
max 
min 
max 
A wins 
min 
P52 P53 P54 P55 p56 
P57 P5a ~ 
Ps.10 
Ps, II 
D 
Player A to move 
0 
Player B to move 
Figure 6.23 Portion of game tree for a hypothetical game. The value of terminal 
nodes is obtained from the evaluation function E(x) for player A 

296 
Basic Search and Travenal Techniques 
eq. (6.1) can be used to get the values of the remaining nodes and hence 
to determine the next move. In a game such as chess it may be possible 
to generate only the next few levels (say 6) of the tree. In such situations 
both the quality of the resulting game and its outcome will depend upon 
the quality of the evaluating functions being used by the two players as well 
as of the algorithm being used to determine V(X) by minimax for the cur-
rent game configuration. The efficiency of this algorithm will limit the 
number of nodes of the search tree that can be generated and so will have 
an effect on the quality of the game. 
Let us assume that player A is a computer and attempt to write an algo-
rithm that A can use to compute V(X). It is clear that the procedure to 
compute V(X) can also be used to determine the next move that A should 
make. A fairly simple recursive procedure to evaluate V(X) using minimax 
can be obtained if we recast the definition of minimax into the following 
form: 
l
e(X) 
V'(X) = 
~~~{ - V'(c;)} 
if X is a leaf of the subtree generated 
If X is not a leaf of the subtree gen-
erated and C;, 1 ~ i ~ d are the 
children of X. 
(6.2) 
where e(X) = E(X) if X is a position from which A is to move and e(X) = 
- E(X) otherwise. 
Starting at a configuration X from which A is to move, one can easily 
prove that eq. (6.2) computes V'(X) = V(X) as given by eq. (6.1). In fact, 
values for all nodes on levels from which A is to move are the same as given 
by eq. (6.1) while values on other levels are the negative of those given by 
eq. (6.1). 
The recursive procedure to evaluate V'(X) based on eq. (6.2) is then 
VE(X, /). This algorithm evaluates V'(X) by generating only l levels of 
the game tree beginning with X as root. One may readily verify that this 
algorithm traverses the desired subtree of the game tree in postorder. A 
postorder traversal is needed as the value of a node can be determined 
only after its children have been evaluated. 

Game Trees 
297 
procedure VE(X, l) 
I I compute V '(X) by looking at most l moves ahead. e(X) is the evaluation/ I 
I /function for player A. For convenience, it is assumed that starting/ I 
I /from any board configuration X the legal moves of the game permit/ I 
I I a transition only to the configurations Ci. C 2, ... , Cd if X is not a/ I 
I /terminal configuration.// 
if Xis terminal or l = 0 then return (e(X)) endif 
ans -
- VE(C,, l -
1) //traverse the first subtree// 
for i -
2 to d do 
I /traverse the remaining subtrees// 
ans -
max(ans, -
VE(C;, l -
1)) 
repeat 
return (ans) 
end VE 
Algorithm 6.14 Postorder evaluation of a game tree 
An initial call to algorithm VE with X = P 11 and l = 4 for the hypothet-
ical game of Figure 6.23 would result in the generation of the complete 
game tree. The values of various configurations would be determined in the 
order: P3i, P32, P2i. Psi, Ps2, P53, P4i, Ps4, Pss, Ps6, P42, P33, ... , P37, 
P 24, P 11. It is possible to introduce, with relative ease, some heuristics into 
algorithm VE that will in general result in the generation of only a portion 
of the possible configurations while still computing V'(X) accurately. 
Consider the game tree of Figure 6.23. After V(P 41) has been computed, 
it is known that V(P 33) is at least V(P 41) = 3. Next, when V(P ss) is deter-
mined to be 2, then we know that V(P42) is at most 2. Since P33 is a max 
position, V(P d cannot affect V(P 33). Regardless of the values of the re-
maining children of P 42, the value of P 33 is not determined by V(P d as 
V(P 42) cannot be more than V(P 41). This observation may be stated more 
formally as the following rule: The alpha value of a max position is defined 
to be the minimum possible value for that position. If the value of a min 
position is determined to be less than or equal to the alpha value of its 
parent, then we may stop generation of the remaining children of this min 
position. Termination of node generation under this rule is known as alpha 
cutoff. Once V(P 41) in Figure 6.23 is determined, the alpha value of P 33 
becomes 3. V(Pss) ~ alpha value of P 33 implies that Ps6 need not be gen-
erated. 

298 
Basic Search and Travenal Techniques 
A corresponding rule may be defined for min positions. The beta value 
of a min position is the maximum possible value for that position. If the 
value of a max position is determined to be greater than or equal to the 
beta value of its parent node, then we may stop generation of the remaining 
children of this max position. Termination of node generation under the 
rule is called beta cutoff. In Figure 6.23, once V(P 35) is determined, the 
beta value of P 23 becomes - 1. Generation of P 57, P 5s, P 59 gives V(P 43) = 0. 
Thus, V(P 43) is greater than or equal to the beta value of P 23 and we may 
terminate the generation of the remaining children of P 36· The two rules 
stated above may be combined together to get what is known as alpha-
beta pruning. When alpha-beta pruning is used on Figure 6.23 the sub-
tree with root P 36 is not generated at all! This is so because when the value 
of P 23 is being determined the alpha value of P 11 is 3. V(P 35) is less than 
the alpha value of P 11 and so an alpha cutoff takes place. It should be 
emphasized that the alpha or beta value of a node is a dynamic quantity. 
Its value at any time during the game tree generation depends upon which 
nodes have so far been generated and evaluated. 
In actually introducing alpha-beta pruning into algorithm VE it is nec-
essary to restate this rule in terms of the values defined by eq. (6.2). Under 
eq. (6.2) all positions are max positions since the values of the min positions 
of eq. (6.1) have been multiplied by -1. The alpha-beta pruning rule now 
reduces to the following rule: let the B-value of a position be the minimum 
value that position can have. For any position X, let B be the B-value of 
its parent and D = - B. Then, if the value of X is determined to be greater 
than or equal to D, we may terminate generation of the remaining children 
of X. Incorporating this rule into algorithm VE is fairly straightforward 
and results in algorithm VEB. This algorithm has the additional parameter 
D which is the negative of the B value of the parent of X. 
procedure VEB (X, l, D) 
//determine V'(X) as in eq. (6.2) using the B-rule and looking// 
//only l moves ahead. Remaining assumptions and notation are// 
I /the same as for algorithm VE. I I 
if Xis terminal or l = 0 then return (e(X)) endif 
ans -
- VEB(Ci. l -
1, oo) 
//current lower bound on V'(x)l I 
for i -
2 to d do 
if an.s ~ D then return (ans) endif //use B-rulel I 
ans -
max (ans, -
VEB(C;, l - 1, - ans)) 
repeat 
return (ans) 
end VEB 
Algorithm 6.15 Postorder evaluation of a game tree using alpha-beta pruning 

Game Trees 
299 
If Y is a position from which A is to move, then the initial call VEB(Y, 
l, oo) correctly computes V'(Y) with an l move look ahead. Further pruning 
of the game tree may be achieved by realizing that the B-value of a node 
X places a lower bound on the value grandchildren of X must have in order 
to affect X's value. Consider the subtree of Figure 6.24(a). If V'(GC(X)) ~ 
B then V'(C(X)) ~ -B. Following the evaluation of C(X), the B-value 
of X is max {B, -
V'(C(X))} = B as V'(C(X)) ~ -B. Hence unless 
V'(GC(X)) > B, it cannot affect V'(X) and so B is a lower bound on the 
value GC(X) should have. Incorporating this lowerbound into algorithm 
VEB yields algorithm AB. The additional parameter LB is a lowerbound 
on the value X should have. 
procedureAB(X, l, LB, D) 
//same as algorithm VEB. LB is a lowerbound on V'(X)l I 
if Xis terminal or I = 0 then return (e(X)) endif 
ans - LB //current lowerbound on V'(X)! I 
for i -
1 to d do 
if ans ~ D then return (ans) endif 
ans -
max (ans, - AB(C;, l -
1, -D, -
ans)) 
repeat 
return (ans) 
endAB 
Algorithm 6.16 Postorder evaluation of a game tree using deep alpha beta 
pruning 
One may easily verify that the initial call AB(Y, l, -
oo, oo) gives the 
same result as the call VE(Y, /). With the addition of LB, the search algo-
rithm is known as deep alpha beta pruning. 
Figure 6.24(b) shows a hypothetical game tree in which the use of algo-
rithm AB results in greater pruning than achieved by algorithm VEB. Let 
us first trace the action of VEB on the tree of Figure 6.24(b). We assume 
the initial call to be VEB(P i. l, oo) where l is the depth of the tree. After 
examining the left subtree of P 1, the B value of P1 is set to 10 and nodes 
P 3 , P4, Ps and P6 are generated. Following this, V'(P6) is determined to 
be 9 and then the B-value of Ps becomes - 9. Using this, we continue to 
evaluate the node P1. In the case of AB however, since the B-value of P1 is 
10, the lowerbound for P 4 is 10 and so the effective B-value of P 4 becomes 
10. As a result the node P1 is not generated since no matter what its value, 
V'(Ps) ~ - 9 and this will not enable V'(P4) to reach its lower bound. 

300 
Basic Search and Travenal Techniques 
B [] 
I 
8 
I I 
GC(X) I 
V' (GC(X)) s; B 
:::} V'(C(X))"" -B 
(a) 
I 
\ 
66 
y 
(b) 
Figure 6.24 Game trees showing lower bounding 
Analysis of Procedures VEB and AB 
Analyzing procedure AB to determine what fraction of nodes in a game 
tree will be generated is exceedingly difficult. Knuth and Moore have 
analyzed procedure VEB for certain kinds of game trees. Some of their 
results are stated below without proof. 

Game Trees 
301 
Definition: A uniform game tree of degree d and height h is a game tree 
is which every node at levels 1, 2, ... , h -
1 has exactly d children. In 
addition, every node at level h is terminal. A random uniform game tree 
is a uniform game tree in which the terminal nodes have independent ran-
dom values. 
Theorem 6.10 [Knuth and Moore] 
The expected number, T(d, h) of 
terminal positions examined by the alpha-beta procedure without deep cut-
offs (i.e. procedure VEB), in a random uniform game tree of degreed and 
height h is less than c(d) r(d)h. r(d) is the largest eigenvalue of the matrix 
Md whose terms Md(i,j) are given by 
1 s i s d and 1 s j s d. 
c(d) is an appropriate constant. 
D 
Note that since procedure AB is at least as good as procedure VEB, the 
bound of Theorem 6.10 is also a bound for AB. Using Theorem 6.10, the 
following theorem may be proved: 
Theorem 6.11 
[Knuth and Moore] 
T(d, h) for a random uniform game 
tree of degreed and height h + 1 satisfies the equality 
limT(d, h) 11h = r(d) 
h-oo 
where 
d 
d 
c 1 log d s r(d) s c 2 log d 
for some positive constants c 1 and c z. 
D 
Knuth and Moore have also analyzed the alpha-beta cutoff procedure on 
a different tree model. 
Definition: A game tree is totally dependent if for every pair p; and p;, 
i .,,t. j of nonterminal positions either all terminal children of p; have greater 
value than the terminal children of pit or they all have lesser value. 

302 
Buie Search and Travenal Tecludque1 
Theorem 6.12 [Knuth and Moore] 
The expected number of terminal 
positions examined by procedure AB (i.e. with deep cutoffs), in a random 
totally dependent uniform game tree of degreed and height h + 1, is 
d - Hd (d ih12l + Hdd Lh12J - Hdh+1 - Hdh) + Hdh 
d - Hi 
where Hd = 1 + 1/2 + · · · + lid. Ford ~ 3, this bound is within a 
constant factor of the minimum number of terminal positions that must be 
examined by any algorithm which evaluates a uniform game tree of degree 
d and height h + 1. 
0 
6.5 BICONNECTED COMPONENTS AND DEPTH FIRST SEARCH 
In this section, by a graph we shall always mean an undirected graph. A 
vertex v in a connected graph G is an articulation point iff the deletion of 
vertex v together with all edges incident to v disconnects the graph into two 
or more nonempty components. In the .connected graph of Figure 6.2S(a) 
vertex 2 is an articulation point as the deletion of vertex 2 and edges (1, 2), 
(2, 3), (2, 5), (2, 7) and (2, 8) leaves behind two disconnected nonempty 
components (Figure 6.2S(b)). Graph G of Figure 6.25(a) has only two other 
articulation points: vertex S and vertex 3. Note that if any of the remaining 
vertices is deleted from G then exactly one component remains. 
A graph G is biconnected iff it contains no articulation point. The graph 
of Figure 6.2S(a) is not biconnected. The graph of Figure 6.26 is biconnected. 
The presence of articulation points in a connected graph can be an unde-
sirable feature in many cases. For example, if G represents a communica-
tion network with the vertices representing communication stations and 
the edges communication lines then the failure of a communication station 
i which is an articulation point would result in loss of communication to 
points other than i too. On the other hand, if G has no articulation point 
then if any station i fails, we can still communicate between every pair of 
stations not including station i. 
In this section we shall develop an efficient algorithm to test if a connected 
graph is biconnected. For the case of graphs that are not biconnected, this 
algorithm will identify all the articulation points. Once it has been deter-
mined that a connected graph G is not biconnected, it may be desirable to 
determine a set of edges whose inclusion will make the graph biconnected. 
Determining such a set of edges is facilitated if we know the maximal sub-
graphs of G that are biconnected. G ' = ( V ', E ') is a maximal biconnected 
subgraph of G iff G has no biconnected subgraph G" = ( V': £") such 

Biconnected Components and Depth First Search 
303 
6 
10 
(a) Graph G 
4 
3 
10 
9 
(bl Result of deletinQ vertex 2 
Figure 6.25 An example graph 
that V' <;;; V" and E' C E". A maximal biconnected subgraph is a bi-
connected component. 
The graph of Figure 6.26 has only one biconnected component (i.e. the 
entire graph). The biconnected components of the graph of Figure 6.2S(a) 
are shown in Figure 6.27. 
It is relatively easy to show that two biconnected components can have 
at most one vertex in common and that this vertex is an articulation point. 
Hence, no edge can be in two different biconnected components (as this 
would require two common vertices). The graph G may be transformed 
into a biconnected graph using the edge addition scheme of Figure 6.28. 
Figure 6.26 A biconnected graph 

304 
Basic Search and Travenal Techniques 
Since every biconnected component of G contains at least two vertices 
(unless G itself has only one vertex), it follows that the V; of step E3 exists. 
Using this scheme to transform the graph of Figure 6.25(a) into a bicon-
nected graph requires us to add edges (4, 10) and (10, 9) (corresponding 
to the articulation point 3); edge (1, 5) (corresponding to the articulation 
point 2) and edge (6, 7) (corresponding to point 5). 
Figure 6.27 Biconnected components of graph of Figure 6.25(a) 
El: 
for each articulation point a do 
E2: 
let B,, B2, ... , Bk be the biconnected components containing 
vertex a 
E3: 
let V;, v; .,,t. a be a vertex in B;, 1 ::s i ::s k 
E4: 
add to G the edges (v;, V;+ 1), 1 ::s i < k 
ES: 
repeat 
Figure 6.28 Scheme to construct a biconnected graph 
Note that once the edges (v ;, v; + 1) of step E4 (Figure 6.28) are added 
vertex a is no longer an articulation point. Hence following the addition 
of the edges corresponding to all articulation points, G has no articulation 
points and so is biconnected. If G has p articulation points and b bicon-
nected components then the scheme of Figure 6.28 introduces exactly b - p 
new edges into G. One may easily show that this scheme may use more than 
the minimum number of edges needed to make G biconnected (exercise 
78). 
Now, let us attack the problem of identifying the articulation points and 
biconnected components of a connected graph G with n ~ 2 vertices. The 

Biconnected Componentl and Depth Fint Search 
305 
problem is efficiently solved by considering a depth first spanning tree of 
G. 
Figures 6.29(a) and (b) show a depth first spanning tree of the graph of 
Figure 6.2S(a). In each figure there is a number outside each vertex. These 
numbers correspond to the order in which a depth first search visits these 
vertices. This number will be referred to as the depth first number (DFN) 
of the vertex. Thus, DFN(l) = 1, DFN(4) = 2 and DFN(6) = 8. In Figure 
6.29(b) solid edges form the depth first spanning tree. These edges are 
called tree edges. Broken edges (i.e. all remaining edges) are called back 
edges. 
I I 
' 
2 4 
I 
4 10 
Figure 6.29 A depth first spanning tree of the graph of Figure 6.25(a) 
Depth first spanning trees have a property that is very useful in identifying 
articulation points and biconnected components. This property is that if 
(u, v) is any edge in G then relative to the depth first spanning tree T either 
u is an ancestor of v or v is an ancestor of u. So, there are no cross edges 
relative to a depth first spanning tree ((u, v) is a cross edge relative to T iff 
neither u is an ancestor of v nor v an ancestor of u). To see this, assume 
that (u, v) E E(G) and (u, v) is a cross edge. (u, v) cannot be a tree edge 
as otherwise u is the parent of v or vice versa. So, (u, v) must be a back 
edge. Without loss of generality, we may assume DFN(u) < DFN(v). Since 
vertex u is visited first, its exploration cannot be complete until vertex v 
is visited. From the definition of depth first search, it follows that u is an 
ancestor of all vertices visited until u is completely explored. Hence u is an 
ancestor of v in T and (u, v) cannot be a cross edge. 

306 
Basic Search and Travenal Techniques 
We next observe that the root node of a depth first spanning tree is an 
articulation point iff it has at least two children. Furthermore, if u is any 
other vertex then it is not an articulation point iff from every child w of u 
it is possible to reach an ancestor of u using only a path made up of de-
scendents of w and a back edge. Note that if this cannot be done for some 
child w of u then the deletion of vertex u will leave behind at least two 
nonempty components (one containing the root and the other containing 
vertex w). This observation leads to a simple rule to identify articulation 
points. For each vertex u define L(u) as follows: 
L(u) = min{ DFN(u), min{L(w) I w is a child of u }, min{ DFN(w) I (u, w) 
is a back edge } } 
It should be clear that L(u) is the lowest depth first number that can be 
reached from u using a path of descendents followed by at most one back 
edge. From the preceding discussion it follows that if u is not the root then 
u is an articulation point iff u has a child w such that L(w) ~ DFN(u). 
For the spanning tree of Figure 6.29(b) the L values are L(l:lO) = (1, 1, 
1, 1, 6, 8, 6, 6, 5, 4). Vertex 3 is an articulation point as child 10 has L(lO) 
= 4 while DFN(3) = 3. Vertex 2 is an articulation point as child 5 has 
L(5) = 6 and DFN(2) = 6. The only other articulation point is vertex 5; 
child 6 has L(6) = 8 while DFN(5) = 7. 
L(u) can be easily computed if the vertices of the depth first spanning 
tree are visited in postorder. Thus, to determine the articulation points, it 
will be necessary to perform a depth first search of the graph G and visit 
the nodes in the resulting depth first spanning tree in postorder. It is pos-
sible to do both these functions in parallel. Procedure ART (Algorithm 6.17) 
carries out a depth first search of G. During this search each newly visited 
vertex gets assigned its depth first number. At the same time, L(i) is com-
puted for each vertex in the tree. This algorithm assumes that the connected 
graph G and the arrays DFN and L are global. In addition, it is assumed 
that the variable num is also global. It is clear from the algorithm that 
when vertex u has been explored and a return made from line 9 then L(u) 
has been correctly computed. Note that in line 5 if w .,,t. v then either 
(u, w) is a back edge or DFN(w) > I)FN(u) ~ L(u). In either case, L(u) 
is correctly updated. The initial call to ART is call ART (1, 0). DFN is 
initialized to zero before invoking ART. 

Biconnected Componentl and Depth Flnt Search 
307 
llne procedure ART(u, v) 
I lu is a start vertex for depth first search. v is its parent/ I 
I /if any in the depth first spanning tree. It is assumed/ I 
I /that the global array DFN is initialized to zero/ I 
I I and that the global variable num is initialized to 1. n isl I 
I /the number of vertices in GI I 
global DFN(n), L(n), num, n 
1 
DFN(u) -
num; L(u) -
num; num -
num + 1 
2 
for each vertex w adjacent from u do 
3 
if. DFN(w) = 0 then caUART(w, u) //w is unvisited// 
4 
L(u) -
min (L(u), L(w)) 
S 
else if. w -;it v then L(u) -
min (L(u), DFN(w)) 
6 
endif. 
7 
endif. 
8 
repeat 
9 
endART 
Algorithm 6.17 Algorithm to compute DFN and L 
Once L(l :n) has been computed the articulation points can be identified 
in O(e) time. Since ART has a complexity O(n + e) where e is the number 
of edges in G, the articulation points of G can be determined in O(n + e) 
time. 
Now, what needs to be done to determine the biconnected components 
of G? If following the call to ART (line 3) L(w) ~ DFN(u) then we know that 
u is either the root or an articulation point. Regardless of whether u is not 
the root or is the root and has either one or more children, the edge (u, w) 
together with all edges (both tree and back) encountered during this call 
to ART (except for edges in other biconnected components contained in 
subtree w) form a biconnected component. A formal proof of this statement 
appears in the proof of Theorem 6.13. The changes needed to ART to obtain 
the biconnected components are: 
(i) introduce a global stack S to hold edges. 
(ii) 
add the line 
2.1 if v -;it w and DFN(w) < DFN(u) then add (u, w) to top of S 
end if 
between lines 2 and 3. Note that (u, w) has already been stacked iff 
either v = w or DFN(w) > DFN(u). 

308 
Buie Search and Traversal Techniques 
(iii) between lines 3 and 4 add the lines: 
3.1 if L(w) ~ DFN(u) then print ('new biconnected component') 
3.2 
loop 
3.3 
delete an edge from the top of stack S 
3.4 
let this edge be (x, y) 
3.5 
print('(',x, ', ',y, ') ') 
3.6 
until((x,y) = (u,w)or(x,y) = (w,u))repeat 
3.7 endif 
One may verify that following these additions to ART, its computing time 
remains O(n + e). The following theorem establishes the correctness of the 
algorithm. 
Theorem 6.13 Algorithm ART with lines 2.1 and 3.1-3.7 added correctly 
generates the biconnected components of the connected graph G when G 
has at least 2 vertices. 
Proof: Note that when G has only one vertex, it has no edges so the algo-
rithm generates no output. In this case G does have a biconnected com-
ponent namely its single vertex. This case can be handled separately. 
When n ~ 2 the algorithm works correctly. This can be shown by induc-
tion on the number of biconnected components in G. Clearly, for all bicon-
nected graphs G the root u of the depth first spanning tree has only one 
child w. Futhermore, w is the only vertex for which L(w) ~ DFN(u) in line 
3.1. By the time w has been explored all edges in G have been output as 
one biconnected component. 
Now assume the algorithm works correctly for all connected graphs G 
with at most m biconnected components. We shall show that it also works 
correctly for all connected graphs with m + 1 biconnected components. 
Let G be any such graph. Consider the first time that L(w) ~ DFN(u) in 
line 3.1. At this time no edges have been output and so all edges in G in-
cident to the descendents ofw are on the stack and are above the edge (u w). 
Since none of the descendents of u is an articulation point and u is one, 
it follows that the set of edges above (u, w) on the stack forms a biconnected 
component together with the edge (u, w). Once these edges have been 
deleted from the stack and output, the algorithm behaves essentially as it 
would on the graph G' obtained by deleting from G the biconnected com-
ponent just output. The behavior of the algorithm on G differs from that 
on G' only in that during the completion of the exploration of vertex u, 
some edges (u, r) such that (u, r) is in the component just output may be 
considered. However, for all such edges, DFN(r) .,,t. 0 and DFN(r) > DFN(u) 

References and Selected Readinp 
309 
~ L(u). Hence, these edges only result in a vacuous iteration of the loop 
of lines 2-8 and do not materially affect the algorithm. 
One may easily establish that G' has at least 2 vertices. Since in addition 
G' has exactly m biconnected components, it follows from the induction 
hypothesis that the remaining components are correctly generated. 
D 
It should be noted that the algorithm described above will work with any 
spanning tree relative to which the given graph has no cross edges. Un-
fortunately, graphs can have cross edges relative to breadth first spanning 
trees. Hence, algorithm ART cannot be adapted to BFS. 
REFERENCES AND SELECTED READINGS: 
Algorithm INORDER2 is due to J. M. Robson and appears in: 
"An improved algorithm for traversing binary trees without auxiliary stack," by 
J. M. Robson, Info. Proc. Let., 2, pp 12-14 (1973). 
Algorithms for other kinds of traversals without using stacks appear in: 
"Simple algorithms for traversing a tree without an auxiliary stack," by B. Dwyer, 
Info. Proc. Let., 2, pp 143-145 (1974). 
"Scanning list structures without stacks or tag bits," by G. Lindstrom, Info. Proc. 
Let., 2, pp 47-51 (1973). 
Two of Dwyer's algorithms are developed in exercises 15 and 16. 
Traversal and search algorithms for threaded trees, A VL trees, B-trees etc. may 
be found in: 
Fundamentals of Data Structures, by E. Horowitz and S. Sahni, Computer Science 
Press, Potomac, Maryland (1976). 
The Art of Computer Programming, Vol. 3, by D. E. Knuth, Addison Wesley, 
Reading, Mass. (1973). 
Our discussion of code optimization for a multiregister machine (i.e. machine 
model B) is based on: 
"The generation of optimal code for arithmetic expressions," by R. Sethi and J. 
Ullman,JACM, 17(4), pp 715-728(1970). 
Early work on code generation was done by Anderson, Floyd, Nakata and Red-
ziejowski. The references are: 

310 
Buie Search and Traversal Techniques 
"A note on some compiling algorithms," by J. Anderson, Comm. ACM, 7(3), pp 
149-150 (1964). 
"An algorithm for coding efficient arithmetic operations," by R. Floyd, Comm. 
ACM, 4(1), pp 42-51 (1961). 
"On compiling algorithms for arithmetic expressions," by I. Nakata, Comm. ACM, 
10(8), pp 492-494 (1967). 
"On arithmetic expressions and trees," by R. Redziejowski, Comm. ACM, 12(2), 
pp 81-84 (1969). 
Further references on code optimization appear in chapter 11. A good discussion 
of state space search techniques, techniques for AND/OR graphs and game trees 
appears in Nilsson's book. This book also provides many applications of these 
techniques. Further applications are discussed in Slagle's book. The references are: 
Problem Solving Methods in Artificial Intelligence, by N. Nilsson, McGraw Hill, 
New York, (1971). 
Artificial Intelligence: The Heuristic Programming Approach, by J. Slagle, McGraw 
Hill, New York (1971). 
Our discussion of alpha-beta cutoffs is from 
"An analysis of alpha-beta cutoffs," by D. Knuth, Artificial Intelligence, 6, pp. 
293-326 (1975). 
The above paper contains proofs for Theorems 6.10, 6.11 and 6.12. Many other re-
sults are also presented. Further analysis of this pruning process may be found in; 
"The efficiency of the alpha-beta search on trees with branch-dependent terminal 
node scores," by M. Newborn, School of Computer Science, McGill University, 
Montreal, Canada (1976). 
"An analysis of the full alpha-beta pruning algorithm," by G. Baudet, Proc. 10th 
Ann. ACM Symp. on Theo. of Comput., San Diego, 1978, pp. 296-313. 
An extension of the minimax rule from trees to graphs is discussed in: 
"Applying the minimax rule over graphs which are not trees," by T. Doffey, Info. 
Proc. Let., 2, pp 79-81 (1973). 
Several applications of depth first search to graph problems are given in: 
"Depth first search and linear graph algorithms," by R. Tarjan, SIAM Jr. on 
Comput., 1(2), pp. 146-160 (1972). 
--- ----- ----

E:urcllea 
311 
The O(n + e) depth first algorithm for biconnected components is due to R. Tarjan 
and appears in the above paper. This paper also contains an O(n + e) algorithm 
to find the strongly connected components of a directed graph. An O(n + e) depth 
first algorithm to find triconnected components can be found in: 
"Dividing a graph into triconnected components," by J. Hopcroft and R. Tarjan, 
SIAM Jr. on Comput., 2(3), pp. 135-158 (1973). 
Some other references to efficient graph algorithms using depth first search are: 
"Efficient planarity testing," by J. Hopcroft and R. Tarjan, JACM, 21(4), pp 549-
568 (1974). 
"Efficient algorithms for graph manipulation," by J. Hopcroft and R. Tarjan, 
CACM, 16(6), pp. 372-378 (1973). 
"Finding all the elementary circuits of a directed graph," by D. Johnson, SIAM Jr. 
on Comput., 4(1), pp 77-84 (1975). 
"Finding dominators in directed graphs," by R. Tarjan, Proc. 7th Annual Prince-
ton Conference on Information Sciences and Systems, pp 414-418 (1973). 
"Testing Flow Graph Reducibility," by R. Tarjan, Proc. 5th Annual ACM Symp. 
on Th. of Comput., pp 96-107 (1973). 
"A fast and usually linear algorithm for global flow analysis," by S. Graham and 
M. Wegman,JACM, 23(1), pp 172-202 (1976). 
The breadth first search algorithms for exercises 37-38 may be found in: 
"Finding spanning trees with differing cost functions," by E. Horowitz and S. 
Sahni (1976). 
An O(n + e) algorithm to find a smallest set of edges which, when added to a graph 
G, produces a biconnected graph can be found in the paper: 
"Smallest augmentations to biconnect a graph, "by A. Rosenthal and A. Goldner, 
SIAM Jr. on Comput .. 6(1), pp. 55-66, (1977). 
EXERCISES 
Unless otherwise stated all binary trees are represented using nodes with three 
fields: LCHILD, DATA and RCHILD. 
1. Give an algorithm to count the number of leaf nodes in a binary tree T. What 
is its computing time? 
2. 
Write an algorithm SW APTREE(T) which takes a binary tree and swaps the 
left and right children of every node. For example, if Tis the binary tree 

312 
Buie Search and Traversal Techniques 
T 
1 
A 
Use one of the three traversal methods discussed in Section 6.1.1. 
3. 
Use one of the three traversal methods discussed in Section 6.1.1 to obtain an 
algorithm EQUIV(T, U) that determines whether or not the binary trees T 
and U are equivalent. Two binary trees T, U are equivalent iff they are struc-
turally equivalent and if the data in corresponding nodes of T and U are the 
same. 
4. Show that 
i) the inorder and postorder sequences of a binary tree uniquely define the 
binary tree. 
* 
ii) 
the inorder and preorder sequences of a binary tree uniquely define the 
binary tree. 
iii) the preorder and postorder sequences of a binary tree do not uniquely 
define the binary tree. 
5. Write an algorithm to construct the binary tree with a given inorder sequence 
I and a given postorder sequence P. Use GETNODE(X) to get a new node. 
What is the complexity of your algorithm? 
6. Do exercise S for a given inorder and preorder sequence. 
7. 
Show that if T has n nodes then Theorem 6.1 holds even for algorithm 
INORDERl. 
8. 
Write a nonrecursive algorithm for preorder traversal of a binary tree T. Your 
algorithm may use a stack. What are the time and space requirements of your 
algorithm? 
9. 
Do problem 8 for postorder traversal. 

ExercUe8 
313 
10. 
Write a nonrecursive algorithm for inorder traversal of a binary tree T. Each 
node has four fields: LCHILD, DATA, PARENT, RCHILD. Your algorithm 
should take no more than 0(1) additional space and O(n) time for an node 
tree. Show that this is true for your algorithm. 
11. 
Do problem 10 for preorder traversal. 
12. 
Do problem 10 for postorder traversal. 
13. 
Using the idea of algorithm INORDER2 write an 0(1) space and O(n) time 
algorithm for preorder traversal of a binary tree T with n nodes. Each node 
has three fields: LCHILD, DATA, RCHILD 
14. 
Do problem 13 for postorder traversal. 
15. 
Write a 0(n) time and 0(1) space algorithm for inorder traversal of a binary 
tree in which each node has a one bit TAG field in addition to the three fields: 
LCHILD, DATA, RCHILD. (Hint: Use the link reversal idea of INORDER2 
but not the LR scheme. Use the TAG bit to distinguish between moves to left 
and right subtrees). 
16. 
Do exercise 15 for preorder traversal. 
17. 
Do exercise 15 for postorder traversal. 
18. 
[Right threaded binary tree] 
In a right threaded binary tree, each node has 
four fields: LCHILD, DATA, RCHILD and TAG. The TAG of every node 
that has a nonempty right subtree is 1. A node with an empty right subtree 
has a TAG of 0 and its RCHILD field points to its inorder successor. Such a 
pointer is called a thread. Every threaded binary tree will have a head node. 
An empty binary tree will be represented by a headnode as: 
LCHILD 
DATA 
RCHILD 
TAG 
I o 
o I 
v 
A nonempty binary tree will appear as the left subtree of its headnode. 
The headnode will also be the inorder successor of the binary tree's last node 
in inorder. The figure below shows a binary tree and the corresponding right 
threaded binary tree. 

314 
Buie Search and Traversal Techniques 
c 
binary tree 
I 
\ 
I 
\ 
I 
\ 
I 
...... -~-a~) 
'-\threods to 
o ~ 
inorder 
'-.J..-..__.,_~ ' 
successors 
right threaded binary tree 
The conventions for the headnode will simplify the traversal algorithms. 
a) 
Write an algorithm INSUC(P) to find the inorder successor of an arbitrary 
node P in a right threaded binary tree. Note that only 0(1) additional 
space is needed and no link reversals are necessary. What is the worst case 
time complexity of your algorithm if the tree has n nodes? 
b) 
Is it possible to obtain an INSUC(P) algorithm for an unthreaded binary 
tree? Why? 
c) 
Using INSUC(P) of part a) obtain an inorder traversal algorithm for a 
binary tree T. Again, note that only 0(1) additional space is needed. No 
link reversals are to be used. Show that the time taken for the traversal is 
0(n) if T has n nodes. 
19. Do parts a), b) and c) of exercise 18 with inorder replaced by preorder. Note 
that a thread is still a pointer to the inorder successor. 
20. 
Do parts a), b) and c) of exercise 18 with inorder replaced by postorder. Note 
that a thread is still a pointer to the inorder successor. 
21. 
[Triple order traversal] 
A triple order traversal of a binary tree T is defined 
recursively by Algorithm 6.18. 
procedure TRIPLE(T) 
if T ;e. 0 then call VISJT(T) 
call TRIPLE(LCHILD(T)) 
call VISIT(T) 
call TRIPLE(RCHILD(T)) 
call VISIT(T) 
end.if 
end TRIPLE 
Algorithm 6.18 Triple order traversal 

Exercises 
315 
B. Dwyer has given a very simple nonrecursive algorithm for such a traversal 
(Algorithm 6.19). In this algorithm P, Q, R point respectively to the present 
node, previously visited node and next node to visit. The algorithm assumes 
that T ;e. 0 and that an empty subtree of node P is represented by a link to P 
rather than a zero. 
line 
procedure TRIP( T) 
! lit is assumed that all LCHILD and RCHILD fields are > 01 I 
1 
P -
T; Q -
-1 
2 
whileP ;e. -1 do 
3 
call VISIT( P) 
4 
R - LCHILD(P); LCHILD(P) -
RCHILD(P) 
S 
RCHILD(P) -
Q; Q -
P; P -
R 
6 
repeat 
7 
end TRIP 
Algorithm 6.19 Dwyer's algorithm for triple order traversal 
a) Prove that Dwyer's algorithm is correct. (Hint. Associated with each node 
S are three links, LCHILD, RCHILD and one from its parent. Each time 
S is visited, the links are rotated anticlockwise and so after three visits 
they are restored to the original configuration and the algorithm backs 
up the tree.) 
b) 
Show that the time and space complexity of algorithm TRIP is 0(n) and 
0(1) respectively. n is the number of nodes in T. 
22. 
Binary trees are often stored such that the children of a node are higher indexed 
nodes than the parent: I.e., LCHILD(P) and RCHILD(P) > P or equal to 0. 
Assume that if P has no left(right) child then LCHILD( P) (RCHILD(P)) = P. 
For this representation and procedure TRIP (Algorithm 6.19) show that 
a) If line 3 is replaced by 
if RCHILD( P) < P then call VISIT(p) endif 
then algorithm TRIP traverses Tin inorder. 
b) If line 3 is replaced by 
if Q < P then call VISIT( P) endif 
then the algorithm traverses Tin preorder. 
c) If line 3 is replaced by 
if LCHILD( P) < P then call VISIT( P) endif 
then a postorder traversal results. 

316 
Buie Search and Traversal Techniques 
23. 
Rewrite algorithm INORDER2 assuming the binary tree T is stored as in ex-
ercise 22. You can now dispense with LR and the stack of nodes from which a 
move to a right subtree has been made. 
24. 
[Level order traversal] 
In a level order traversal of a binary tree T all nodes 
on level i are visited before any node on level i + 1 is visited. Within a level, 
nodes are visited left to right. In level order the nodes of the tree of Figure 6.4 
will be visited in the order ABC DEF G. Write an algorithm LEVEL(T) to 
traverse the binary tree Tin level order. How much time and space are needed 
by your algorithm? Assume each node has three fields: LCHILD, DATA and 
RCHILD. 
25. 
Show that if a tree of degree k is represented using nodes with k child fields 
each then n(k -
1) + 1 of the total nk child fields present in an n node tree 
will be zero. 
26. 
Prove that traversing a tree in tree preorder gives the same results as traversing 
the corresponding binary tree in preorder (i.e. the nodes are visited in the 
same order). 
27. 
Prove that traversing a tree in tree in order gives the same results as traversing 
the corresponding binary tree in inorder (i.e. the nodes are visited in the same 
order). 
28. 
Show that if a tree is traversed in tree postorder then the nodes may be visited 
in a different order than when the corresponding binary tree is traversed in 
postorder. 
29. 
Write a nonrecursive algorithm Tl(T, k) for tree inorder. The tree T is of 
degree k and node P has k child fields CHILD(P, i), 1 :s i :s k. What are the 
time and space requirements of your algorithm? 
30. Do exercise 29 for tree preorder. 
31. 
Do exercise 29 for tree inorder. 
32. 
Assume tree Tis represented as in exercise 29. Write an algorithm LEVEL(T, 
k) to traverse T by levels. Within each level nodes are to be visited left to right 
assuming the subtree ordering CHILD(P, i) is to the left of CHILD(P, i + 1). 
What are the time and space requirements of your algorithm? 
33. 
Show that for any undirected graph G = (V, E) a call to BFS(v) with v E V 
results in the visiting of all vertices in the connected component containing v. 

Exercises 
317 
34. 
Rewrite BFS and BFT so that all the connected components of the undirected 
graph G get printed out. Assume that G is input in adjacency list form with 
HEAD(i) the headnode for the adjacency list for vertex i. 
35. 
Write an algorithm using the idea of BFS to find a shortest (directed) cycle 
containing a given vertex v. Prove that your algorithm finds a shortest cycle. 
What are the time and space requirements of your algorithm? 
36. 
Prove that if G is a connected undirected graph with n vertices and n -
1 
edges then G is a tree. 
37. a) The radius of a tree is its depth. Show that the forward edges used in 
BFS(v) define a spanning tree with root v having minimum radius amongst 
all spanning trees, for the undirected connected graph G, having root v. 
b) 
Using the result of a) write an algorithm to find a minimum radius span-
ning tree for G. What are the time and space requirements of your algo-
rithm? 
38. The diameter of a tree is the maximum distance between any two vertices. 
Let d be the diameter of a minimum diameter spanning tree for an undirected 
connected graph G. Let r be the radius of a minimum radius spanning tree 
for G. 
a) 
Show that '2r -
1 :s d :s '2r. 
b) Write an algorithm to find a minimum diameter spanning tree for G. 
(Hint: Use breadth first search followed by some local modification.) 
c) 
Prove that your algorithm is correct. 
d) 
What are the time and space requirements of your algorithm? 
39. 
Show that DFS visits all vertices in G reachable from v. 
40. 
Prove that the bounds of Theorem 6.3 hold for DFS. 
41. A bipartite graph G = (V, E) is an undirected graph whose vertices can be 
partitioned into two disjoint sets V 1 and V 2 = V -
V 1 with the properties (i) 
no two vertices in V 1 are adjacent in G and (ii) no two vertices in V 2 are adja-
cent in G. The graph G of Figure 6.6(a) is bipartite. A possible partitioning 
of Vis: V1 = { l, 4, 5, 6, 7} and V2 = {2, 3, 8}. Write an algorithm to deter-
mine whether a graph G is bipartite. In case G is bipartite your algorithm 
should obtain a partitioning of the vertices into two disjoint sets V 1 and V 2 
satisfying properties (i) and (ii) above. Show that if G is represented by its ad-
jacency lists, then this algorithm can be made to work in time O(n + e) where 
n = IVI ande = IEI. 

318 
Buie Search and Traversal Techniques 
42. It is easy to see that for any graph G, both DFS and BFS will take almost the 
same amount of time. However the space requirements may be considerably 
different. 
43. 
a) 
Give an example of an n vertex graph for which the depth of recursion of 
DFS starting from a particular vertex v is n -
1 whereas the queue of 
BFS will have at most 1 vertex at any given time if BFS is started from 
the same vertex v. 
b) Give an example of an n vertex graph for which the queue of BFS will 
have n -
1 vertices at one time whereas the depth of recursion of DFS 
is at most one. Both searches are started from the same vertex. 
Another way to search a graph is D-search. This method differs from BFS in 
that the next vertex to explore is the vertex most recently added to the list of 
unexplored vertices. Hence, this list operates as a stack rather than a queue. 
a) 
Write an algorithm for D-search. 
b) Show that D-search starting from vertex v visits all vertices reachable 
from v. 
c) 
What are the time and space requirements of your algorithm? 
d) 
Modify your algorithm so that it produces a spanning tree for an undirected 
connected graph. 
44. 
Write an algorithm to find the reflexive transitive closure matrix, A*, of a 
directed graph G. Show that if G has n vertices and e edges and if G is repre-
sented by its adjacency lists then this can be done in time 0(n 2 + ne). (Hint: 
use either BFS or DFS). How much space does your algorithm take in addition 
to that needed for G andA*? 
45. 
Write an algorithm to evaluate an arithmetic expression represented as a binary 
tree T. Assume that the only operators are binary + , - , * and I. Each node 
in the binary tree has three fields LCHILD, DATA and RCHILD. If P is a 
leaf node then DATA(P) is the address in memory corresponding to the var-
iable or constant represented by P. VAL(DATA(P)) is the current value of 
that variable or constant. What is the computing time of your algorithm? 
46. The postfix representation of an infix arithmetic expression L O R is defined 
recursively to be the postfix representation of L followed by the postfix repre-
sentation of R followed by Q. L and R are respectively the left and right op-
erands of Q. Consider some examples: 
Infix 
Postfix 
i) a + b 
ab+ 
ii) 
(a + b)*C 
ab +C* 
iii) 
(a - bl(c*d) 
ab - Cd*/ 
---------

Exerclles 
319 
In postfix form there are no parenthesis. 
a) 
What is the postfix form of the following expressions: 
i) 
(a + b*c)/(c - d) 
ii) a + (b - c)*(b + c) + dl(e - f) 
iii) al(b + c) + d*(e - f) 
b) Write an algorithm to evaluate a postfix expression E. Assume Eis pre-
sented as a string and that there exists an algorithm NEXT_TOKEN(E) 
that returns the next token (i.e. operator or operand) in E. When all 
tokens in E have been extracted, NEXT_TOKEN(E) returns oo. Assume 
that the only operators in E are binary + , - , * and I. (Hint: make a 
left to right scan of E using a stack to store operands and results. When-
ever an operator is seen in E, the top two operands on the stack are its 
right and left operands.) What is the complexity of your algorithm? 
c) 
Write an algorithm to obtain the postfix form of an infix expression E. 
Again assume E has only the binary operators + , - , * and I. (Hint: 
make a left to right scan of E using a stack to store operators until both 
the left and right operands of an operator have been output in postfix 
form.) Note that E may contain parenthesis. What is the complexity of 
your algorithm? 
47. Write an algorithm to obtain a binary expression tree for the postfix expression 
E. Assume E has the same operators as in the above exercise. You may use 
an algorithm GETNODE(X) to get a new node X. Each node has three fields: 
LCHILD, DATA and RCHILD. What is the complexity of your algorithm? 
48. Prove Theorem 6.5. 
49. 
Complete Table 6.2 to incude all possibilities for the code of an expression 
containing some commutative operators. 
50. 
Modify algorithm CODEl so that it generates optimal code even when the ex-
pression T contains some commutative operators. Show that your algorithm 
generates optimal code. 
51. 
Do exercise SO for the case when T contains some associative operators. 
52. 
For the following expression obtain an expression tree. Label the nodes with 
their MR value and obtain the optimal code generated by CODE2 for the two 
cases N = 1 and N = 2. Assume that no operator is either commutative or 
associative. 
i) 
(a + b )*(c + d *(e + f)l(g + h)) 
ii) a * b* cl(e - f + g*(h - k)*(l + m)) 
iii) a*(b - C)*(d + f)/(g*(h + j) - k *I) 

320 
Basic Search and Travenal Techniques 
53. Write an algorithm to compute MR(P) for each node Pin a binary expression 
tree T. See Theorem 6.6 for the definition of MR(P). Assume each node P has 
four fields LCHILD, DATA, MR and RCHILD. 
54. Prove Theorem 6.7. 
55. Prove Theorem 6.8. 
56. 
Show that the time complexity of CODE2 is 0(n) where n is the number of 
nodes in T. 
57. 
Show that if MR(T) s N then CODE2 generates code using the minimum 
possible number of registers when no stores are allowed. 
58. Prove Lemma 6.1. 
59. 
The number of memory references needed to evaluate a code of length I for an 
expression E is I (to fetch the instructions) plus one reference for each LOAD, 
STORE and instruction of type OP Rl, M, R2. Show that if the expression 
tree for E has n nodes and every code for E has at leasts STOREs then the 
minimum number of memory references needed to evaluate any code for E is 
at least I + n + 3s. Show that the code generated by CODE2 requires exactly 
this many references. 
60. Write an algorithm FLIP(T) to interchange left and right subtrees of nodes in 
the expression tree T representing commutative operators. The resulting tree 
should be such that the sum of major and minor nodes is minimum for every 
given N. N is the number of registers. What is the complexity of FLIP? 
61. 
Extend CODE2 to account for associative operators. 
62. Write an algorithm to determine whether or not a given AND/OR graph G 
represents a solvable problem. Devise a suitable representation for the graph 
G. 
63. 
Modify Algorithm 6.12 so that it identifies a solution subtree of T. 
64. Write out the algorithm ASOLVE used in algorithm BFGEN. 
65. 
Write an algorithm PRUNE to remove from the solution tree T generated by 
BFGEN, all nodes that need not be solved. I.e., the output tree is one in which 
all nodes must be solved in order to solve the whole problem. 
66. 
Consider the hypothetical game tree: 

Exerei-
321 
a) 
Using the minimax technique (eq. (6.1)) obtain the value of the root node. 
b) What move should player A make? 
c) 
List the nodes of this game tree in the order in which their value is com-
puted by algorithm VE. 
d) 
Using eq. (6.2) compute V'(X) for every node X in the tree. 
e) 
Which nodes of this tree are not evaluated during the computation of 
the value of the root node using algorithm AB with X = root, I = oo, 
LB= -ooandD = oo? 
67. 
Show that V'(X) computed by eq. (6.2) is the same as V(X) computed by eq. 
(6.1) for all nodes on levels from which A is to move. For all other nodes show 
that V(X) computed by eq. (6.1) is the negative of V'(X) computed by eq. 
(6.2). 
68. 
Show that algorithm AB when initially called with LB 
yields the same results as VE does for the same X and/. 
-oo and D 
00 
69. 
For the following graphs identify the articulation points and draw the bicon-
nected components. 
(i) 
(ii) 

322 
Buie Search IUld Traveral TeclmiqDN 
70. Show that if G is a connected undirected graph then no edge of G can be in 
two different biconnected components. 
71. Let G; = (V;, £;), 1 :s; i :s; k be the biconnected components of a connected 
graph G. Show that 
a) if i ¢ j then V; n Vj contains at most one vertex 
b) vertex v is an articulation point of G iff { v} = V; n Vj for some i andj, 
i ¢ j. 
72. 
Let G be a connected undirected graph. Write an algorithm to find the mini-
mum number of edges that have to be added to G so that G becomes bicon-
nected. Your algorithm should output such a set of edges. What are the time 
and space requirements of your algorithm? 
73. Show that if T is a breadth first spanning tree for an undirected connected 
graph G then G may have cross edges relative to T. 
74. Prove that a non root vertex, u, is an articulation point iff L(w) ~ DFN(u) for 
some child w of u. 
75. 
Prove that when the additions 2.1 and 3.1-3.6 are made to algorithm ART 
then if either v = w or DFN(w) > DFN(u) then edge (u, w) is either already 
on the stack of edges or has been output as part of a biconnected component. 
76. Write an algorithm of time complexity 0(n 2) to find the transitive closure 
matrix A + of an undirected graph G. 
77. 
Write an algorithm, using DFS, to find the transitive closure matrix A+ of a 
directed graph G. Show that this can be done in time 0(n 2 + ne ). 

Chapter 7 
BACKTRACKING 
7.1 
THE GENERAL METHOD 
In the search for fundamental principles of algorithm design, backtracking 
represents one of the most general techniques. Many problems which deal 
with searching for a set of solutions or which ask for an optimal solution 
satisfying some constraints can be solved using the backtracking formula-
tion. The name backtrack was first coined by D. H. Lehmer in the 1950's. 
Early workers who studied the process were R. J. Walker who gave an algo-
rithmic account of it in 1960 and Golomb and Baumert who presented a 
very general description of backtracking coupled with a variety of applica-
tions. (See the references for further details). 
In order to apply the backtrack method, the desired solution must be 
expressible as an n-tuple (xi, ... , Xn) where the x1 are chosen from some 
finite set S 1• Often the problem to be solved calls for finding one vector 
which maximizes (or minimizes or satisfies) a criterion function P(xi, ... , 
Xn). Sometimes it seeks all such vectors which satisfy P. For example, 
sorting the integers in A(l:n) is a problem whose solution is expressible by 
an n-tuple where x; is the index in A of the ith smallest element. The 
criterion function Pis the inequality A(x;) :5 A(x;+ 1) for 1 :5 i < n. The 
set S; is finite and includes the integers 1 through n. Though sorting is not 
usually one of the problems solved by backtracking, it is one example of a 
familiar problem whose solution can be formulated as an n tuple. In this 
chapter we will study a collection of problems whose solution is best viewed 
using backtracking. 
Suppose m; is the size of set S;. Then there are m = mi m2 · · · mn n-
tuples which are possible candidates for satisfying the function P. The 
brute force approach would be to form all of these n-tuples and evaluate 
each one with P, saving those which yield the optimum. The backtrack 
algorithm has as its virtue the ability to yield the same answer with far fewer 
than m trials. Its basic idea is to build up the same vector one component 
323 

324 
Backtracking 
at a time and to use modified criterion functions P ;(xi. ... , x;) (sometimes 
called bounding functions) to test whether the vector being formed has 
any chance of success. The major advantage of this method is this: if it is 
realized that the partial vector (x 1, x2, ... , x;) can in no way lead to an 
optimal solution, then m;+ 1 • • • mn possible test vectors may be ignored 
entirely. 
Many of the problems we shall solve using backtracking require that all 
the solutions satisfy a complex set of constraints. For any problem these con-
straints may be divided into two categories: explicit and implicit. Explicit 
constraints are rules which restrict each x; to take on values only from a 
given set. Common examples of explicit constraints are 
X;;:,:: 0 
x; = 0 or 1 
[; ~ X; ~ U; 
or S; 
or S; 
or S; 
{all nonnegative real numbers} 
{ 0, 1} 
{a:[; ~a ~ u;} 
The explicit constraints may or not depend on the particular instance I of 
the problem being solved. All tuples that satisfy the explicit constraints 
define a possible solution space for I. The implicit constraints determine 
which of the tuples in the solution space of I actually satisfy the criterion 
function. Thus implicit constraints describe the way in which the x; must 
relate to each other. 
Example 7.1 (g-queens) 
A classic combinatorial problem is to place eight 
queens on an g x g chessboard so that no two "attack", that is so that 
no two of them are on the same row, column or diagonal. Let us number 
the rows and columns of the chessboard 1 through g (figure 7 .1). The 
queens may also be numbered 1 through g_ Since each queen must be on 
a different row, we can without loss of generality assume queen i is to be 
placed on row i. All solutions to the g.queens problem can therefore be 
represented as g-tuples (x 1, ••• , xs) where x; is the column on which queen 
i is placed. The explicit constraints using this formulation are S; = { 1, 2, 
3, 4, 5, 6, 7, g}, 1 ~ i ~ n. Therefore the solution space consists of gs 
g-tuples. The implicit constraints for this problem are that no two x;'s can 
be the same (i.e. all queens must be on different columns) and no two 
queens can be on the same diagonal. The first of these two constraints 
implies that all solutions are permutations of the g.tuple (1, 2, 3, 4, 5, 6, 
7, g). This realization reduces the size of the solution space from gs tuples 
to g! tuples. We shall see later how to formulate the second constraint in 
terms of the x;. Expressed as an g.tuple, the solution in figure 7.1 is (4, 6, 
g, 2, 7, 1, 3, 5). 
D 
--------~ 

The General Method 
325 
column 
2 
3 
4 
5 
6 
7 
8 
row 
I 
2 
3 
4 
5 
6 
7 
8 
Q 
Q 
Q 
Q 
Q 
Q 
Q 
Q 
Figure 7 .1 One solution to the 8-queens problem 
Example 7.2 (Sum of subsets) Given n + 1 positive numbers: w;, 1 5 
i 5 n and M, this problem calls for finding all subsets of the w; whose sum 
is M. For example, if n = 4, (wi, w2, W3, W4) = (11, 13, 24, 7) and M = 
31 then the desired subsets are (11, 13, 7) and (24, 7). Rather than repre-
sent the solution vector by thew; which sum to M, we could represent the 
solution vector by giving the indices of these w;. Now the two solutions 
are described by the vectors (1, 2, 4) and (3, 4). In general, all solutions 
are k-tuples (xi, x2, ... , X.t), 1 5 k 5 n and different solutions may have 
different size tuples. The explicit constraints require x; i: {j lj is an integer 
and 1 5 j 5 n }. The implicit constraints require that no two be the same 
and that the sum of the corresponding w; be M. Since we wish to avoid 
generating multiple instances of the same subset (e.g. (1, 2, 4) and (1, 4, 2) 
represent the same subset), another implicit constraint which is imposed is 
thatx; < X;+i, 1 5 i < n. 
In another formulation of the sum of subsets problem, each solution 
subset is represented by an n-tuple (x1, x2, ... , Xn) such that x; i: {O, l}, 
1 5 i 5 n. x; = 0 if w; is not chosen and x; = 1 if w; is chosen. The 
solutions to the above instance are (1, 1, 0, 1) and (0, 0, 1, 1). This formu-
lation expresses all solutions using a fixed size tuple. Thus we conclude 
that there may be several ways to formulate a problem so that all solutions 
are tuples that satisfy some constraints. One may verify that for both of 
the above formulations, the solution space consists of 2n distinct tuples. 
D 
Backtracking algorithms determine problem solutions by systematically 
searching the solution space for the given problem instance. This search is 
facilitated by using a tree organization for the solution space. 1 For a given 

326 
Backtracking 
solution space many tree organizations may be possible. The next two ex-
amples examine some of the ways to organize a solution space into a tree. 
Example 7 .3 (n-queens) The n-queens problem is a generalization of 
the 8-queens problem of Example 7.1. n queens are to be placed on an x 
n chessboard so that no two attack, i.e., no two queens are on the same row, 
column or diagonal. Generalizing our earlier discussion, the solution space 
consists of all n! permutations of the n-tuple (1, 2, ... , n). Figure 7.2 
shows a possible tree organization for the case n = 4. A tree such as this is 
called a permutation tree. The edges are labeled by possible values of x;. 
Edges from level 1 to level 2 nodes specify the values for x 1 • Thus, the 
leftmost subtree contains all solutions with x 1 = 1; its leftmost subtree 
contains all solutions with x 1 = 1 and x2 = 2, etc. Edges from level i to 
level i + 1 are labeled with the values of x;. The solution space is defined 
by all paths from the root node to a leaf node. There are 4! = 24 leaf 
nodes in the tree of figure 7.2. 
0 
•. •4 
Figure 7 .2 Tree organization of the 4-queens solution space. 
Nodes are numbered as in depth first search. 

The General Method 
327 
Example 7.4 (Sum of subsets) In Example 7.2 we gave two possible 
formulations of the solution space for the sum of subsets problem. Figures 
7 .3 and 7.4 show a possible tree organization for each of these formulations 
for the case n = 4. The tree of Figure 7.3 corresponds to the variable tuple 
size formulation. The edges are labeled such that an edge from a level i node 
to a level i + 1 node represents a value for x;. At each node, the solution 
space is partitioned into subsolution spaces. The solution space is defined 
by all paths from the root node to any node in the tree. The possible paths 
are ( ) (this corresponds to the empty path from the root to itself); (1); (1, 
2); (1, 2, 3); (1, 2, 3, 4); (1, 2, 4); (1, 3, 4); (2); (2, 3); etc. Thus, the left-
most subtree defines all subsets containing w 1, the next subtree defines all 
subsets containing w2 but not w 1; etc. 
The tree of Figure 7.4 corresponds to the fixed tuple size formulation. 
Edges from level i nodes to level i + 1 nodes are labeled with the value of 
x; which is either zero or one. All paths from the root to a leaf node define 
the solution space. The left subtree of the root defines all subsets containing 
w 1 while the right subtree defines all subsets not containing w 1 etc. Now 
there are 2 4 leaf nodes which represent 16 possible tuples. 
D 
•1 =I 
x, =4 
5 
8 
Figure 7 .3 A possible solution space organization for the sum of subsets 
problem. Nodes are numbered as in breadth first search. 
At this point it is useful to develop some terminology regarding tree 
organizations of solution spaces. Each node in this tree defines a problem 

• 
I 
328 
Backtracking 
state. All paths from the root to other nodes define the state space of the 
problem. Solution states are those problem states S for which the path 
from the root to S defines a tuple in the solution space. In the tree of Fig-
ure 7.3 all nodes are solution states while in the tree of Figure 7.4 only 
leaf nodes are solution states. Answer states are those solution states S for 
which the path from the root to S defines a tuple which is a member of the 
set of solutions (i.e., it satisfies the implicit constraints) of the problem. 
The tree organization of the solution space will be referred to as the state 
space tree. 
18 
19 
4 
X4•I 
x4•I 
•4•1 
•4•1 
~·~ ~·~ 
4·~ ~x~O 
Figure 7.4 Another possible organization for the sum of subsets problems. 
Nodes are numbered as in D-search. 
At each internal node in the state space trees of Examples 7.3 and 7.4 
the solution space is partitioned into disjoint sub-solution spaces. For ex-
ample at node 1 of Figure 7.2 the solution space is partitioned into four 
disjoint sets. Subtrees 2, 18, 34, and 50 respectively represent all elements 
of the solution space with x 1 = 1, 2, 3 and 4. At node 2 the sub-solution 
space with x 1 = 1 is further partitioned into three disjoint sets. Subtree 
3 represents all solution space elements with x 1 = 1 and x2 = 2. For all 
of the state space trees we shall be studying in this chapter, the solution 
space will be partitioned into disjoint sub-solution spaces at each internal 
node. It should be noted that this is not a requirement on a state space 

The General Method 
329 
tree. The only requirement is that every element of the solution space be 
represented by at least one node in the state space tree. 
The state space tree organizations described in example 7.4 will be called 
static trees. This terminology follows from the observation that the tree 
organizations are independent of the problem instance being solved. For 
some problems it is advantageous to use different tree organizations for 
different problem instances. In this case the tree organization is determined 
dynamically as the solution space is being searched. Tree organizations 
that are problem instance dependent are called dynamic trees. As an ex-
ample, consider the fixed tuple size formulation for the sum of subsets 
problem (example 7.4). Using a dynamic tree organization one problem 
instance with n = 4 may be solved using the organization given in figure 
7.4 while another problem instance with n = 4 may be solved using a tree 
in which at level 1 the partitioning corresponds to x2 = 1 and x2 = 0. At 
level 2 the partitioning could correspond to x 1 = 1 and x 1 = 0 while at 
level 3 it could correspond to X3 = 1 and XJ = 0, and so on. We shall see 
more of dynamic trees in sections 7.6 and 8.3. 
Once a state space tree has been conceived of for any problem, this 
problem may be solved by systematically generating the problem states, 
determining which of these are solution states and finally determining which 
solution states are answer states. There are two fundamentally different 
ways in which to generate the problem states. Both of these begin with the 
root node and generate other nodes. A node which has been generated 
and all of whose children have not yet been generated is called a live node. 
The live node whose children are currently being generated is called the E-
node (node being expanded). A dead node is a generated node that is either 
not to be expanded further or one for which all of its children have been 
generated. In both methods of generating problem states we will have a 
list of live nodes. In the first of these two methods as soon as a new child, C, 
of the current E-node, R, is generated, this child will become the new E-
node. R will become the E-node again when the subtree C has been fully 
explored. This corresponds to a depth first generation of the problem 
states. In the second state generation method, the E-node remains the E-
node until it is dead. In both methods, bounding functions will be used 
to kill live nodes without generating all their children. This will be done 
carefully enough so that at the conclusion of the process at least one answer 
node is always generated, or all answer nodes are generated if the problem 
requires us to find all solutions. Depth first node generation with bounding 
functions is called backtracking. State generation methods in which the E-
node remains theE-node until it is dead lead to branch-and-bound methods. 
The branch-and-bound technique is discussed in chapter 8. 

330 
Backtracking 
The nodes of Figure 7 .2 have been numbered in the order they would 
be generated in a depth first generation process. The nodes in Figures 7.3 
and 7.4 have been numbered according to two generation methods in which 
the E-node remains the E-node until it is dead. In Figure 7.3 each new 
node is placed into a queue. When all of the children of the current E-node 
have been generated, the next node at the front of the queue becomes the 
new E-node. In Figure 7.4 new nodes are placed into a stack instead of a 
queue. Current terminology is not uniform when referring to these two 
alternatives. Typically the queue method is called breadth first generation 
while the stack method is called D-search (depth search). 
Example 7 .5 ( 4-queens) Let us see how backtracking works on the 4-
queens problem of Example 7.3. As a bounding function we will use the 
obvious criteria that if (xi, x 2, ••• , x;) is the path to the current E-node 
then all children nodes with parent-child labelings x; + 1 are such that (xi, 
... , X;+t) represents a chessboard configuration in which no two queens 
are attacking. We start with the root node as the only live node. This be-
comes the E-node and the path is ().We generate one child. Let us assume 
that children are generated in ascending order. Thus, node number 2 of 
Figure 7.2 is generated and the path is now (1). This corresponds to placing 
queen 1 on column 1. Node 2 becomes the E-node. Node 3 is generated 
and immediately killed. The next node generated is node 8 and the path 
becomes (1, 3). Node 8 becomes the E-node. However, it gets killed as all 
of its children represent board configurations that cannot lead to an answer 
node. We backtrack to node 2 and generate another child, node 13. The 
path is now (1, 4). Figure 7.5 shows the board configurations as backtracking 
proceeds. Figure 7.5 shows graphically the steps that the backtracking 
algorithm goes through as it tries to find a solution. The dots indicate place-
ments of a queen which were tried and rejected because another queen was 
attacking. In (b) the second queen is placed on columns 1, 2 and finally 
settles on column 3. In (c) the algorithm tries all four columns and is unable 
to place the next queen on a square. Backtracking now takes place. In (d) 
the second queen is moved to the next possible column, column 4 and the 
third queen is placed on column 2. The boards in Figure 7.5 (e, f, g, h) show 
the remaining steps that the algorithm goes through until a solution is 
found. 
Figure 7.6 shows the part of the tree of Figure 7.2 that is actually gen-
erated. Nodes are numbered in the order in which they are generated. A 
node that gets killed as a result of the bounding function has a B under it. 
Contrast this tree with Figure 7.2 which contains 31 nodes. 
D 

The General Method 
331 
mm am 
•• 2 
2 
2 
• 
• 
• 
• 
• 
3 
(a) 
(b) 
(c) 
(d) 
(•) 
( f) 
(Q) 
(h) 
Figure 7 .5 Example of a backtrack solution to the four queens problem 
•2=2 
•2=3 
•2=4 
•2=4 
3 A 
19 
B 
B 
B 
B 
B 
B 
B 
Figure 7.6 Portion of tree of Figure 7.2 that is generated during backtracking 
With this example completed, we are now ready to present a precise 
formulation of the backtracking process. We will continue to treat back-
tracking in a general way. We shall assume that all answer nodes are to 
be found and not just one. Let (xi, x2, ... , x;) be a path from the root 
to a node in a state space tree. Let T(x 1, x 2, ... , x;) be the set of all pos-
sible values for x;+1 such that (xi, x2, ... , X;+1) is also a path to a problem 

• 
I 
332 
Backtracking 
state. We shall assume the existence of bounding functions B;+ 1 (expressed 
as predicates) such that B; + 1 (x i. x 2, ••• , x; + 1) is false for a path (x i. x 2, 
... , x; + 1) from the root node to a problem state only if the path cannot 
be extended to reach an answer node. Thus the candidates for position 
i + 1 of the solution vector X(l :n) are those values which are generated 
by T and satisfy B;+t· Algorithm 7.1, procedure BACKTRACK, is the 
general backtracking schema making use of T and B; + 1. 
procedure BACKTRACK(n) 
I /This is a program schema which describes the backtracking process./ I 
I I All solutions are generated in X(l :n) and printed as soon as they are/ I 
//determined. T(X(l), ... , X(k -
1)) gives all possible values of// 
I !X(k) given that X(l), ... , X(k -
1) have already been chosen./ I 
I /The predicates B k(X(l), ... , X(k)) determine those// 
I /elements X(k) which satisfy the implicit constraints./ I 
integer k, n; local X(l :n) 
k -
1 
while k > 0 do 
if there remains an untried X(k) such that 
X(k)E T(X(l), .. .,X(k -
l))andBk(X(l), ... ,X(k)) 
then if (X(l), ... , X(k)) is a path to an answer node 
then print (X(l), ... , X(k)) endif 
k -
k + 1 I I consider the next set/ I 
else k -
k -
1 I /backtrack to previous set/ I 
endif 
repeat 
end BACKTRACK 
Algorithm 7 .1 General backtracking method 
true 
Note that T( ) will yield the set of all possible values which can be placed 
as the first component, X(l), of the solution vector. X(l) will take on those 
values for which the bounding function B 1(X(l)) is true. Also note how the 
elements are generated in a depth first manner. k is continually incremented 
and a solution vector is grown until either a solution is found or no untried 
value of X(k) remains. When k is decremented, the algorithm must resume 
the generation of possible elements for the kth position which have not yet 
been tried. Therefore one must develop a procedure which generates these 
values in some order. If only one solution is desired, a return after the 
print will suffice. 

The General Method 
333 
Algorithm 7.2 presents a recursive formulation of the backtracking algo-
rithm. It is natural to describe backtracking in this way since it is essentially 
a postorder traversal of a tree (see section 6.1). This recursive version is 
initially invoked by 
call RBACKTRACK(l) 
procedure RBACKTRACK(k) 
I /This is a program schema which describes the backtracking process/ I 
//usingrecursion.Onentering,thefirstk - lvaluesX(l), .. .,X(k -1)// 
I !of the solution vector X(l:n) have been assigned./ I 
global n, X(l :n) 
for each X(k) such that 
X(k) E T(X(l), ... , X(k -
1)) and Bk (X(l), ... , X(k)) 
true do 
if (X(l), ... , X(k)) is a path to an answer node 
then print (X(l), ... , X(k)) endif 
call RBACKTRACK(k + 1) 
repeat 
end RBACKTRACK 
Algorithm 7 .2 Recursive backtracking algorithm 
The solution vector (xi, ... , Xn) is treated as a global array X(l:n). All 
of the possible elements for the kth position of the tuple which satisfy Bk 
are generated, one by one, and adjoined to the current vector (X(l), ... , 
X(k -
1)). Each time X(k) is attached a check is made to determine if a 
solution has been found. Then the algorithm is recursively invoked. When 
the for loop is exited, no more values for X(k) exist and the current copy of 
RBACKTRACK ends. The last unresolved call now resumes, namely the 
one which continues to examine the remaining elements assuming only k -
1 
values have been set. 
Note that when k exceeds n, T(X(l), ... , X(k -
1)) returns the empty 
set and hence the for loop is never entered. Note also that this program 
causes all solutions to be printed and assumes that tuples of various sizes 
may comprise a solution. If only a single solution is desired, then a flag 
may be added as a parameter to indicate the first occurrence of success. 
Efficiency 
The efficiency of both of the backtracking programs we've just seen 
depends very much upon 4 factors: (i) the time to generate the next 

' 
I 
• 
i 
I 
334 
Backtracking 
X(k); (ii) the number of X(k) satisfying the explicit constraints; (iii) the 
time for the bounding functions B;; and (iv) the number of X(k) satisfying 
the B; for all i. Bounding functions are regarded as good if they substan· 
tially reduce the number of nodes that are generated. However there is us· 
ually a trade off in that bounding functions that are good also take more 
time to evaluate. What is desired is a reduction in the overall computing 
time and not just a reduction in the number of nodes generated. For many 
problems, the size of the state space tree is too large to permit the genera-
tion of all nodes. Bounding functions must be used and hopefully at least 
one solution will be found in a reasonable time span. Yet for many prob-
lems (e.g. n-queens) no sophisticated bounding methods are known. 
One general principle of efficient searching is called rearrangement. For 
many problems the sets S; can be taken in any order. This suggests that 
all other things being equal, it is more efficient to make the next choice 
from the set with the fewest elements. This strategy doesn't pay off for 
the n-queens problem and examples can be constructed which prove this 
principle won't always work. But from an information-theoretic point of 
view, it can be shown that on the average a choice from the smallest set is 
more efficient. The potential value of this heuristic is exhibited in Figure 
7. 7 by the two backtracking search trees for the same problem. If we are 
able to remove a node on level one of Figure 7. 7(a) then we are effectively 
removing twelve possible 4-tuples from consideration. Whereas if we remove 
a node from level one of the tree in Figure 7. 7(b) then only eight tuples 
are eliminated. More sophisticated rearrangement strategies will be studied 
in conjunction with dynamic state space trees. 
As stated previously, there are four factors that determine the time re-
quired by a backtracking algorithm. Once a state space tree organization 
is selected, the first three of these are relatively independent of the problem 
instance being solved. Only the fourth, the number of nodes generated, 
varies from one problem instance to another. A backtracking algorithm on 
one problem instance might generate only O(n) nodes while on a different 
(and even closely related) instance it might generate almost all the nodes in 
the state space tree. If the number of nodes in the solution space is 2n or n ! 
the worst case time for a backtracking algorithm will generally be O(p(n)2n) 
or O(q(n)n!) respectively. p(n) and q(n) are polynomials inn. The impor-
tance of backtracking lies in its ability to solve some instances with large n 
in a very small amount of time. The only difficulty is in predicting the be-
havior of a backtracking algorithm for the problem instance we wish to 
solve. 
We can estimate the number of nodes that will be generated by a back-
tracking algorithm working on a certain instance I by using Monte Carlo 

The General Method 
335 
level 
0 
2 
3 
(a) 
0 
2 
3 
(b) 
Figure 7. 7 Rearrangement 
methods. The general idea in the estimation method is to generate a random 
path in the state space tree. Let X be a node on this random path. Assume 
that Xis at level i of the state space tree. The bounding functions are used 
at node X to determine the number, m;, of its children that do not get 
bounded. The next node on the path is obtained by randomly selecting one 
of these m; children that do not get bounded. The path generation ter-
minates at a node which is either a leaf or at a node all of whose children 
get bounded. Using these m;s we can estimate the total number, m, of 
nodes in the state space tree that will not get bounded. This number is par-
ticularly useful when all answer nodes are to be searched for. In this case 
all unbounded nodes need to be generated. When only a single solution 
is desired, m may not be such a good estimate for the number of nodes 
generated as the backtracking algorithm may arrive at a solution by gen-
erating only a small fraction of the m nodes. To estimate m from the m;s 
we need to make an assumption on the bounding functions. We shall 
assume that these functions are static. I.e. the backtracking algorithm 
does not change its bounding functions as it gathers information during 
its execution. Moreover, exactly the same function is used for aU nodes 
on the same level of the state space tree. This assumption is not true of 

# 
I 
336 
Backtracking 
most backtracking algorithms. In most cases the bounding functions 
get stronger as the search proceeds. In these cases, our estimate for m 
will be higher than one that could be obtained if the change in the bounding 
functions is taken into consideration. 
Continuing with the assumption of static bounding functions, we see that 
the number of unbounded nodes on level 2 is m 1. If the search tree is such 
that nodes on the same level have the same degree then we would expect 
each level 2 node to have on the average mi unbounded children. This 
yields a total of m 1 m 2 nodes on level 3. The expected number of unbounded 
nodes on level 4 is m1m2m3. In general, the expected number of nodes on 
level i + 1 is m 1m 2, •• ., m;. Hence, the estimated number, m, of un-
bounded nodes that will be generated in solving the given problem instance 
I ism = 1 + m1 + m1m2 + m1m2m3 + .... 
Procedure ESTIMATE is an algorithm to determine the value m. It 
selects a random path from the root of the state space tree. The function 
SIZE returns the size of the set Tk. The function CHOOSE makes a ran-
dom choice of an element in Tk. The desired sum is built using the variables 
m and r. 
procedure ESTIMATE 
I /This procedure follows a random path in a state space tree/ I 
I I and produces an estimate of the number of nodes in the tree .I I 
m -
1; r -
1; k -
1 
loop 
Tk -
{X(k): X(k) E T(X(l), ... , X(k -
1)) and Bk(X(l), ... , X(k))} 
if SIZE(T k) = 0 then exit endif 
r -
r * size (T k) 
m -m + r 
X(k) -
CHOOSE(Tk) 
k -
k + 1 
repeat 
retum(m) 
end ESTIMATE 
Algorithm 7 .3 Estimating the efficiency of backtracking 
We will use this estimator in later sections as we examine backtracking 
solutions to various problems. 

The 8-Queens Problem 
337 
A better estimate of the number of unbounded nodes that will be gen-
erated by a backtracking algorithm can be obtained by selecting several 
different random paths (typically no more than 20) and determining the 
average of these values. 
7 .2 THE 8-QUEENS PROBLEM 
Now we are ready to tackle the 8-queens problem via a backtracking solu-
tion. In fact we will trivially generalize the problem and consider an n x 
n chessboard and try to find all ways to place n nonattacking queens. We 
observed from the 4-queens problem that we can let (x1, ... , Xn) represent 
a solution where X; is the column of the ith row where the ith queen is 
placed. The X;S will all be distinct since no two queens can be placed in 
the same column. Now how do we test if two queens are on the same 
diagonal? 
Ifwe imagine the squares of the chessboard being numbered as the indices 
of the two dimensional array A(l:n, l:n) then we observe that for every 
element on the same diagonal which runs from the upper left to the lower 
right, each element has the same "row -
column" value. Also, every ele-
ment on the same diagonal which goes from the upper right to the lower 
left has the same "row + column" value. Suppose two queens are placed 
at positions (i,j) and (k, /).Then by the above they are on the same diagonal 
only if 
i - j = k - I or i + j = k + I. 
The first equation implies 
j-l=i-k 
while the second implies 
j-l=k-i 
Therefore two queens lie on the same diagonal if and only if lj - /I = 
Ii - kl. 
Procedure PLACE(k) returns a boolean value which is true if the kth 
queen can be placed at the current value of X(k). It tests both if X(k) is 
distinct from all previous values X(l), ... , X(k -
1) and also if there is 
no other queen on the same diagonal. Its computing time is O(k -
1). 

338 
Backtracking 
procedure PLACE(k) 
I /returns true if a queen can be placed in kth row and/ I 
I IX(k)th column. Otherwise it returns false.I I 
/IX is a global array whose first k values have been set.I I 
I I ABS(r) returns the absolute value of rl I 
global X(l: k ); integer i, k 
for i - 1 to k do 
if X(i) = X(k) 
I !two in the same column// 
or ABS(X(i) - X(k)) = ABS(i -
k) //in the same diagonal// 
then retum(false) 
endif 
repeat 
return( true) 
end PLACE 
Algorithm 7 .4 Can a new queen be placed? 
Using procedure PLACE we can now refine the general backtracking 
method as given by Algorithm 7.1 and give a precise solution to the n· 
queens problem. 
procedure NQUEENS(n) 
I /using backtracking this procedure prints all possible placements of/ I 
I In queens on an n x n chessboard so that they are nonattacking/ I 
integer k, n, X(l:n) 
X(l) -
0; k -
1 Ilk is the current row; X(k) the current column/ I 
wblle k > 0 do //for aU rows do// 
X(k) - X(k) + 1 I /move to the next column/ I 
whlleX(k) ~ n and not PLACE(k) do //can this queen be placed?// 
X(k) -
X(k) + 1 
repeat 
if X(k) ~ n 
! la position is found// 
then if k = n 
I !is a solution complete? I I 
then print(X) //yes, print the array/ I 
else k -
k + 1; X(k) -
0 
I Igo to the next row// 
endif 
else k -
k -
1 I /backtrack/ I 
endif 
repeat 
endNQUEENS 
Algorithm 7 .5 All solutions to the n·queens problem 

Sum of Subsets 
339 
At this point we might wonder how effective procedure NQUEENS is 
over the brute force approach. For an 8 x 8 chessboard there are ('t) pos-
sible ways to place 8 pieces or approximately 4.4 billion 8-tuples to ex-
amine. However by only allowing placements of queens on distinct rows 
and columns we require the examination of at most 8! or only 40,320 
8-tuples. 
We may use procedure ESTIMATE to estimate the number of nodes 
that will be generated by NQUEENS. Note that the assumptions which 
are needed for procedure ESTIMATE do hold for NQUEENS. The 
bounding function is static. No change is made to the function as the search 
proceeds. In addition, all nodes on the same level of the state space tree 
have the same degree. In Figure 7.8 we see five 8 x 8 chessboards which 
were created as a result of procedure ESTIMATE. As required the place-
ment of each queen on the chessboard was chosen randomly. With each 
choice we kept track of the number of columns a queen could legitimately 
be placed on. These numbers are listed in the vector beneath each chess-
board. The number following the vector represents the value that procedure 
ESTIMATE would produce from these sizes. The average of these five trials 
is 1625. The total number of nodes in the 8-queens state space tree is 
1 + j~O Cu (8 - o) = 69,281. 
So the estimated number of unbounded nodes is only about 2.34% of the 
total number of nodes in the 8-queens state space tree. (See the exercises 
for more ideas about the efficiency of NQUEENS.) 
7 .3 SUM OF SUBSETS 
Suppose we are given n distinct positive numbers (usually called weights) 
and we desire to find all combinations of these numbers whose sum is M. 
This is called the sum of subsets problem. Examples 7.2 and 7.4 showed 
how we could formulate this problem using either fixed or variable size 
tuples. We will consider a backtracking solution using the fixed tuple size 
strategy. In this case the element X(i) of the solution vector is either one 
or zero depending upon whether the weight W(i) is included or not. 
The children of any node in Figure 7.4 are easily generated. For a node 
at level i the left child corresponds to X(i) = 1 and the right to X(i) = 0. 

340 
Backtracking 
I 
I 
I 
•I 
2 
2 
3 
3 
4 
4 
5 
5 
6 
(8,5,4,3,2) = 1649 
( 8,5,3,1,2,1) = 769 
I 
I 
2 
2 
3 
3 
4 
4 
5 
5 
6 
7 
(8,6,4,2,1,1,1) •1401 
( 8,6,4, 3,2) • 197 7 
I 
2 
3 
4 
5 
6 
7 
8 
# 
I 
(8,5,3,2,2,1,1,1) • 2329 
Figure 7 .8 Five walks through the 8 queens problem plus estimates of the tree size 
-----· ·----- --
----~ 

Sum of Subsets 
341 
A simple choice for the bounding functions is Bk(X(l), ... , X(k)) = true iff 
k 
n 
E W(i)X(i) + E W(i) ~ M 
i=I 
i=k+ 1 
Clearly X(l), ... , X(k) cannot lead to an answer node if this condition is 
not satisfied. The bounding functions may be strengthened if we assume 
the W(i)s are initially in nondecreasing order. In this case X(l), ... ,X(k) 
cannot lead to an answer node if 
k 
E W(i)X(i) + W(k + 1) > M 
i=I 
The bounding functions we shall use are therefore: 
( 
k 
n 
Bk(X(l), ... , X(k)) = true iff 
;~i W(i)X(i) + i=Et W(i) ~ M 
k 
and ;~i W(i)X(i) + W(k + 1) ~ M) 
(7.1) 
Since our algorithm will not make use of B n, we need not be concerned by 
the appearance of W(n + 1) in this function. While we have now specified 
all that is needed to directly use either of the backtracking schemas, a 
simpler algorithm results if we tailor either of these schemas to the problem 
at hand. This simplification results from the realization that if X(k) = 1 
then 
E 
W(i)X(i) + E W(i) > M 
i= I 
i=k+ 1 
For simplicity we shall refine the recursive schema. The resulting algo-
rithm is SUMOFSUB. 

• i 
•I 
• 
i 
342 
Backtracking 
procedure SUMOFSUB(s, k, r) 
//find all subsets of W(l:n) that sum toM. The values of// 
k-1 
I IX(j), 1 $ j < k have already been determined. s = E W(j)X(j)// 
n 
j=l 
//and r = E W(j) The W(j)s are in nondecreasing order./ I 
i=k 
. 
n 
//It is assumed that W(l) $ Mand E W(i) 2: M.I I 
i= 1 
1 global integer M, n; global real W(l:n); global booleanX(l:n) 
2 real r, s; integer k, j 
I I generate left child. Note thats + W(k) $ M because Bk_ 1 = true/ I 
3 X(k) -
1 
4 ifs + W(k) = M 
//subset found// 
5 
then print (X(j), j -
1 to k) 
6 
7 
8 
9 
I /there is no recursive call here as W(j) > 0, 1 $ j $ n/ I 
else 
ifs+ W(k) + W(k + 1) $ Mthen I/Bk= true// 
call SUMOFSUB(s + W(k), k + 1, r -
W(k)) 
endif 
10 endif 
//generate right child and evaluate Bk! I 
11 
ifs + r -
W(k) 2: M and s + W(k + 1) ~ M 
I I Bk = true/ I 
12 
thenX(k) -
0 
13 
callSUMOFSUB(s, k + 1, r -
W(k)) 
14 endif 
15 
end SUMOFSUB 
Algorithm 7.6 
Recursive backtracking algorithm for sum of subsets problem 
k 
n 
Procedure SUMOFSUB avoids computing E W(i)X(i) and E W(i) 
i=I 
i=k+I 
each time by keeping these values in variables s and r respectively. The 
n 
algorithm assumes W(l) $ M and E W(i) 2: M. The initial call is call 
n 
i=l 
SUMOFSUB(O, 1, E W(i) ). It is interesting to note that the algorithm 
i=I 
does not explicitly use the test k > n to terminate the recursion. This test 
is not needed as on entry to the algorithms .,e. Mand s + r 2: M. Hence, 
r .,e. 0 and so k can be no greater than n. Also note that in line 7, since 
s + W(k) < Mand s + r 2: M it follows that r .,e. W(k) and hence k + 
1 $ n. Observe also that ifs + W(k) = M (line 4) then X(k + 1), ... , 
X(n) must be zerb. These zeros are omitted from the output of line 5. In 

Graph Coloring 
343 
k 
n 
line 7 we do not test for E W(i)X(i) + 
i=I 
E W(i) ~ M as we already know 
i=k+I 
s + r ~Mand X(k) = 1. 
Example 7.6 Figure 7.9 shows the portion of the state space tree generated 
by procedure SUMOFSUB while working on the instance n = 6, M = 30 
and W(l :6) = (5, 10, 12, 13, 15, 18). The rectangular nodes list the values 
of s, k, r on each of the calls to SUMOFSUB. Circular nodes represent 
points at which a subset with sum Mis printed out. At nodes A, Band C 
the output is respectively (1, 1, 0, 0, 1), (1, 0, 1, 1) and (0, 0, 1, 0, 0, 1). 
Note that the tree of Figure 7.9 contains only 23 rectangular nodes. The 
full state space tree for n = 6 contains 26 -
1 = 63 nodes from which 
calls could be made (this count excludes the 64 leaf nodes as no call need 
be made from a leaf). 0 
7.4 GRAPH COLORING 
Let G be a graph and m be a given positive integer. We want to discover 
if the nodes of G can be colored in such a way that no two adjacent nodes 
have the same color yet only m colors are used. This is termed the m-
colorability decision problem and it is discussed again in Chapter 11. The 
m-colorability optimization problem asks for the smallest integer m for 
which the graph G can be colored. This integer is referred to as the 
chromatic number of the graph. 
A graph is said to be planar iff it can be drawn in a plane in such a way 
that no two edges cross each other. A famous special case of the m-color-
ability decision problem is the 4-color problem for planar graphs. This 
problem asks the following question: given any map, can the regions be 
colored in such a way that no two adjacent regions have the same color yet 
only four colors are needed. This turns out to be a problem where graphs 
are very useful, because a map can easily be transformed into a graph. 
Each region of the map becomes a node and if two regions are adjacent 
then the corresponding nodes are joined by an edge. Figure 7 .10 shows 
a map with 5 regions and its corresponding graph. This map requires 4 
colors. For many years it was known that 5 colors were sufficient to color 
any map, but no map had ever been found which required more than 4 
colors. After several hundred years this problem has just recently been 
solved (to most people's satisfaction) by a group of mathematicians with the 
help of a computer. They showed that in fact 4 colors are sufficient. In 
this section we consider not only graphs which are produced from maps 

344 
Backtracking 
c 
Figure 7.9 Portion of state space tree generated by SUMOFSUB 
4 
15 
2 
I 
I 
-
I 
3 
Figure 7.10 A map and its planar graph representation 

Graph Coloring 
345 
but aU graphs. We are interested in determining an the different ways in 
which a given graph may be colored using at most m colors. 
Suppose we represent a graph by its adjacency matrix GRAPH(l:n, l:n), 
where G RAPH(i, j) = true if (i, j) is an edge of G and otherwise GRAPH 
(i,j) = false. We prefer to use Boolean values since the algorithm wiJl only 
be interested in whether or not an edge exists. The colors will be represented 
by the integers 1, 2, ... , m and the solutions will be given by the n-tuple 
(X(l), ... , X(n)) where X(i) is the color of node i. Using the recursive back-
tracking formulation as given in Algorithm 7.2 the resulting program is 
MCOLORING. The underlying state space tree used is a tree of degree 
m and height n + 1. Each node at level i has m children corresponding 
to them possible assignments to X(i), 1 ~ i ~ n. Nodes at level n + 1 are 
leaf nodes. Figure 7.11 shows the state space tree when n = 3 and m = 3. 
procedure MCOLORING(k) 
I /This program was formed using the recursive backtracking schema.I I 
//The graph is represented by its boolean adjacency matrix GRAPH(l:// 
I In, 1 :n ). All assignments of 1, 2, ... , m to the vertices of the graph/ I 
//such that adjacent vertices are assigned distinct integers are printed./ I 
I /k is the index of the next vertex to color/ I 
global integer m, n, X(l :n) boolean GRAPH(l :n, 1 :n) 
integer k 
loop 
I /generate all legal assignments for X(k)I I 
call NEXTVALUE(k) //assign to X(k) a legal color// 
if X(k) = 0 then exit endif //no new color possible// 
ilk = n 
then print(X) 
I I at most m colors are assigned to n vertices/ I 
else call MCOLORING(k + 1) 
endif 
repeat 
end MCOLORING 
Algorithm 7.7 
Finding all m·colorings of a graph 
Procedure MCOLORING is begun by first assigning the graph to its 
adjacency matrix, setting the array X to zero, and then invoking the state· 
ment call MCOLORING(l). 
Notice the similarity between this algorithm and the general form of the 
recursive backtracking procedure of Algorithm 7.2. Procedure NEXT-
VALUE produces the possible colors for X(k) after X(l) through X(k -
1) 

• 
• 
• 
• 
346 
Backtracking 
Figure 7.11 State space tree for MCOLORING when n = 3 and m = 3 
have been defined. The main loop of MCOLORING repeatedly picks an 
element from the set of possibilities, assigns it to X(k), and then calls 
MCOLORING recursively . 
procedure NEXTV ALUE(k) 
I IX(l), ... , X(k -
1) have been assigned integer values in the range/ I 
//[l, m] such that adjacent vertices have distinct integers. A value for// 
I IX(k) is determined in the range [O, m]. X(k) is assigned the next/ I 
I /highest numbered color while maintaining distinctness from the/ I 
//adjacent vertices of vertex k. If no such color exists then X(k) -
0.1 I 
global integer m, n, X(l:n) boolean GRAPH(l:n, l:n) 
integerj, k 
loop 
X(k) -
(X(k) + 1) mod (m + 1) 
I !next highest color// 
if X(k) = 0 then return endif I I all colors have been exhausted/ I 
forj -
1 ton do 
I I check if this color is distinct from adjacent colors/ I 
if GRAPH(k,j) and //if (k,j) is an edge// 
X(k) = X(j) //and if adjacent vertices have identical colors// 
then exit endif 
repeat 
ifj = n + 1 then return endif //new color found// 
repeat I /otherwise try to find another color/ I 
end NEXTVALUE 
Algorithm 7 .8 Generating a next color 

Graph Coloring 
347 
An upper bound on the computing time of Algorithm 7.7 may be arrived 
at by noticing that the number of internal nodes in the state space tree is 
n-1 
E mi. At each internal node, O(mn) time is spent by NEXTVALUE to 
i=O 
determine the children corresponding to legal colorings. Hence, the total 
n 
time is bounded by E min = n (mn+I -
1) I (m -
1) = O(nmn). 
i=l 
Figure 7.12 shows a simple graph containing four nodes. Below that is 
the tree which is generated by procedure MCOLORING. Each path to a 
leaf represents a coloring using at most 3 colors. Note that only twelve 
solutions exist with exactly 3 colors. 
1---------12 
4 1---------1 3 
•1 = 
Figure 7 .12 A 4 node graph and all possible 3 colorings 

•1 
I 
. 
' 
• 
• 
348 
Backtracking 
7.5 HAMILTONIAN CYCLES 
Let G = ( V, E) be a connected graph with n vertices. A Hamiltonian cycle 
(suggested by Sir William Hamilton) is a round trip path along n edges of 
0 which visits every vertex once and returns to its starting position. In 
other words if a Hamiltonian cycle begins at some vertex v 1 E G and the 
vertices of G are visited in the order v 1, v 2, ••• , Vn+ 1 then the edges (v;, 
V;+1) are in E, 1 ::5 i ::5 n and the v; are distinct except for v1 and Vn+1 
which are equal. 
The graph Gl of Figure 7.13 contains the Hamiltonian cycle 1, 2, 8, 7, 6, 
5, 4, 3, 1. The graph G2 of Figure 7.13 contains no Hamiltonain cycle. 
There seems to be no easy way to determine if a given graph contains a 
Hamiltonian cycle. We shall now look at a backtracking algorithm which 
finds all the Hamiltonian cycles in a graph. The graph may either be 
directed or undirected. Only distinct cycles will be output. 
GI: 
G2: 
Figure 7.13 Two graphs, one containing a Hamiltonian cycle 
The backtracking solution vector (x1, ... , Xn) is defined so that X; rep-
resents the ith visited vertex of the proposed cycle. Now all we need to do is 
determine how to compute the set of possible vertices for Xk if x1, ... , 
x k -1 have already been chosen. If k = 1 then X(l) can be any one of the 
n vertices. In order to avoid the printing of the same cycle n times we re-
quire that X(l) = 1. If 1 < k < n then X(k) can be any vertex v which is 
distinct from X(l), X(2), ... , X(k -
1) and v is connected by an edge to 
X(k -
1). X(n) can only be the one remaining vertex and it must be con-

Hamiltonian Cycles 
349 
nected to both X(n -
1) and X(l). We begin by presenting procedure 
NEXTVALUE(k) which determines a possible next vertex for the proposed 
cycle. 
procedure NEXTV ALUE(k) 
I IX(l), ... , X(k -
1) is a path of k -
1 distinct vertices. If X(k) = 01 I 
//then no vertex has as yet been assigned to X(k). After execution X(k)! I 
I !is assigned to the next highet numbered vertex which (i) does not/ I 
//already appear in X(l), ... , X(k -
1); (ii) is connected by an edge// 
I Ito X(k -
1). Otherwise X(k) = 0. If k = n then in addition X(k)/ I 
I !is connected to X(l).I I 
global integer n, X(l:n), boolean GRAPH(l:n, l:n) 
integer k,j 
loop 
X(k) -
(X(k) + 1) mod (n + 1) 
I !next vertex/ I 
if X(k) = 0 then return endif 
if GRAPH(X(k -
1), X(k)) //is there an edge// 
then forj -
1 to k -
1 do //check for distinctness// 
if X(j) = X(k) 
then exit 
I !exit this for loop/ I 
endif 
repeat 
ifj = k 
I !if true then the vertex is distinct/ I 
then if k < nor (k = n and GRAPH(X(n), 1)) then return 
endif 
endif 
endif 
repeat 
end NEXTVALUE 
Algorithm 7 .9 Generating a next vertex 
Using procedure NEXTVALUE we can particularize the recursive back-
tracking schema to find all Hamiltonian cycJes. 

• 
• 
• 
• 
350 
Backtracking 
procedure HAMILTONIAN(k) 
I /This procedure uses the recursive formulation of backtracking/ I 
I Ito find all the Hamiltonian cycles of a graph. The graph/ I 
//is stored as a boolean adjacency matrix in GRAPH(l:n, l:n). All// 
I I cycles begin at vertex 1./ I 
global integer X(l :n) 
local integer k, n 
loop //generate values for X(k)I / 
call NEXTVALUE(k) //assign a legal next vertex to X(k)! ! 
if X(k) = 0 then return endif 
if k = n 
then print (X, '1 ') I I a cycle is printed/ I 
else call HAMILTONIAN(k + 1) 
endif 
repeat 
end HAMILTONIAN 
Algorithm 7.10 Finding all Hamiltonian cycles 
This procedure is started by first initializing the adjacency matrix GRAPH 
(l:n, l:n), then setting X(2:n) -
0, X(l) -
1 and then executing call 
HAMILTONIAN(2). 
Recall from section 5.8 the traveling salesperson problem which asked 
for a "tour" which has minimum cost. This tour is a Hamiltonian cycle. 
For the simple case of a graph all of whose edge costs are identical, pro-
cedure HAMILTONIAN will find a minimum cost tour if a tour exists. 
If the common edge cost is c, the cost of a tour is en since there are n 
edges in a Hamiltonian cycle . 
7.6 KNAPSACK PROBLEM 
In this section we reconsider a problem which was defined and solved by a 
dynamic programming algorithm in Chapter 5, the zero-one knapsack 
optimization problem. Given n positive weights w;, n positive profits p;, 
and a positive number M which is the knapsack capacity, this problem 
calls for choosing a subset of the weights such that 
E w;x; :s; M 
and 
l:si:sn 
E p ;X; is maximized 
l:si:sn 
(7.2) 
The x's constitute a zero-one valued vector. 
---~- - - -

Knapsack Problem 
351 
The solution space for this problem consists of the 2n distinct ways to 
assign zero or one values to the x's. Thus the solution space is the same 
as that for the sum of subsets problem. Two possible tree organizations are 
possible. One corresponds to the fixed tuple size formulation (Figure 7.4) 
and the other to the variable tuple size formulation (Figure 7.3). Back-
tracking algorithms for the knapsack problem may be arrived at using 
either of these two state space trees. Regardless of which is used, bounding 
functions are needed to help kill some live nodes without actually expanding 
them. A good bounding function for this problem is obtained by using an 
upper bound on the value of the best feasible solution obtainable by ex-
panding the given live node and any of its descendants. If this upper bound 
is not higher than the value of the best solution determined so far then 
that live node may be killed. 
We shall continue the discussion using the fixed tuple size formulation. 
If at node Z the values of x;, 1 :s; i :s; k have already been determined, 
then an upper bound for Z can be obtained by relaxing the requirement 
x; = 0 or 1 to 0 ::5 x; :s; 1 fork + 1 :s; i :s; n and using the greedy algo-
rithm of section 4.3 to solve the relaxed problem. Procedure BOUND 
(p, w, k, M) determines an upper bound on the best solution obtainable 
by expanding any node Z at level k + 1 of the state space tree. The object 
weights and profits are W(i) and P(i). p = I;7=1 P(i)X(i) and it is assumed 
that P(i)IW(i) ~ P(i + l)IW(i + 1), 1 :s; i < n. 
procedureBOUND(p, w, k, M) 
I Ip, the current profit total// 
I /w, the current weight total/ I 
I I k, the index of the last removed item/ I 
I IM, the knapsack size/ I 
I /the result is a new profit/ I 
global n, P(l:n), W(l:n) 
integer k, i; real b, c, p, w, M 
b - p; c -
w 
fori-k + ltondo 
c -
c + W(i) 
if c < M then b -
b + P(i) 
endif 
repeat 
retum(b) 
end BOUND 
else retum(b + (1 - (c - M)!W(i))*P(i)) 
Algorithm 7 .11 A bounding function 

• 
• 
. 
' 
• 
352 
Backtracking 
From Algorithm 7.11 it follows that the bound for a feasible left child 
of a node Z is the same as that for Z. Hence, the bounding function need 
not be used whenever the backtracking algorithm makes a move to the 
left child of a node. Since the backtracking algorithm will attempt to 
make a left child move whenever given a choice between a left and right 
child, we see that the bounding function need be used only after a series 
procedure BKNAPl(M, n, W, P,jw,fp, X) 
I IM, the size of the knapsack/ I 
I In, the number of weights and profits/ I 
//W(l:n), the weights// 
//P(l:n), the corresponding profits; P(i)IW(i) ~ P(i + 1)/W(i + 1)// 
I lfw, the final weight of the knapsack/ I 
//fp, the final maximum profit/ I 
//X(l:n), either zero or one. X(k) = 0 if W(k) is not in the knapsack// 
//else X(k) = 111 
1 integer n, k, Y(l:n), i, X(l:n); real M, W(l:n), P(l:n),fw,fp, cw, cp; 
2 cw -
cp -
O; k -
l;fp -
-1 I !cw = current weight, cp = cur-// 
I /rent profit/ I 
loop 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
while k :s; n and cw + W(k) :s; M do //place k into knapsack// 
cw -
cw + W(k); cp -
cp + P(k); Y(k) -
l; k -
k + 1 
//place W(k) in the knapsack/ I 
repeat 
if k > n then fp -
cp;fw -
cw; k -
n; X -
Y //update so-// 
//lution// 
else Y(k) -
0 I IM is exceeded so object k does not fit/ I 
endif 
while BOUND(cp, cw, k, M) ::5 fp do //afterfp is set above,/ I 
//BOUND = fp// 
while k ;e 0 and Y(k) ;e 1 do 
k -
k -
1 //find the last weight included in the knapsack// 
repeat 
if k = 0 then return endif I /the algorithm ends here/ I 
Y(k) -
O; cw -
cw - W(k); cp -
cp - P(k)//remove the kth// 
//item// 
17 
repeat 
k-k+l 
18 
repeat 
19end BKNAPl 
Algorithm 7.12 Backtracking solution to the 0/1 knapsack problem 

Knapsack Problem 
353 
of successful left child moves (i.e. moves to feasible left children). The 
resulting algorithm is procedure BKNAPl (Algorithm 7.12). It was obtained 
from the iterative backtracking schema. 
n 
When f p ;C 
- 1, X(i), 1 :S i :S n is such that .E P(i)X(i) = fp. In the 
l= 1 
while loop of lines 4-6 successive moves are made to feasible left children. 
k-1 
Y(i). 1 :S i :S k is the path to the current node. cw = E W(i) Y(i) and 
k-1 
i=I 
cp = E P(i) Y(i). If at line 7, k > n then cp > fp as otherwise the path 
i= I 
to this leaf would have been terminated the last time the bounding function 
was used. If k :S n then W(k) does not fit and a right child move has to be 
made. So, Y(k) is set to 0 in line 8. If in line 10, BOUND :S fp, then the 
present path may be terminated as it cannot lead to a better solution than 
the best found so far. In lines 11-13 we trace back along the path to the 
most recent node from which an as yet untried move may be made. If there 
is no such node then the algorithm terminates in line 14. Otherwise Y(k), 
cw and cp are appropriately updated to correspond to a right child move. 
The bound for this new node is computed. The back-up process of lines 
10-16 continues until a move is made to a right child from which there is a 
possibility of obtaining a solution with value greater thanfp. Note that the 
bounding function of line 10 is not static as f p changes as more of the tree 
is searched. Hence the bounding function gets stronger dynamically. 
Example 7. 7 Consider the following instance of the knapsack problem: P 
(11, 21, 31, 33, 43, 53, 55, 65), W = (1, 11, 21, 23, 33, 43, 45, 55), M = 110, 
n = 8 
Figure 7.14 shows the tree that gets generated as various choices are 
made for the vector Y. The ith level of the tree corresponds to an assign-
ment of one or zero to Y(i), either including or excluding the weight W(i). 
The two numbers contained in a node are the weight (cw) and profit (cp) 
(reading downwards), given the assignments down to the level of the node. 
Nodes containing no numbers imply that the weight and profit is the same 
as their parent. The number outside each right child and outside the root 
is the bound corresponding to that node. The bound for a left child is the 
same~as that for its parent. The variablefp of Algorithm 7.12 is updated 
at each of the nodes A, B, C and D. Each time fp is updated, X is also 
updated. On termination fp = 159 and X = (1, 1, 1, 0, 1, 1, 0, 0). Of 
the 2 9 -
1 = 511 nodes in the state space tree only 33 are generated. This 
number could have been reduced to 26 by noticing that since all the P(i)'s 

., 
• 
• 
• 
354 
Backtracking 
Figure 7.14 Tree generated by Algorithm 7.12 
are integer, the value of all feasible solutions is also integer. Hence a better 
upper bound is LBOUND(p, w, k, M)j . Consequently the nodes E and F 
need not be expanded. D 
Algorithm BKNAPl can be improved further by realizing that the loop 
of lines 4 to 6 is essentially executed each time a call to BOUND is made 
in line 10. Hence, there should be no need to redo this work. To avoid the 
work done in lines 4 to 6 of BKNAPl we need to change BOUND to a 
function with side effects. The new algorithms BOUNDl and BKNAP2 
appear as Algorithms 7.13 and 7.14. All variables have the same meanings 
as in Algorithms 7.11 and 7.12. 

Knapsack Problem 
355 
procedure BOUNDl(p, w, k, M, pp, ww, i) 
I /pp and ww are the profit and weight corresponding to the last left/ I 
I /child move. i is the index of the first object that does not fit./ I 
I !It is n + 1 if no objects remain./ I 
global n, P(l:n), W(l:n), Y(l:n) 
integer k, i; realp, w, pp, ww, M, b 
pp - p; WW -
W 
for i -
k + 1 to n do 
if ww + W(i) s M then ww - ww + W(i); pp - pp + P(i); Y(i) -
1 
else return (pp + (M - ww)*P(i)IW(i)) 
end if 
repeat 
retum(pp) 
endBOUNDl 
Algorithm 7.13 Generating a bound 
procedure BKNAP2(M, n, W, P,jw,fp, X) 
//same as BKNAPl// 
integer n, k, Y(l:n), i,j, X(l:n) 
real W(l:n), P(l:n), M,fw,fp, pp, ww, cw, cp 
cw -
cp -
k -
O;fp -
-
1 
loop 
while BOUNDl(cp, cw, k, M, pp, ww,j) :S fp do 
while k ;e 0 and Y(k) ;e 1 do 
k-k-1 
repeat 
if k = 0 then return endif 
Y(k) -
O; cw -
cw -
W(k); cp - cp - P(k) 
repeat 
cp - pp; cw -
ww; k - j 
I I equivalent to loop of lines 4-6 in/ I 
//BKNAPl// 
ifk > n thenfp - cp;fw -
cw; k - n;X -
Y 
else Y(k) -
0 
endif 
repeat 
endBKNAP2 
Algorithm 7.14 Modified knapsack algorithm 

• 
• 
• 
• 
356 
Backtracking 
So far, all our backtracking algorithms have worked on a static state 
space tree. We shall now see how a dynamic state space tree may be used 
for the knapsack problem. One method for dynamically partitioning the 
solution space is based upon trying to obtain an optimal solution using the 
greedy algorithm of section 4.3. We first replace the integer constraint 
X; = 0 or 1 by the constraint 0 :s; X; :s; 1. This yields the relaxed problem 
max E p;x; 
1 !Si:Sn 
subject to E w; x; :s; M 
lsisn 
(7.3) 
0 :s; X; :s; 1, 
l:s;i:s;n 
If the solution generated by the greedy method has all x;s equal to zero or 
one, then it is also an optimal solution to the original zero-one knapsack 
problem. If this is not the case then exactly one X; will be such that 0 < X; 
< 1. We shall partition the solution space of (7.2) into two subspaces. In 
one X; = 0 and in the other X; = 1. Thus the left subtree of the state space 
tree will correspond to x; = 0 and the right to x; = 1. In general, at each 
node Z of the state space tree the greedy algorithm will be used to solve 
(7.3) under the added restrictions corresponding to the assignments already 
made along the path from the root to this node. In case the solution is all 
integer then an optimal solution for this node has been found. If not then 
there is exactly one x; such that 0 < x; < 1. The left child of Z corresponds to 
x; = 0 and the right to X; = 1. 
The justification for this partitioning scheme is that the noninteger x; is 
what prevents the greedy solution from being a feasible solution to the 
zero-one knapsack problem. So, we would expect to reach a feasible greedy 
solution quickly by forcing this x; to be integer. Choosing left branches to 
correspond to x; = 0 rather than x; = 1 is also justifiable. Since the greedy 
algorithm requirespi/wi ~ Pi+1IWj+1, we would expect most objects with 
low index (i.e. small j and hence high density) to be in an optimal filling 
of the knapsack. When X; is set to zero, we are not preventing the greedy 
algorithm from using any of the objects with j < i (unless xi has already 
been set to zero). On the other hand, when x; is set to 1, some of the x;s 
with .i < i will not be able to get into the knapsack. Therefore we expect 
to arrive at an optimal solution with X; = 0. So we wish the backtracking 
algorithm to try this alternative first. Hence the left subtree corresponds 
tox; = 0 . 
Example 7 .8 Let us try out a backtracking algorithm and the above dy-
namic partitioning scheme on the data of Example 7.7. The greedy solution 

Knapsack Problem 
357 
corresponding to the root node (i.e. Equation (7.3)) is x = (1, 1, 1, l, 1, 
21145, 0, 0). Its value is 164.88. The two subtrees of the root correspond 
to Xo = 0 and Xo = 1 respectively (Figure 7.15). The greedy solution at 
node 2 is x = (1, 1, 1, 1, 1, 0, 21145, 0). Its value is 164.66. The solution 
space at node 2 is partitioned using x 7 = 0 and x 7 = 1. The next £-node 
is node 3. The solution here has x 8 = 21155. The partitioning now is with 
xs = 0 and xs = 1. The solution at node 4 is all integer so there is no need 
to expand this node further. The best solution found so far has value 139 
and x = (1, 1, 1, 1, 1, 0, 0, 0). Node 5 is the next £-node. The greedy 
solution for this node isX = (1, 1, 1, 22/23, 0, 0, 0, 1). Its value is 159.56. 
The partitioning is now with x 4 = 0 and X4 = 1. The greedy solution at 
n0de 6 has value 156.66 and xs = 213. Next, node 7 becomes the £-node. 
The solution here is (1, 1, 1, 0, 0, 0, 0, 1). Its value is 128. Node 7 is not 
expanded as the greedy solution here is all integer. At node 8 the greedy 
solution has value 157.71 andx3 = 417. The solution at node 9 is all integer 
and has value 140. The greedy solution at node 10 is (1, 0, 1, 0, 1, 0, 0, 1). 
Its value is 150. The next £-node is node 11. Its value is 159.52 and X3 = 
20/21. The partitioning is now on X3 = 0 and X3 = 1. The remainder of 
the backtracking process on this knapsack instance is left as an exercise. D 
Experimental work cited in the references indicates that backtracking 
algorithms for the knapsack problem generally work in less time when using 
a static tree than when using a dynamic tree. The dynamic partitioning 
scheme is, however, very useful in the solution of integer linear programs. 
The general integer linear program is mathematically stated in (7.4) 
minimize E c i Xj 
lsjsn 
subject to E a;.;Xi ::5 b;, 
1 :S i :S m 
lsjsn 
and x jS are nonnegative integers 
(7.4) 
If the integer constraints on the x;s in (7.4) are replaced by the constraint 
x; ~ 0 then we obtain a linear program whose optimal solution has a value 
at least as large as the value of an optimal solution to (7.4). Linear pro-
grams may be solved using the simplex method (see the references). If 
the solution is not all integer then a noninteger x; is chosen to partition 
the solution space. Let us assume that the value of x; in the optimal so-
lution to the linear program corresponding to any node Z in the state space 
is v and v is not an integer. The left child of Z corresponds to X; :S Lv J 
while the right child of Z corresponds to x; ~ I v l . Since the resulting 
state space tree has a potentially infinite depth (note that on the path from 

• 
• 
• 
• 
358 
Backtracking 
Figure 7.15 Part of the dynamic state space tree generated in Example 7.7 
the root to a node Z the solution space can be partitioned on one x; many 
times as each x; can have as value any nonnegative integer) it is almost 
always searched using a branch-and-bound method (see chapter 8) . 
Testing 
Algorithm BKNAPl was programmed in Pascal by N. R. Venkatesh and 
run on a CDC Cyber 74 Computer. The objective of the experiment was to 
determine how the computing time required by BKNAPl varied with 
changes in the profits and weights and also with a change in n. The num-
ber of data sets one can design is potentially infinite. We shall report the 
results of the experiment for the following data sets: 
(i) 
random weights and profits in the range [l, 1000] 
(ii) random weights and profits in the range [l, 100] 
(iii) random weights in the range [ 1, 100] and p; = w; + 10 
(iv) 
random weights in the range [l, 100] andp; = 1.lw;. 

References and Selected Readings 
359 
In each of the above data sets M was set to be half the sum of the weights. 
For each n chosen, 10 different problem instances from each data set were 
generated. Table 7.1 gives the average and maximum times as well as the 
standard deviation in the times. These figures are reported only for selected 
values of n. For data set (iii) more than 2 minutes were needed to solve 
ten instances for each n, n > 40. As is readily observable the computing 
times for any fixed n are very much dependent on the actual weights and 
profits. 
In another test conducted by N. R. Venkatesh it was determined that 
the backtracking algorithm using a variable tuple size formulation required 
between 8%-12% less time than BKNAPl. 
Table 7.2 presents corresponding computing times for the dynamic pro-
gramming algorithm discussed in chapter 5. This algorithm was modified 
to include the heuristics described at the end of section 5.6. It was found 
that the addition of these heuristics reduced the time for DKNAP by more 
than 50% on data sets (i), (ii) and (iv). While there was a decrease in 
computing time for data set (iii), it wasn't quite as significant. In general, 
the dynamic programming algorithm performed worse than BKNAPl. 
This observation should be contrasted with the findings of an independent 
test conducted by Horowitz and Sahni (see the references). Their tests show 
that the divide-and-conquer dynamic programming algorithm discussed in 
the exercises of chapter 5 is superior to BKNAPl. The exercises explore 
the relative efficiency of BKNAP2 as well as strengthening of the bounding 
function. 
REFERENCES AND SELECTED READINGS 
An early modern account of backtracking is given in 
"An enumerative technique for a class of combinatorial problems" by R. J. Walker, 
Proceedings of Symposia in Applied Mathematics, vol. X, American Mathematical 
Society, Providence, R. I., 1960. 
Another description of the method plus a set of applications can be found in 
"Backtrack programming" by S. Golomb and L. Baumert, J. ACM, vol. 12, (1965), 
516-524. 

. . 
.. 
. . 
. . 
g 
r 
ll'r' i 
ll'r' 
Table 7.1 
Computing times for BKNAPl on Cyber 74 (Times in milliseconds) 
i 
(Table prepared by N. R. Venkatesh) 
Data set 
(i) 
(ii) 
(iii) 
(iv) 
-
n 
avg 
max 
std 
avg 
max 
std 
avg 
max 
std 
avg 
max 
std 
10 
2.15 
5 
1.1 
2.2 
4 
0.81 
7.6 
14 
4.14 
8.3 
28 
7.1 
20 
7.45 
15 
3.22 
7.2 
13 
2.93 
46.3 
261 
59.34 
7.8 
26 
5.68 
30 
14.5 
42 
9.02 
11.3 
25 
4.64 
217.8 
1026 
300.12 
8.1 
13 
2.53 
40 
16.05 
28 
4.71 
15.85 
27 
4.67 
1286.25 
11954 
2736.21 
10.25 
20 
4.55 
75 
44.8 
68 
11.18 
41.5 
60 
8.53 
21.0 
52 
8.69 
100 
81.5 
174 
32.62 
64.95 
111 
13.5 
31.7 
73 
11.12 
125 
107.9 
291 
47.17 
106.9 
163 
26.12 
39 
57 
7.69 
150 
166.85 
426 
70.02 
126.3 
187 
23.97 
53.4 
86 
11.11 
175 
191.5 
338 
54.06 
185.6 
262 
30.25 
62.45 
72 
4.5 
190 
227.6 
413 
70.44 
211.0 
333 
48.48 
73.9 
95 
7.51 

Data Set 
n 
avg 
10 
5.15 
20 
26.4 
30 
67.5 
40 
147.4 
75 
823.7 
100 
Table 7.2 Computing times for dynamic programming algorithm (Times in milliseconds) 
(table prepared by N. R. Venkatesh) 
(i) 
(ii) 
(iii) 
-
max 
std 
avg 
max 
std 
avg 
max 
std 
avg 
8 
1.06 
5.4 
10 
1.62 
19.4 
27 
2.8 
12.75 
51 
8.2 
26.3 
54 
8.98 
170.15 
245 
27 
134 
93 
14.8 
66.6 
94 
13.68 
528.6 
658 
69.6 
423.4 
244 
32.3 
135.5 
199 
27.8 
989.5 
1146 
95.2 
788.4 
1468 
249.6 
689.1 
1273 
190.6 
Excessive storage needed 
Excessive storage needed 
(iv) 
max 
22 
195 
571 
989 
std 
4.13 
34.85 
77.7 
102.4 
~ 
;-i 
l ' 
[ 
::a 
~ 
flj 
~ 
.... 

362 
Backtracking 
A set of backtrack programs in FORTRAN is presented in 
Combinatoral Algorithms by A. Nijenhuis and H. S. Wilf, Academic Press, New 
York, 1975. 
and a method for improving the efficiency of backtracking using assembly language 
macros is given in 
"Backtrack programming techniques" by J. R. Bitner and E. M. Reingold, C.ACM, 
vol. 18, (1975), 651-656. 
The technique for estimating the efficiency of a backtrack program was first 
proposed in 
"Combinatorial analysis and computers" by M. Hall and D. E. Knuth, American 
Mathematical Monthly, vol. 72, Part II, Feb. 1965, 21-28. 
and was later published in 
"Estimating the efficiency of backtrack programs" by D. E. Knuth, Mathematics 
of Computation, vol. 29, (1975), 121-136. 
The dynamic partitioning scheme for the zero-one knapsack problem was proposed 
by Greenberg and Hegerich. Their algorithm appears in 
"A branch-and-search algorithm for the knapsack problem" by H. Greenberg and 
R. Hegerich, Manag. Sci, 16(5), 327-332 (1970). 
Experimental results showing static trees to be superior for this problem may be 
found in 
"Computing partitions with applications to the knapsack problem" by E. Horowitz 
and S. Sahni, J.ACM, 21(2), 277-292 (1974). 
Data presented in the above paper by Horowitz and Sahni shows that the divide-
and-conquer dynamic programming algorithm for the knapsack problem is superior 
to BKNAPl. 
A good reference for the use of dynamic state space trees in the solution of integer 
linear programs is: 
"Integer Programming" by R. Garfinkel and G. Nemhauser, John Wiley, 1973. 

Exercises 
363 
A discussion of the simplex method for solving linear programs may be found in: 
"Linear Programming," by S. Gass, McGraw Hill, New York, 1969. 
EXERCISES 
1. Change the two backtracking control abstractions, Algorithms 7.1 and 7.2 so 
that they find only a single solution rather than all solutions. 
2. Using the rules given in section 1.3, translate the recursive backtracking 
algorithm, Algorithm 7 .2, into an equivalent iterative one. Then apply all 
of the simplifications you can think of and compare the result to Algorithm 7 .1. 
3. 
Procedure NQUEENS can be made more efficient by redefining procedure 
PLACE(k) so that it either returns the next legitimate column on which to 
place the kth queen or an illegal value. Rewrite both procedures so they imple-
ment this alternative strategy. 
4. For then-queens problem we observe that some solutions are simply reflections 
or rotations of others. For example when n = 4 the two solutions given below 
are equivalent under reflection. 
I 
I 
2 
2 
3 
3 
4 
4 
Figure 7 .16 Equivalent solutions to the 4-queens problem 
Observe that for finding inequivalent solutions the algorithm need only set 
X(l) = 2, 3, ... , r n/21 . Modify procedure NQUEENS so that only inequiv-
alent solutions are computed. 
5. 
Run then-queens algorithm devised above for n = 8, 9, 10. Tabulate the num-
ber of solutions your program finds for each value of n. 
6. Given an n x n chessboard, a knight is placed on an arbitrary square with 
coordinates (,x, y). The problem is to determine n 2 -
1 knight moves such 

364 
Backtracking 
that every square of the board is visited once if such a sequence of moves exists. 
Write an algorithm to solve this problem. 
7. 
Suppose you are given n men and n women and two n x n arraysP and Q such 
that P(i, j) is the preference of man i for woman j and Q(i, j) is the preference 
of woman i for man j. Give an algorithm which finds a pairing of men and 
women such that the sum of the product of the preferences is maximized. 
8. Prove that the size of the set of all subsets of n elements is 2n. 
9. 
Let A(l:n, l:n) be an n x n matrix. The determinant of A is the number 
det(A) = Esgn(s>a1 .• o>'l2..r<2> ... an..r<n> 
where the sum is taken over all permutations s(l), ... , s(n) of { 1, 2, ... , n} 
and sgn(s) is + 1 or - 1 according to whether s is an even or odd permutation. 
The permanent of A is defined as 
per(A) = Ea1,.,0>'12..r<2> ... an,.,<n> 
The determinant can be computed as a byproduct of Gaussian elimination 
requiring O(n 3) operations, but no polynomial time algorithm is known for 
computing permanents. Write an algorithm which computes the permanent of 
a matrix by generating the elements of s using backtracking. Analyze the time 
of your algorithm. 
10. 
Let MAZE{l:n, l:n) be a zero or one valued, two-dimensional array which 
represents a maze. A one means a blocked path while a zero stands for an 
open position. You are to develop an algorithm which begins at MAZE(l, 1) 
and tries to find a path to position MAZE(n, n). Once again backtracking 
will be necessary here. See if you can analyze the time of your algorithm. 
11. The assignment problem is usually stated in this way: there are n people to be 
assigned to n jobs. The cost of assigning the ith man to the jth job is COST 
(i,j). You are to develop an algorithm which assigns every job to a person and 
at the same time minimizes the total cost of the assignment. 
12. Let W = (5, 7, 10, 12, 15, 18, 20) and M = 35. Find all possible subsets of 
W which sum to M. Do this using SUMOFSUB. Draw the portion of the 
state space tree which is generated. 
13. 
Run procedure SUMOFSUB on the data M 
35 and (i) W = (5, 7, 10, 12, 

Exercises 
365 
15, 18, 20), (ii) W = (20, 18, 15, 12, 10, 7, 5), and (iii) W = (15, 7, 20, 5, 18, 
10, 12). Are there any discernible differences in the computing times? 
14. 
Write a backtracking algorithm for the sum of subsets problem using the state 
space tree corresponding to the variable tuple size formulation. 
15. 
[Programming Project] Write a program for the sum of subsets problem. Use 
backtracking and a fixed tuple size formulation and assume the objects are in 
nonincreasing order of weights. Program algorithm SUMOFSUB. Design 
several data sets to compare the performance of the two programs (see section 
7.6). Obtain computing times for the two programs. What conclusions can you 
draw? 
16. Run Algorithm 7.7, MCOLORING using as data the complete graphs of size 
n = 2, 3, 4, 5, 6 and 7. Let the desired number of colors be k = n and k = 
n 12. Tabulate the computing times for each value of n and k. 
17. 
Determine the order of magnitude of the worst case computing time for the 
backtracking procedure which finds all Hamiltonian cycles. 
18. 
Draw the portion of the state space tree generated by Algorithm 7.10 for the 
graph Gl of Figure 7.13. 
19. 
Generalize procedure HAMILTONIAN so that it processes a graph whose 
edges have costs associated with them and finds a Hamiltonian cycle with 
minimum cost. You may assume that all edge costs are positive. 
20. (i) 
Write a backtracking program for solving the knapsack optimization prob-
lem using the variable size tuple formulation. 
(ii) Draw the portion of the state space tree your algorithm will generate when 
solving the knapsack instance of Example 7. 7. 
21. 
Complete the state space tree of Figure 7.15. 
22. 
Write a backtracking algorithm for the knapsack problem using the dynamic 
state space tree discussed in section 7.6. 
23. 
[Programming project] (i) Program the knapsack algorithms BKNAPl, 
BKNAP2 and the algorithms of exercises 19 and 21. Run these four algorithms 
using the same data as in section. 7.6. Determine average times, maximum 
times and standard deviations as in Table 7.1. Which algorithm is expected to 
perform best? 
(ii) Now program the dynamic programming algorithm of chapter 5 for the 
knapsack problem. Use the heuristics suggested at the end of section 5.6. 

366 
Backtracking 
Obtain computing times and compare this algorithm with the backtracking 
algorithms. 
24. 
(i) Obtain a knapsack instance for which more nodes are generated by the 
backtracking algorithm using a dynamic tree than when using a static tree. 
(ii) Obtain a knapsack instance for which more nodes are generated by the 
backtracking algorithm using a static tree than when a dynamic tree is used. 
(iii) Strengthen the heuristic used in the backtracking algorithms of (i) by 
first building on array MINW(i) with the property that MINW(i) is the index 
of the object which has least weight amongst objects i, i + 1, ... , n. Now 
any £-node at which decision for x1, ... , Xi-t have been made and at which 
the unutilized knapsack capacity is less than W(MINW(i)) may be terminated 
provided the profit earned up to this node is no more than the maximum 
determined so far. Incorporate this into your programs of (i). Rerun the new 
programs on the same data sets and see what (if any) improvements result. 
This strengthening of the heuristic is due to Antonio Albano and Renzo Orsini 
and appears in their paper: "A tree search approach to the M-Partition and 
Knapsack Problem," Instituto di Scienze dell' Informazione, Pisa, Italy, 1977. 
25. This problem is called the postage stamp problem. Envision a country which 
issues n different denominations of stamps but allows no more than m stamps 
on a single letter. For given values of m and n write an algorithm which com-
putes the greatest consecutive range of postage values, from one on up, and 
all possible sets of denominations that realize that range. For example for n = 
4 and m = 5 the stamps with values (1, 4, 12, 21) allow the postage values 1 
through 71. Are there any other denominations of four stamps which have 
the same range? 
26. Here is a game one can buy in most toy stores. It's called Hi-Q. Thirty-two 
pieces are arranged on a board as shown in Figure 7 .17. Only the center position 
is unoccupied. A piece is only allowed to move by jumping over one of its 
neighbors into an empty space. Diagonal jumps are not permitted. When a piece 
is jumped it is removed from the board. Write an algorithm which determines a 
series of jumps so that all of the pieces except one are eventually removed, and 
that final piece ends up at the center position. 
27. 
Imagine a set of 12 plane figures each composed of five equal-sized squares. 
Each figure differs in shape from the others but together they can be arranged 
to make different size rectangles. In Figure 7.18 there is a picture of 12 pento-
minoes which are joined to create a 6 x 10 rectangle. Write an algorithm 
which finds all possible ways to place the pentominoes so that a 6 x 10 rec-
tangle is formed. 

Exercises 
367 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
CJ 
CJ 
CJ 
0 
CJ 
CJ 
CJ 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
D 
Figure 7.17 A Hi-Q board in its initial state 
2 
3 
4 
5 
6 
7 
8 
9 
II 
10 
12 
Figure 7.18 A pentominoe configuration 
28. Suppose a set of electrical components such as transistors are to be placed on 
a circuit board. We are given a connection matrix CONN where CONN(i, j) 
equals the number of connections between component i and component j and 
a matrix DIST where DIST(r, s) is the distance between position rand position 
s on the circuit board. The "wiring" of the board consists of placing each of 
n components at some location. The cost of a wiring is the sum of the products 
of CONN(i,j)*DIST(r, s) where component i is placed at location rand com-
ponentj is placed at locations. Compose an algorithm which finds an assign-
ment of components to locations which minimizes the total cost of the wiring. 
29. 
Suppose there are n jobs to be executed but only k processors which can work 

368 
Backtracking 
in parallel. The time required by job i is t;. Write an algorithm which deter-
mines which jobs are to be run on which processors and the order in which 
they should be run so that the finish time of the last job is minimized. 
30. 
Two graphs G(V, E) and H(A, B) are called isomorphic if there is a one-to-one 
onto correspondence of the vertices that preserves the adjacency relationships. 
More formally if f is a function from V to A and if (v, w) is an edge in E then 
(f(v),flw)) is an edge in H. Figure 7.19 shows two directed graphs which are 
isomorphic under the mapping that 1, 2, 3, 4, 5, goes to a, b, c, d, e. A brute 
force algorithm to test two graphs for isomorphism would try out all n ! possible 
correspondences and then test to see if adjacency was preserved. A back-
tracking algorithm can do better than this by applying some obvious pruning 
to the resultant state space tree. First of all we know that for a correspondence 
to exist between two vertices they must have the same degree. Another strategy 
one can use is to select at an early stage vertices of degree k for which the 
second graph has the fewest number of vertices of degree k. This exercise calls 
for devising an isomorphism algorithm which is based on backtracking that 
makes use of these ideas. 
Figure 7.19 Two isomorphic graphs 
31. 
A graph is called complete if all of its vertices are connected to all of the 
other vertices in the graph. A maximal complete subgraph of a graph is called 
a clique. By maximal we mean that this subgraph is contained within no other 
subgraph which is also complete. A clique of size k has (~) subcliques of size 
i, 1 s i s k. This implies that any algorithm which looks for a maximal 
clique must be careful about generating each subclique the fewest number of 
times that is possible. One way to generate the cliques is to extend a clique 

Exercises 
369 
of size m to size m + 1 and to continue this process by trying out all possible 
vertices. But this strategy will generate the same clique many times and this 
can be avoided by using the following rules. Given a clique X suppose node v 
is the first node which is added to produce a clique of size one greater. After 
the backtracking process examines all possible cliques which are produced 
from X and v, then no vertex which is adjacent to v need be added to X and 
examined. Let X and Y be cliques where X is properly contained i~ Y. If all 
cliques containing X and vertex v have been generated, then all cliques with 
Y and v can be ignored. Write a backtracking algorithm which generates 
the maximal cliques of a directed graph and makes use of these last rules for 
pruning the state space tree. 
32. 
Define the following terms: state space, tree organization, rearrangement, 
explicit constraints, implicit constraints, permutation tree, problem state, 
solution states, answer states, static trees, dynamic trees, live node, £-node, 
dead node, bounding functions. 

. 
' 
# 
I 
Chapter 8 
BRANCH-AND-BOUND 
8.1 THE METHOD 
This chapter makes extensive use of terminology defined in Section 7 .1. 
The reader is urged to review this section before proceeding. 
The term branch-and-bound refers to all state space search methods in 
which all children of the £-node are generated before any other live node 
can become the £-node. We have already seen two graph search strategies, 
BFS and D-search, in which the exploration of a new node cannot begin 
until the node currently being explored is fully explored. Both of these gen-
eralize to branch-and-bound strategies. In branch-and-bound terminology, 
a BFS-like state space search will be called FIFO (First In First Out) search 
as the list of live nodes is a first-in-first-out list (or queue). A D-search-
like state space search will be called LIFO (Last In First Out) search as the 
list of live nodes is a last-in-first-out list (or stack). As in the case of back-
tracking, bounding functions are used to help avoid the generation of sub-
trees that do not contain an answer node. 
Example 8.1 
(4-queens) Let us see how a FIFO branch-and-bound algo-
rithm would search the state space tree (Figure 7 .2) for the 4-queens pro-
blem. Initially, there is only one live node, node 1. This represents the case 
when no queen has been placed on the chessboard. This node becomes the 
£-node. It is expanded and its children, nodes 2, 18, 34 and 50 are gen-
erated. These nodes represent a chessboard with queen 1 in row 1 and 
columns 1, 2, 3 and 4 respectively. The only live nodes now are nodes 2, 18, 
34 and 50. If the nodes were generated in this order, then the next £-node 
is node 2. It is expanded and nodes 3, 8 and 13 are generated. Node 3 is 
immediately killed using the bounding function of example 7.5. Nodes 8 
and 13 are added to the queue of live nodes. Node 18 becomes the next £-
node. Nodes 19, 24 and 29 are generated. Nodes 19 and 24 are killed as a 
result of the bounding functions. Node 29 is added to the queue of live 
370 

The Method 
371 
nodes. The next £-node is node 34. Figure 8.1 shows the portion of the tree 
of Figure 7.2 that is generated by a FIFO branch-and-bound search. Nodes 
that get killed as a result of the bounding functions have a B under them. 
Numbers inside the node correspond to the numbers in figure 7 .2. Numbers 
outside the node give the order in which the nodes are generated by FIFO 
branch-and-bound. At the time the answer node, node 31, is reached the 
only live nodes remaining are nodes 38 and 54. A comparison of figures 
7 .6 and 8.1 indicates that backtracking is a superior search method for 
this problem. 
0 
18 
9 
e 
Answer node 
17 
61 
e 
F1gure 8.1 
Portion of 4-queens state space tree generated by FIFO branch-and-
bound 
LC-Search 
In both LIFO and FIFO branch -and-bound the selection rule for the next 
£-node is rather rigid and in a sense "blind". The selection rule for the 
next £-node does not give any preference to a node that has a very good 
chance of getting the search to an answer node quickly. Thus, in example 
8.1 when node 30 is generated, it should have become obvious to the search 
algorithm that this node will lead to an answer node in one move. However, 
the rigid FIFO rule requires first the expansion of all live nodes generated 
before node 30 was generated. 
The search for an answer node can often be speeded by using an "intel-
ligent" ranking function, c(. ), for live nodes. The next £-node is selected 
on the basis of this ranking function. If in the 4-queens example we use a 

372 
Branch-and-Bound 
ranking function that assigns node 30 a better rank than all other live 
nodes, then node 30 will become the £-node following node 29. The re-
maining live nodes will never become £-nodes as the expansion of node 30 
results in the generation of an answer node (node 31). 
The ideal way to assign ranks would be on the basis of the additional 
computational effort (or cost) needed to reach an answer node from the 
live node. For any node X, this cost could be (i) the number of nodes in the 
subtree X that need to be generated before an answer node is generated or 
more simply, (ii) it could be the number of levels the nearest answer node (in 
the subtree X) is from X. Using this latter measure, the cost of the root of 
the tree of figure 8.1 is 4 (node 31 is four levels from node 1). The cost of 
nodes (18 and 34); (29 and 35) and (30 and 38) is respectively 3, 2 and 1. 
The cost of all remaining nodes on levels 2, 3 and 4 is respectively greater 
than 3, 2 and 1. Using these costs as a basis to select the next £-node, the 
£-nodes are nodes 1, 18, 29 and 30 (in that order). The only other nodes 
to get generated are nodes 2, 34, 50, 19, 24, 32 and 31. It should be easy 
to see that if cost measure (i) is used then the search would always generate 
the minimum number of nodes every branch-and-bound type algorithm 
must generate. If cost measure (ii) is used then the only nodes to become 
£-nodes are the nodes on the path from the root to the nearest answer 
node. The difficulty with using either of these "ideal" cost functions is that 
computing the cost of a node will usually involve a search of the subtree 
X for an answer node. Hence, by the time the cost of a node is determined, 
that subtree has been searched and there is no need to explore X again. 
For this reason, search algorithms usually rank nodes based only on an 
estimate, g( · ), of their cost. 
Let g (X) be an estimate of the additional effort needed to reach an an-
swer node from X. Node X is assigned a rank using a function c ( · ) such 
that c (X) = f(h(X)) + g (X) where h (X) is the cost of reaching X from the 
root and/{·) is any nondecreasing function. At first, we may doubt the use-
fullness of using an/(·) other than/(h(X)) = 0 for all h(X). We can "jus-
tify" such an /( ·) on the grounds that the effort already expended in 
reaching the live nodes cannot be reduced and all we are concerned with 
now is minimizing the additional effort we will be spending to find an 
answer node. Hence, the effort already expended need not be considered. 
Using/{·) = 0 usually biases the search algorithm to make deep probes 
into the search tree. To see this note that we would normally expect 
g(Y) ~ g(X) for Ya child of X. Hence, following X, Y will become the 
£-node; then one of Ys children will become the £-node; next one of Ys 
grandchildren will become the £-node and so on. Nodes in subtrees other 
than the subtree X will not get generated until the subtree X is fully 

The Method 
373 
searched. This would be no cause for concern if g (X) was the true cost of 
X. Then, we would not wish to explore the remaining subtrees in any case 
(as X is guaranteed to get us to an answer node quicker than any other 
existing live node). However, g(X) is only an estimate of the true cost. So, 
it is quite possible that for two nodes Wand Z, g(W) < g(Z) and Z is 
actually much closer to an answer node than W It is therefore desirable 
not to over bias the search algorithm in favor of deep probes. By using 
f( · ) ~ 0 we can force the search algorithm to favor a node Z close to the 
root over a node W which is many levels below Z. This would reduce the 
possibility of deep and fruitless searches into the tree. 
A search strategy that uses a cost function c(X) = f(h(X)) + g(X) to 
select the next £-node would always choose for its next £-node a live node 
with least c( · ). Hence, such a search strategy is called an LC-search 
(Least Cost search). It is interesting to note that BFS and D-search are 
special cases of LC-search. If we use g(X) = 0 andf(h(X)) = level of node 
X then an LC-search generates nodes by levels. This is essentially the same 
as a BFS search. If f(h(X)) = 0 and g(X) ;:::: g{ Y) whenever Y is a child of 
X then the search is essentially a D-search. An LC-search coupled with 
bounding functions will be called an LC branch-and-bound search. 
In discussing LC-searches we will sometimes make reference to a cost 
function c( ·) defined as follows: if X is an answer node then c(X) is the 
cost (level, computational difficulty etc.) of reaching X from the root of 
the state space tree. If X is not an answer node then c (X) = oo if the sub-
tree X contains no answer node otherwise c (X) equals the cost of a mini-
mum cost answer node in the subtree X. It should be easy to see that c ( · ) 
withf(h(X) = h(X) is an approximation to c( · ). From now on c(X) will be 
referred to as the cost of X. 
The 15-puzzle-An Example 
The 15-puzzle (invented by Sam Loyd in 1878) consists of 15 numbered 
tiles on a square frame with a capacity of 16 tiles (Figure 8.2). We are given 
an initial arrangement of the tiles and the objective is to transform this 
arrangement into the goal arrangement of Figure 8.3(b) through a series of 
legal moves. The only legal moves are ones in which a tile adjacent to the 
empty spot (ES) is moved to ES. Thus from the initial arrangement of 
Figure 8.2(a), four moves are possible. We can move any one of the tiles 
numbered 2, 3, 5 or 6 to the empty spot. Following this move, other 
moves can be made. Each move creates a new arrangement of the tiles. 
These arrangements will be called the states of the puzzle. The initial and 
goal arrangements are called the initial and goal states. A state is reachable 

• 
I 
37 4 
Branch-and-Bound 
1 
3 
4 15 
1 
2 
3 
4 
2 
5 12 
5 
6 
7 
8 
7 
6 11 14 
9 10 11 12 
8 
9 10 13 
13 14 15 
-
(a) An arrangement 
(b) Goal arrangement 
(c) 
Figure 8.2 15-puzzle arrangements 
from the initial state iff there is a sequence of legal moves from the initial 
state to this state. The state space of an initial state consists of all states 
that can be reached from the initial state. The most straightforward way 
to solve the puzzle would be to search the state space for the goal state 
and use the path from the initial state to the goal state as the answer. It 
is easy to see that there are 16! (16! :::: 20.9 x 10 12) different arrangements 
of the tiles on the frame. Of these only one half are reachable from any 
given initial state. Indeed, the state space for the problem is very large. Be-
fore attempting to search this state space for the goal state, it would be 
worthwhile to determine whether or not the goal state is reachable from 
the initial state. There is a very simple way to do this. Let us number the 
frame positions 1-16. Position i is the frame position containing tile num-
bered i in the goal arrangement of Figure 8.2(b). Position 16 is the empty 
spot. Let POSITION(i) be the position number in the initial state of the tile 
numbered i. POSITION(16) will denote the position of the empty spot. For 
any state let LESS(z) be the number of tiles j such that j < i and POSI-
TION(j) > POSITION(i). For the state of Figure 8.2(a) we have, for exam-
ple, LESS{l) = 0, LESS(4) = 1 and LESS{12) = 6. Let X = 1 if in the 
initial state, the empty spot is at one of the shaded positions of Figure 8.2(c) 
and X 
= 0 if it is at one of the remaining positions. Then, we have 
the following theorem: 
Theorem 8.1 The goal state of Figure 8.2(b) is reachable from the initial 
state iff El~ 1 LESS(i) + X is even. 
Proof: Left as an exercise. 
D 
Theorem 8.1 may be used to determine whether or not the goal state is 
in the state space of the initial state. If it is, then we may proceed to 
determine a sequence of moves leading to the goal state. In order to carry 

The Method 
37 5 
out this search, the state space may be organized into a tree. The children 
of each node X in this tree represent the states reachable from state X by one 
legal move. It is convenient to think of a move as actually involving a move 
of the empty space rather than a move of a tile. The empty space, on each 
move, moves either up, right, down or left. Figure 8.3(a) shows the first 
three levels of the state space tree of the 15-puzzle beginning with the 
initial state shown in the root. Parts of levels 4 and 5 of the tree are also 
shown. The tree has been pruned a little. No node P has a child state 
that is the same as P's parent. The subtree eliminated in this way is al-
ready present in the tree and has root PARENT(P). As can be seen, there 
is an answer node at level 4. 
A depth first generation of the state space tree will generate the subtree 
of Figure 8.3(b) when next moves are attempted in the order: move the 
empty space up, right, down, left. It is clear from successive board con-
figurations that each move gets us farther from the goal rather than closer. 
The search of the state space tree is blind. It will take the leftmost path 
from the root regardless of the starting configuration. As a result, an 
answer node may never be found (unless the left most path ends in such a 
node). In a FIFO search of the tree of Figure 8.3(a), the nodes will be 
generated in the order numbered. A breadth first search will always find 
a goal node nearest to the root. However, such a search is also "blind" in 
the sense that no matter what the initial configuration, the algorithm 
attempts to make the same sequence of moves. A FIFO search always 
generates the state space tree by levels. 
What we would like, is a more "intelligent" search method. One that 
seeks out an answer node and adapts the path it takes through the state 
space tree to the specific problem instance being solved. With each node X 
in the state space tree we can associate a cost c(X). c(X) is the length of a 
path from the root to a nearest goal node (if any) in the subtree with root 
X. Thus, in Figure 8.3(a), c(l) = c(4) = c(lO) = c(23) = 3. When such a 
cost function is available, a very efficient search can be carried out. We be-
gin with the root as the £-node and generate a child node with c( ) value 
the same as the root. Thus children nodes 2, 3 and 5 are eliminated and 
only node 4 becomes a live node. This becomes the next E-node. Its first 
child, node 10, has c(lO) = c(4) = 34. The remaining children are not 
generated. Node 4 dies and node 10 becomes the £-node. In generating 
node lO's children, node 22 is killed immediately as c(22) > 3. Node 23 is 
generated next. It is a goal node and the search terminates. In this search 
strategy, the only nodes to become E-nodes are nodes on the path from the 
root to a nearest goal node. Unfortunately, this is an impractical strategy 
as it is not possible to easily compute the function c( · ) specified above. 

. . 
. . 
.. 
. . 
1 
~ 
0-. 
1 
2 
3 
4 
5 
6 
8 
9 10 7 11 
13 14 15 12 
ht 
down 
-
-
4 
1 
2 
4 
1 
2 
3 
4 
3 
4 
5 
1 
2 
3 
4 
5 
6 
3 
8 
5 
6 
8 
7 
8 
5 
6 
8 
9 10 7 11 
9 10 7 11 
11 
9 10 7 11 
f 
l r 
13 14 15 12 
13 14 15 12 
12 
13 14 15 12 
a. 
6 
ri~~t 
8 
10 
13 /. ~t 
5 
1 
2 
4 
1 
2 
4 
1 
2 
3 
1 
2 
3 
4 
1 
3 
4 
1 
2 
3 
3 
4 
1 
3 4 
1 2 3 4 
1 2 3 4 
5 
6 
3 
8 
5 
6 
3 
8 
5 
6 
8 
4 
5 
6 
8 11 
5 
8 
5 
6 
7 
7 
8 
5 2 6 8 
5ho 6 8 
5 6 8 
9 10 7 11 
9 10 7 11 
9 10 7 11 
9 10 7 
9 
9 10 15 
10 11 
911 7 11 
9 
7 1 9 0 7h1 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 
13 14 
14 15 12 
311 5 2 
3h4 5 2 
3 4 5 2 
1 6 
down 
do 
left 19 
eft 
22 
up 
down 
23 
1 
2 
4 
8 
1 
2 
3 
1 
2 
3 
4 
1 
2 
3 
4 
5 
6 
3 
5 
6 
8 
4 
5 
6 
7 
5 
6 
7 
8 
9 10 7 11 
9 10 7 11 
9 10 11 8 
9 10 11 12 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 14 15 
17 
18 
20 
1 
6 
2 
4 
1 
2 
4 
1 
2 
3 
4 
3 
4 
5 
3 
8 
5 
6 
3 
8 
5 
6 
8 11 
8 11 
9 10 7 11 
9 10 7 11 
9 10 7 12 
7 
13 14 15 12 
13 14 15 12 
13 14 15 
15 12 
edges are labeled according to the direction in which the empty space moves 
Figure 8.3(a) Part of the state space tree for the 15-puzzle 

1 
2 
3 
4 
5 
6 
1 
2 
3 
4 
1 
2 
4 right 1 
2 
4 
down 
1 
2 
4 
8 
down 
1 
2 
4 
8 
down 
1 
2 
4 
8 
5 
6 
8 
up 
5 
6 
3 
8 
5 
6 
3 
8 
5 
6 
3 
5 
6 
3 11 
5 
6 
3 11 
' 
' 
' 
9 10 7 11 
9 10 7 11 
9 10 7 11 
9 10 7 11 
9 10 7 
, 
9 10 7 12 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 14 15 12 
13 14 15 
left 
' 
l2 
11 
10 
9 
8 
7 
1 
2 
8 11 down 
1 
2 
8 
right 1 
2 
8 
1 
2 
4 
8 
1 
2 
4 
8 
1 
2 
4 
8 
5 
6 
4 
5 
6 
4 11 
5 
6 
4 11 
up 
5 
6 
11 
up 
5 
6 
3 11 
up 
5 
6 
3 11 
9 10 3 12 
9 10 3 12 
9 10 3 12 
9 10 3 12 ' 
9 10 
12" 
9 10 7 12 
13 14 
7 15 
13 14 7 15 
13 14 7 15 
13 14 7 15 
13 14 7 15 
13 14 
15 
Figure 8.3(b) 
First ten steps in a depth first search 
t 
a= t 
a. 
t,,; 
::j 

• 
• 
• 
• 
• 
378 
Branch-and-Bound 
We can arrive at an easy to compute estimate c(X) of c(X). We can 
write c(X) = f(X) + g(X) where f {X) is the length of the path from the 
root to node X and g{X) is an estimate of the length of a shortest path from 
X to a goal node in the subtree with root X. One possible choice for g(X) 
is: 
g(X) = number of nonblank tiles not in their goal position 
Clearly, at least g(X) moves will have to be made to transform state X to 
a goal state. It is easy to see that more than g(X) moves may be needed to 
achieve this. To see this, examine the problem state of Figure 8.4. g(X) = 1 
as only tile 7 is not in its final spot (the count for g(X) excludes the blank 
tile). However, the number of moves needed to reach the goal state is many 
more than g(X). c(X) is a lower bound on the value of c(X). 
An LC search of Figure 8.3(a) using c(X) will begin by using node 1 as 
the £-node. All its children are generated. Node 1 dies leaving behind the 
live nodes 2, 3, 4 and 5. The next node to become the E-node is a live node 
with least c(X). c(2) = 1 + 4, c(3) = 1 + 4, c(4) = 1 + 2 and c(5) = 1 
+ 4. Node 4 becomes the £-node. Its children are generated. The live nodes 
at this time are 2, 3, 5, 10, 11 and 12. c(lO) = 2 + 1, c(11) = 2 + 3, 
c(12) = 2 + 3. The live node with least c is node 10. This becomes the next 
£-node. Nodes 22 and 23 are next generated. Node 23 is determined to be 
a goal node and the search terminates. In this case LC-search was almost 
as efficient as using the exact function c( ). It should be noted that with 
a suitable choice for c( ), an LC-search will be far more selective than any 
of the other search methods we have discussed. 
1 
2 
3 
4 
5 
6 
8 
9 
10 11 
12 
13 
14 
15 
7 
Figure 8.4 Problem state 
Control Abstractions for LC-Search 
Let T be a state space tree and c( ) a cost function for the nodes in T. If 
X is a node in T then c(X) is the minimum cost of any answer node in the 
subtree with root X. Thus, c(T) is the cost of a minimum cost answer node 

The Method 
379 
in T. As remarked earlier, it will usually not be possible to find an easily 
computable function c( ) as defined above. Instead, a heuristic c( ) that es-
timates c ( ) will be used. This heuristic should be easy to compute and will 
generally have the property that if X is either an answer node or a leaf 
node then c(X) = c(X). Procedure LC (Algorithm 8.1) uses c to find an 
answer node. The algorithm uses two subalgorithms LEAST(X) and 
ADD(X) to respectively delete and add a live node from or to the list of 
live nodes. LEAST{X) finds a live node with least c( ). This node is deleted 
from the list of live nodes and returned in variable X. ADD(X) adds the 
new live node X to the list of live nodes. The list of live nodes will usually 
be implemented as a min-heap (Section 2.3). Procedure LC outputs the 
path from the answer node it finds to the root node T. This is easy to do 
if with each node X that becomes live, we associate a variable PARENT(X) 
which gives the parent of node X. When an answer node G is found, the 
path from G to T can be determined by following a sequence of PARENT 
values starting from the current £-node (which is the parent of G) and 
ending at node T. 
The correctness of algorithm LC is easy to establish. Variable E always 
points to the current £-node. By definition of LC-search, the root node is 
line procedure LC (T, c) 
I I search T for an answer node/ I 
O 
if Tis an answer node then output T; return; endif 
1 
E -
T 
/IE-node/I 
2 
initialize the list of live nodes to be empty 
3 
loop 
4 
for each child X of E do 
5 
if X is an answer node then output the path from X to T 
6 
return 
7 
endif 
8 
call ADD(X) I IX is a new live node/ I 
9 
PARENT(X) -
E 
//pointer for path to root/ I 
10 
repeat 
11 
if there are no more live nodes then print ('no answer node') 
12 
stop 
13 
endif 
14 
callLEAST(E) 
15 
repeat 
16 
endLC 
Algorithm 8.1 LC-search 

• 
• 
• 
• 
• 
380 
Branch-and-Bound 
the first £-node (line 1). Line 2 initializes the list of live nodes. At any time 
during the execution of LC, this list contains all live nodes except the£-
node. Thus, initially this list should be empty (line 2). The for loop of lines 
4-10 examines all the children of the £-node. If one of the children is an 
answer node then the algorithm outputs the path from X to T and termi-
nates. If a child of E is not an answer node then it becomes a live node. 
It is added to the list of live nodes (line 8) and its PARENT field set to E. 
When all the children of E have been generated, E becomes a dead node 
and line 11 is reached. This happens only if none of E's children is an 
answer node. So, the search must continue further. In case there are no 
live nodes left then the entire state space tree has been searched and no 
answer nodes found. The algorithm terminates in line 12. Otherwise, 
LEAST(X), by definition correctly chooses the next £-node and the search 
continues from here. 
From the preceding discussion, it is clear that LC terminates only when 
either an answer node is found or when the entire state space tree has been 
generated and searched. Thus, termination is guaranteed only for finite 
state space trees. Termination can also be guaranteed for infinite state 
space trees that have at least one answer node provided a "proper" choice 
for the cost function, c( ), is made. This is the case, for example, when 
c(X) > c(Y) for every pair of nodes X and Y such that the level number of 
Xis "sufficiently" higher than that of Y. For infinite state space trees with 
no answer nodes, LC will not terminate. Thus, it is advisable to restrict the 
search to find answer nodes with a cost no more than a given bound C. 
One should note the similarity between algorithm LC and algorithms 
for a breadth first search and D-search of a state space tree. If the list of 
live nodes is implemented as a queue with LEAST(X) and ADD(X) being 
algorithms to delete an element from and add an element to the queue 
then LC will be transformed to a FIFO search schema. If the list of live 
nodes is implemented as a stack with LEAST(X) and ADD(X) being algo-
rithms to delete and add elements to the stack then LC will carry out a 
LIFO search of the state space tree. Thus, the algorithms for LC, FIFO 
and LIFO search are essentially the same. The only difference is in the 
implementation of the list of live nodes. This is to be expected as the three 
search methods differ only in the selection rule used to obtain the next 
£-node. 
Properties of LC-Search 
Let us explore some properties of procedure LC. In many applications it is 
desirable to find an answer node that has minimum cost among all answer 

The Method 
381 
nodes. Does LC necessarily find an answer node G with minimum cost 
c(G)? The answer to this is no. Consider the state space tree of Figure 8.5. 
Square leaf nodes are answer nodes. Associated with each node is a pair of 
numbers. The upper number is the value of c and the lower the estimate 
c. Thus, c(root) = 10 and c(root) = 0. It is clear that LC will first generate 
the two children of the root and then the node with c( ) = 2 will become 
the E-node. The expansion of this node leads us to the answer node G 
with c{G) = c(G) = 20 and the algorithm terminates. The minimum cost 
answer node G has cost c(G) = 10. The reason LC did not get to the mini-
mum cost answer node is that the function c is such that there exist two 
nodes X and Y such that c(X) < c(Y) while c(X) > c(Y). As a result LC 
will choose node X as an £-node before node Y and possibly terminate 
finding an answer node which is a descendent of X. Even if c(X) < c(Y) for 
every pair of nodes X, Ysuch that c(X) < c(Y), procedure LC may not find 
a minimum cost answer node. As an example, consider the state space tree 
of Figure 8.6. Procedure LC terminates at node 3 while the min cost answer 
node is node 4. 
20 
20 
20 
2 
Cl) 
Cl) 
10 
0 
Cl) 
Cl) 
Figure 8.5 LC-search 
10 
10 

• 
• I ., 
• 
• 
• 
382 
Branch-and-Bound 
We can often find a c( 
) that is c;asy to compute and has the property that 
for each node X, C(X) ~ c(X). In this case, algorithm LC does not 
necessarily find a minimum cost answer node (Figure 8.5). When C(X) ~ 
c(X) for every node X and c(X) = c(X) for X an answer node, a slight 
modification to LC results in a search algorithm that terminates when a 
minimum cost answer node is reached. In this modification the search con-
tinues until an answer node becomes the £-node. The new algorithm is LCl 
(Algorithm 8.2) . 
line 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
procedure LCl (T, c) 
I I search T for a minimum cost answer node.I I 
E -
T 
I /first £-node/ I 
initialize the list of live nodes to be empty 
loop 
if E is an answer node then output path from E to T 
re tum 
endif 
for each child X of E do 
call ADD(X); PARENT(X) -
E 
repeat 
if there are no more live nodes then print ('no answer node') 
stop 
endif 
call LEAST(E) 
repeat 
endLCl 
Algorithm 8.2 LC - search for least cost answer node 
Theorem 8.2 Let c( ·) be such that c(X) ::s: c(X) fot every node X in a state 
space tree T and c(X) = c(X) for every answer node X in T. If algorithm 
LCl terminates in line 5 then the answer node found is of minimum cost . 

20 
20 
Figure 8.6 State space tree 
25 
25 
The Method 
383 
Proof: At the time the £-node Eis an answer node, c(E) ~ c(L) for every 
live node L on the list of live nodes. By assumption, c(E) = c(E) and c(L) 
s c(L) for every live node L. Hence c{E) ~ c(L) and so Eis a minimum 
cost answer node. 
D 
Bounding 
A branch-and-bound method searches a state space tree using any search 
mechanism in which all the children of the £-node are generated before 
another node becomes the £-node. We shall assume that each answer node 
X has a cost c(X) associated with it and that a minimum cost answer node 
is to be found. Three common search strategies are FIFO, LIFO and LC. 
(Another method, Heuristic search, is discussed in the exercises.) A cost 
function c(-) such that c(X) ~ c(X) is used to provide lower bounds on 
solutions obtainable from any node X. If U is an upper bound on the cost 
of a minimum cost solution then all live nodes X with c(X) > U may be 
killed as all answer nodes reachable from X have cost c(X) ~ c(X) > U. In 
case an answer node with cost U has already been reached then all live 
nodes with c(X) ~ U may be killed. The starting value for U may be ob-
tained by some heuristic or may be set to oo. Clearly, so long as the initial 
value for U is no less than the cost of a minimum cost answer node, the 
above rules to kill live nodes will not result in the killing of a live node 
that can reach a minimum cost answer node. Each time a new answer node 
is found, the value of U may be updated. 
Let us see how these ideas may be used to arrive at branch-and-bound 
algorithms for optimization problems. In this section we shall deal directly 
only with minimization problems. A maximization problem is easily con-

384 
Branch-and-Bound 
verted into a minimization problem by changing the sign of the objective 
function. We need to be able to formulate the search for an optimal solu-
tion as a search for a least cost answer node in a state space tree. To do 
this it is necessary to define the cost function c( ·) such that c(X) is mini-
mum for all nodes representing an optimal solution. The easiest way to do 
this is to use the objective function itself for c( · ). For nodes representing 
feasible solutions c(X) is the value of the objective function for that feasible 
solution. Nodes representing infeasible solutions have c(X) = oo. For 
nodes representing partial solutions c(X) is the cost of the minimum cost 
node in the subtree with root X. Since c(X) will in general be as hard to 
compute as solving the original optimization problem, the branch-and-
bound algorithm will use an estimate c(X) such that c(X) :s: c(X) for all X. 
In general then, the c( ·) function used in the branch-and-bound solution 
to optimization functions will estimate the objective function value and not 
the computational difficulty of reaching an answer node. In addition, to be 
consistent with the terminology used in connection with the 15-puzzle, any 
node representing a feasible solution (a solution node) will be an answer 
node. However, only minimum cost answer nodes will correspond to an 
optimal solution. Thus, answer nodes and solution nodes are indistinguish-
able. 
As an example optimization problem, consider the job sequencing with 
deadlines problem introduced in section 4.4. We shall generalize this prob-
lem to allow jobs with different processing times. We are given n jobs and 
one processor. Each job i has associated with it a three tuple (p;, d;, t;). 
Job i requires t; units of processing time. If its processing is not completed 
by the deadline d; then a penalty p; is incurred. The objective is to select a 
subset J of the n jobs such that all jobs in J can be completed by their dead-
lines. Hence, a penalty can be incurred only on those jobs not in J. J 
should be a subset such that the penalty incurred is minimum among all 
possible subsets J. Such a J is optimal. 
Consider the following instance: n = 4; (pi, di. t1) = (5, 1, 1); (p2, di, t2) 
= (10, 3, 2); (p3, dJ, t3) = (6, 2, 1) and (p4, d4, t4) = (3, 1, 1). The 
solution space for this instance consists of all possible subsets of the job 
index set { 1, 2, 3, 4 }. The solution space may be organized into a tree using 
either of the two formulations used for the sum of subsets problem (exam-
ple 7 .3). Figure 8. 7 corresponds to the variable tuple size formulation while 
Figure 8.8 corresponds to the fixed tuple size formulation. In both figures 
square nodes represent infeasible subsets. In Figure 8. 7 all nonsquare nodes 
are answer nodes. Node 9 represents an optimal solution and is the only 
minimum cost answer node. For this node]= {2, 3} and the penalty (cost) 

The Method 
385 
is 8. In Figure 8.8 only nonsquare leaf nodes are answer nodes. Node 25 
represents the optimal solution and is also a minimum cost answer node. 
This node corresponds to J = { 2, 3} and a penalty of 8. The costs of the 
answer nodes of Figure 8.8 is given below the nodes. 
Figure 8. 7 State space tree corresponding to variable tuple size formulation 
x •O 
0 4 
5 10 
5 6 
~ 
5 
9 
13 
19 
8 
II 
14 
15 
18 
21 
24 
Figure 8.8 State space tree corresponding to fixed tuple size formulation 
We can define a cost function c( ) for the state space formulations of 
Figures 8. 7 and 8.8. For any circular node X, c (X) is the minimum penalty 
corresponding to any node in the subtree with root X. c(X) = oo for a 
square node. In the tree of Figure 8. 7, c(3) = 8, c(2) = 9 and c(l) = 8. 
In the tree of Figure 8.8, c(l) = 8, c(2) = 9, c(S) = 13 and c(6) = 8. 
Clearly, c(l) is the penalty corresponding to an optimal selection J. 

386 
Branch-and-Bound 
A bound c(X) such that c(X) ::s: c(X) for all X is easy to obtain. Let S x 
be the subset of jobs selected for J at node X. If m = max{ i Ii E S x} then 
c(X) = E p; is an estimate for c(X) with the property c(X) ::s: c(X). For 
i<m 
ifSX 
each circular node, X, in Figures 8. 7 and 8.8 the value of c(X) is the num-
ber outside node X. For a square node c(X) = oo. A simple upper bound 
u(X) on the cost of a minimum cost answer node in the subtree X is u(X) = 
'f.;fSxP ;. Note that u(X) is the cost of the solution S x corresponding to node 
x. 
A FIFO branch-and-bound algorithm for the job sequencing problem 
can begin with U = oo (or U = E 1sisnP;) as an upper bound on the cost 
of a minimum cost answer node. Starting with node 1 as the £-node and 
using the variable tuple size formulation of Figure 8.7, nodes 2, 3, 4 and 5 
are generated (in that order). u(2) = 19, u(3) = 14, u(4) = 18 and u(S) 
= 21. U is updated to 14 when node 3 is generated. Since c(4) and c(S) 
are greater than U, nodes 4 and 5 get killed (or bounded). Only nodes 2 
and 3 remain alive. Node 2 becomes the next £-node. Its children, nodes 
6, 7, and 8 are generated. u(6) = 9 and so U is updated to 9. c(7) = 10 
> U and node 7 gets killed. Node 8 is infeasible and so it is killed. Next, 
node 3 becomes the £-node. Nodes 9 and 10 are now generated. u(9) = 8 
and so U becomes 8. c(lO) = 11 > U and this node is killed. The next 
£-node is node 6. Both its children are infeasible. Node 9's only child is 
also infeasible. The minimum cost answer node is node 9. It has a cost of 
8. 
When implementing a FIFO branch-and-bound algorithm, it is not eco-
nomical to kill live nodes with c(X) > U (or c(X) ~ U in case a node with 
cost U has been found) each time U is updated. This is so because live 
nodes are in the queue in the order in which they were generated. Hence, 
nodes with c(X) > U (or c(X) ~ U) are distributed in some random way 
in the queue. Instead, live nodes with c(X) > U (or c(X) ~ U) are killed 
when they are about to become £-nodes. Procedure FIFOBB is a program 
schema for a FIFO branch-and-bound algorithm. It uses a small positive 
constant E such that if for any two feasible nodes X and Y u(X) < u(Y), 
then u(X) < u(X) + E < u(Y). This E is needed to distinguish between 
the case when a solution with cost u(X) has been found and the case 
when such a solution has not been found. If the latter is the case then U 1s 
updated to min{ U, u(X) + E }. When U is updated in this way, live nodes 
Y with c( Y) ;;?; U may be killed. This does not kill the node that promised 
to lead to a solution with value ::s: U. We may dispense with this use of 
E if every feasible node X that is generated defines a feasible solution and 
u(X) = cost of X. This is true, for example, for Figure 8. 7 with u( ·) 

The Method 
387 
as defined above. FIFOBB also uses the subalgorithms ADDQ(X) and 
DELETEQ(X). These algorithms respectively add a node to a queue and 
delete a node from a queue. For every solution node X in the state space 
tree cost(X) is the cost of the solution corresponding to node X. FIFOBB 
assumes c(X) = oo for infeasible nodes and c(X) $ c(X) s; u(X) for fea-
sible nodes. 
line procedure FIFOBB (T, c, u, E, cost) 
I /Search T for a least cost answer (solution) node. It is/ I 
I I assumed that T contains at least one solution node and/ I 
! lc(X) s; c(X) s; u(X).I I 
1 
E -
T; PARENT(E) -
O; 
2 
if Tis a solution node then U -
min(cost(T), u(T) + E); ans -
T 
3 
else U -
u(T) + E; ans -
0 
4 
endif 
5 
initialize queue to be empty 
6 
loop 
7 
for each child X of E do 
8 
if c(X) < U then callADDQ(X); PARENT(X) - E 
9 
case 
10 
:X is a solution node and cost(X) < U: 
11 
U -
min (cost(X), u(X) + E) 
12 
ans -
X 
13 
:u(X) + E < U:U -
u(X) + E 
14 
endcase 
15 
endif 
16 
repeat 
17 
loop I I get next £-node/ I 
18 
if queue is empty then print ('least cost = ', U) 
19 
while ans -;C 0 do 
20 
print (ans) 
21 
ans -
PARENT(ans) 
22 
repeat 
23 
endif 
24 
call DELETEQ(E) 
25 
if c(E) < U then exit 
I /kill nodes with c(E) ~ U I I 
26 
repeat 
27 
repeat 
28 
end FIFO BB 
Algorithm 8.3 FIFO branch-and-bound to find minimum cost answer node 

388 
Branch-and-Bound 
LC Branch-and-Bound 
An LC branch-and-bound search of the tree of Figure 8. 7 will begin with 
U = oo and node 1 as the first £-node. When node 1 is expanded, nodes 
2, 3, 4 and 5 are generated in that order. As in the case of FIFO branch-
and-bound, U is updated to 14 when node 3 is generated and nodes 4 and 
5 are killed as c(4) > U and c(5) > U. Node 2 is the next £-node as 
c(2) = 0 while c(3) = 5. Nodes 6, 7 and 8 are generated. U is updated to 
9 when node 6 is generated. So, node 7 is killed as c(7) = 10 > U. Node 
8 is infeasible and so killed. The only live nodes now are nodes 3 and 
6. Node 6 is the next £-node as c(6) = 0 < c(3). Both its children are 
infeasible. Node 3 becomes the next £-node. When node 9 is generated U 
is updated to 8 as u(9) = 8. So, node 10 with c(lO) = 11 is killed upon 
generation. Node 9 becomes the next £-node. Its only child is infeasible. 
No live nodes remain. The search terminates with node 9 representing the 
minimum cost answer node. An LC branch-and-bound algorithm may also 
terminate when the next £-node E has c(E) ~ U. 
The control abstraction for LC branch-and-bound is LCBB. It operates 
under the same assumptions as FIFOBB. ADD and LEAST are algo-
rithms to respectively add a node to a min-heap and delete a node from a 
min-heap. 

The Method 
389 
line procedureLCBB (T, c, u, E, cost) 
I /search T for a least cost answer (solution) node. It is assumed// 
//that T contains at least one solution node and c(X) ::s: c(X) ::s:// 
I !u(X).11 
1 
E -
T; PARENT(£) -
0 
2 
if Tis a solution node then U -
min(cost(T), u(T) + E); ans -
T 
3 
else U -
u(T) + E; ans - 0 
4 
endif 
5 
initialize the list of live nodes to be empty 
6 
loop 
7 
for each child X of E do 
8 
if c(X) < U then callADD(X) 
9 
PARENT(X) - E 
10 
case 
11 
:Xis a solution node and cost(X) < U: 
12 
U -
min(cost (X), u(X) + E) 
13 
14 
:u(X) + E < U: U -
u(X) + E 
15 
endcase 
16 
endif 
17 
repeat 
18 
if there are no more live nodes or the next E-node 
19 
has c ~ U then print ('least cost = ', U) 
20 
while ans -,e 0 do 
21 
print (ans) 
22 
ans - PARENT( ans) 
23 
repeat 
24 
return 
25 
endif 
26 
call LEAST(£) 
27 
repeat 
28 
endLCBB 
Algorithm 8.4 LC branch-and-bound to find minimum cost answer node 

• 
I 
390 
Branch-and-Bound 
8.2 ZERO-ONE KNAPSACK PROBLEM 
In order to use the branch-and-bound technique to solve any problem, it is 
first necessary to conceive of a state space tree for the problem. We have 
already seen two possible state space tree organizations for the knapsack 
problem (Section 7.6). Still, we cannot directly apply the techniques of 
Section 8.1 since these were discussed with respect to minimization problems 
whereas the knapsack problem is a maximization problem. This difficulty 
is easily overcome by replacing the objective function E p;x; by the func-
tion - Ep;x;. Clearly, Ep;x; is maximized iff - Ep;x; is minimized. This 
modified knapsack problem is stated as (8.1). 
n 
minimize - E p;x; 
i=l 
n 
subject to E w;x; s; M 
i=l 
X; = 0 or 1, 
ls;is;n 
(8.1) 
We continue the discussion assuming a fixed tuple size formulation for 
the solution space. The discussion is easily extended to the variable tuple 
size formulation. Every leaf node in the state space tree representing an 
assignment for which E1,,;,,n w;x; s; Mis an answer (or solution) node. 
All other leaf nodes are infeasible. In order for a minimum cost answer 
node to correspond to any optimal solution, we need to define c(X) = 
-E 1s;sn p;X; for every answer node X. c(X) = oo for infeasible leaf nodes. 
For nonleaf nodes, c(X) is recursively defined to be min{ c(LCHILD(X)), 
c(RCHILD(X)) }. 
We now need two functions c(X) and u(X) such that c(X) s; c(X) s; u(X) 
for every node X. c( ·)and u( ·)satisfying this requirement may be obtained 
as follows. Let X be a node at level j, 1 s; j s; n + 1. At node X assign-
ments have already been made to x;, 1 s; i < j. The cost of these assign-
ments is - E1,,;<.;p;x;. So, c(X) s; - E1,,;<.;P;X; and we may use u(X) = 
- E1,,i<;p;x;. If q = 
- E1,,i<JP;X; then an improved upper bound func-
tion u(X) is u(X) = UBOUND(q, E1,,;<_;w;x;,J -
1, M) where UBOUND 
is defined by Algorithm 8.5. As for c(X), it is clear that - BOUND( - q, 
E1,,;<.;w;x;,J -
1, M) s; c(X) where BOUND is Algorithm 7.11. 

Zero-one Knapsack Problem 
391 
procedure UBOUND (p, w, k, M) 
/Ip, w, k and M have the same meaning as in Algorithm 7.11// 
I I W(i) and P(i) are respectively the weight and profit of the ith object/ I 
global W(l:n), P(l:n); integer i, k, n 
b - p; c -
w 
for i -
k + 1 to n do 
if c + W(i) ~ M then c -
c + W(i); b -
b - P(i) endif 
repeat 
return (b) 
end UBOUND 
Algorithm 8.5 Function u( ·) for knapsack problem 
LC Branch-and-Bound Solution 
Example 8.2 (LCBB) 
Consider the knapsack instance: n = 4; (pi, p2, 
p3, p4) = (10, 10, 12, 18); (wi. w2, w 3, w4) = (2, 4, 6, 9) and M = 15. 
Let us trace the working of an LC branch-and-bound search using c( ·)and 
u( ·) as defined above. We shall continue to use the fixed tuple size for-
mulation. The search begins with the root as the £-node. For this node, 
node 1 of Figure 8.9, we have c(l) = -38 and u(l) = -32. Since this is 
not a solution node, procedure LCBB sets ans = 0 and U = 
- 32 + E. 
The £-node is expanded and its two children, nodes 2 and 3 generated. c 
(2) = -38, c(3) = -32, u(2) = -32 and u(3) = -27. Both nodes are 
put onto the list of live nodes. Node 2 is the next £-node. It is expanded 
and nodes 4 and 5 generated. Both nodes get added to the list of live nodes. 
Node 4 is the live node with least c value and becomes the next E -node. 
Nodes 6 and 7 are generated. Assuming node 6 is generated first, it gets 
onto the list of live nodes. Next node 7 gets onto this list and U is updated 
to - 38 + E. The next E -node will be one of nodes 6 and 7. Let us assume 
it is node 7. Its two children are nodes 8 and 9. Node 8 is a solution node, 
U is updated to - 38 and node 8 is put onto the live nodes list. Node 9 
has c(9) > U and is killed immediately. Nodes 6 and 8 are two live nodes 
with least c. Regardless of which becomes the next £-node, c(E) ~ U and 
the search terminates with node 8 the answer node. At this time, the value 
- 38 together with the path 8, 7, 4, 2, 1 is printed out and the algorithm 
terminates. From the path one cannot figure out the assignment of values 
to the x;'s such that E p;x; = U. Hence, a proper implementation of pro-

• 
392 
Branch-and-Bound 
cedure LCBB will have to keep additional information from which the values 
of the X;s may be extracted. One way is to associate with each node a one 
bit field, TAG. The sequence of TAG bits from the answer node to the 
root give the X; values. Thus, we will have TAG(2) = TAG(4) = TAG(6) 
= TAG(8) = 1 and TAG(3) = TAG(S) = TAG(7) = TAG(9) = 0. The 
TAG sequence for the path 8, 7, 4, 2, l, is 1 0 1 1 and so X4 = l, XJ = 0, 
x2 = landx1 = 1. 
D 
-38 
-32 
-38 
-32 
6 
-38 
-38 
8 
-38 
-32 
3 
-36 
-22 
K -20 
-20 
9 
Upper number • c 
Lower number • u 
-32 
-27 
Figure 8.9 LC Branch-and-bound tree for Example 8.2 
In order to use procedure LCBB (Algorithm 8.5) to solve the knapsack 
problem, we need to specify (i) the structure of nodes in the state space 
tree being searched, (ii) how to generate the children of a given node; (iii) 
how to recognize a solution node; (iv) a representation of the list of live 
nodes and subalgorithms ADD and LEAST. The node structure needed 
will depend on which of the two formulations for the state space tree is 
being used. Let us continue with a fixed size tuple formulation. Each node 
X that is generated and put onto the list of live nodes must have a PARENT 
field. In addition, as noted in Example 8.2, each node should have a one 
bit TAG field. This field is needed to output the x; values corresponding 
to an optimal solution. In order to be able to generate X's children, we 
----------

Zero-one Knapsack Problem 
393 
shall need to know the level of node X in the state space tree. For this we 
shall use a field LEVEL. The left child of X is chosen by setting XLEVEL<X> 
= 1 and the right child by setting XLEVEL<x> = 0. In order to determine the 
feasibility of the left child, we need to know the amount of knapsack space 
available at node X. This can be determined either by following the path 
from node X to the root or by explicitly retaining this value in the node. 
We choose to retain this value in a field CU (capacity unused). The eval-
uation of c(X) and u(X) requires knowledge of the profit E 1,,; < LEVEL<x>P;X; 
earned by the filling corresponding to node X. This may be computed by 
following the path from X to the root. Alternatively, this value may be 
explicitly retained in a field PE. Finally, in order to determine the live node 
with least c value or to insert nodes properly into the list of live nodes, 
we need to know c(X). Again, we have a choice. c(X) may be stored ex-
plicitly in a field UB or may be computed when needed. Assuming all in-
formation is kept explicitly, we need nodes with six fields each: PARENT, 
LEVEL, TAG, CU, PE and UB. 
Using this six field node structure, the children of any live node X may 
be easily determined. The left child, Y, is feasible iff CU(X) ~ WLEVEUX>· 
In this case, PARENT(Y) = X; LEVEL(Y) = LEVEL(X) + 1; CU(Y) 
= CU(X) -
WLEvEux>; PE(Y) = PE(X) + PLEVEUX>; TAG(Y) = 1 and 
UB(Y) = UB(X). The right child may be generated similarly. Solution 
nodes are easily recognized too. Node X is a solution node iff LEVEL(X) = 
n + 1. 
We are now left with the task of specifying the representation of the 
list of live nodes. The functions we wish to perform on this list are: a) test 
if the list is empty b) add nodes and c) delete a node with least UB. We 
have seen a data structure that allows us to perform these three functions 
efficiently: a min-heap. If there are m live nodes then function a) can be 
carried out in 8(1) time while b) and c) require only O(log n) time. 
While the preceding discussion together with procedure LCBB result 
in a complete specification of an LC branch-and-bound algorithm for the 
knapsack problem, some improvement in algorithm efficiency results if 
we tailor LCBB to this specific problem. First, our tailored algorithm will 
compute -c and -u, which are nonnegative quantities, rather than c and 
u. In addition, we shall retain L = 
- U rather than U. Also, for any live 
node X, UB(X) = 
- c(X). These changes only result in minor changes in 
procedure LCBB. These changes are: 
i) 
The conditional of line 8 becomes if UB(X) > L then 
ii) 
the conditional of line 11 becomes :LEVEL(X) = n + 1 and 
PE(X) > L: 

394 
Branch-and-Bound 
iii) line 12 becomes L -
PE(X) 
iv) 
line 14 becomes : - u(X) -
E > L: L -
-u(X) -
E 
v) 
the conditional of line 19 becomes UB(X) :s: L 
vi) 
in line 25 the next E node is the live node with maximum UB. 
While these changes do not materially affect the running time of the 
resulting algorithm, they result in an algorithm that mirrors the "maximiza-
tion" formulation of the problem rather than the "minimization" formula-
tion (8.1). Thus L is a lower bound on the value of an optimal filling and 
UB(X) is an upper bound on the maximum filling obtainable from any so-
lution node in the subtree with root X. The remaining changes we shall 
make will reduce the running time of the search algorithm. The final algo-
rithm is procedure LCKNAP. 
LCKNAP makes use of the subalgorithms LUBOUND (Algorithm 8.6); 
NEWNODE (Algorithm 8. 7(a)); FINISH (Algorithm 8. 7(b)), INIT and 
GETNODE. LUBOUND computes - c( ·) and - u( · ). NEWNODE creates 
a new six field node, sets the fields appropriately and adds this node to the 
list of live nodes. Procedure FINISH prints out the value of the optimal 
solution as well as the objects· with x; = 1 in an optimal solution. INIT 
initializes the list of available nodes and also the list of live nodes. Since 
nodes are never freed by the algorithms, nodes may be used sequentially 
i.e. nodes 1 through m may be assigned in the order 1, 2, ... , m. GETNODE 
gets a free node. In accordance with conventions established in Section 
8.1, L will be the larger of the value of the best solution found so far and 
the highest lower bound computed by LUBOUND less E. E is a "small" 
positive number. 
The parameters to LCKNAP are P, W, M and N. N is the number of 
objects. P(i) and W(i), 1 :s: i :s: N are the profits and weights respectively. 
The objects are indexed such that P(i)! W(i) ;;:::: P(i + 1)/ W(i + 1), 1 :s: i 
< N. M is the capacity of the knapsack. Lines 1-5 initialize the list of free 
nodes and the root node of the search tree. This root node Eis the first£-
node. The loop of lines 6-24, successively examines each of the live nodes 
generated. The loop terminates either when there are no live nodes remaining 
(line 22) or when the next node, E, selected for expansion (the next £-node) 
is such that UB(E) :s: L (line 24). The termination at line 24 is valid as the 
node selected to be the next £-node is a live node with maximum UB(E). 
Hence, for all other live nodes X, UB(X) :s: UB(E) :s: L and none of them 
can lead to a solution node with value greater than L. Within this loop, 
the new £-node Eis examined. This node is either a leaf node (LEVEL(£) 
= n + 1) or it has exactly two children. In case it is a leaf, then it is a 
solution node and may be a new candidate for the answer node. Lines 9-11 
determine this. In case E is not a leaf node, its two children are generated. 

Zero-one Knapsack Problem 
395 
The left child, X, corresponds to x; = 1 and the right, Y, to x; = 0 where 
i = LEVEL(£). The left child is feasible (i.e. can lead to a solution node) 
iff there is enough space left in the knapsack to accommodate x; (cap ~ 
W(i)). In case this child is feasible and from the way the upper bound is 
computed by LUBOUND, it follows that UB(X) = UB(E). Since UB(E) > 
L (line 24) or L = LBB -
E < UBB (line 5) it follows that Xis to be added 
to the list of live nodes. Note that there is no need to recompute the lower 
and upper bound values for this node. They are the same as for £! The 
right child R is always feasible since E is feasible. For this node the lower 
and upper bound values may differ from those of node E. Hence, a call 
to LUBOUND is made (line 16). UB(R) = UBB. Node R may be killed if 
UB(R) ::s: L. Line 18 adds R to the list of live nodes when R is not be killed. 
Line 19 updates the value of L. 
procedureLUBOUND(P, W, rw, cp, N, k, LBB, UBB) 
I /rw is the remaining capacity and cp is the profit already earned/ I 
I I objects k, ... , N have yet to be considered/ I 
I /LBB = 
-
u(X) and UBB = -
c(X)! I 
LBB -
cp; c -
rw 
for i -
k to N do 
if c < W(i) then UBB -
LBB + c * P(i)!W(i) 
for} - i + 1 to N do 
endif 
if c ~ W(J) then c -
c -
W(J) 
endif 
repeat 
re tum 
LBB - LBB + P(J) 
c -
c -
W(i); LBB -
LBB + P(i) 
repeat 
UBB -LBB 
endLUBOUND 
Algorithm 8.6 Algorithm to compute lower and upper bounds 
procedure NEWNODE (par, lev, t, cap, prof, ub) 
I /create a new node I and add it to the list of live nodes./ I 
call GETNODE(I) 
PARENT(!) -
par; LEVEL(!) -
lev; TAG(!) -
t 
CU(!) -
cap; PE(!) - prof; UB(l) -
ub 
call ADD(!) 
endNEWNODE 
Algorithm 8. 7 (a) 
Creating a new node 

396 
Branch-and-Bound 
procedure FINISH(L, ANS, N) 
I /print solution/ I 
real L; global TAG, PARENT 
print ('VALUE OF OPTIMAL FILLING IS', L) 
print ('OBJECTS IN KNAPSACK ARE') 
for j -
N to 1 by - 1 do 
if TAG(ANS) = 1 then print(j) endif 
ANS - PARENT(ANS) 
repeat 
end FINISH 
Algorithm 8. 7 (b) 
Printing the answer 
-38 
-32 
-38 
-32 
-32 
-27 
2 
3 
-38 
-36 
-32 
-32 
-22 
-27 
K 
5 
6 
7 
-38 
-38 
-32 
-38 
Upper number 
:I: a 
Lower number • u 
x 
-20 
-20 
10 
II 
12 
13 
-28 
-28 
Figure 8.10 FIFO branch-and-bound tree for Example 8.3 

Zero-one Knapsack Problem 
397 
line procedure LCKNAP(P, W, M, N, E) 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
I /least cost branch-and-bound algorithm for the 0/1 knapsack/ I 
I /problem. A fixed tuple size formulation is used. It is assumed// 
//that P(l)!W(l) ~ P(2)/W(2) ~ ... ~ P(N)!W(N)l I 
realP(N), W(N),M,L,LBB, UBB,cap,prof 
integer ANS, X, N 
call !NIT I /initialize list of available nodes and list of live nodes/ I 
call GETNODE(E) I !root node/ I 
PARENT(E) -
O; LEVEL(E) -
l; CU(E) -
M; PE(E) -
0 
callLUBOUND (P, W, M, 0, N, 1, LBB, UBB) 
L - LBB -
E; UB(E) -
UBB 
loop 
i -
LEVEL(E); cap -
CU(E); prof -
PE(E) 
case 
:i = N + 1: 
I I solution node/ I 
if prof > L then L - prof; ANS - E 
endif 
:else: 
I IE has two children/ I 
if cap ~ W(i) then I /feasible left child/ I 
callNEWNODE(E, i + 1, I, cap - W(i),prqf + P(i), UB(E) 
endif 
I !see if right child is to live/ I 
callLUBOUND (P, W, cap, prof, N, i + 1, LBB, UBB) 
if UBB > L then I /right child is to live/ I 
call NEWNODE(E, i + 1, 0, cap, prof, UBB) 
L -
max (L, LBB -
E) 
endif 
endcase 
22 
if there are no more live nodes then exit endif 
23 
call LARGEST(£) I !next £-node is node with largest UB( ·)/I 
24 
until UB(E) $ L repeat 
25 
call FINISH(L, ANS, N) 
26 end LCKNAP 
Algorithm 8.8 LC-branch-and-bound algorithm for knapsack problem 

398 
Branch-and-Bound 
FIFO Branch-and-Bound Solution 
Example 8.3 (FIFOBB) Now, let us trace through procedure FIFOBB 
(Algorithm 8.3) using the same knapsack instance as in Example 8.2 and 
using the knapsack formulation (8.1). Initially the root node, node 1 of 
Figure 8.10, is the £-node and the queue of live nodes is empty. Since this 
is not a solution node, U is initialized to u(l) + E = -32 + E. We shall 
assume the children of a node are generated left to right. Nodes 2 and 3 
are generated and added to the queue (in that order). The value of U re-
mains unchanged. Node 2 becomes the next £-node. Its children, nodes 
4 and 5, are generated and added to the queue. Node 3, the next £-node, 
is expanded. It's children nodes are generated. Node 6 gets added to the 
queue. Node 7 is immediately killed as c(7) ;;:::: U. Node 4 is next expanded. 
Nodes 8 and 9 are generated and added to the queue. U is updated to 
u(9) + E = -38 + E. Nodes 5 and 6 are the next two nodes to become 
£-nodes. Neither is expanded as for each, c( ) ;;:::: U. Node 8 is the next 
£-node. Nodes 10 and 11 are generated. Node 10 is infeasible and so killed. 
Node 11 has c(ll) ~ U and so is also killed. Node 9 is next expanded. 
When node 12 is generated U and ans are updated to - 38 and 12 re-
spectively. Node 12 joins the queue of live nodes. Node 13 is killed before 
it can get onto the queue of live nodes as c(13) > U. The only remaining 
live node is node 12. It has no children and the search terminates. The 
value of U and the path from node 12 to the root is output. As in the case 
of Example 8.2 additional information is needed to determine the x; values 
on this path. 
D 
As in the case of LCKNAP, we shall tailor the FIFO branch-and-bound 
algorithm, FIFOKNAP to the problem at hand as well as to the state space 
tree formulation chosen. Since nodes will be generated and examined (i.e. 
become £-nodes) by levels, it is possible to keep track of the level of a node 
by the use of an end of level marker, '#', on the queue of live nodes. This 
leaves us with five fields per node: CU, PE, TAG, UB and PARENT. Pro-
cedure NNODE (Algorithm 8.9) generates a new live node, sets the fields 
and adds it to the queue of live nodes. 
-------------
----

Zero-one Knapsack Problem 
399 
procedure NNODE(.par, t, cap, prof, ub) 
I I create a new live node I and add it to the queue of live nodes/ I 
call GETNODE(I) 
PARENT(/) - par; TAG(/) -
t 
CU(!) -
cap; Pl(/) - prof; UB(I) -
ub 
call ADDQ(I) 
endNNODE 
Algorithm 8.9 Creating a new node 
Algorithm FIFOKNAP works with the maximization formulation of the 
knapsack problem. L represents a lower bound on the value of an optimal 
solution. Since no solution nodes can be reached until nodes at level N + 
1 are generated, we can dispense with E as used in LCKNAP. Lines 3-6 
initialize the list of free nodes, the root node E, L and the queue of live 
nodes. This queue initially contains the root node E and the end of level 
marker'#'. i is the level counter. During the algorithm, i will have as value 
the level number corresponding to the current £-node. Initially, i = 1. 
In each iteration of the main while loop (lines 7-26), all live nodes at level 
i are removed from the queue. In the loop of lines 8-23, nodes are removed 
from the queue one by one. In case the end of level marker is removed 
then the loop is exited (line 11). Otherwise, node E is expanded only if 
UB(E) ~ L. Lines 13-21 generate the left and right children of node E 
and are similar to the corresponding code in procedure LCKNAP. When 
we exit from the while loop, the only live nodes on the queue are nodes at 
level N + 1. Each of these is a solution node. A node with maximum PE 
value is an answer node. Such a node may be easily found by examining 
the PE values of the remaining live nodes one by one. Procedure FINISH 
(Algorithm 8. 7) prints out the value of an optimal solution as well as the 
objects that must be included into the knapsack in order to obtain this 
profit. 

400 
Branch-and-Bound 
procedure FIFOKNAP(P, W, M, N) 
//same function and assumptions as LCKNAP// 
1 
realP(N), W(N),M,L,LBB, UBB,E,prof,cap 
2 
integer ANS, X, N 
3 
call/NIT; i -
1 
4 
callLUBOUND(P, W, M, 0, N, 1, L, UBB) 
5 
call NNODE(O, 0, M, 0, UBB) 
I !root node/ I 
6 
call ADDQ('#') I /level marker/ I 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
while i ~ N do I !for all live nodes on level ii I 
loop 
call DELETEQ(E) 
case 
:E = '#': exit //end of level i.Exit to line 24// 
:UB(E) <:::: L: 
I IE is to live/ I 
cap -
CU(E); prof - PE(E) 
if cap <:::: W(i) then I /feasible left child/ I 
call NNODE(E, 1, cap -
W(i), prof + P(i), UB(E)) 
endif 
call LUBOUND(P, W, cap, prof, N, i + 1, LBB, UBB) 
if UBB <:::: L then I /right child is to live/ I 
call NNODE(E, 0, cap, prof. UBB) 
L -
max(L, LBB) 
endif 
endcase 
repeat 
call ADDQ('#') //end of level// 
25 
i -
i + 1 
26 
repeat 
27 
ANS -
live node X with PE(X) = L 
28 
call FINISH(L, ANS, N) 
29 end FIFO KNAP 
Algorithm 8.10 FIFO branch-and-bound knapsack algorithm 
At first, we may be tempted to discard FIFOKNAP in favor of LCKNAP. 
Our intuition leads us to believe that LCKNAP will examine fewer nodes 
in its quest for an optimal solution. However, we should keep in mind that 
insertions into and deletions from a heap are far more expensive (propor-
tional to the logarithm of the heap size) than the corresponding operations 

Zero-one Knapsack Problem 
401 
on a queue (8(1)). Consequently, the work done for each £-node is more 
in LCKNAP than in FIFOKNAP. Unless LCKNAP uses far fewer £-nodes 
than FIFOKNAP, FIFOKNAP will outperform (in terms of real computa· 
tion time) LCKNAP. 
We have now seen four different approaches to solving the knapsack 
problem: dynamic programming; backtracking; LC branch-and-bound and 
FIFO branch-and-bound. If we compare the dynamic programming algo-
rithm DKNAP (Algorithm 5.7) and FIFOKNAP we see that there is a cor-
respondence between generating the su>s and generating nodes by levels. 
S(i) contains all pairs (P, W) corresponding to nodes on level i + 1, 0 ~ i 
~ n. Hence, both algorithms generate the state space tree by levels. The 
dynamic programming algorithm, however, keeps the nodes on each level 
ordered by their profit earned (P) and capacity used (W) values. No two 
tuples have the same P or W value. In FIFOKNAP we may have many 
nodes on the same level with the same P or W value. It is not easy to 
implement the dominance rule of Section 5.5 into FIFOKNAP as nodes on 
a level are not ordered by their P or W values. However, the bounding 
rules can easily be incorporated into DKNAP. Towards the end of Section 
5.5 we discussed some simple heuristics to determine if a pair (P, W) E SU> 
should be killed. These heuristics are readily seen to be bounding functions 
of the type discussed here. Let the algorithm resulting from the inclusion 
of the bounding functions into DKNAP be DKNAPl. DKNAPl is expected 
to be superior to FIFOKNAP as it uses the dominance rule in addition 
to the bounding functions. In addition, the overhead incurred each time 
a node is generated is less. 
To determine which of the knapsack algorithms is best, it is necessary 
to program them and obtain real computing times for different data sets. 
Since the effectiveness of the bounding functions and the dominance rule 
is highly data dependent, we expect a wide variation in the computing time 
for different problem instances having the same number of objects n. In 
order to get representative times, it is necessary to generate many problem 
instances for a fixed n and obtain computing times for these instances. The 
generation of these data sets and the problem of conducting the tests is 
discussed in a programming project at the end of this chapter. The results 
of some tests may be found in the references to this chapter. 
Before closing our discussion of the knapsack problem, we briefly discuss 
a very effective heuristic to reduce a knapsack instance with large n to an 
equivalent one with smaller n. This heuristic, REDUCE, actually uses 
some of the ideas developed for the branch-and-bound algorithm. It clas-
sifies the objects { 1, 2, ... , n} into one of three categories Jl, I2, and 13. 
J1 is a set of objects for which x; must be 1 in every optimal solution. I2 

402 
Branch-and-Bound 
is a set for which Xi must be 0. 13 is { 1, 2, ... , n} - l1 - 12. Once ll, 
12, and 13 have been determined only the reduced knapsack instance: 
maximize E DiXi 
iEIJ° 
subject to E w;x; ~ M -
E W;Xi 
iE/3 
iE/1 
Xi = 0 or 1 
(8.2) 
has to be solved. From the solution to (8.2) an optimal solution to the 
original knapsack instance is obtained by setting Xi = 1 if i E l1 and Xi = 
0 ifi E 12. 
Procedure REDUCE makes use of two functions UBB(ll, 12) and LBB 
(Jl, 12). UBB(Jl, 12) is an upper bound on the value of an optimal solution 
to the given knapsack instance with the added constraints X; = 1 if i E l1 
and Xi = 0 if i E 12. LBB(ll, 12) is a lower bound under the constraints of 
l1 and 12. Note that UBB(ll, 12) and LBB(ll, 12) are the same as UBB 
and LBB of LUBOUND provided they are computed at a node X repre-
senting the assignment x; = 1 if i E Jl and x; = 0 if i E 12. Procedure 
REDUCE needs no further explanation. It should be clear that l1 and l2 
are such that from an optimal solution to (8.2) we can easily obtain an 
optimal solution to the original knapsack problem. 
procedure REDUCE (P, W, n, M, ll, 12) 
//variables are as described above. P(i)!W(i) <:::: P(i + l)!W(i + 1),11 
//1 ~ i < n// 
Il-12-<P 
L -
LBB(</J, </J) 
k -
largestj such that E W(i) < M 
lsisj 
for i -
1 to k do I I determine Ill I 
case 
: UBB (¢, {i}) < L :Jl - l1 U {i} 
:LBB(</J, {i}) > L :L - LBB(¢,{i}) 
end case 
repeat 
for i -
k + 1 ton do //determine 1211 
case 
:UBB( {i}, ¢) < L: l2 -
l2 U {i} 
:LBB( {i}, ¢) > L:L - LBB( {i}, ¢) 
endcase 
repeat 
end REDUCE 
Algorithm 8.11 
Reduction algorithm for knapsack problem 

Traveling Salesperson 
403 
The time complexity of REDUCE is O(n 2). Because the reduction pro-
cedure is very much like the heuristics used in DKNAP1, LCKNAP, 
BKNAPl and BKNAP2, the use of REDUCE does not decrease the overall 
computing time by as much as may be expected by the reduction in number 
of objects. These algorithms do dynamically what REDUCE does. The ex-
ercises explore the value of REDUCE further. 
8.3 TRAVELING SALESPERSON 
An O(n 2 2•) dynamic programming algorithm for the traveling salesperson 
problem was arrived at in Section 5.7. We shall now investigate branch-
and-bound algorithms for this problem. While the worst case complexity 
of these algorithms will not be any better than O(n 22"), the use of good 
bounding functions will enable these branch-and-bound algorithms to solve 
some problem instances in much less time than required by the dynamic 
programming algorithm. 
Let G = (V, E) be a directed graph defining an instance of the traveling 
salesperson problem. Let Cij be the cost of edge (i, j), Cij = oo if (i, j) 
~ E and let I VI 
= n. Without loss of generality, we may assume that 
every tour starts and ends at vertex 1. So, the solution space S is given by 
S = {l, 7r, ll 7r is a permutation of(2, 3, ... , n)}. ISi = (n -
1)!. The 
size of Smay be reduced by restricting S so that (1, i1, i2, ... , i.-1, 1) E 
S iff (ih ij+1) E £, 0 ~ j 
~ n -
1, io = i. = 1. S may be organized 
into a state space tree similar to that for the n-queens problem (see Figure 
7.2). Figure 8.11 shows the tree organization for the case of a complete 
graph with I VI = 4. Each leaf node L is a solution node and represents 
the tour defined by the path from the root to L. Node 14 represents the 
tourio = l,i1 = 3,i2 = 4,iJ = 2andi4 = 1. 
Figure 8.11 State space tree for the traveling salesperson problem with n = 4 
and i o = i 4 = 1 

404 
Branch-and-Bound 
In order to use LC-branch-and-bound to search the traveling salesperson 
state space tree, we need to define a cost function c( ·) and two other func-
tions c( ·) and u( ·) such that c(R) ~ c(R) ~ u(R) for all nodes R. c( ·) 
is such that the solution node with least c( ·) corresponds to a shortest tour 
in G. One choice for c( ·) is: 
{
length of tour defined by the path from the root to A if A is a leaf 
c(A) = 
cost of a minimum cost leaf in the subtree A if A is not a leaf 
A simple c( ·) such that c(A) ~ c(A) for all A is obtained by defining 
c(A) to be the length of the path defined at node A. For example, the 
path defined at node 6 of Figure 8.11 is io, i 1, i2 = 1, 2, 4. It consists of 
the edges (1, 2) and (2, 4). A better c(·) may be obtained by using the 
reduced cost matrix corresponding to G. A row (column) is said to reduced 
iff it contains at least one zero and all remaining entries are non-negative. 
A matrix is reduced iff every row and column is reduced. As an example 
of how to reduce the cost matrix of a given graph G, consider the matrix 
of Figure 8.12(a). This corresponds to a graph with five vertices. Since every 
tour on this graph includes exactly one edge (i,j) with i = k, 1 ~ k ~ 5 
and exactly one edge (i, j) withj = k, 1 ~ k ~ 5, subtracting a constant 
t from every entry in one column or one row of the cost matrix reduces 
the length of every tour by exactly t. A minimum cost tour remains a 
minimum cost tour following this subtraction operation. If t is chosen to 
be the minimum entry in row i (column j), then subtracting it from all 
entries in row i (column j) will introduce a zero into row i (column j). 
Repeating this as often as needed, the cost matrix may be reduced. The 
total amount subtracted from all the columns and rows is a lower bound 
on the length of a minimum cost tour and may be used as the c value for 
the root of the state space tree. Subtracting 10, 2, 2, 3, 4, 1 and 3 from 
rows 1, 2, 3, 4, 5 and columns 1 and 3 respectively of the matrix of Figure 
8.12(a) yields the reduced matrix of Figure 8.12(b). The total amount sub-
tracted is 25. Hence, all tours in the original graph have a length at least 25. 
With every node in the traveling salesperson state space tree we may 
associate a reduced cost matrix. Let A be the reduced cost matrix for node 
R. Let S be a child of R such that the tree edge (R, S) corresponds to in-
cluding edge (i, j) in the tour. If S is not a leaf then the reduced cost 
matrix for S may be obtained as follows (i) change all entries in row i and 
columnj of A to oo. This prevents the use of any more edges leaving vertex 
i or entering vertex j. (ii) set A(j, 1) to oo. This prevents the use of edge 
(j, 1). (iii) reduce all rows and columns in the resulting matrix except for 

Traveling Salesperson 
405 
00 
20 30 
10 
11 
00 
10 
17 
0 
1 
15 
00 
16 
4 
2 
12 
00 
11 
2 
0 
3 
5 
00 
2 
4 
0 
3 
00 
0 
2 
19 
6 
18 
00 
3 
15 
3 
12 
00 
0 
16 
4 
7 16 
00 
11 
0 
0 
12 
00 
(a) Cost Matrix 
(b) Reduced Cost 
Matrix 
L = 25 
Figure 8.12 An example 
rows and columns containing only oo. Let the resulting matrix be B. Steps 
(i) and (ii) are valid as no tour in the subtree S can contain edges of the 
type (i, k) or (k, j) or (j, 1) (except for edge (i, j)). If r is the total 
amount subtracted in step (iii) then c(S) = c(R) + A(i, j) + r. For leaf 
nodes c( ·) = c( ) is easily computed as each leaf defines a unique tour. 
For the upper bound function u, we may use u(R) = oo for all nodes R. 
Let us now trace the progress of the LC branch-and-bound algorithm, 
LCBB(Algorithm 8.4), on the problem instance of Figure 8.12(a). We shall 
use c and u as above. The initial reduced matrix is that of Figure 8.12(b) 
and U = oo. The portion of the state space tree that gets generated is 
shown in Figure 8.13. Starting with the root node as the £-node, nodes 
2, 3, 4, and 5 are generated (in that order). The reduced matrices corre-
sponding to these nodes are shown in Figure 8.14. The matrix of Figure 
8.14(b) is obtained from that of 8.12(b) by (i) setting all entries in row 1 
and column 3 to oo; (ii) the element at position (3, 1) is set to oo; (iii) 
column 1 is reduced by subtracting by 11. The c for node 3 is therefore 
25 + 17 (cost of edge (1, 3) in reduced matrix) + 11 = 53. The matrices 
and c values for nodes 2, 4, and 5 are obtained similarly. U is unchanged 
and node 4 becomes the next E node. Its children 6, 7 and 8 are generated. 
The live nodes at this time are nodes 2, 3, 5, 6, 7 and 8. Node 6 has least 
c value and becomes the next E node. Nodes 9 and 10 are generated. Node 
10 is the next E node. The solution node, node 11, is generated. The tour 
length for this node is c(l 1) = 28 and U is updated to 28. For the next 
£-node, node 5, c(5) = 31 > U. Hence, LCBB terminates with 1, 4, 2, 5, 
3, 1 as the shortest length tour. 
An exercise examines the implementation considerations for the algorithm 
described above. A different LC branch-and-bound algorithm may be 

406 
Branch-and-Bound 
arrived at by considering a different tree organization for the solution space. 
This organization is arrived at by regarding a tour as a collection of n 
edges. If G = (V, E) has e edges then every tour contains exactly n of the 
e edges. However, for each i, 1 ~ i ~ n there is exactly one edge of the 
form (i, j) and one of the form (k, i) in every tour. A possible organization 
for the state space is a binary tree in which a left branch represents the 
inclusion of a particular edge while the right branch represents the exclusion 
of that edge. Figures 8.lS(b) and (c) represent the first two levels of two 
possible state space trees for the three vertex graph of Figure 8.lS(a). As is 
true of all problems, many state space trees are possible for a given prob-
lem formulation. Different trees differ in the order in which decisions are 
made. Thus, in Figure 8.lS(b) we first decide the fate of edge (1, 3) 
while in Figure 8.lS(c) we first decide the fate of edge (1, 2). Rather 
than use a static state space tree, we shall now consider a dynamic state 
space tree (see Section 7.1). This will also be a binary tree. However, the 
order in which edges will be considered will depend on the particular prob-
lem instance being solved. We shall compute c in the same way as we did 
using the earlier state space tree formulation. 
As an example of how LCBB would work on the dynamic binary tree 
formulation, consider the cost matrix of Figure 8.12(a). Since a total of 25 
needs to be subtracted from the rows and columns of this matrix in order 
Numbers outside the node ore c voluH 
Figure 8.13 State space tree generated by procedure LCBB. 
-------· 

Traveling Salesperson 
407 
[j 
00 
00 
00 
]] r1 
00 
00 
00 
]J [~ 
00 
00 
00 
]] 
00 
11 
2 
00 
00 
2 
00 
11 
00 
00 
00 
0 
3 
00 
0 
3 
00 
00 
00 
12 
00 
3 
00 
00 
3 
12 
00 
11 
00 
0 12 
0 
00 
12 
11 
0 
0 
00 
a) path 1,2; node 2 
b) path 1,3; node 3 
c) path l,4;node 4 
[~ 
00 
00 
00 
~] [~ 
00 
00 
00 
~] [~ 
00 
00 
00 
]] 
00 
9 
0 
00 
11 
00 
00 
00 
00 
3 
00 
0 
00 
00 
00 
1 
00 
00 
12 
0 
9 
00 
00 
00 
00 
00 
00 
00 
00 
0 
0 
12 
00 
0 
00 
0 
00 
00 
d) path 1,5; node 5 
e) path 1,4,2; node 6 
f) path 1,4,3; node 7 
[~ 
00 
00 
00 
~] [~ 
00 
00 
00 
~] [~ 
00 
00 
00 
~] 
00 
0 
00 
00 
00 
00 
00 
00 
00 
3 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
00 
0 
0 
00 
00 
00 
00 
00 
0 
00 
g) path 1,4,5; node 8 
h) path 1,4,2,3; node 9 
i) path 1,4,2,5; node IO 
Figure 8.14 Reduced cost matrices corresponding to nodes in Figure 8.13 
include 
<1,2> 
4 
exclude 
include 
<1,3> 
<1,2> 
<l,2> 
include 
<3,1> 
7 
4 
exclude 
<2,3> 
7 
(a) Graph 
(b) Part of a state space tree 
(c) Part of a state space tree 
Figure 8.15 An example 
to obtain the reduced matrix of Figure 8.12(b), all tours have a length 
at least 25. This fact is represented by the root of the state space tree of 
Figure 8.16. Now, we must decide which edge to use to partition the solution 
space into two subsets. If edge (i, j) is used then the left subtree of the 
root will represent all tours including edge (i, j) and the right subtree 

408 
Branch-and-Bound 
will represent all tours that do not include edge (i, j). If an optimal tour 
is included in the left subtree then only n -
1 edges remain to be selected. 
If all optimal tours lie in the right subtree then we have still to select n 
edges. Since the left subtree selects fewer edges, it should be easier to find 
an optimal solution in it than to find one in the right subtree. Consequently, 
we would like to choose as the partitioning edge an edge (i, j) that has 
highest probability of being in an optimal tour. Several heuristics for deter· 
mining such an edge may be formulated. A selection rule that is commonly 
used is: select that edge which results in a right subtree that has highest 
c value. The logic behind this is that we will soon have right subtrees (per-
haps at lower levels) for which the c value is higher than the length of an 
optimal tour. Another possibility is to choose an edge such that the dif· 
ference in the c values for the left and right subtrees is maximum. Other 
selection rules are also possible. 
25 I 
~c~de 
exc~de 
<3,1> 
< 3,1> 
25 2 
3 36 
include 
exclude 
<5,3> 
<5,3> 
28 4 
5 36 
include 
exclude 
<1,4> 
<l,4> 
28 6 
7 37 
Figure 8.16 State space tree for Figure 8.12(a) 
When procedure LCBB is used with the first of the two selection rules 
stated above and the cost matrix of Figure 8.12(a), the tree of Figure 8.16 
is generated. At the root node, we have to determine an edge (i,j) that will 
maximize the c value of the right subtree. If we select an edge (i, j) whose 
cost in the reduced matrix (Figure 8.12(b)) is positive then the c value of 
the right subtree will remain 25. This is so as the reduced matrix for the 
right subtree will have B(i, j) = oo and all other entries will be identical 
to those in Figure 8.12(b). Hence B will be reduced and c cannot increase. 
So, we must choose an edge with reduced cost 0. If we choose (1, 4) then 
B(l, 4) = oo and we need to subtract 1 from row 1 to obtain a reduced 

Traveling Salesperson 
409 
matrix. In this case c will be 26. If ( 3, 1) is selected then 11 needs to be 
subtracted from column 1 in order to obtain the reduced matrix for the 
right subtree. So, c will be 36. IfA is the reduced cost matrix for nodeR then 
the selection of edge (i, j) (A(i, j) = 0) as the next partitioning edge will in-
crease the c of the right subtree by t. = min.h•;{A(i, k)} + min.1:.,;{A(k, j)} 
as this much needs to be subtracted from row i and column j in order 
to introduce a zero into both. For edges (1, 4>, (2, 5>, (3, 1>, (3, 4>, 
(4, 5), (5, 2) and (5, 3), A. 
= 1, 2, 11, 0, 3, 3 and 11 respectively. 
So, either of the edges (3, 1) or (5, 3) may be used. Let us assume that 
LCBB selects edge (3, 1). c(2) (Figure 8.16) may be computed in a manner 
similar to that for the state space tree of Figure 8.13. In the corresponding 
reduced cost matrix all entries in row 3 and column 1 will be oo. Moreover 
the entry (1, 3) will also be oo as inclusion of this edge will result in a cycle. 
The reduced matrices corresponding to nodes 2 and 3 are given in Figures 
8.17(a) and (b). The c values for nodes 2 and 3 (as well as for all other 
nodes) appears outside the respective node. 
oolOoo 
0 
1 
000011 
2 
0 
00 
00 
00 
00 
00 
00 
3 
12 
00 
0 
00 
0 
0 
12 
00 
(a) node 2 
00 
10 
00 
0 
1 
0000 
0 
2 
0 
00 
00 
00 
00 
00 
oo 
3 
loo 
0 
00 
0 
00 
12 
00 
(d) node 5 
00 
10 
17 
0 
1 
looll 
2 
0 
00 
300 
0 
2 
4 
3 12 
00 
0 
0 
0 
0 
12 
00 
(b) node 3 
00 
00 
00 
00 
00 
00000000 
0 
00 
00 
00 
00 
00 
00 
0000000 
00 
00 
00 
00 
00 
(e) node 6 
oo 
700 
Ooo 
000000 
2 
0 
00 
00 
00 
00 
00 
oo 
Ooooo 
0 
00 
00 
00 
00 
00 
(c) node 4 
00 
0000000 
000000 
0 
0 
00 
00 
00 
00 
00 
oo 
Ooooo 
0 
00 
00 
00 
00 
00 
(f) node 7 
Figure 8.17 
Reduced cost matrices for Figure 8.16 
Node 2 is the nextE-node. Now, for edges (1, 4), (2, 5), (4, 5), (5, 2) 
and (5, 3), A. 
= 3, 2, 3, 3 and 11 respectively. Edge (5, 3) is selected 
and nodes 4 and 5 generated. The corresponding reduced matrices are given 
in Figures 8.17(c) and (d). c(4) becomes 28 as we need to subtract 3 from 
column 2 in order to reduce this column. Note that entry (1, 5) has been set 
to oo in Figure 8.17( c). This is necessary as the inclusion of edge ( 1, 5) to 
the collection { (3, 1), (5, 3)} will result in a cycle. In addition, entries in 

410 
Branch-and-Bound 
column 3 and row 5 are set to oo. Node 4 is the nextE-node. The A. values 
corresponding to edges (1, 4), (2, 5) and (4, 2) are 9, 2 and 0 respectively. 
Edge (1, 4) is selected and nodes 6 and 7 generated. The edge selection 
at node 6 is { (3, 1), (5, 3), (1, 4) }. This corresponds to the path 5, 3, 1, 4. 
So, entry (4, 5) is set to oo in Figure 8.17(e). In general if edge (i, j) is 
selected then the entries in row i and columnj are set to oo in the left sub-
tree. In addition, one more entry needs to be set to oo. This is an entry 
whose inclusion in the set of edges would create a cycle (an exercise examines 
how to determine this). The next £-node is node 6. At this time three of 
the five edges have already been selected. The remaining two may be 
selected directly. The only possibility is { (4, 2), (2, 5) }. This gives the 
path 5, 3, 1, 4, 2, 5 with length 28. U is updated to 28. Node 3 is the next 
£-node. LCBB terminates now as c(3) = 36 > U. 
In the preceding example, LCBB was modified slightly to handle nodes 
"close" to a solution node differently from other nodes. Node 6 is only two 
levels from a solution node. Rather than evaluate c at the children of 6 and 
then obtain their grandchildren, we just obtained an optimal solution for 
that subtree by a complete search with no bounding. We could have done 
something similar when generating the tree of Figure 8.13. Since node 6 is 
only two levels from the leaf nodes, we can simply skip computing c for 
the children and grandchildren of 6 and generate all of them, picking up 
the best. This works out to be quite efficient as it is easier to generate a 
subtree with a small number of nodes and evaluate all the solution nodes 
in it than it is to compute c for one of the children of 6. This latter state-
ment is true of many applications of branch-and-bound. Branch-and-bound 
is used on large subtrees. Once a small subtree is reached (say one with 4 
or 6 nodes in it) then that subtree is fully evaluated without using the 
bounding functions. 
The exercises examine yet another LC branch-and-bound algorithm for 
the traveling salesperson problem. This algorithm also uses a dynamic 
state space tree. Associated with each node in the state space tree is a 
graph. Each node represents a subproblem requiring us to find a minimum 
length tour in the graph associated with that node. The original graph 
G = (V, E) is associated with the root node. A lower bound c on the length 
of a shortest tour in the graph H = (V, A) associated with any node X is 
obtained by solving the following assignment problem: 

Traveling Salesperson 
411 
minimizel: l:c;jXij 
j= 1 i= 1 
n 
subject to I: xii = 1, 1 s. j s. n 
i=l 
n 
I: x ij = 1, 1 5 i 5 n 
J=l 
Xij = 0 if (i,j) EA 
(8.2) 
Xif = 0 or 1, 
ls.is.n,ls.js.n 
Note that I VI = n and c;1 is the length of edge (i,j). c;1 = oo if (i,j) 
EE. Algorithms to solve the assignment problem (8.2) are discussed in the 
texts: Linear Programming (pp. 227-228) by S. Gass, McGraw-Hill, New 
York, 1969and Flows in Networks(pp.111-112)byL. Ford and D. Fulkerson, 
Princeton University Press, 1962. 
In case the solution to (8.2) is a tour then the length of a shortest tour 
in H has been obtained. Usually, however, the solution to (8.2) will be 
made up to several disjoint cycles. One of these cycles is used to partition 
the solution space of H. Let C be any one of the cycles in a solution to 
(8.2) (assume there are at least two cycles). Let W == { w1, w2, ... , w,} 
be the vertices in C. Define R; and R; as: 
R 1 = 
{(w;,j) jj E W} 
R; = 
{(w;,j) jj E W} 
Now, define the edge sets: 
E1=A-R1 
E2=A-R1-R2 
£3=A-R1-R2-R3 
E,=A-R1-R2 ... -R, 

412 
Branch-and-Bound 
The children of X correspond to the graphs (V, Ei), 1 s i s r. The cor-
rectness of this partitioning rule follows from the following theorem: 
Theorem 8.3 
[Garfinkel] If T is a tour in H then T is a tour in exactly 
oneofthegraphs(V,Ei),l Si Sr. 
Proof: Left as an exercise. 
0 
We have now seen several branch-and-bound strategies for the traveling 
salesperson problem. It is not possible to determine analytically which of 
these is best. The exercises describe computer experiments that determine 
empirically the relative performance of the strategies suggested. 
8.4 EFFICIBNCY CONSIDERATIONS 
One can pose several questions concerning the performance characteristics 
of branch-and-bound algorithms that find least cost answer nodes. We 
might ask questions such as: 
(i) Will the use of a better starting value for U always decrease the num-
ber of nodes generated? 
(ii) Is it possible to decrease the number of nodes generated by actually 
expanding some nodes with c( ) > U? 
(iii) Will the use of a better c always result in a decrease in (or at least 
will not increase) the number of nodes generated? (c2 is better than 
c 1 iff c 1(X) s c 2(X) s c(X) for all nodes X). 
(iv) Does the use of dominance relations ever result in the generation of 
more nodes than will otherwise be generated? 
In this section we shall answer these questions. While the answers to 
most of the questions examined will agree with our intuition, the answers 
to others will be contrary to intuition. However, even in cases where the 
answer does not agree with intuition we can expect the performance of the 
algorithm to generally agree with the intuitive expectations. All of the fol-
lowing theorems assume that the branch-and-bound algorithm is to find 
a minimum cost solution node. Consequently, c(X) = cost of minimum 
cost solution node in subtree X. 
Theorem 8.4 Let T be a state space tree. The number of nodes of T gen-
erated by FIFO, LIFO and LC branch-and-bound algorithms cannot be 
decreased by the expansion of any node X with c(X) ~ U where U is the 
current upper bound on the cost of a minimum cost solution node in T. 

Efficiency Considerations 
413 
Proof: The theorem follows from the observation that the value of U can-
not be decreased by expanding X(as c(X) ~ U). Hence, such an expansion 
cannot affect the operation of the algorithm on the remainder of the tree. 
0 
Theorem 8.S 
Let U, and U2, U1 < U2 be two initial upper bounds on 
the cost of a minimum cost solution node in the state space tree T. FIFO, 
LIFO and LC branch-and-bound algorithms beginning with U 1 will gen-
erate no more nodes than they would if they started with U 2 as the initial 
upper bound. 
Proof: Left as an exercise. 
0 
Theorem 8.6 
The use of a better c function in conjunction with FIFO 
and LIFO branch-and-bound algorithms will not increase the number of 
nodes generated. 
Proof: Left as an exercise. 
0 
Theorem 8. 7 
If a better c function is used in a LC branch-and-bound 
algorithm, the number of nodes generated may increase. 
Proof: Consider the state space tree of Figure 8.18. All leaf nodes are 
solution nodes. The value outside each leaf is its cost. From these values 
it follows that c(l) = c(J) = 3 and c(2) = 4. Outside each of nodes 1, 2, 
and 3 is a pair of numbers(~~). Clearly, c2 is a better function than c,. 
However, if c2 is used, node 2 can become the £-node before node 3 (as 
ci(2) = c2(3)). In this case all 9 nodes of the tree will get generated. When 
c 1 is used, nodes 4, 5 and 6 are not generated. 
0 
6 
6 
4 
4 
4 
3 
Figure 8.18 
Example tree for Theorem 8. 7 

• 
• 
• 
• 
414 
Branch-and-Bound 
Now, let us look at the effect of dominance relations. Formally, a dom-
inance relation Dis given by a set of tuples, D = {(i1, i2), (i3, i4), (is, i6) 
... }. If (i,j) E D then node i is said to dominate nodej. By this we mean 
that subtree i contains a solution node with cost no more than the cost of a 
minimum cost solution node in subtree j. Dominated nodes may be killed 
without expansion . 
Since every node dominates itself, (i, i) ED for all i and D. The relation 
(i, i) should not result in the killing of node i. In addition, it is quite pos-
sible for D to contain tuples (i,, i2), (i2, iJ), (i3, i4) · · · (in, ii). In this case, 
the transitivity of D implies that each node i" dominates all nodes ii> 1 s 
j s n. Care should be taken to leave at least one of the i/s alive. A dom-
inance relation D2 is said to be stronger than another dominance relation 
D1, iff D1 C D 2. In the following theorems I will donote the identity rela-
tion { (i, i) I 1 s i s n } . 
Theorem 8.8 
The number of nodes generated during a FIFO or LIFO 
branch-and-bound search for a least cost solution node may increase when 
a stronger dominance relation is used. 
Proof: Just consider the state space tree of Figure 8.19. The only solution 
nodes are leaf nodes. Their cost is written outside the node. For the re-
maining nodes the number outside each node is its c value. The two dom-
inance relations to use are D1 = I and D2 = I U { (5, 2), (5, 8) }. Clearly, 
D 2 is stronger than D 1 and fewer nodes are generated using D 1 rather than 
D 2· I = { (i' i) Ii E D}. 
0 
4 
8 
7 
8 
7 
9 
3 
2 
7 
9 
7 
8 
4 
8 
Figure 8.19 
Example tree for Theorem 8.8 
Theorem 8.9 
Let D1 and D2 be two dominance relations. Let D2 be 
stronger than D1 and such that (i,j) E D2, i ~ j, implies c(i) < cU). An 

References and Selected Readings 
415 
LC branch-and-bound using Di generates at least as many nodes as one 
using D2. 
Proof: 
Left as an exercise. 
0 
Theorem 8.10 If the condition c(i) < c(j) in Theorem 8.10 is removed 
then an LC branch-and-bound using D 1 may generate fewer nodes than 
one using D2. 
Proof: 
Left as an exercise. 
0. 
REFERENCES AND SELECTED READINGS 
LC branch-and-bound algorithms have been extensively studied by researchers 
in areas such as artificial intelligence and operations research. Some of the more 
interesting general references are: 
"Problem Solving Methods in Artificial Intelligence" by N. J. Nilsson, McGraw-
Hill, New York, 1971. 
"Integer Programming," by R. S. Garfinkel and G. L. Nemhauser, John Wiley 
and Sons, Inc., New York, 1972. 
"Branch-and-bound methods: a survey," by E. L. Lawler and D. W. Wood, Oper. 
Res .. 14, pp. 699-719, 1966. 
"Branch-and-bound methods: general formulation and properties," by L. Mitten, 
Oper. Res., 18, pp. 24-34, 1970. 
Branch-and-bound algorithms using dominance relations in a manner similar to 
that suggested by FIFOKNAP (resulting in DKNAPl) may be found in: 
"A dynamic programming approach to sequencing problems," by M. Held and 
R. Karp.Jr. of SIAM, 10, pp.196-210, 1962. 
"Algorithms for scheduling independent tasks," by S. Sahni, J. ACM, 23(1), pp. 
116-127, 1976. 
"Exact and approximate algorithms for scheduling nonidentical processors," by 
E. Horowitz and S. Sahni,]. ACM, 23, pp. 317-327, 1976. 
"General techniques for combinatorial approximation," by S. Sahni, Oper. Res., 
25(6), pp. 920-936, 1977. 
. 
"Branch-and-bound strategies for dynamic programming," by T. Morin and R. 
Marsten, Oper. Res., 24, pp. 611-627, 1976. 

• I 
*I 
416 
Branch-and-Bound 
The algorithms in the above five papers are very similar to dynamic programming 
type algorithms. Further branch-and-bound algorithms for scheduling problems 
appear in: 
"Sequencing by enumerative methods," by J. Lenstra, Math Centre. Tract 69, 
Mathematisch Centrum, Amsterdam, 1976. 
"Job-shop scheduling by implicit enumeration," by B. Lageweg, J. Lenstra and A. 
Rinnooy Kan, Manag. Sci., 24(4), pp. 441-450, 1977. 
"Application of the branch-and-bound technique to some flow-shop scheduling 
problems," by E. Ignall and L. Schrage, Oper. Res., 13, pp. 400-412, 1965. 
The reduction technique for the knapsack problem is due to Ingargiola and 
Korsh. It appears in: 
"A reduction algorithm for zero-one single knapsack problems," by G. Ingargiola 
and J. Korsh, Manag. Sci., 20(4), pp. 460-663, 1973. 
A related reduction technique may be found in: 
"A general algorithm for one dimensional knapsack problems," by G. Ingargiola 
and J. Korsh, Oper. Res., 25(5), pp. 752-759, 1977. 
Branch-and-bound algorithms for the traveling salesperson problem have been 
proposed by many researchers. A survey of these algorithms appears in: 
"The traveling salesman problem: a survey," by M. Bellmore and G. Nemhauser, 
Oper. Res., 16, pp. 538-558, 1968. 
The reduced matrix technique to compute c is due to Little, Murty, Sweeny and 
Karel. It appears in the paper: 
"An algorithm for the traveling salesman problem," by J. Little, K. Murty, D. 
Sweeny and C. Karel, Oper Res., 11(6), pp. 972-989, 1963 . 
The above paper uses the dynamic state space tree approach. The partitioning 
scheme (8.3) is due to Garfinkel. His work is reported in: 
"On partitioning the feasible set in a branch-and-bound algorithm for the asym-
metric traveling salesman problem," by R. Garfinkel, Oper. Res., 21(1), pp. 340-
342, 1973. 
A more efficient branch-and-bound algorithm for the traveling salesperson prob-
lem has been proposed by Held and Karp. Their algorithm can be used only when 
C;j = Cji for all i andj. The following two papers describe the algorithm: 
- ------------
- --
-- -

Exercises 
417 
"The traveling salesman problem and minimum spanning trees," by M. Held and 
R. Karp, Oper. Res., 18, pp. 1138-1162, 1970. 
"The traveling salesman problem and minimum spanning trees: part II," by M. 
Held and R. Karp, Math Prog., 1, pp. 6-25, 1971. 
The results of section 8.4 are based on the work of Kohler, Steiglitz and Ibaraki. 
The relevent papers are: 
"Characterization and theoretical comparison of branch-and-bound algorithms for 
permutation problems," by W. Kohler and K. Steiglitz, J. ACM 21(1), pp. 140-
156, 1974. 
"Computational efficiency of approximate branch-and-bound algorithms," by 
T. Ibaraki, Math of Oper. Res., 1(3), pp. 287-298, 1976. 
"Theoretical comparisons of search strategies in branch-and-bound algorithms," by 
T. lbaraki, Int. Jr. of Comp. and Info. Sci., 5(4), pp. 315-344, 1976. 
"On the computational efficiency of branch-and-bound algorithms," by T. lbaraki, 
Jr. of the Oper. Res. Soc. of Japan, 20(1), pp. 16-35, 1977. 
"The power of dominance relations in branch-and-bound algorithms," by T. lbaraki, 
J. ACM, 24(2), pp. 264-279, 1977. 
The papers by T. lbaraki cited above also contain a discussion of heuristic search. 
More ideas on heuristic search can be found in N. Nilsson's book which was cited 
earlier. 
EXERCISES 
1. Prove Theorem 8.1. 
2. 
Write a program schema DFBB, for a LIFO branch-and-bound search for a 
least cost answer node. 
3. Draw the portion of the state space tree generated by FIFOBB, LCBB and a 
LIFO branch-and-bound for the job sequencing with deadlines instance n = 5; 
(pi, pz, ... , ps) = (6, 3, 4, 8, 5); (t1, ti, ... , ts) = (2, 1, 2, 1, 1); (d1, di, 
... , d s) = (3, 1, 4, 2, 4). What is the penalty corresponding to an optimal 
solution? Use a variable tuple size formulation and c( ·) and u( ·) as in Section 
8.1. 
4. 
Write a complete LC branch-and-bound algorithm for the job sequencing with 
deadlines problem. Use the fixed tuple size formulation. 

418 
Branch-and-Bound 
5. Work out Example 8.2 using the variable tuple size formulation. 
6. Work out Example 8.3 using the variable tuple size formulation. 
7. Draw the portion of the state space tree generated by LCKNAP for the knap-
sack instances: 
(i) 
n = 5, (p1, pz, ... , ps) = (10, 15, 6, 8, 4), (wi, w2, ... , ws) = (4, 6, 
3, 4, 2) and M = 12. 
(ii) n = 5, (p1,p2,p3,p4,ps) = (w1, w2, WJ, W4Ws) = (4, 4, 5, 8, 9) and 
M = 15. 
8. Do problem 7 using a LC branch-and-bound on a dynamic state space tree 
(see Section 7.6). Use the fixed tuple size formulation. 
9. Write a LC branch-and-bound algorithm for the knapsack problem using the 
fixed tuple size formulation and the dynamic state space tree of Section 7.6. 
10. [Programming Project] Program algorithms DKNAP (Alg. 5.7), DKNAPl 
(see pag. 401), LCKNAP (Alg. 8.8), and BKNAP2 (Alg. 7.13). Compare these 
algorithms empirically using randomly generated data as below: 
11. 
Data Set 
(i) 
Random w 1 andp1, w1 E [1, 100], p1 E [1, 100], M = E'!wJ2. 
(ii) 
Random w 1 and p1, w1 E [1, 100], p 1 E [1, 100]; M = 2max{ w 1} 
(iii) 
Randomw1,w1E [1, lOO];p1 = w1 + lO;M = E;'w,/2 
(iv) 
Same as (iii) except M = 2*max{ w 1} 
(v) 
Randomp1,p1 E [1, 100]; w1 = p1 + 10; M = E!w1/2 
(vi) 
Same as (v) except M = 2*max{ w 1} 
Obtain computing times for n = 5, 10, 20, 30, 40, .... For each n generate 
(say) 10 problem instances from each of the above data sets. Report average 
and worst case computing times for each of the above data sets. From these 
times can you say anything about the expected behavior of these algorithms? 
Now, generate problem instances with p 1 = w 1, 1 :s; i :s; n, M = Ew 112 and 
Ew;x; ,c. M for any 0, 1 assignment to the x;'s. Obtain computing times for 
your four programs for n = 10, 20 and 30. 
If you still have computer time available, then study the effect of changing 
the range to [1, 1000] in data sets (i) through (vi). In sets (iii) to (vi) replace 
Pl = W; + 10 by Pl = WI + 100 and WI = Pi + 10 by WI =Pl + 100 re-
spectively. 
[Programming Project] (a) Program the reduction heuristic REDUCE of 
Section 8.2. Generate several problem instances from the data sets of Exercise 
10 and determine the size of the reduced problem instances. Use n = 100, 
200, 500 and 1000. 

Exercises 
419 
(b) Program DKNAPl and the backtracking algorithm BKNAP2 for the 
knapsack problem. Compare the effectiveness of REDUCE by running several 
problem instances (as in Exercise 10). Obtain average and worst case com-
puting times for DKNAPl and BKNAP2 for the generated problem instances 
and also for the reduced instances. To the times for the reduced problem in-
stances add the time required by REDUCE. What conclusions can you draw 
from your experiments? 
12. a) 
Write a branch-and-bound algorithm for the job sequencing with deadlines 
problem using a dominance rule. Your algorithm should work with a fixed 
tuple size formulation and should generate nodes by levels. Nodes on each 
level should be kept in an order permitting easy use of your dominance 
rule. 
b) 
Convert your algorithm into a computer program and using randomly 
generated problem instances, determine the worth of the dominance rule 
as well as the bounding functions. To do this, you will have to run four 
versions of your program: PROGA ... bounding functions and dominance 
rules are removed; PROGB ... dominance rule is removed; PROGC ... 
bounding function is removed and PROGD ... bounding functions and 
dominance rules are included. Determine both computing time figures 
as well as the number of nodes generated. 
13. Consider the traveling salesperson instance defined by the cost matrix: 
[1 
7 
3 12 
8 l 
00 
6 14 
9 
8 
00 
6 
18 
3 
5 
00 
11 
14 
9 
8 
00 
a) Obtain the reduced cost matrix 
b) Using a state space tree formulation similar to that of Figure 8.11 and 
c( ·) as described in Section 8.3, obtain the portion of the state space tree 
that will be generated by LCBB. Label each node by its c value. Write out 
the reduced matrices corresponding to each of these nodes. 
c) 
Do part b) using the reduced matrix method and the dynamic state space 
tree approach discussed in Section 8.3. 
d) 
Solve the above traveling salesperson instance using the assignment prob-
lem formulation. Draw the state space tree and describe the progress of 
the method from node to node. 
e) Solve the given traveling salesperson problem using backtracking and the 
same c(.) function as above. Use the static state space tree formulation. 
f) Do part e) using a dynamic state space tree. 

• 
• 
• 
420 
Branch-and· Bound 
14. Do problem 13 using the following traveling salesperson cost matrix: 
[1 
11 
10 
9 
1] 
00 
7 
3 
4 
00 
4 
10 
s 
00 
9 
s s 
15. 
a) Describe an efficient implementation for a LC branch-and-bound traveling 
salesperson problem using the reduced cost matrix approach and (i) a 
dynamic state space tree and (ii) a static tree as in Figure 8.11. 
b) Are there any problem instances for which the LC branch-and-bound will 
generate fewer nodes using a static tree than using a dynamic tree? Prove 
your answer. 
16. 
Consider the LC branch-and-bound traveling salesperson algorithm described 
using the dynamic state space tree formulation. Let A and B be nodes. Let B 
be a child of A. If the edge (A, B) represents the inclusion of edge (i, j} in 
the tour then in the reduced matrix for Ball entries in row i and columnj are 
set to oo. In addition, one more entry is set to oo. Obtain an efficient way to 
determine this entry. 
17. [Programming Project]. Write computer programs for the following traveling 
salesperson algorithms: 
i) 
the dynamic programming algorithm of chapter 5 
ii) 
a backtracking algorithm using the static tree formulation of Section 8.3 
iii) 
a backtracking algorithm using the dynamic tree formulation of Section 
8.3 
iv) 
a LC branch-and-bound algorithm corresponding to (ii) 
v) 
a LC branch-and-bound algorithm corresponding to (iii) 
Design data sets to be used to compare the efficiency of the above algorithms. 
Randomly generate problem instances from each of these data sets and obtain 
computing times for your programs. Obtain tables along the lines of those in 
Section 7.6. What conclusions can you draw from your computing times? 
18. 
Prove theorem 8.3. 
19 • Prove theorem 8.5. 
20. 
Prove theorem 8.6. 

Exercises 
421 
21. 
Prove theorem 8.9. 
22. Prove theorem 8 .10. 
23. 
[Heuristic Search] Heuristic search is a generalization of FIFO, LIFO and LC 
search. A heuristic function h( ·) is used to evaluate all live nodes. The next 
£-node is the live node with least h( · ). Discuss the advantages of using a heu-
ristic function h( ·) different from c( · ) in the search for a least cost answer 
node. Consider the knapsack and traveling salesperson problems as two ex-
ample problems. Also consider any other problems you wish to. For these 
problems devise "reasonable" functions h( ·) (different from c( · )). Obtain 
problem instances on which heuristic search performs better than LC search. 

Chapter 9 
ALGEBRAIC SIMPLIFICATION AND 
TRANSFORMATION 
9.1 
THE GENERAL METHOD 
In this chapter we shift our attention away from the problems we've dealt 
with previously to concentrate on methods for dealing with numbers and 
polynomials. Though computers have the ability already built-in to manipu-
late integers and reals, they are not directly equipped to manipulate sym-
bolic mathematical expressions such as polynomials. One must determine 
a way to represent them and then write procedures which perform the 
desired operations. A system which allows for the manipulation of mathe-
matical expressions, (usually including arbitrary precision integers, poly-
nomials and rational functions), is called a mathematical symbol manipu-
lation system. These systems have been fruitfully used to solve a variety of 
scientific problems for many years. The techniques we will study here have 
often led to efficient ways to implement the operations offered by these 
systems. 
The first design technique we present is called algebraic transformation. 
Assume we have an input I which is a member of set S 1 and a functionf{J) 
which describes what must be computed. Usually the outputf{J) is also a 
member of St· Though a method may exist for computingf{D using opera-
tions on elements in S 1, this method may be inefficient. The algebraic 
transformation technique suggests that we alter the input into another form 
producing a member of set S 2• S 2 contains exactly the same elements as S 1 
except it assumes a different representation for them. Why would we trans-
form the input into another form? Because it may be easier to compute the 
function f for elements of S 2 than for elements of St· Once the answer in 
S 2 is computed an inverse transformation is performed to yield the result 
insetS 1• 
For example let S 1 be the set of integers represented using decimal no-
tation and S 2 the set of integers using binary notation. Given two integers 
422 

The General Method 
423 
from set Si, plus any arithmetic operations to carry out on these numbers, 
todays computers will transform the numbers into elements of set S 2, 
perform the operations and transform the result back into decimal form. 
The algorithms for transforming the numbers are familiar to most students 
of computer science. To go from elements of set S 1 to set S 2 repeated divi-
sion by 2 is used and from set S 2 to set S 1 repeated multiplication is used. 
The value of binary representation is the simplification which results in the 
internal circuitry of a computer. 
For another example let S 1 be the set of n-degree polynomials (n ~ 0) 
with integer coefficients represented by a list of their coefficients, e.g. 
The set S 2 consists of exactly the same set of polynomials but represented 
by their values at 2n + 1 points, namely the 2n + 1 pairs (x;, A(x;)), 
1 ::5 i ::5 2n + 1 would represent the polynomial A. (At this stage we won't 
worry about what the values of x, are, but for now you can consider them 
as consecutive integers.) The function f to be computed is the one which 
determines the product of two polynomials A(x), B(x) assuming the set 
S 1 representation to start with. Rather than forming the product directly 
using the conventional method, (which requires O(n 2) operations where n 
is the degree of A and B and ignoring any possible growth in the size of 
the coefficients), we could transform the two polynomials into elements 
of set S 2. We do this by evaluating A(x) and B(x) at 2n + 1 points. The 
product can now be computed simply, by multiplying the corresponding 
points together. The representation of A(x)•B(x) in set S 2 is given by the 
tuples (x 1, A(x1)•B(x1)) 1 ::5 i ::;; 2n + 1, and requires only O(n) operations 
to compute. We may determine the product of A(x)•B(x) in coefficient 
form by finding the polynomial which interpolates (or satisfies) these 2n + 1 
points. It is easy to show that there is a unique polynomial of degree ::5 2n 
which goes through 2n + 1 points. 
Figure 9.1 describes these transformations in a graphical form indicating 
the two paths one may take to reach the coefficient product domain, either 
directly using conventional multiplication or by algebraic transformation. 
The tram:formation in one direction is affected by evaluation while the in-
verse transformation is accomplished by interpolation. The value of the 
scheme rests entirely on whether or not these transformations can be car-
ried out efficiently. 
The world of algebraic algorithms is so broad that we will only attempt 
to cover a few of the interesting topics. In Section 9.2 we discuss the ques-
tion of polynomial evaluation at one or more points and the inverse opera-

424 
Algebraic Sbnplltlcation and Transformation 
Convent ion al 
multiplication 
s1 :coefficients--------- Product 
... ,,.,;., 1 
r, ... ,,,, .. ;., 
s2 : points--------- Point product 
Pairwise 
multiplication 
Figure 9.1 Transformation technique for polynomial products 
tion of polynomial interpolation at n points. Then in Section 9.3 we discuss 
the same problems as in section 9.2 but this time assuming the n points 
are nth roots of unity. This is shown to be equivalent to computing the 
Fourier transform and in that section we show how the divide-and-conquer 
strategy leads to the fast Fourier transform algorithm. In Section 9.4 we 
shift our attention to integer problems, in this case the processes of modular 
arithmetic. Modular arithmetic can be viewed as a transformation scheme 
which is useful for speeding up large precision integer arithmetic operations. 
Moreover we will see that transformation into and out of modular form is 
a special case of evaluation and interpolation. Thus there is an algebraic 
unity to Sections 9.2, 9.3, and 9.4. Finally, in Section 9.5 we present the 
asymptotically best known algorithms for n-point evaluation and inter-
polation. 
9.2 EVALUATION AND INTERPOLATION 
In this section we examine the operations on polynomials of evaluation and 
interpolation. As we search for efficient algorithms, we will see examples 
of another design strategy called algebraic simplification. When applied to 
algebraic problems, algebraic simplification refers to the process of re-ex-
pressing computational formulas so that the required number of operations 
to compute these formulas is minimized. One issue we will ignore here is 
the numerical stability of the resulting algorithms. Though this is often an 
important consideration it is too far from our purposes. See the references 
for some pointers to the literature regarding numerical stability. 
A univariate polynomial is generally written as 
A(x) = anxn + an-lxn-I + ... + a 1X + ao 

Evaluation and Interpolation 
425 
where x is an indeterminate and the a; may be integers, floating point 
numbers or more generally elements of a commutative ring or a field. If 
an -:;t 0 then n is called the degree of A. 
When considering the representation of a polynomial by its coefficients, 
there are at least two alternatives. The first calls for storing the degree 
followed by degree + 1 coefficients: 
This is termed the dense representation because it explicitly stores all co-
efficients whether or not they are zero. We observe that for a polynomial 
such as x 1000 + 1 the dense representation is wasteful since it requires 1002 
locations while there are only 2 nonzero terms. 
The second representation calls for storing only each nonzero coefficient 
and its corresponding exponent e.g. if all the a; are nonzero then 
(n , an, n -
1, a n - 1, ••• , 1, a I , Q, a o). 
This is termed the sparse representation because the storage depends di-
rectly upon the number of nonzero terms and not on the degree. For a 
polynomial of degree n, all of whose coefficients are nonzero, this second 
representation requires roughly twice the storage of the first. However that 
is the worst case. For high degree polynomials with few nonzero terms, 
the second representation will be many times better than the first. 
Secondarily we note that the terms of a polynomial will often be linked 
together rather than sequentially stored. However we will avoid this com-
plication in the following programs and assume that we can access the ith 
coefficient by writing a ; . 
Suppose we are given the polynomial A(x) = anx" + ... + a 0 and we 
wish to evaluate it at a point v, i.e. compute A(v). The straightforward 
or right-to-left method adds a 1v to a 0, a 2v2 to this sum and continues as 
described in Algorithm 9.1. The analysis of this algorithm is quite simple, 
namely 2n multiplications, n additions and 2n + 2 assignments are made 
(excluding the for loop). 
procedure STRAITEVAL(A, n, v) 
s - ao;r -
1 
fori -
1 to n do 
r -
r * v 
s-a 1*r+s 
repeat 
return{s) 
end STRAITEV AL 
Algorithm 9.1 Straightforward evaluation 

426 
Algebraic Simplification and Transformation 
An improvement to this procedure was,devised by Isaac Newton in 1711. 
The same improvement was used by W. G. Horner in 1819 to evaluate 
the coefficients of A(x + c). The method came to be known as Horner's 
rule. They suggest rewriting the polynomial in the following way: 
A(x) = ( ... ((anX + lln-l)x + lln-z)x + ... + a1)x + ao. 
This is our first and perhaps most famous example of algebraic simplifica-
tion. The procedure for evaluation which is based on this formula is given 
in Algorithm 9.2. 
procedureHORNER(A, n, v) 
S -
lln 
for i -
n -
1 to 0 by - 1 do 
S -
S*V + a; 
repeat 
return(s) 
end HORNER 
Algorithm 9.2 Homer's rule 
Horner's rule requires n multiplications, n additions and n + 1 assignments 
(excluding the for loop). Thus we see that it is an improvement over the 
straightforward method by a factor of 2. In fact in Chapter 10 we shall see 
that Horner's rule yields the optimal way to evaluate an nth degree poly-
nomial. 
Now suppose we consider the sparse representation of a polynomial, 
A(x) = amx•m + ... + a1x•1 where the a;~ 0 and em> em-I> ... > 
e 1 ~ 0. The straightforward algorithm (Algorithm 9.1) when generalized 
to this sparse case is given in Algorithm 9.3. 
procedureSSTRAITEVAL(A, m, v) 
I /sparse straightforward evaluation. m is the number of nonzero terms// 
s - 0 
fori -
1 to m do 
s - s + a;* v I e; 
repeat 
return(s) 
end SSTRAITEV AL 
Algorithm 9.3 Sparse evaluation 

Evaluation and Interpolation 
427 
Assuming that v I e is computed by repeated multiplication with v, this 
operation requires e -
1 multiplications and Algorithm 9 .3 requires em + 
em_ 1 + . . . + e 1 multiplications, m additions and m + 1 assignments. 
This is horribly inefficient and can easily be improved by an algorithm 
based on computing 
procedureNSTRAITEVAL(A, m, v) 
s -
eo -
O; t -
1 
fori -
1 tom do 
r -
v I (e,. -
e1-1) 
t -
r * t 
s - s + a,. * t 
repeat 
retum(s) 
end NSTRAITEVAL 
Algorithm 9.4 Evaluating a polynomial represented in coefficient-exponent form 
Algorithm 9.4 requires em + m multiplications, 3m + 3 assignments, 
m additions and m subtractions. 
A more clever scheme is to generalize Horner's strategy yielding the re-
vised formula, 
A(x) = (( .. . ((amx•m-•m-1 + am-1)x•m-1-•m-2 + ... + a2)xe,-e, + 
a 1)xe, 
The program below is based on this formula. 
procedureSHORNER(A, m, v) 
s -
eo -
0 
for i - m to 1 by - 1 do 
s -
(s + a,) * v I (e,. -
e 1-1) 
repeat 
retum(s) 
endSHORNER 
Algorithm 9.5 Homer's rule for a sparse representation 

428 
Algebraic Simplification and Transformation 
The number of required multiplications is 
(em -
em-I -
1) + . •. + (e1 -
eo -
1) + m = em 
which is the degree of A. In addition there are m additions, m subtractions 
and m + 2 assignments. Thus we see that Horner's rule is easily adapted 
to either the sparse or dense polynomial model and in both cases the num-
ber of operations is bounded and linear in the degree. With a little more 
work one can find an even better method, assuming a sparse representation, 
which requires only m + log2 em multiplications. (See the exercises for a 
hint.) 
Interpolation 
Given n points (x;, y;) our task is to find the coefficients of the unique 
polynomial A(x) of degree ::;; n -
1 which goes through these n points. 
Mathematically the answer to this problem was given by Lagrange 
( 
(x - Xj)) 
A(x) = 
E 
II 
y;. 
l:s;isn 
i~.i (Xi -
Xj) 
t s.1:s;n 
To verify that A(x) does satisfy then points we observe that 
A(x,) 
( II (x; - XJ))y; = y; 
i;<j (X; -
Xj) 
l:s;j:s;n 
(9.1) 
(9.2) 
since every other term will become zero. The numerator of each term is a 
product of n -
1 factors and hence the degree of A is ::5 n - 1. 
We now give a program which produces the coefficients of A(x) based 
upon this formula. We will need to perform some addition and multipli-
cation of polynomials so we assume the existence of functions PADD(A, B) 
and PMULT(A, B) with the obvious interpretations. 

Evaluation and Interpolation 
429 
procedureLAGRANGE(X, Y, n, ANS) 
I IX, Y are one-dimensional arrays containing n points (x;, y;)./ I 
I I ANS is a polynomial which interpolates these points/ I 
integer den, n; polynomial POLY, ANS; realX(l:n), Y(l:n); 
ANS-0 
fori -
1 ton do 
POLY - den -
1 
for j -
1 to n do 
if i ~ j 
then POLY -PMULT(POLY,x - X{j)) ! Ix - X(J) is a degree// 
I I one polynomial in x I I 
den - den * (X(i) - X{j)) 
I I X(i) - X{j) is a constant/ I 
endif 
repeat 
ANS - PADD(ANS, PMULT(Y(i)!den, POLY)) 
repeat 
end LAGRANGE 
Algorithm 9.6 Lagrange interpolation 
An analysis of the computing time of LAGRANGE is instructive. The 
innermost if statement is executed n 2 times. The time to compute each new 
value of den is one subtraction and one multiplication, but the execution 
of PMUL T requires more than constant time per call. Since the degree of 
x - X{j) is one, the time for one execution of PMULT is proportional to 
the degree of POLY, which is at mostj -
1 on theJth iteration. 
Therefore the total cost of the polynomial multiplication step is 
lsisn 
E {j -
1) = E (n(n + 1) - n) 
l:s;j:s;n 
l:s;i:s;n 
2 
= n 2 (n + 1)/2 - n 2 
= O(n 3). 
(9.3) 
This result is discouraging because it is so high. Perhaps we should search 
for a better method. Suppose we already have an interpolating polynomial 
A(x) such that A(x,) = y; for 1 :s;; i :s;; n and we want to add just one more 
point (Xn+i. Yn+1). How would we compute this new interpolating poly-
nomial given the fact that A(x) was already available? If we could solve this 
problem efficiently, then we could apply our solution n times to get an n 
point interpolating polynomial. 

430 
Algebraic Simplification and Transformation 
Let Gj-1(x) interpolatej - 1 points (xk, yk) 1 ::; k < j such that Gj-1(xk) 
= Yk· Also let Dj-1(x) = (x -
x1) ... (x -
Xj-1). Then we can compute 
G,{x) by the formula 
Gj(x) = (yj -
Gj-1(xj))(Dj-1(x)!Dj-1 (x)) + Gj-1(x) 
We observe that 
Also we observe that 
= yj 
Having verified that this formula is correct, we present an algorithm for 
computing the interpolating polynomial which is based upon this formula. 
Notice that from the formula, two applications of Horner's rule are re-
quired, once for evaluating Gj_ 1(x) at Xj and the other for evaluating 
Dj-1(x) atxj. 
procedure INTERP(X, Y, n, G) 
//assume n ~ 2. X(l:n), Y{l:n) are then pairs of points// 
I /The coefficients of the unique interpolating polynomial/ I 
I I of degree < n is returned in G .I I 
realX(l:n), Y{l:n), num, denom; polynomial G, D; 
G -
Y{l) //G begins as a constant/ I 
D -
x - X(l) / ID(x) is a linear polynomial.// 
for i -
2 to n do 
denom -HORNER(D,i - 1,X(i)) //evaluateDatx;// 
num - HORNER(G, i - 2,X{i)) //evaluateG atx,// 
G - PADD(PMULT((Y(i) - num)I denom, D), G) 
D - PMULT(D, x - X(i)) 
repeat 
endINTERP 
Algorithm 9. 7 Newtonian interpolation 

The Fast Fourier Transform 
431 
On the ith iteration D has degree i - 1 and G has degree i - 2. There-
fore the invocations of HORNER require 
E (i + i -
1) = n(n -
1) -
(n -
1) = (n -
1)2 
(9.4) 
lsisn-1 
multiplications in total. The term (Y(i) -
num)I denom is a constant. 
Multiplying this constant by D requires i + 1 multiplications and multipli-
cation of D by x - X{i) requires i + 1 multiplications. The addition with 
G requires zero multiplications. Thus the remaining steps require 
E (2i + 2) = n(n -
1) + 2(n -
1) = (n -
1) (n + 2) 
(9.5) 
l'5i'5n-l 
operations and so we see that the entire procedure INTERP requires O(n 2) 
operations. 
In conclusion we observe that for a dense polynomial of degree n, eval-
uation can be accomplished using O(n) operations or for a sparse poly-
nomial with m nonzero terms and degree n, evaluation can be done using 
at most O(m + n) = O(n) operations. Also, given n points we can produce 
the interpolating polynomial in O(n 2) time. In chapter 10 we will discuss 
the question of the optimality of Horner's rule for evaluation. Section 9.5 
presents an even faster way to perform interpolation of n points as well as 
evaluation of a polynomial at n points. 
9.3 THE FAST FOURIER TRANSFORM 
If one is able to devise an algorithm which is an order of magnitude faster 
than any previous method, that is a worthy accomplishment. When the 
improvement is for a process which has many applications then that ac-
complishment will have a significant impact upon researchers and prac-
titioners. This is the case of the fast Fourier transform. No algorithm im-
provement has had a greater impact in the recent past than this one. The 
Fourier transform is used by electrical engineers in a variety of ways in-
cluding speech transmission, coding theory, and image processing. But 
before this fast algorithm was developed the use of this transform was 
considered inpractical. 
The Fourier transform of a continuous function a(t) is given by 
A(j) = [,, a(t)ehifi dt 
{9.6) 

• 
• 
• 
• 
• 
432 
Algebraic Simplification and Transformation 
while the inverse transform is 
a(t) = l/(27r) [,, A(f)e -Z...ift df. 
(9.7) 
The i in the above two equations stands for the square root of - 1. The 
constant e is the base of the natural logarithm. The variable t is often re-
garded as time while f is taken to mean frequency and then the Fourier 
transform is interpreted as taking a function of time into a function of 
frequency. 
Corresponding to this continuous Fourier transform is the discrete Fourier 
transform which handles sample points of a(t), namely a 0, a1, ... , aN-1· 
The discrete Fourier transform is defined by 
A j = 
E 
a ke hijk!N, 
OsksN-1 
and the inverse is 
ak = (l!N) E Aje -hijklN, 
OsjsN-1 
0 ~j ~ N -
1 
(9.8) 
O~k~N-1 
(9.9) 
In the discrete case a set of N sample points is given and a resulting set 
of N points is produced. An important fact to observe is the close connec-
tion between the discrete Fourier transform and polynomial evaluation. 
If we imagine the polynomial 
a(x) = aN-IXN-I + aN-2xN-l + ... + a 1X + ao 
then the Fourier element Aj is the value of a(x) at x = wj where w = e 2"i1N. 
Similarly for the inverse Fourier transform if we imagine the polynomial 
with the Fourier coefficients 
then each ak is the value of A(x) at x = (w -l)k where w = ehilN. Thus, the 
discrete Fourier transform corresponds exactly to the evaluation of a poly-
nomial at N points: w 0, w 1, ••• , wN-1. 
From the preceding section we know that we can evaluate an Nth degree 
polynomial at N points using O(N2) operations. We apply Homer's rule 
once for each point. The fast Fourier transform (abbreviated as FFT) is 
an algorithm for computing these N values using only O(N log N) operations. 
This algorithm was popularized by Cooley and Tukey in 1965 and the long 

The Fast Fourier Transform 
433 
history of this method was traced by Cooley, Lewis and Welch (see the 
references). 
A hint that the Fourier transform can be computed faster than by 
Homer's rule comes from observing that the evaluation points are not 
arbitrary, but are in fact very special. They are the N powers wJ for 0 :5 j 
:5 N -
1 where w = ehilN. The point w is a primitive Nth root of unity 
in the complex plane. 
Definition: An element w in a commutative ring is called a primitive Nth 
root of unity if 
(i) 
w :;C 1 
(ii) 
WN = 1 
(9.10) 
(iii) 
E 
wiP = 0, 
l:s;j:s;N-1 
OspsN-1 
We now present two simple properties of Nth roots from which we can see 
how the FFT algorithm can easily be understood. 
Theorem 9.1 Let N = 2n and suppose w is a primitive Nth root of unity. 
Then -wJ = wJ+n. 
Proof: (wJ+n)2 = (wi) 2(wn)2 = (wi)2w 2n = (wi) 2 since wN = 1. Since the 
wi are distinct we know that wi :;C wJ+n so we can conclude that wi+n = 
-wi. 
D 
Theorem 9.2 Let N = 2n and w a primitive Nth root of unity. Then w 2 
is a primitive nth root of unity. 
Proof: Since wN = w 2n = 1, (w2 )n = 1 implying w2 is an nth root of unity. 
In addition we observe that ( w2 )i :;C 1 for 1 :5 j :5 n -
1 since otherwise 
we would have wk = 1 for 1 :5 k < 2n = N which would contradict 
the fact that w is a primitive Nth root of unity. Therefore w 2 is a primitive 
nth root of unity. 
D 
From this theorem we can conclude that if wi, 0 < j :5 N -
1 are the 
primitive Nth roots of unity, N = 2n, then w2J, 0 < j :5 n -
1 are primi-
tive nth roots of unity. Using these two theorems we are now ready to show 
how to derive a divide-and-conquer algorithm for the Fourier transform. 

• 
• 
• 
• 
434 
Algebraic Simplification and Transformation 
The complexity of the algorithm is O(Mog N), an order of magnitude faster 
than the O(N2) conventional algorithm which uses polynomial evaluation. 
Again let a N _ 1, ••• , a o be the coefficients to be transformed and let 
a(x) = aN-1xN-I + ... + a1x + ao. Webreakupa(x)intotwoparts,one 
which contains even numbered exponents and the other odd numbered 
exponents. 
Letting y = x 2 we can rewrite a(x) as a sum of two polynomials 
a(x) = (aN-1yn-I + llN-Jyn-l + · · · + ai)x 
+ (aN-2yn-I + llN-4Yn-l + · · · + ao) 
= c(y)*X + b(y) 
Recall that the values of the Fourier transform are a(wj), 0 :5 j :5 N - 1. 
By the above the values of a(x) at the points wi, 0 :5 j :5 n -
1 are now 
expressible as 
a(wi) = c(w 2i)wi + b(w 2i) 
a(wJ+n) = -c(w 2i)wi + b(w 2i) 
These two formulas are computationally valuable in the following way. They 
reveal how to take a problem of size N and transform it into 2 identical 
problems of size n = N/2. These subproblems are the evaluation of b(y) 
and c(y), each of degree n -
1, at the points (w 2)i, 0 :5 j :5 n -
1 and 
these points are primitive nth roots. This is an example of divide-and-con-
quer and we can apply the divide-and-conquer strategy again as long as the 
number of points remains even. This leads us to always choose N as a 
power of 2, N = 2m, for then we can continue to carry out the splitting 
procedure until a trivial problem is reached, namely evaluating a constant 
polynomial. 
Procedure FFT in Algorithm 9.8 combines all of these ideas into a re-
cursive version of the fast Fourier transform algorithm . 

The Fast Fourier Transform 
435 
procedure FFT(N, a(x), w, A) 
I IN = 2m, a(x) = aN-1xN-I + ... + a 0, w is al/ 
I /primitive N-th root of unity A(O:N -
1) is set to/ I 
I /the values a(wi), 0 5 j 5 N - 1./ I 
integer N real A(O:N -
1), B(O:(N 12) -
1), C(O:(N 12) -
1), 
WP( -1 :(N/2) -
1) 
ifN = 1 thenA(O) -
ao 
else n - N/2 
b(x) -
aN_zxn-I + ... + a2x + a 0 
//divide the coefficients// 
c(x) -
a N - 1X n - I + . . . + a 3X + a I 
I I into 2 sets/ I 
call FFT(n, b(x), w2, B) //apply this algorithm again// 
callFFT(n, c(x), w2, C) //and again// 
WP(-1) -
l/w 
for} -
0 ton - 1 do 
WP(}) -
w *WP(} -
1) 
A(j) - B(j) + WP(j)*C(j) 
A(j + n) -
B(j) -
WP(j)*C(j) 
repeat 
endif 
endFFT 
Algorithm 9.8 Recursive fast Fourier transform 
Now let us derive the computing time of FFT. Let T(N) be the time for 
the algorithm applied to N inputs. Then we have 
T(N) = 2T(Nl2) + cN 
where c is a constant and cN is a bound on the time needed to form b(x), 
c(x), A and B. Since T(l) = d, where d is another constant, we can re-
peatedly simplify this recurrence relation to get 
T(2m) = 2T(2m- 1) + elm 
= ... = cm2m + T(1)2m 
= cN log 2 N + dN 
= O(N log2N) 

• 
• 
• 
• 
• 
436 
Algebraic Simpliftcation and Transformation 
Suppose we return briefly to the problem considered at the beginning of 
this chapter, the multiplication polynomials. The transformation technique 
calls for evaluating A(x) and B(x) at 2N + 1 points, computing the 2N + 1 
products A(x ;) * B(x ;) and then finding the product A(x)B(x) in coefficient 
form by computing the interpolating polynomial which satisfies these points. 
In Section 9.2 we saw that N point evaluation and interpofation required 
O(N2) operations, so that no asymptotic improvement is gained by using 
this transformation over the conventional multiplication algorithm. How-
ever, in this section we have seen that if the points are especially chosen 
to be the N = 2m distinct powers of a primitive Nth root of unity, then 
evaluation and interpolation can be done using at most O(N log N) opera-
tions. Therefore by using the fast Fourier transform algorithm we can mul-
tiply two N-degree polynomials in O(N log N) operations. 
The divide-and-conquer strategy plus some simple properties of primitive 
Nth roots of unity leads to a very nice conceptual framework for under-
standing the FFT. The above analysis shows that asymptotically it is better 
than the direct method by an order of magnitude. However the version we 
have produced may still not be faster! The reason for this is the consid-
erable overhead that is required to implement the recursive calls. We need 
to study this algorithm more closely to eliminate this overhead. However 
uninterested readers may skip directly to Section 9.4 . 
An iterative version of the FFT 
Recall that if we view the elements of the vector (a o, ... , a N _ 1) to be trans-
formed as coefficients of a polynomial a(x), then the Fourier transform is 
the same as computing a(wi) for 0 :5 j < N. This transformation is also 
equivalent to computing the remainder when A(x) is divided by the linear 
polynomial x 
wi, for if q(x) and c are the quotient and remainder such 
that 
A(x) = (x -
wi)q(x) + c 
then A(wi) = 0 * q(x) + c = c. We could divide A(x) by these N linear 
polynomials, but that would require O(N2) operations. Instead we are going 
to make use of the principle called balancing and compute these remainders 
with the help of a process which is structured like a binary tree. 
Consider the product of the linear factors (x -
w 0) (x -
w 1) 
• • • (x -
w 7) = xs -
w 0• All of the intermediate terms cancel leaving only exponents 
eight and zero with nonzero coefficients. If we select out from this product 
the even and odd degree terms a similar phenomenon occurs, namely 

The Fast Fourier Transform 
437 
(x - w0 ) (x - w2 ) (x - w4 ) (x - w6 ) = (x 4 -
w0 ) and (x -
w 1 ) (x - w3 ) 
(x -
w5 ) (x -
w7 ) = x 4 -
w4 • Continuing in a similar fashion we see in 
Figure 9.2 that the selected products have only two nonzero terms and we 
can continue this splitting until only linear factors are present. 
Now suppose we want to compute the remainders of A(x) by eight linear 
factors (x - w0), ••• , (x - w7). We begin by computing the remainder of 
A(x) divided by the product D(x) = (x -
wO) .•. (x -
w 7). If A(x) = 
Q(x)D(x) + R(x) thenA(wi) = R(wi), 0 :5 j :5 7, since D(wi) = 0 and the 
degree of R(x) is less than the degree of D(x) which equals 8. Now we 
divide R(x) by x 4 -
w 0 obtaining S(x) and by x 4 -
w4 obtaining T(x). 
A(wJ) = R(wJ) = S(wJ) for} = 0, 2, 4, 6 andA(wJ) = R(wJ) = T(wJ) forj = 
1, 3, 5, 7 and the degrees of S and Tare less than 4. Next we divide S(x) 
by xi -
w 0 and x 2 
-
w4 obtaining remainders U(x) and Y(x) where 
A(wJ) = U(wJ) for j = 0, 4 and A(wJ) = V(wi) for j = 2, 6. Notice how 
each divisor has only two nonzero terms and so the division process will be 
fast. By continuing in this way we will eventually conclude with the eight 
valuesA(x) mod (x - wf) for}= 0, 1, ... , 7. 
By carrying out successive divisions down the binary tree of Figure 9.2 
we will eventually arrive at the appropriate coefficients of the Fourier 
transform. The order of these coefficients will be permuted in the same 
way the x -
wi appear at the bottom of the tree, but this can be corrected 
at the end of the algorithm. Since this permutation caused the polynomials 
4 
0 
x - w 
8 
0 
x - w 
4 
4 
x-w 
A 
A 
2 0 
2 
4 
2 
2 
2 
6 
x -w 
x -w 
x -w 
x -w 
/\ /\ /\ /\ 
0 
4 
2 
6 
I 
5 
3 
7 
x-w 
x-w 
x-w 
x-w 
x-w 
x-w 
x-w 
x-w 
Figure 9.2 Divisors in the FIT algorithm of size 8 

• 
• 
• 
• 
438 
Algebraic Simplification and Transformation 
at each node of the tree to have such a simple form, the division at each 
stage is simple and the resulting computation time for the entire transform 
reduces to O(N log N). One can see this in a simple way by observing that 
the tree has log N levels, 2; nodes on each level, where a dividend poly-
nomial on level i has at most 2k-i terms. Thus the work on the ith level is 
proportion to 2i*2k-i = 2k = N and hence O(N log N) bounds the time 
procedure NFFT(A, m) 
I /nonrecursive FFT algorithm where A(l :n) contains/ I 
I /the input coefficients, n = 2 t m. The elements/ I 
I I of the transform are computed in-place./ I 
I /Complex arithmetic is assumed and w = ehilj/ I 
//is expressed in terms of sines and cosines./ I 
integer i,j, k, I, m, n, ndiv2, pow2, pow2ml, index 
complexA(l:n), r, s, t 
n - 2tm; ndiv2 -N/2;j -
1 
for i -
1 ton -
1 do I /permute the input/ I 
if i < j then t -
A(j); A(j) - A(i); A(i) -
t 
endif 
k -
ndiv2 
while k < j do 
j - j - k; k - k/2 
repeat 
j-j + k 
repeat 
pi -
3.14159265 //a constant// 
for/-ltomdo /lm=log2nll 
pow2 -
2 t /; pow2ml - pow212 
r -
(1.0, 0.) I Ir is a complex number, a pair of real numbers/ I 
s -
cmplx (cos(pilpow2ml), sin (pilpow2ml)) //an nth root// 
forj - 1 to pow2m1 do 
for i - j to n by pow2 do 
index - i + pow2ml 
t -
A(index)*r 
A(index) - A(i) -
t 
//compute the next pair// 
A(i) - A(i) + t 
repeat 
r - r*s 
repeat 
repeat 
endNFFT 
Algorithm 9.9 Nonrecursive FFF 

The Fast Fourier Transform 
439 
for the entire algorithm. Algorithm 9. 9 uses this point of view to produce 
an FFT algorithm which is iterative in nature. 
Procedure NFFT is an in-place, iterative version of the fast Fourier trans-
form. It begins by rearranging the input so that at the end of the algorithm 
the correct values are in their proper positions. Complex arithmetic is as-
sumed and w = ehi!J is expressed in terms of sines and cosines. To verify 
that the complexity of NFFT is truly O(n log n), assume n = 2m and ex-
amine the triply nested for loops. The statements contained in the inner-
most for loop require no more than constant time per iteration. The inner-
most for loop is executed no more than jnf211 < 2m-l+I times. This 
implies that the total time of NFFT is bounded by 
E 
E 
c2m-t+I = 
E elm = c2mm = O(n log n). 
J,;/,;m i,;j,;21-1 
J,;/,;m 
Now suppose we simulate the algorithm as it works on the particular 
case n = 4. We assume as inputs the symbolic quantities A(l) = a i, A(2) 
= a2, A(3) = a3 and A(4) = a4. Initially m = 2 and n = 4. After the 
first for loop is completed, the array contains the elements permuted as 
A(l) = a i, A(2) = a3, A(3) = a2, A(4) = a4. The main for loop is executed 
for I = 1 and I = 2. After the I = 1 pass is completed the array contains 
A(l) = a1 + a3, A(2) = a1 -
a3, A(3) = a2 + a4, A(4) = a2 -
a4. At 
this point one should observe in general that w n12 = 
- 1 or for this case w 2 
= -1 and the complex number expressed as a 2-tuple, (cos 11", sin 7r) is 
equal to w. At the end of the algorithm the final values in the array A are 
A(l) = a1 + a2 + a 3 + a4, A(2) = a1 + wa2 + w 2a3 + w 3a4, A(3) 
a1 + w 2a2 + a3 + w 2a4,A(4) = a1 + w 3a2 + w 2a3 + wa4. 
Some remaining points 
Up to now we have been treating the value w as ehilN. This is a complex 
number (it has an imaginary part) and its value cannot be represented 
exactly in a digital computer. Thus the arithmetic operations performed in 
the Fourier transform algorithm were assumed to be operations on complex 
numbers and this implies they are approximations to the actual values. 
When the inputs to be transformed are readings from a continuous signal, 
approximations of w do not cause any significant loss in accuracy. However 
there are occasions when one would prefer an exact result, for instance 
when one is using the FFT for polynomial multiplication in a mathematical 
symbol manipulation system. It is possible to circumvent the need for 
approximate, complex arithmetic by working in a finite field. 
Let p be chosen such that it is a prime which is less than your computer's 

• 
• 
• 
• 
440 
Algebraic Simplification and Transformation 
word size and such that the integers 0, 1, ... , p - 1 contain a primitive nth 
root of unity. By doing all of the arithmetic of the fast Fourier transform 
modulo p, all of the results will be single precision. By choosing p to be a 
prime the integers 0, 1, ... , p -
1 form a field and all arithmetic opera-
tions including division can be performed. If all values during the com-
putation are bounded by p -
1 then the exact answer will be formed since 
x mod p = x if 0 :5 x < p. However if one or more values exceeds p -
1 
the exact answer can still be produced by repeating the transform using 
several different primes followed by the Chinese Remainder Theorem as 
described in the next section. So the question which remains is, given an 
N can one find a sufficient number of primes of a certain size which con-
tain Nth roots. From finite field theory { 0, 1, .. ., p -
1} contains a prim-
itive Nth root iff N divides p -
1. Therefore, to transform a sequence of 
size N = 2 m, primes of the form p = 2 ek + 1 where m :s; e must be found. 
Call such a number a Fourier prime. J. Lipson has shown that there are 
more than x/(2•- 1 In x) Fourier primes less than x with exponent e and 
hence there are more than enough for any reasonable application. For 
example if the work size is 32 bits let x = 231 and e = 20. Then there are 
approximately 182 primes of the form 2/k + 1 wheref ~ 20. Any of these 
Fourier primes would suffice to compute the FFT of a sequence of at most 
220• See the exercises for more details. 
9.4 MODULAR ARITHMETIC 
Another example of a useful set of transformations is modular arithmetic. 
Modular arithmetic is useful in one context because it allows one to refor-
mulate the way addition, subtraction, and multiplication are performed. 
This reformulation is one which exploits parallelism whereas the normal 
methods for doing arithmetic are serial in nature. The growth of special 
computers which make it desirable to perform parallel computation make 
modular arithmetic attractive. A second use of modular arithmetic is with 
systems which allow for symbolic mathematical computation. These soft-
ware packages usually provide operations which permit arbitrarily large 
integers and rational numbers as operands. Modular arithmetic has been 
found to yield efficient algorithms for the manipulation of large numbers. 
Finally there is an intrinsic interest in finite field arithmetic (the integers 
0, 1, ... , p -
1 wher p is a prime form a field) by number theorists and 
electrical engineers specializing in communications and coding theory. In 
this section we will study this subject from a computer scientists point of 
view, namely the development of efficient algorithms for the required opera-
tions. 
--
-
- ---------

Modular Arithmetic 
441 
The mod operator is defined as 
x mod y = x - y Lxly J, 
x mod 0 = x 
if y "" 0 
Note that (xly) corresponds to fixed point integer division which is com-
monly found on most current day computers. 
We will denote the set of integers { 0, 1, ... , p -
1} where p is a prime 
by GF(p), (the Galois field withp elements), named after the mathematician 
Galois who studied and characterized the properties of these fields. Also 
we will assume that p is a single precision number for the computer you 
plan to execute on. It is, in fact, true that the set GF(p) forms a field under 
the following definitions of addition, subtraction, multiplication and division: 
If a, b E GF(p), then 
(a + b) mod p = {a 
a+b 
+b-p 
(a - b) mod p = { 
a - b 
a-b+p 
ifa + b < p 
ifa + b ~ p 
ifa-b~O 
ifa-b<O 
(ab) mod p = r such that r is the remainder when the product ab is di-
vided by p, ab = qp + r where 0 ~ r < p 
(alb) mod p = (ab - 1) mod p = r, the unique remainder when ab - 1 is 
divided by p, ab - 1 = qp + r, 0 ~ r < p 
b - 1 is the multiplicative inverse of b in GF(p). For every element b in 
GF(p) except zero there exists a unique element called b - 1 such that bb - 1 
mod p = 1. We shall see how to compute this value very soon. 
Now what are the computing times for these operations? We have assumed 
that p is a single precision integer, which implies that all a, b E GF(p) 
are also single precision integers. The time for addition, subtraction and 
multiplication mod p given the formulas above are easily seen to be 0(1). 

• 
• 
• 
• 
• 
442 
Algebraic Simplification and Transformation 
But before we can determine the time for division we must develop an algo-
rithm to compute the multiplicative inverse of an element b E GF(p). 
By definition we know that to find x = b - 1 there must exist an integer 
k, 0 ::S k < p such that bx = kp + 1. For example, if p = 7 
b: 
1 2 3 4 5 6 (element) 
b- 1: 
1 4 5 2 3 6 (inverse) 
k: 0 1 2 1 2 5 
An algorithm for computing the inverse of b in GF(p) is provided by 
generalizing Euclid's algorithm for the computation of greatest common 
divisors (see Section 1.3). Recall that given two nonnegative integers a, b 
Euclid's algorithm computes their gcd. It does so by making use of the 
theorem that if a > b 2!: 0 then gcd(a, b) = gcd(b, a mod b) if b is non-
zero and otherwise gcd(a, 0) = a. It is also possible to compute two more 
integers x, y such that ax + by = gcd(a, b). Letting a be a prime p and 
b E GF(p), then the gcd(p, b) = 1 (since the only divisors of a prime are 
itself and one) and Euclid's generalization reduces to finding integers x, y 
such that px + by = 1. This implies that y is the multiplicative inverse of 
b modp . 
procedure EXEUCLID(b, p) 
/lb E GF(p), pis a prime. EXEUCLID is a function// 
I /whose result is the integer x such that bx + kp = 1/ I 
//The statement (e,j) -
(g, h) is// 
I /interpreted as e - g;f - hi I 
(c, d, x,y) -
(p, b, 0, 1) //initialize// 
while d ¢. 1 do 
q - cl d 
I I compute quotient/ I 
e -
c - d*q //compute new remainder// 
w -
x -y*q 
(c, d, x,y) -
(d, e,y, w) 
repeat 
ify < 0 theny - y + p 
retum(y) 
end EXEUCLID 
Algorithm 9.10 Extended Euclidean algorithm 
A close examination of EXEUCLID shows that Euclid's gcd algorithm is 
carried out by the steps q -
c/d; e -
c - d * q; c -
d; and d -
e. The 

Modular Arithmetic 
443 
only other steps are the updating of x and y as the algorithm proceeds. In 
order to analyze the time for EXEUCLID we need to know the number of 
divisions Euclid's algorithm may require. This was answered in the worst 
case by Lame' in 1845. 
Theorem 9.6 (G. Lame', 1845): For n ~ 1 let a, b be integers a > b > 0 
such that Euclid's algorithm applied to a, b requires n division steps. Then 
n :5 5 log1ob. 
Thus the while loop is executed no more than O(log 10P) times and this 
is the computing time for the extended Euclidean algorithm and hence for 
modular division. By modular arithmetic we will mean the operations of 
addition, subtraction, multiplication and division modulo p as previously 
defined. 
Now lets see how we can use modular arithmetic as a transformation 
technique to help us work with integers. We begin by looking at how we 
can represent integers using a set of moduli, then how we perform arith-
metic on this representation and finally how to produce the proper integer 
result. 
Let a and b be integers and suppose that a is represented by the r-tuple 
(a i, ••• , a,) where a, = a mod p, and b is represented as (bi, ... , b ,) 
where b; = b mod p;. The p; are typically single precision primes. This is 
called a mixed radix representation which contrasts with the conventional 
representation of integers using a single radix such as 10 (decimal) or 2 
(binary). The following rules for addition, subtraction and multiplication 
using a mixed radix representation are as follows: 
(a1, ... ,a,)+ (bi, ... ,b,) = ((a1 + b1)modpi, ... ,(a,+ b,)modp,), 
(a 1, ... , a,) * (b 1, ... , b ,) = (a 1h 1 mod p i, ••• , a ,b r mod p ,), 
For example let the moduli be p 1 = 3, p 2 = 5, and p J = 7 and suppose 
we start with the integers 10 and 15. 
10 = (10 mod 3, 10 mod 5, 10 mod 7) = (1, 0, 3) 
15 = (15 mod 3, 15 mod 5, 15 mod 7) = (0, 0, 1) 
Then 
10 + 15 = (25 mod 3, 25 mod 5, 25 mod 7) = (1, 0, 4) 
= (1 + 0 mod 3, 0 + 0 mod 5, 3 + 1 mod 7) = (1, 0, 4) 

• 
• 
• 
• 
444 
Algebraic Simplification and Transformation 
Also 
Also 
15 -
10 = (5 mod 3, 5 mod 5, 5 mod 7) = (2, 0, 5) 
= (0 - 1 mod 3, 0 - 0 mod 5, 1 - 3 mod 7) = (2, 0, 5) 
10* 15 = (150 mod 3, 150 mod 5, 150 mod 7) 
= (1*0 mod 3, O*O mod 5, 3*1 mod 7) 
(O, 0, 3) 
(O, 0, 3) 
After we have performed some desired sequence of arithmetic operations 
using these r-tuples, we are left with some r-tuple (ci, ... , er). We now 
need some way of transforming back from modular form with the assurance 
that the resulting integer is the correct one. The ability to do this is guar-
anteed by the following theorem which was first proven in full generality 
by L. Euler in 1734. 
Theorem 9. 7 (Chinese Remainder Theorem): Let p 1, •• ., pr be positive 
integers which are pairwise relatively prime (no two integers have a com-
mon factor). Let p = p 1 ••• Pr and let b, a i. ..• , ar be integers. Then, 
there is exactly one integer, a, which satisfies the conditions 
b :5 a < b + p, and a = a; (mod p;) for 1 :5 i :5 r. 
Proof: Let x be another integer, different from a, such that a = x(mod p,) 
for 1 :5 i :5 r. Then a -
x is a multiple of p; for all i. Since the p; are 
pairwise relatively prime it follows that a -
x is a multiple of p. Thus, 
there can be only one solution which satisfies the above relations. We will 
show how to construct this value in a moment. 
D 
A pictorial view of these transformations when applied to integer mul-
tiplication is given in Figure 9.3. Instead of using conventional multiplica-
tion, which requires O((log a)2) operations (a = max(a, b)) we choose a 
set of primes p 1, ••• , p,, compute a; = a mod p ;, b; = b mod p; and 
then c; = a ;b; mod p ;. These are all single precision operations and so 
they require O(r) steps. r must be sufficiently large so that ab < pi, ... , 
Pr. The precision of a is proportional to log a and hence the precision of 
ab is no more than 2 log a = O(log a). Thus r = O(log a) and the time 
for transforming into modular form and computing the r products is O(loga). 
Therefore the value of this method rests upon how fast we can perform the 
inverse transformation by the Chinese Remainder Algorithm . 

Modular Arithmetic 
445 
Suppose we consider how to compute the value in the Chinese Remainder 
Theorem for only two moduli: Given a mod p and b mod q we wish to 
determine the unique c such that c mod p = a and c mod q = b. The 
value for c which satisfies these two constraints is easily seen to be 
c = (b - a)sp + a 
where s is the multiplicative reciprocal of p mod q, i.e. an s which satisfies 
ps mod q = 1. To show that this formula is correct we note that 
((b -
a)sp +a) modp =a 
since the term (b - a)sp hasp as a factor. Secondly 
((b -
a)sp + a) mod q = (b - a)sp mod q + a mod q 
= (b - a) mod q + a mod q 
= (b -
a + a) mod q 
=b 
Procedure ONESTEPCRA below uses procedure EXEUCLID and arithmetic 
modulo p to compute the formula we've just described. 
procedure ONESTEPCRA(a, p, b, q) 
I la, bare in GF(p), gcd(p, q) = 111 
I I returns a value c such that c mod p = a and c mod q = b ./I 
integer a, b, p, q, t, pb, r, u 
t -
a modq 
pb - p modq 
s -EXEUCLJD(pb, q) 
u -
(b -
t) * s mod q 
retum(u*p + a) 
end ONESTEPCRA 
Algorithm 9.11 One step Chinese Remainder Algorithm 
The computing time is dominated by the call to EXEUCLID which requires 
O(log q) operations. 
The simplest way to use this procedure to implement the Chinese Re-
mainder Theorem for r moduli is to apply it r -
1 times in the following 
way. Given a set of congruences a; mod p;, 1 :s i :s r we let procedure 

• 
• 
• 
• 
446 
Algebraic Simpliftcation and Transformation 
Conventional 
multiplication 
Integers--------- Integer 
l 
products 
Division 
i 
Chinese Remainder 
I 
algorithm 
Integers ________ _.Products 
mod 
P 
Mod 
p 
mod P 
multiplication 
Figure 9.3 Integer multiplication by mod p transformations 
ONESTEPCRA be called r -
1 times with the following set of values for 
the parameters . 
a 
p 
b 
q 
output 
first time 
a1 
P1 
a2 
P2 
CJ 
second time 
C1 
P1P2 
a3 
p3 
C2 
third time 
C2 
P1P2PJ 
a4 
p4 
CJ 
r-1-st time 
Cr-2 
PIP2• ••Pr-I 
ar 
Pr 
Cr-I 
The final result c r- I is an integer such that c r - I mod pi = a i for 1 :s i :s r 
and Cr-1 < p1 ... Pr· The total computing time is O(r log q) = O(r 2) • 
An example: Suppose we wish to take 4, 6, 8 and compute 4 + 8 * 6 = 52. 
Letp1 = 7, p2 = 11. 
4 
(4 mod 7, 4 mod 11) = (4, 4) 
6 = (6 mod 7, 6 mod 11) = (6, 6) 
8 = (8 mod 7, 8 mod 11) = (1, 8) 
8*6 = (6*1mod7, 8*6 mod 11) = (6, 4) 
4 + 8*6 = (4 + 6mod7,4 + 4modll) = (3,8) 
So, we must convert the 2-tuple (3, 8) back to integer notation. Using 
procedure ONESTEPCRA with a = 3, b = 8, p = 7, q = 11 we get 

Even Faster Evaluation and Interpolation 
447 
1) t -
a mod q = 3 mod 11 = 3 
2) pb -
p mod q = 7 mod 11 = 7 
3) s - EXEUCLID(pb, q) = 8; k = 5 
4) 
u -
(b - t)s mod q = (8 -
3)8 mod 11 
40 mod 11 = 7 
5) return(u*p + a) = 7*7 + 3 = 52 
In conclusion we review the computing times for modular arithmetic. If 
a, b E GF(p) where p is single precision then 
Operation 
a+ b 
a· b 
alb 
Computing Time 
c -
(c1, .. ., c,) 
c, = c modp; 
0(1) 
0(1) 
O(logp) 
O(r log c) 
c -
(ci, .. ., c,) 
O(r 2) 
9.5 EVEN FASTER EVALUATION AND INTERPOLATION 
In this section we will study four problems 
(1) from an n-precision integer compute its residues modulo n single pre-
cision primes; 
(2) from an n-degree polynomial compute its values at n points; 
(3) from n single precision residues compute the unique n-precision in-
teger which is congruent to the residues; 
(4) from n points compute the unique interpolating polynomial through 
those points; 
We have seen in Sections 9.2 and 9.4 that the classical methods for prob-
lems (1)-(4) take O(n 2) operations. Here we will show how to use the fast 
Fourier transform to speed up all four problems. In particular we will 
derive algorithms for problems (1) and (2) whose times are O(n (log n)2) 
and for problems (3) and (4) whose time is O(n (log n)3). These algorithms 
will rely on the fast Fourier transform as it is used to perform n-precision 
integer multiplication in time O(n log n log log n). This algorithm, developed 
by Schonhage and Strassen is the fastest known way to multiply. Because 
this algorithm is complex to describe and already appears in several places 
(see the references), we will simply assume its existence here. Moreover to 

• 
• 
448 
Algebraic Simplification and Transformation 
simplify things somewhat we will assume that for n-precision integers and 
for n-degree polynomials the time to add or subtract is O(n) and the time 
to multiply or divide is O(n log n ). In addition we will assume that an ex-
tended gcd algorithm is available, (see Algorithm 9 .10) for integers or 
polynomials whose computing time is O(n (log n)2). 
Now consider the binary tree as shown in Figure 9.4. As we "go down" 
the tree the level numbers increase, while the root of the tree is at the top 
at level 1. The ith level has 2; - 1 nodes and a tree with k levels has a total 
of 2k -
1 nodes. We will be interested in computing different functions at 
every node of such a binary tree. So for example an algorithm for moving 
up the tree is 
procedure MOVEUPATREE(T, n) 
/In= 2k- 1 valuesarestoredinT(l:k, l:n)// 
//in locations T(k, 1), ... , T(k, n)l I 
I /The algorithm causes the nodes of a binary tree to be/ I 
I /visited such that at each node an abstract binary operation/ I 
I I denoted by * is performed. The resulting values are/ I 
//stored in the array T as indicated in Figure 9.4./ I 
for i - k -
1 to 1 by - 1 do 
p-1 
for j -
1 to 2 t (i -
1) do 
T(i. j) -
T(i + 1, p) * T(i + 1, p + 1) 
p -p + 2 
repeat 
repeat 
end MOVEUPATREE 
Algorithm 9.12 Moving up a tree 
Subsequently we will be concerned about the cost of the operation *• 
which is denoted by C( * ). Given the value of C( *) on the ith level and the 
above algorithm, the total time needed to compute every node in a tree is 
E 
2i-1C(*) 
(9.17) 
l.:5i.:5k-1 
Similarly an algorithm which computes elements as we go down the tree 
would be 

Even Faster Evaluation and Interpolation 
449 
Figure 9.4 A binary tree 
procedureMOVEDOWNATREE(S, T, n) 
/In = 2k- 1 and T(l, 1) is given./ I 
level 
I I Also S(l :n, 1 :k) is given containing a binary tree of values./ I 
I /The algorithm produces elements and stores them/ I 
I /in the array T(l :k, l:n) at the positions which// 
I I correspond to the nodes of the binary tree in Figure 9.4./ I 
fori -
2 to k do 
p-1 
for j -
1 to 2 t (i -
1) by 2 do 
T(i,j) - S(i,j)*T(i - l,p) 
T(i,j + 1) - S(i,j + O*T(i -
1, p) 
p-p+l 
repeat 
repeat 
endMOVEDOWNATREE 
Algorithm 9.13 Moving down a tree 
We now proceed to the specific problems. 
Problem 1. Let u be an n-precision integer and p 1, ••• , p,, single precision 
primes. We wish to compute then residues u; = u modp; which gives the 
mixed radix representation for u. We consider the binary tree in Figure 
9.5. 

• 
• 
• 
• 
• 
450 
Algebraic Simplification and Transformation 
Figure 9.5 Binary tree with moduli 
If n = 2 k - 1 then products on the ith level have precision 2 k -i, 1 s i s k. 
Using our fast integer multiplication algorithm we can compute the ele-
ments going up the tree. Therefore C( *) at the ith level is 2 k-i - t(k - i -
1) 
and the total time to complete the tree is 
r; 
2i-12k-i-l(k _ i _ 1) 
!Sisk-I 
(9.18) 
\ 
k(k + 1) 
/ 
= 2k-\k2 -
2 
+ 1 j = O(n(log n) 2) 
Now to compute the n residues u; = u mod p; we reverse direction and 
proceed to compute functions down the tree. Since u is n-precision and 
the primes are all near the maximum size of a single precision number we 
first compute u mod p 1 ••• Pn = ub. Then the algorithm will continue by 
computing next 
U2,1 = ubmodp1 ···Pn12andu2.2 = ubmodpn12+1 ···Pn· 
Then we compute 
U 3,1 = U 2,1 mod p 1 ••• p n/4, U 3,2 = U 2,1 mod p n/4+ 1 • • • p n/2 
U3,3 = U2,2modpn12+1 ... P3n/4, U3,4 = U2,2modp3nl4+1. ·. Pn 
and so on down the tree until we have 

Even Faster Evaluation and Interpolation 
451 
A node on level i is computed using the previously computed product of 
primes at that position plus the element u J. 1- 1 at the descendant node. 
The computation requires a division operation so C( *) at the ith level is 
2k-•+l(k -
i + 1) and the total time for problem 1 is 
2'-12k-i+ l(k -
i + 1) 
1 $:i:Sk 
(9.19) 
\ 
k(k - l)J 
2k(k 2 -
2 
= O(n(log n)2) 
Problem 2. 
Let P(x) be an n-degree polynomial and xi, ... , x" n single 
precision points. We wish to compute then values P(x;) 1 .::5 i .::5 n. We 
consider the binary tree in Figure 9.6. 
If n = 2k- 1 the products on the ith level have degree 2k-'. Using fast 
polynomial multiplication we compute the elements going up the tree. 
Therefore C( *) on the ith level is 2 k - 1- 1 (k -
i -
1) and the total time to 
complete the tree is 
1 $:i..Sk-l 
(9.20) 
\ 
k(k + 1) 
J 
2k- 2(k 2 -
2 
+ 1 
= O(n(log n)2) 
Note that this process shows how to compute the elementary symmetric 
functions of x1, ... , x. in O(n(log n)2) operations. 
Now to compute then values P(x,) we reverse direction and proceed to 
compute functions down the tree. If D(x) = (x -
x 1) 
• • • (x -
x .) then 
we can divide P(x) by D(x) obtaining a quotient and remainder as follows 
P(x) = D(x)Q(x) + R 11(x) 
where the degree of R 11 is less than the degree of D. By substitution it 
follows that 
P(x 1) = R 1 i(x ,) , 1 .::5 i .::5 n. 
The algorithm would continue by next dividing R 11(x) by the first n/2 

452 
Algebraic Simplification and Transformation 
Figure 9.6 A binary tree with linear moduli 
factors of D(x) and then by the second n/2 factors. Calling these poly-
nomials D 1(x) andD2(x) we get the quotients and remainders 
R11(x) = D1(x)Q 1(x) + Rn(x) 
R11(x) = D2(x)Qi(x) + R22(x) 
By the same argument we see that 
1 s; i s; n/2 
n/2 + 1 s; i s; n 
(9.21) 
Eventually we will arrive at constants Rkl, ..• , Rk,ll(k-1> where P(x;) = 
Rk,1 for 1 s; i s; n. Since the time for multiplication and division of poly-
nomials is the same, C( *) on the ith level is 2k-i(k -
i) and the total for 
problem 2 is 
E 2i-12k-i(k -
i) 
lsisk 
(9.22) 

Even Faster Evaluation and Interpolation 
453 
Problem 3. 
Given n residues u; of n single precision primes p; we wish to 
find the unique n-precision integer u such that u mod P• = u;, 1 s: i s: n. 
It follows from the Chinese remainder theorem, Theorem 9.7, that this 
integer exists and is unique. For this problem as for problem 1 we will 
assume the binary tree in Figure 9.5 has already been computed. What 
we need to do is go up the tree and at each node compute a new integer 
which is congruent to the product of the integers at the children nodes. For 
example, at the first level let u; = Uk.1, 1 s: i :s: n = 2k- 1 • Then for i odd we 
compute from Uk.1 mod p; and Uk.1+1 mod p1+1 the unique integer Uk-1.i = 
u k,, mod p; and u k-1.1 = u k.• + 1 mod p 1+ 1. Thus u k-1.1 lies in the range [O, 
P•P1+1). Repeating this process up the tree we will eventually produce the 
integer u in the interval [O, p 1 ••• p n). So we need to develop an algo-
rithm which proceeds from level i to i -
1. But we already have such an 
algorithm, the one step Chinese remainder algorithm or procedure 
ONESTEPCRA. The time for this algorithm was shown to be dominated 
by the time for EXEUCLID. Using our assumption that EXEUCLID can 
be done in O(n(log n )2) operations, where n is the maximum precision of 
the moduli, then this is also the time for ONESTEPCRA. Note the dif-
ference between its use in this section and in Section 9.4. In the latter sec-
tion only one of the moduli was growing. 
We now apply this one step algorithm to an algorithm which proceeds 
up the tree of Figure 9.5. The total time for problem 3 is seen to be 
ls..isk-1 
(9.23) 
2k- 2 E (k - i -
1)2 = O(n(log n)3) 
1 sislc-1 
Problem 4. 
Given n values y1, ... , Yn at n = 2k - 1 points (x1, ... , Xn) we 
wish to compute the unique interpolating polynomial P(x) of degree s: 
n -
1 such that P(x 1) 
= y 1• For this problem as for problem 2 we will 
assume that the binary tree in Figure 9.6 has already been computed. 
Again we need an algorithm which goes up the tree and at each node com-
putes a new interpolating polynomial from its two ancestors. For example 
at level k we compute polynomials Rk1(x), ... , Rkn(x) such that Rk;(x;) 
y ;. Then at level k -
1 we compute R k- i.i. .. ., R k -1.n12 such that 
Rk-1,;(X;) = y; 
Rk-1,;(X;+1) = Yi+I 
and so on until R 11(x) = P(x). Therefore we need an algorithm which com-

454 
Algebraic Simplification and Transformation 
bines two interpolating polynomials to give a third which interpolates at 
both sets of points. This requires a generalization of procedure INTERP, 
Algorithm 9. 7. 
procedureBALANCEDINTERP(Ul, U2, Ql, Q2, m) 
I !Ul, U2, Ql, Q2 are all polynomials in x such that// 
I !Ul interpolates the points x1, ... , Xm12! I 
I I U2 interpolates x m12 + 1, .. ., xml I 
//Ql = (x -
X1) ... (x -
Xmd, Q2 = (x -
Xm12+1) ... (x -
Xm)I / 
/ lgcd(Ql, Q2) = 1. A polynomial U3(x) is computed and returned./ I 
I !U3(x;) = Ul(x;) for 1 s. i s. m/211 
I IU3(x;) = U2(x;) for m/2 + 1 ::5 i ::5 ml! 
I /and the degree of U3 is ::5 m - 1./ I 
UlB -
PMOD(Ul, Q2) //PMOD(A, B) computes the polynomial re-// 
//mainder// 
CB - PMOD(Ql, Q2) //of A(x) divided by B(x)I / 
CBI -
EXEUCLID(CB, Q2) //the extended Euclidean algorithm for// 
I I polynomials/ I 
C - PMOD(CBl, Q2) 
U3 -
PADD(Ul, PMUL(PMUL(PSUB(U2 - UIB), C), Ql)) 
end BALANCEDINTERP 
Algorithm 9.14 Balanced interpolation 
We note that steps one, two and three above imply that there exists 
quotients Cl, C2, C3 such that 
U1 = Q2*Cl + UlB, deg(U1B) < deg(Q2) (a) 
Q1 = Q2~C2 + CB, deg(CB) 
< deg(Q2) (b) 
C*CB + C3*Q2 = 1, deg(C) < deg(Q2) 
(c) 
C is the multiplicative inverse of CB modulo Q2. Therefore 
U3 = Ul + (U2 -
UlB)*C*Ql (i) 
U3 = Ul + (U2 + Q2*Cl - Ul)((l - C3*Q2)/CB)*Ql (ii) 
using (a) and (c). By (i) U3(x;) = Ul(x;) for 1 ::5 i ::5 m/2 since Ql(x) 
evaluated at those points is zero. By (ii) it is easy to see that U3(x) = U2(x) 
at the pointsxm12+1, ... , Xm. 
Now steps 1 and 2 take O(m log m) operations. In order to compute the 

References and Selected Readings 
455 
multiplicative inverse of CB we use the extended gcd algorithm for poly-
nomials which takes O(m(log m)2) operations. The time for step 4 is no 
more than O(m log m) so the total time for one step interpolation is O(m 
(log m)2). 
Applying this one step algorithm as we proceed up the tree gives a total 
computing time for problem 4 of 
E 2i-l2k-i- 1(k - i -
1)2 = O(n(log n) 3) 
(9.24) 
I S:iS:k-1 
The exercises show how one can further reduce the time for problems 
3 and 4 using the idea of preconditioning. 
REFERENCES AND SELECTED READINGS 
The paper which presented the fast Fourier transform to a modern audience was 
"An algorithm for the machine calculation of complex Fourier series" by J. M. 
Cooley and J. W. Tukey, Math. Comp. 19, 196S, 297-301. 
Other papers of interest concerning this algorithm are 
"History of the fast Fourier transform" by J. M. Cooley, P. A. Lewis, and P. D. 
Welch, Proc. IEEE, SS, 1967, 167S-1679. 
"The fast Fourier transform: its role as an algebraic algorithm" by John D. Lipson, 
Proc. XX th ACM Conj, Houston, 436-441. 
"Algebraic theory of finite Fourier transforms" by P. J. Nicholson, J. Computer 
and System Sciences, S:S, 1971, S24-S49. 
"The fast Fourier transform on a finite field" by J. M. Pollard, Math. Comp., 2S, 
114, April 1971, 36S-374. 
"Discrete Fourier transform when the number of data points is prime" by C. M. 
Rader, Pree. IEEE, S6, 1968, 1107-1108. 
An even faster method for computing the Fourier transform has just recently ap-
peared. A description of it can be found in 
"On computing the discrete Fourier transform" by S. Winograd, IBM Research 
Report RC6291, Watson Research Center, Yorktown Heights, N.Y. December 1976. 

456 
Algebraic Simplification and Transformation 
For an interesting collection of papers which deal with evaluation, interpolation, 
and modular arithmetic see 
"Evaluating polynomials at many points" by A. B. Borodin and I. Munro, Infor-
mation Processing Letters, 1 :2, 1971, 66-68. 
The Computational Complexity of Algebraic and Numeric Problems, by Borodin, 
A. B., and I. Munro, American Elsevier, New York, 1975. 
"Polynomial evaluation via the division algorithm-the fast Fourier transform re-
visited" by C. M. Fiduccia, Proc. 4th Annual ACM Symposium on Theory of 
Computing, 1972, 88-93. 
"On decreasing the computing time for modular arithmetic" by L. E. Heindel and 
E. Horowitz, Proc. IEEE 12th Annual Symposium on Switching and Automata 
Theory, 1971, 126-128. 
"A fast method for interpolation using preconditioning" by E. Horowitz, Information 
ProcessingLetters, 1:4, 1972, 157-163. 
"A unified view of the complexity of evaluation and interpolation" by E. Horowitz, 
Acta Informatica, 3, 1974, 123-133. 
"Chinese remainder and interpolation algorithms" by J. Lipson, Proc. 2nd Sympo-
sium on Symbolic and Algebraic Manipulation, 1971, 372-391. 
"Fast modular transforms via division" by R. Moenck and A. B. Borodin, Proc. 
IEEE 13th Annual Symposium on Switching and Automata Theory, 1972, 90-96. 
For more on mathematical symbol manipulation systems which allow for operations 
on mathematical expressions see 
"Computer algebra of polynomials and rational functions" by G. E. Collins, Amer-
ican Math. Monthly, 80:7, 1973, 725-754. 
ALTRAN Users Manual by W. S. Brown, 3rd edition, Bell laboratories, Murray 
Hill, New Jersey. 
MACSYMA Users Manual by J. Moses et. al., M.l.T., Cambridge, Mass. 
REDUCE Users Manual by A. Hearn, Computer science, University of Utah, Salt 
Lake City, Utah. 
The use of the FFT plus modular arithmetic for multiplying large precision integers 
was originally given by 
"Schnelle multiplikation grosser zahlen" by A. Schonhage and V. Strassen, Com-
puting, 7, 1971, 281-292. 

Exercises 
457 
English accounts of the method, which requires O(n log n log log n) operations to 
multiply two n bit integers, can be found in 
The Design and Analysis of Computer Algorithms by A. V. Aho, J. E. Hopcroft, 
and J. D. Ullman, Addison Wesley, Reading, Mass. 1974. 
and 
The Art of Computer Programming: Semi-Numerical Algorithms by D. E. Knuth, 
vol. 11, Addison Wesley, 1969. 
EXERCISES 
1. Devise an algorithm which accepts a number in decimal and produces the 
equivalent number in binary. 
2. 
Devise an algorithm which performs the inverse transformation of exercise 1. 
3. 
Show the tuples which would result :tJy representing the polynomials 5.x 2 + 
3x + 10 and 7x + 4 at the values x = 0, 1, 2, 3, 4, 5, 6. What set of tuples 
are sufficient to represent the product of these two polynomials. 
4. If A(x) = anxn + ... + a ix + ao then the derivative of A(x), A'(x) 
nanxn-i + ... + ai. Devise an algorithm which produces the value of a 
polynomial and its derivative at a point x = v. Determine the number of re-
quired arithmetic operations. 
5. Devise a divide-and-conquer algorithm to evaluate a polynomial at a point. 
Analyze carefully the time for your algorithm. How does it compare to Homer's 
rule? 
6. A polynomial of degree n > 0 has n derivatives, each one obtained by taking 
the derivative of the previous one. Devise an algorithm which produces the 
values of a polynomial and its n derivatives. 
7. Assume that polynomials such as A(x) = a.x" + ... + ao are represented 
by an array POLY(O:n + 1) where P(O) = n and P(i) = an-i+l for 1 :5 i :5 
n + 1. Write a procedure PADD(R, S, T) which takes the polynomials in 
the arrays R and S and places their sum in the array T. 
8. 
Using the same assumptions as for problem 7, write a procedure PMUL(R, 
S, T) which computes the product of the polynomials in R and S and places 
the result in T. 

458 
Algebraic SimpllDcatlon and Transformation 
9. Let A(x) = anx" + ... + ao, p = n/2 and q 
of Horner' s rule states that 
f n/21 . Then a variation 
A(x) = ( .. . (a2pX 2 + a2p-2)x 2 + .. . )x2 + ao 
+ (( .. . (a2q-1x 2 + a2q-3)x 2 + .. . )x2 + a1)x 
Show how to use thiS formula to evaluate A(x) at x = v and x = -v. 
10. Given the polynomial A(x) as above devise an algorithm which computes the 
coefficients of the polynomial A(x + c) for some constant c. 
11. Suppose the polynomial A(x) has real coefficients but we wish to evaluate A 
at the complex number x = u + iv, u and v being real. Develop an algorithm 
to do this. 
12. Suppose the polynomial A(x) = amx•m + ... + a 1xel where a; ~ 0 and 
em > em -
1 > ... > el ~ 0 is represented by an array POLY(0:2m) 
where P(O) = m, P(l) = em, P(2) = am, ..• , P(1m -
1) = e 1, P(1m) = a 1. 
Write a procedure PADD(R, S, n which computes the sum of two such poly· 
nomials and stores the result in the array T. 
13. Using the same assumptions as in exercise 9 write a procedure PMUL(R, S, T) 
which computes the product of the polynomials represented in R and S and 
places the result in T. What is the computing time of your algorithm? 
14. Determine the polynomial of smallest degree which interpolates the points 
(0, 1), (1, 2), (2, 3). 
15. Given n points (x;, y;), 1 :s i :s n devise an algorithm which computes both 
the interpolating polynomial A(x) and its derivative at the same time. How 
efficient is your algorithm? 
16. Prove that the polynomial of degree :s n which interpolates n + 1 points is 
unique. 
17. The binary method for exponentiation uses the binary expansion of the ex-
ponent, n, to determine when to square the temporary result and when to 
multiply it by x. Since there are Llog nJ + 1 bits inn the algorithm requires 
O(log n) operations which is an order of magnitude faster than iteration. Algo-
rithm 9.15 below describes the procedure precisely. Show how to use the 
binary method to evaluate a sparse polynomial in time m + loge m. 

procedureEXPONENTIATE(x, n) 
I /returns xn for an integer n ~ 0.1 I 
integer m, n real x 
m - n;y -
1; z -
x 
while m > 0 do 
while mod(m, 2) = 0 do 
m -
m/2; z -
z *z 
repeat 
m -m - l;y-y*z 
repeat 
retum(y) 
end EXPONENTIATE 
Algorithm 9.15 Binary exponentiation 
Exercises 
459 
18. 
Show the result of applying the Fourier transform to the sequence (ao, ... , a 1). 
19. 
The iterative version of the FFT, Algorithm 9.9, rests upon the fact that dividing 
a polynomial by x' -
c can be done efficiently. Prove that if P(x) = 
a21-1x 21 - 1 + ... + ao then the remainder of P(x)/(x' -
c) is the sum of 
(aJ + caJ+i)xiforj = 0, ... ,t -
1. 
20. 
Given the finite field A = (0, 1, ... , p -
1) one of these elements x, is such 
that x 0, x, x 2, ••• , xP- 2 is equal to all of the nonzero elements of A.xis called 
a primitive element. If x is a primitive element and n divides p -
1 then 
x< P - llln is a primitive nth root of unity. To find such a value x we use the 
fact that x < p- l)lq ~ 1 for each prime factor q of p -
1. Use this fact to 
write an algorithm which, when given a, b and e finds the a largest Fourier 
primes less than or equal to b of the form 2fk + 1 with f ~ e. For example 
if a = 10, b = 2 31 and e = 20 the answer is 
p 
f 
least primitive element 
2130706433 
24 
3 
2114977793 
20 
3 
2113929217 
25 
5 
2099249153 
21 
3 
2095054849 
21 
11 
2088763393 
23 
5 
2077229057 
20 
3 
2070937601 
20 
6 
2047868929 
20 
13 
2035286017 
20 
10 
Table 9.1 
Fourier primes 

460 
Algebraic Slmpllftcatlon and Transformation 
21. 
The Fourier transform can be generalized to k dimensions. For example the 
2-dimensional transform takes the matrix a(O:n -
1, O:n -
1) and yields the 
transformed matrix 
A(i,j) = 
E 
E 
ak,/W -(ik+jl)ln 
Osk sn -1 Os/ sn -1 
(9.25) 
for an n x n matrix with elements in GF(p). The inverse transformation is 
a(i,j) = (1/n 2) 
E 
E 
A(k, l)w-(ik+jl)ln 
O.sk.sn -1 Os/.sn-1 
(9.26) 
Define the 2-dimensional convolution C(i, j) = A(i, j)*B(i, j) and derive an 
efficient algorithm for computing it. 
22. 
Investigate the problem of evaluating an nth degree polynomial at the n points 
2;, 0 s; i s; n -
1. Note thatA(2i) requires no multiplications, only n additions 
and n shifts. 
23. Given then points (2i, y;), 0 s; i s; n -
1 where y; is an integer, determine 
an algorithm which produces the unique interpolating polynomial of degree 
s; n. Try to minimize the number of multiplications. 
24. 
In Section 9.5 the time for the n value Chinese remainder algorithm and n 
point interpolation is shown to be O(n(log n) 3). However it is possible to get 
modified algorithms whose complexity is O(n(log n) 2) if we allow certain values 
to be computed in advance without cost. Assuming the moduli and the points 
are so known, what should be computed in advance to lower the complexity 
of these two problems? 
25. 
[Diffie, Hellman, Rivest, Shamir, Adelman] Some people are cormected to a 
computer network. They need a mechanism by which they can send messages 
to one another which can't be decoded by a third party (security) and in addi-
tion be able to prove that any particular message was actually sent by a given 
person (a signature). In short each person needs an encoding mechanism 
E and a decoding mechanism D such that D(E(M)) = M for any message M. 
A signature feature is possible if the sender, A, first decodes his message, 
sends it and it is encoded by the receiver using As encoding scheme E, (E(D(M)) 
= M). The E for all users is published in a public directory. The scheme to 
implement D and E proposed by the last three people above relies on the 
difficulty of factoring versus the simplicity of determining several large (100 
digit) primes. Using modular arithmetic see if you can construct an encoding 
function which is invertible, but only if the factors o.f a number are known. 

Chapter 10 
LOWER BOUND THEORY 
Lower Bound Techniques 
In the previous nine chapters we have surveyed a broad range of problems 
and their algorithmic solution. Our main task for each problem has been 
to obtain a correct and efficient solution. If two algorithms for solving the 
same problem were discovered and if their times differed by an order of 
magnitude, then the one with the smaller order was generally regarded as 
superior. But still we are left with the question "is there a faster method". 
The purpose of this chapter is to expose you to some techniques that have 
been used to establish that a given algorithm is the most efficient possible. 
The way this is done is by discovering a function, g(n), which is a lower 
bound on the time that any algorithm must take to solve the given problem. 
If we have an algorithm whose computing time is the same order as g(n) 
then we know that asymptotically we can do no better. 
Recall from chapter one that there is a mathematical notation for ex-
pressing lower bounds. lf.ftn) is the time for some algorithm, then we write 
f(n) = O(g(n)) to mean that g(n) is a lower bound for f(n). Formally this 
equation can be written if there exists positive constants c and no such 
that Jf(n) J 
:::::: c J g(n) J for all n > n 0• In addition to developing lower 
bounds to within a constant factor, we will also be concerned with deter-
mining more exact bounds whenever this is possible. 
Deriving good lower bounds is often more difficult than devising efficient 
algorithms. Perhaps this is because a lower bound states a fact about all 
possible algorithms for solving a problem. Usually we cannot enumerate 
and analyze all of these algorithms, so lower bound proofs are often hard 
to obtain. 
However, for many problems it is possible to easily observe that a lower 
bound identical to n exists, where n is the number of inputs (or possibly 
outputs) to the problem. For example consider all algorithms which find the 
maximum of an unordered set of n integers. Clearly every integer must be 
461 

462 
Lower Bound Theory 
examined at least once and so O(n) is a lower bound for any algorithm which 
solves this problem. Or, suppose we wish to find an algorithm which ef-
ficiently multiplies two n x n matrices. Then O(n 2) is a lower bound on 
any such algorithm since there are 2n 2 inputs which must be examined and 
n 2 outputs to be computed. Bounds such as these are often referred to as 
trivial lower bounds because they are so easy to obtain. We know how to 
find the maximum of n elements by an algorithm which uses only n -
1 
comparisons so there is no gap between the upper and lower bound for 
this problem. But for matrix multiplication the best known algorithm re-
quires O(n 2+E) * operations ( E > O) and so there is no reason to believe that 
a better method cannot be found. 
In section 10.1 we present the computational model called comparison 
trees. These are useful for determining lower bounds for sorting and 
searching problems. In section 10.2 we examine the technique for estab-
lishing lower bounds called an oracle and also we study a closely related 
method called an adversary argument. In section 10.3 we study some argu-
ments which have been used to find lower bounds for the arithmetic and 
algebraic problems discussed in Chapter 9. Then in section 10.4 we ex-
amine some lower bound results assuming that more than one processor 
is available. 
10.1 COMPARISON TREES FOR SORTING AND SEARCHING 
In this section we will study the use of comparison trees for deriving lower 
bounds on problems which are collectively called sorting and searching. 
We will see how these trees are especially useful for modeling the way in 
which a large number of sorting and searching algorithms work. By ap-
pealing to some elementary facts about trees the lower bounds are obtained. 
Suppose that we are given a setS of distinct values upon which an ordering 
relation "<" holds. The sorting problem calls for determining a permuta-
tion of the integers 1 to n, say p(l) to p(n) such that then distinct values 
from S stored in A(l:n) satisfy A(p(l)) < A(p(2)) < ... < A(p(n)). The 
ordered searching problem asks if a given element x E S occurs within the 
elements in A(l:n) which are ordered so that A(l) < ... < A(n). If x is 
inA(l:n) then we are to determine an i between 1 and n such thatA(i) = x. 
The merging problem assumes that two ordered sets of distinct inputs 
from Sare given in A(l:m) and B(l:n) such that A(l) < ... < A(m) and 
B(l) < . . . < B(n); these m + n values are to be rearranged into an 
array C(l:m + n) so that C(l) < ... < C(m + n). For all of these prob-
lems we will restrict the class of algorithms we are considering to those 
which work solely by making comparisons between elements. No arithmetic 
*see chapter 3 for more details 

Comparison trees for sorting and searching 
463 
involving elements is permitted, though it is possible for the algorithm to 
move elements around. This class of algorithms is referred to as comparison 
based algorithms. We rule out algorithms such as radix sort which decom-
pose the values into subparts. 
In obtaining the lower bound for the ordered searching problem, we 
shall consider only those comparison based algorithms in which every com-
parison between two elements of S is of the type "compare x and A(i)". 
Any searching algorithm which satisfies this restriction can be described 
by an extended binary tree (see section 3.2 and 5.3). Each internal node 
in this tree represents a comparison between x and an A(i). There are 
three possible outcomes of this comparison: x < A(i), x = A(i), and x > 
A(i). We may assume that if x = A(i) then the algorithm terminates. 
Hence the progress of the algorithm may be described by a binary tree in 
which the left branch is taken if x < A(i) and the right branch is taken if 
x > A(i). If the algorithm terminates following a left or right branch (but 
before another comparison between x and A(i)) then no i has been found 
such that x = A (i) and the algorithm must declare the search unsuccessful. 
Figure 10.1 shows two comparison trees, one modeling a linear search 
algorithm and the other a binary search (see Algorithm 3.3). It should be 
easy to see that the comparison tree for any search algorithm must contain 
at least n internal nodes corresponding to the n different values of i for 
which x = A(i) and at least one external node corresponding to an unsuc-
cessful search. 
Theorem 10.1 Let A(l:n), n ~ 1, contain n distinct elements, ordered so 
that A(l) < ... < A(n). Let FIND(n) be the minimum number of com-
parisons needed, in the worst case, by any comparison based algorithm to 
recognize if x E A(l:n). Then FIND(n) ;?: pog (n + 1)1. 
Proof: 
Consider all possible comparison trees which model algorithms 
to solve the searching problem. FIND(n) is bounded below by the distance 
of the longest path from the root to a leaf in such a tree. There must be 
n internal nodes in all of these trees corresponding to the n possible suc-
cessful occurrences of x in A. If all internal nodes of a binary tree are at 
levels less than or equal to k, then there are at most 2 k -
1 internal nodes. 
Thus n :s 2k - 1 and FIND(n) = k ~ pog (n + 1)1. 
D 
From the above theorem and theorem 3.2 we can conclude that binary 
search is an optimal worst case algorithm for solving the searching problem. 
Now let's consider the sorting problem. We can describe any sorting algo-
rithm which satisfies the restrictions of the comparison tree model by an 

464 
Lower Bound Theory 
Figure 10.1 Comparison trees for two searching algorithms 
extended binary tree. Since the keys are distinct, any comparison between 
A(i) and A(j) must result in one of two possibilities: either A(i) < A(j) or 
A(i) > A(j). Thus this tree will be a binary tree where the value of any 
internal node is the pair i;j which represents the comparison A(i) with 
A(j). If A(i) is less than A(j) then the algorithm proceeds down the left 
branch of the tree and otherwise it proceeds down the right branch. The 
external nodes represent termination of the algorithm. Associated with 
every path from the root to an external node is a unique permutation. To 
see that this permutation is unique, note that the algorithms we allow are 
only permitted to move data and make comparisons. The data movement 
on any path from the root to an external node is the same no matter what 
the initial input values are. As there are n! different possible permutations 
of n items, and any one of these might legitimately be the only correct 

Comparison trees for sorting and searching 
465 
answer for the sorting problem on a given instance, the comparison tree 
must have at least n! external nodes. 
Figure 10.2 shows a comparison tree for sorting 3 items. The first com-
parison is A(l) : A(2). If A(l) is less than A(2) then the next comparison 
is A(2) with A(3). If A(2) is less than A(3) then the left branch leads to an 
external node containing 1,2,3. This implies that the original set was al-
ready sorted for A(l) < A(2) < A(3). The other five external nodes cor-
respond to the other possible orderings which could yield a sorted set. 
We consider the worst case for all comparison based sorting algorithms. 
Let T(n) be the minimum number of comparisons which are sufficient to 
sort n items in the worst case. Using our knowledge of binary trees once 
again, if all internal nodes are at levels less than k then there are at most 
2k external nodes, (one more than the number of internal nodes). There-
fore, letting k = T(n) 
n! :S 2 T(11) 
Since T(n) is an integer we get the lower bound 
T(n) ~ pog n!l 
By Stirling's approximation (see exercise 7) it follows that 
l!og n!l = n log n - n!ln 2 + (1/2) log n + 0(1) 
where In 2 refers to the natural logarithm of 2 while log n is the logarithm 
to the base 2 of n. This formula shows that T(n) is of the order n log n. 
Figure 10.2 A comparison tree for sorting three items 

466 
Lower Bound Theory 
Hence we say that no comparison based sorting algorithm can work in 
less than O(n log n) time. (This bound can be shown to hold even when 
operations more complex than just comparisons are allowed, e.g. see in 
the references the paper by N. Friedman who considers operations such as 
addition, subtraction and in some cases arbitrary analytic functions). 
How close do the known sorting methods get to this lower bound of T(n)? 
Consider the "bottom-up" version of mergesort which first orders consecu-
tive pairs of elements, and then merges adjacent groups of size 2, 4, 8, ... 
until the entire sorted set is produced. The worst case number of compari-
sons required by this algorithm is bounded by 
E (n/2i) (2; -
1) ::5 n log n - O(n) 
1 :s;i.sk 
(10.1) 
Thus we know at least one algorithm which requires slightly less than n log 
n comparisons. Is there still a better method? 
The sorting strategy called binary insertion sorting works in the following 
way. The next unsorted item is chosen and a binary search (see Algorithm 
3.3) is performed on the sorted set to determine where to place this new 
item. Then the sorted items are moved to make room for the new value. 
This algorithm will require O(n 2) data movements to sort the entire set 
but far fewer comparisons. Let BISORT(n) be the number of comparisons 
it requires. Then by the results of section 3.2 
BISORT(n) = 
E 1log2 kl 
lsksn 
(10.2) 
which is equal to 
pog n l - 2 pog n l + 1 
Now suppose we compare BISORT(n) with the theoretical lower bound. 
This is done in Table 10.1. 
n 
1 2 3 4 5 
6 
7 
8 
9 10 
11 
12 13 
T(n) 
0 1 3 5 7 10 
13 
16 19 22 
26 29 33 
BISORT(n) 
0 1 3 5 8 11 
14 17 21 
25 29 33 37 
Table IO.I 
Bounds for minimum comparison sorting 

Comparison trees for sorting and searching 
467 
Scanning Table 10.1 we observe that for n = 1, 2, 3, and 4 the values 
are the same so binary insertion is optimal. But for n = S there is a dif-
ference of one and so we are left with the question of whether 7 or 8 is the 
minimum number of comparisons in the worst case needed to sort S items. 
This question has been answered by Lester Ford Jr. and Selmer Johnson 
who presented a sorting algorithm which requires even fewer comparisons 
than the binary insertion method. In fact their method requires exactly 
T(n) comparisons for 1 :s n :s 11 and 20 :s n :s 21. 
Merge insertion sorting 
To see how the Ford-Johnson method works suppose we consider the sorting 
of 17 items which originally reside in SORTED(l :17). We begin by com-
paring consecutive pairs SORTED(l): SORTED(2), SORTED(3) : SOR-
TED(4), ... , SORTED(lS) : SORTED(16) placing the larger items into 
the array HIGH and the smaller items into the array LOW. SORTED(l 7) 
is placed into LOW(9). Then we sort the array HIGH using this algorithm 
recursively. When this is done we have that LOW(l) < HIGH(l) < ... < 
HIGH(8) and though LOW(2) through LOW(9) remain unsorted, we do 
know that LOW(i) :s HIGH(i) for 2 :s i :s 8. Now if we insert LOW(2) 
into the sorted set, that will possibly require two comparisons and at the 
same time cause the insertion of LOW(3) to possibly require 3 comparisons 
for a total of S. A better approach is to first insert LOW(3) among the 
items LOW(l), HIGH(l), HIGH(2) using binary insertion followed by in-
serting LOW(2). Each insertion requires only 2 comparisons and the merged 
elements are stored back into the array SORTED. This gives us the new 
relationships SORTED(l) < SORTED(2) < . . . < SORTED(6) < 
HIGH(4) < HIGH(S) < HIGH(6) < HIGH(7) < HIGH(8) and LOW(i) 
:s HIGH(i), for 4 :s i :s 8. Eleven items are now sorted while six remain 
to be merged. If we insert LOW(4) followed by LOW(S), 3 and 4 compari-
sons may be needed respectively. Once again it is more economical to first 
insert LOW(S) followed by LOW(4), each insertion requiring at most 3 
comparisons. This gives us the new situation SORTED(l) < . . . < 
SORTED(lO) < HIGH(6) < HIGH(7) < HIGH(8) and LOW(i) < 
HIGH(i), 6 :s i :s 8. If we insert LOW(7), which will require only four 
comparisons then LOW(8) will require S comparisons. However if we in-
sert LOW(9) followed by LOW(8), LOW(7), and LOW(6) then each item 
will require at most four comparisons. We do the insertions in the order 
LOW(9) to LOW(6) yielding the completely sorted set of seventeen items. 
A count of the total number of comparisons needed to sort the seventeen 
items is: 8 to compare SORTED(i): SORTED(i + 1), 16 to sort HIGH(1:8) 

468 
Lower Bound Theory 
using merge insertion recursively, 4 to insert LOW(3) and LOW(2), 6 to 
insert LOW(S) and LOW(4), and 16 to insert LOW(9) to LOW(6) requiring 
a total of SO. The value of T(n) for n = 17 is 49 so merge insertion requires 
only one more comparison than the theoretical lower bound. 
In general, merge insertion can be summarized as follows: Let SORTED(l 
:n) contain the n items to be sorted. Make pairwise comparisons of 
SORTED(i) and SORTED(i + 1) placing the larger items into an array 
HIGH and the smaller items into array LOW. If n is odd then the last 
item of SORTED is appended to LOW. Now apply merge insertion to the 
elements of HIGH. After that we know that HIGH(l) :s HIGH(2) :s ... :s 
HIGH( Lnl2J) and LOW(i) :s HIGH(i) for 1 :s i :s Ln12J. Now we 
insert the items of LOW into the HIGH array using binary insertion. How-
ever, the order in which we insert the LOW's is important. We want to 
select the maximum number of items in LOW such that the number of 
comparisons required to insert each one into the already sorted list is a 
constant}. As we have seen from our example the insertion will proceed in 
the order LOW(t), LOW(tJ -
1), ... , LOW(tJ-t + 1) where the tJ are a 
set of increasing integers. In fact t J has the form t J = 2J -
t J _ 1 and in 
the exercises it is shown that this recurrence relation can be solved to give 
the formula tJ = (2J+t + ( -1)1)/3. Thus items are inserted in the order 
LOW(3), LOW(2); LOW(S), LOW(4); LOW(ll), LOW(lO), LOW(9), 
LOW(8), LOW(7), LOW(6); etc. 
It can be shown that the time for this algorithm is 
(10.3) 
For n = 1 to 21 the values of this sum are 
0, 1, 3, S, 7, 10, 13, 16, 19,22, 26, 30, 34, 38,42, 46, SO, S4, S8, 62, 66. 
Comparing these values with the values of the lower bound T(n), we see 
that merge insertion is truly optimal for 1 :s n :s 11 and n = 20, 21. 
Is it the case that the Ford-Johnson algorithm actually requires the fewest 
number of comparisons needed to sort n items for all values n? Recently 
Glenn Manacher has exhibited an algorithm which uses fewer comparisons 
than the Ford-Johnson algorithm for infinitely many n. The smallest such 
value is n = 189. His algorithm makes use of a minimum comparison 
merging algorithm which we will see in the next section. For more on 
Manacher's result see his paper in the references. 

Oracles and Advenary Arguments 
469 
Lower bounds on selection 
From our previous discussion it should be clear that any comparison tree 
which models comparison based algorithms for finding the maximum of n 
elements has at least 2 n - 1 external nodes. This follows since each path 
from the root to an external node must contain at least n -
1 internal 
nodes implying at least n -
1 comparisons, for otherwise at least two of 
the input items never lose a comparison and the largest is not yet found. 
Now suppose we let L k(n) denote a lower bound for the number of 
comparisons necessary for a comparison based algorithm to determine the 
largest, 2nd largest, ... , kth largest out of n elements, in the worst case. 
L 1(n) = n -
1 from above. Since the comparison tree must contain 
enough external nodes to allow for any possible permutation of the input 
it follows immediately that L k(n) ~ IJog n(n -
1) ... (n -
k + 1)1 . 
Theorem 10.2 L k(n) ~ n -
k + IJog n(n -
1) ... (n -
k + 2) l for 
all integers k, n where 1 :s k :s n. 
Proof: 
As before internal nodes of the comparison tree contain integers of 
the form i:j which imply a comparison between the input items A(i) and 
A(j). If A(i) < A(j) then the algorithm proceeds down the left branch 
and otherwise it proceeds down the right branch. Now consider the set of 
all possible inputs and place inputs into the same equivalence class if 
their k -
1 largest values appear in the same positions. There will be 
n(n -
1) ... (n -
k + 2) equivalence classes which we denote by£;. Now 
consider the external nodes for the set of inputs in the equivalence class 
E ;. The external nodes of the entire tree are also partitioned into classes 
called X ;. For all external nodes in X; the positions of the largest, ... , 
k -
1st largest are identical. If we examine the subtree of the original 
comparison tree which defines the class X ;, then we observe that all com-
parisons are made on the position of the n -
k + 1 smallest elements, in 
essence trying to determine the kth largest element. Therefore this subtree 
can be viewed as a comparison tree for finding the largest of n -
k + 1 
elements and therefore it has at least 2 n -k external nodes. 
Therefore the original tree contains at least n(n -
1) ... (n - k + 2)2n-k 
external nodes and the theorem follows. 
D 
10.2 ORACLES AND ADVERSARY ARGUMENTS 
One of the proof techniques which is useful for obtaining lower bounds 
consists of making use of an "oracle". The most famous oracle in history 

• I 
-1 
-' 
470 
Lower Bound Theory 
was called the Delphic oracle, located in Delphi, Greece. This oracle can 
still be found, situated in the side of a hill embedded in some rocks. In 
olden times people would approach the oracle and ask it a question. After 
some period of time elapsed, the oracle would reply and a caretaker would 
interpret the oracles answer. 
A similar phenomenon takes place when we use an oracle to establish 
a lower bound. Given some model of computation such as comparison 
trees, the oracle tells us the outcome of each comparison. In order to 
derive a good lower bound, the oracle tries its best to cause the algorithm 
to work as hard as it might. It does this by choosing as the outcome of the 
next test, the result which causes the most work to be required to deter-
mine the final answer. And by keeping track of the work that is done a 
worst case lower bound for the problem can be derived. 
Now we consider the merging problem. Given the sets A(l:m) and 
B(l:n) where the items in A and the items in B are sorted, we investigate 
lower bounds for algorithms which merge these two sets to give a single 
sorted set. As was the case for sorting we will assume that all of the m + n 
elements are distinct and that A(l) < A(2) < . . . < A(m) and B(l) < 
B(2) < ... < B(n). It is possible that after these two sets are merged, the 
n elements of B may be interleaved within A in every possible way. Elemen-
tary combinatorics tells us that there are C +")ways thattheA's and B's may 
merge together while still preserving the ordering within A and B. Thus if 
we use comparison trees as our model for merging algorithms, then there 
will beC + ") external nodes and therefore at least 
m 
comparisons are required by any comparison based merging algorithm. 
The conventional merging procedure which was given in section 3.4 (Algo-
rithm 3.8) takes m + n -
1 comparisons. If we let MERGE(m, n) be the 
minimum number of comparisons need to merge m items with n items then 
we have the inequality 
flog ( m; n) l :5 MERGE(m, n) :5 m 
+ n - 1 

Oracles and Adversary Arguments 
471 
The exercises show that these upper and lower bounds can get arbitrarily 
far apart as m gets much smaller than n. This should not be a surprise 
because the conventional algorithm is designed to work best when m and n 
are approximately equal. In the extreme case when m = 1 we observe that 
binary insertion would require the fewest number of comparisons needed 
to merge A(l) into B(l), ... , B(n). 
When m and n are equal then the lower bound given by the comparison 
tree model is actually too low and the number of comparisons for the 
conventional merging algorithm can be shown to be optimal. 
Theorem 10.3 MERGE(m, m) = 2m -
1, form ~ 1. 
Proof: 
Consider any algorithm which merges the two sets A(l) < ... < 
A(m) and B(l) < ... < B(m). We already have an algorithm which re-
quires 2m -
1 comparisons. If we can show that MERGE(m, m) ~ 
2m -
1 then the theorem follows. Consider any comparison based algo-
rithm for solving the merging problem and an instance for which the final 
result is B(l) < A(l) < B(2) < A(2) < ... < B(m) < A(m), i.e. where 
the B's and A's alternate. Any merging algorithm must make each of the 
2m -
1 comparisons B(l) : A(l), A(l) : B(2), B(2) : A(2), ... , B(m) : 
A(m) while merging the given inputs. To see this suppose that a com-
parison of type A(i):B(i) is not made for some i. Then the algorithm can-
not distinguish between the previous ordering and the one where B(l) < 
A(l) < . . . < A(i -
1) < A(i) < B(i) < B(i + 1) < . . . < B(m) < 
A(m). So the algorithm will not necessarily merge the A's and B's properly. 
If a comparison of type A(i):B(i + 1) is not made, then the algorithm 
will not be able to distinguish between the cases when B(l) < A(l) < B(2) 
< ... < B(m) < A(m) and when B(l) < A(l) < B(2) < A(2) < ... < 
A(i -
1) < B(i) < B(i + I) < A(i) < A(i + 1) < ... < B(m) < A(m). 
So any algorithm must make all 2m -
1 comparisons to produce this final 
result. The theorem follows. 
D 
Theorem 10.3 shows us that the conventional merging procedure actually 
uses the minimum number of comparisons when m = n. Since it is known 
that this procedure gets worse as m gets small why not try to develop an 
algorithm which works well for small m. When m = 1 we have already 
observed that binary insertion would require the fewest number of com-
parisons. A hybrid algorithm which combines the merits of binary merging 
and conventional merging has been developed by F. K. Hwang and S. Lin. 

472 
Lower Bound Theory 
procedureBINARYMERGE(A, m, B, n, C) 
I I A(l :m) and B(l :n) satisfy A(l) :s A(2) :s . . . :s A(m) and/ I 
I I B(l) :s B(2) :s . . . :s B(n). The result is to store the/ I 
I /items in A and B into C such that C(l) :s C(2) :s . . . :s C(m + n).I I 
while m -;t. 0 and n -;t. 0 do 
ifm:sn 
thent -
Llogn/mj 
ifA(m) < B(n + 1 - 2* *t) 
then C -
B(n + 1 - 2 * * t), ... , B(n) I I !move 2' items intoi/ I 
! IC.I/ 
n-n-2**t 
else call BINSRCH(B, n + 1 - 2 * *t, n, A(m), k) 
Ilk is the greatest integer: A(m) > B(k)I I 
C -
A(m), B(k + 1), ... , B(n) //Move n -
m + 111 
I /items into Cl I 
m -
m - l; n -
k 
endif 
elset -
Llogm/nj 
ifB(n) < A(m + 1 - 2* *t), 
then C -A(m + 1 - 2* *t), ... , A(m) //move 2' items// 
I !into Cl I 
m-m-2**t 
elsecallBINSRCH(A,m + 1- 2**t,m,B(n),k) 
Ilk is the greatest integer: B(n) > A(k)I I 
C -
B(n), A(k + 1), ... , A(m) I !move m -
n + 1 items/ I 
I /into Cl I 
n -
n - l; m -
k 
endif 
end if 
repeat 
if n = 0 then C - A(l), ... , A(m) 
else C - B(l), ... , B(n) 
endif 
end BINARYMERGE 
Algorithm IO.I Minimum comparison merging 
As one can see the algorithm is essentially symmetric in the sense that 
the main then else clauses work in the same way only depending upon 
whether m or n is greater. Procedure BINSRCH (see section 3.2) allows 
for a lower and upper bound of an array to be specified and it returns an 

Oracles and Adversary Arguments 
473 
index k which points to the largest item in the array which is less than the 
item to be inserted. The notation C -
A(k), A(k + 1), ... means that 
everything to the right of the assignment statement is placed into the ap-
propriate place in the output array C. 
BINARYMERGE essentially works in the following way. Assuming that 
m :s n, the last element in the smaller array, A(m), is compared with an 
element of B which is near the high index end of the array, but not too 
near. Essentially it is as if the B array were segmented intom + 1 groups 
of r n/ ml elements each and the last element in the next to the last group 
is compared with A(m). If A(m) < B(k), then all of B(k), B(k + 1), ... , 
B(n) can be copied into the output. Otherwise A(m) is inserted into the 
rightmost group using binary search. A(m) and the values of B which are 
greater than A(m) can then be inserted into the output. Then the algo-
rithm continues in this way. 
Table 10.2 shows an example of BINARYMERGE form = 21 and n = 
3. The three columns m, n, t show how these variables change throughout 
the algorithm. The next columns show the comparisons that are made and 
every time a call to binary search is executed the value of k is set. The out-
put vector is C and you will notice that for this example more than one 
element is placed there each time through the loop. The conventional mer-
ging algorithm would require 17 comparisons on this example while 
BINARYMERGE requires only S. 
The original input ism = 21, n = 3 and 
A = (100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 
440, 460, 480, 500) 
B = (170, 250, 370) 
m 
n 
comparisons 
k 
output 
21 
3 
2 
B(3) < A(18) 
C - A(18), A(19), A(20), A(21) 
17 
3 
2 
B(3) > A(14) 
14 
C -
B(3), A(15), A(16), A(l 7) 
14 
2 
2 
B(2) < A(ll) 
C - A(ll), A(12), A(13), A(14) 
10 
2 
2 
B(2) > A(7) 
8 
C -
B(2), A (9), A (10) 
8 
1 
3 
B(l)>A(l) 
4 
C - B(l), A(5), A(6), A(7), A(8) 
4 
0 
C - A(l), A(2), A(3), A(4) 
Table 10.2 An example of binary merging 
For another example which we can solve using oracles, consider the 
problem of finding the largest and the 2nd largest elements out of a set of 
n. What is a lower bound on the number of comparisons required by any 
algorithm which finds these two quantities? Theorem 10.2 has already 
provided us with an answer using comparison trees. An algorithm which 

474 
Lower Bound Theory 
makes n -
1 comparisons to find the largest and then n -
2 to find the 
second largest gives an immediate upper bound of 2n - 3. So a large gap 
still remains. 
This problem was originally stated in terms of a tennis tournament where 
the values are called players and the largest value is interpreted as the win-
ner, the second largest as the runner-up. Figure 10.3 shows a sample 
tournament among eight players. The winner of each match (which is the 
larger of the two values being compared) is promoted up the tree until the 
final round which, in this case, determines McMahon as the winner. Now, 
who are the candidates for second place? The runner-up must be someone 
who lost to McMahon but who did not lose to anyone else. In Figure 10.3 
that means that either Guttag, Rosen, or Francez are the possible candi-
dates for second place. 
McMahon 
McMahon 
Guttag 
A 
Rosen 
McMahon 
Guttag 
Oaks 
/\ /\ /\ /\ 
Rosen 
Cline 
McMahon 
Francez 
Guttag 
Taylor 
Oaks 
Lynch 
Figure 10.3 A tennis tournament 
Figure 10.3 leads us to another algorithm for determining the runner-up 
once the winner of a tournament has been found. The players who have 
lost to the winner play a second tournament to determine the runner-up. 
This second tournament need only be replayed along the path that the 
winner, in this case McMahon, followed as he rose through the tree. For 
a tournament with n players there are IJog n l levels and hence only 
IJog n l - 1 comparisons are required for this second tournament. This 
new algorithm, which was first suggested by J. Schreier in 1932, requires 
a total of n -
2 + IJog n l comparisons. Therefore we have an identical 
agreement between the known upper and lower bounds for this problem. 

Oracles and Adversary Arguments 
475 
Now we show how the same lower bound can be derived using an oracle. 
Theorem 10.4 Any comparison based algorithm which computes the 
largest and second largest of a set of n unordered elements requires n 
2 + IJog n l comparisons. 
Proof: 
Assume that a tournament has been played obtaining the largest 
element and the second largest element by some method. Since we cannot 
determine the second largest element without having determined the 
largest element we see that at least n -
1 comparisons are necessary. 
Therefore all we need to show is th~t there is always some sequence of 
comparisons which forces the second largest to be found in IJog n l - 1 
additional comparisons. 
Suppose that the winner of the tournament has played x matches. Then 
there are x people who are candidates for the runner-up position. The run-
ner-up has lost only once, to the winner, and the other x -
1 candidates 
must have lost to one other person. Therefore we produce an oracle which 
decides the results of matches in such a way that the winner plays IJog n l 
other people. 
In a match between a and b the oracle declares a as the winner if a is 
previously undefeated and b has lost at least once or if both a and b are 
undefeated but a has won more matches than b. In any other case the 
oracle can decide arbitrarily as long as it remains consistent. 
Now, consider a tournament in which the outcome of each match is 
determined by the above oracle. Corresponding to this tournament imagine 
drawing a directed graph with n vertices. Each vertex corresponds to one 
of then players. Draw a directed edge from vertex b to a, b .,t. a iff either 
player a has defeated b or a has defeated another player who has defeated 
b. It is easy to see by induction that any player who has played and won 
only x matches can have at most 2x- 1 edges pointing into its corresponding 
node. Since for the overall winner there must be an edge from each of the 
remaining n -
1 vertices, it follows that the winner must have played at 
least IJog n l matches. 
D 
Another technique for establishing lower bounds which is related to 
oracles is the state space description method. Often it is possible to de-
scribe any algorithm for solving a given problem by a set of n-tuples. A 
state space description is a set of rules which show the possible states 
(n-tuples) which an algorithm can assume from a given state and a single 
comparison. Once the state transitions are given it is possible to derive 

476 
Lower Bound Theory 
lower bounds by arguing that the finish state cannot be reached using any 
fewer transitions. As an example of the state space description method we 
consider a problem originally defined and solved in section 3.3, given n 
distinct items find the maximum and the minimum. Recall that the divide-
and-conquer based solution required j3n/2l - 2 comparisons. We would 
like to show that this algorithm is indeed optimal. 
Theorem 10.5 Any algorithm which computes the largest and smallest 
element of a set of n unordered elements requires 1Jnl2 l - 2 compar-
isons. 
Proof: 
The technique we use to establish a lower bound is to define an 
oracle by a state table. We consider the state of a comparison based algo-
rithm as being described by a four tuple (a, b, c, d) where a is the number 
of items which have never been compared; b is the number of items which 
have won but never lost; c is the number of items which have lost but never 
won; and d is the number of items which have both won and lost. Originally 
the algorithm is in state (n, 0, 0, 0) and concludes with (0, 1, 1, n -
2). 
Then, after each comparison the tuple (a, b, c, d) can make progress only 
if it assumes one of five possible states: 
(a -
2, b + 1, c + 1, d) 
(a -
1, b, c + 1, d) or (a 
or (a -
1, b, c, d + 1) 
(a, b -
1, c, d + 1) 
(a, b, c -
1, d + 1) 
if a ~ 2 
I !two items from a 
are compared// 
1, b + 1, c, d) if a ~ 1 
I I an item from a 
compared 
with 
one from b or c I I 
if b ~ 2 
I !two items from b 
are compared// 
if c ~ 2 
I !two items from c 
are compared/ I 
In order to get the state (0, 1, 1, n -
2) from the state (n, 0, 0, 0) it fol-
lows that 1Jn12l -
2 comparisons are needed. To see this observe that 
the quickest way to get the a component to zero requires n/2 state changes 
yielding the tuple (0, n/2, n/2, 0). Next the b and c components are re-
duced, requiring an additional n - 2 state changes. 
D 
We end this section by deriving another lower bound on the selection 
problem. We originally studied this problem in Chapter 3 where we pre-
sented several solutions. One of the algorithms presented there has a worst 
case complexity of O(n) no matter what value is being selected. There-

Oracles and Adversary Arguments 
477 
fore we know that asymptotically any selection algorithm requires 8(n) 
time. Let SEL~n) be the minimum number of comparisons needed for 
finding the kth element of an unordered set of size n. We have already seen 
that fork = 1 SEL1(n) = n -
1 and fork = 2, SELi(n) = n -
2 + 
pog n l . In the following paragraphs we present a state table which shows 
that n -
k + (k -
1) pog nl(k -
1)1 
:s; SELk(n). We continue to use 
the terminology which refers to an element of the set as a "player" and 
to a comparison between two players as a "match" which must be won by 
one of the players. A procedure for selecting the kth largest element is re-
ferred to as a tournament which finds the kth best player. 
In order to derive this lower bound on the selection problem, an oracle 
will be constructed in the form of a state transition table which will cause 
any comparison based algorithm to make at least n -
k + (k -
1) pog 
nl(k -
1) l comparisons. The tuple size for states in this case is two, (it 
was four for the max-min problem), and the components of a tuple, say 
(Map, Set), stand for the following: Map is an onto mapping from the 
integers 1, 2, ... , n to itself and Set is an ordered subset of the input. The 
inital state is the identity mapping and the empty set. At any time period 
t the oracle is assumed to be given two unordered elements from the input, 
say a and b, and the oracle acts as follows: 
(i) if a and b are both in Set at time t then a wins iff a > b. The tuple 
(Map, Set) remains unchanged. 
(ii) If a is in Set and b is not in Set then a wins and the tuple (Map, Set) 
remains unchanged. 
(iii) if a and b are both not in Set, then if Map(a) > Map(b) at time t 
then a wins. If Map(a) = Map(b) then it doesn't matter who wins as 
long as no inconsistency with any previous decision is made. If 
Map(a) + Map(b) ~ nl(k -
1) at time t then Map is unchanged 
and the winner is inserted into Set as the new smallest value. Other-
wise Set stays the same and Map(the loser) -
0 at time t + 1 and 
Map(the winner) -
Map(a) + Map(b) at time t + 1 and for all 
items w, w .,t. a, w .,t. b, Map(w) stays the same. 
Lemma 10.1 
Using the oracle just defined, the k -
1 best players will 
have played at least (k -
1) pog (nl(k -
l))l matches when the tourna-
ment is completed. 
Proof; At time t the number of matches won by any player x is ~ pog 
Map(x)l . The elements in Set are ordered so that x 1 < ... < x;. Now 
for all w in the input I:(Map(w)) = n. Let W = { y:y is not in Set but 

478 
Lower Bound Theory 
Map(y) > O}. Since for all win the input Map(w) < nl(k -
1) it follows 
that the size of Set plus the size of W is > k -
1. However since the ele-
ments y in W can only be less than some x; in Set, if the size of Set is < 
k -
1 at the end of the tournament then any player in Set or W is a can-
didate for the k -
1 best players. This is a contradiction so it follows that 
at the end of the tournament the k -
1 best players are ordered and in 
Set. 
0 
We are now in a position to establish the main theorem. 
Theorem 10.6 [Hyafil] The function SELk(n) satisfies 
n - k + (k - 1) pog n!(k - 1)1 :s SELk(n). 
Proof: 
According to the lemma the k - 1 best players have played at least 
(k -
1) pog n!(k - 1)1 matches. Any player who is not among the k 
best players has lost at least one match against a player which is not among 
k -
1 best. Thus there are n -
k additional matches which were not in-
cluded in the count of the matches played by the k -
1 top players. Thus 
the statement of the proof follows. 
0 
10.3 TECHNIQUES FOR ALGEBRAIC PROBLEMS 
In this section we will examine two methods, substitution and linear in-
dependence, for deriving lower bounds on arithmetic and algebraic prob-
lems. The algebraic problems we are considering here are operations on 
integers, polynomials and rational functions. Solutions to these problems 
were presented in Chapter 9. In addition we also include matrix multiplica-
tion and related operations which were discussed in Chapter 3. 
The model of computation we will use is called a straight line program. 
It is called this because there are no branching instructions allowed. This 
implies that if we know a way of solving a problem for n inputs, then a set 
of straight line programs, one each for solving a different size n, can be 
given. The only statement in a straight line program is the assignment 
which has the forms -
p op q. s, p, q are variables of bounded size and 
op is typically one of the arithmetic operations: addition, subtraction, mul-
tiplication or division. Moreover s is a variable which has not yet appeared 
in any previous step, while p and q are either constants, an input variable 
or a variable which has already appeared on the left of an assignment 
statement. For example one possible straight-line program which computes 
the value of a degree two polynomial has the form 

Techniques for Algebraic Problems 
479 
vl -
a 2* x 
vl -
vl + a 1 
vl -
vl * x 
ans -
vl + a o 
In order to determine the complexity of a straight line program we assume 
that each instruction takes one unit of time and requires one unit of space. 
Then the time complexity of a straight line program is the number of as-
signments, or its length. A more realistic assumption takes into account 
the fact that an integer n requires L log n J + 1 bits to represent it. But 
in this section we will assume that all operands are small enough to occupy 
a fixed size register and hence the unit cost assumption is appropriate. 
Now we need to consider the class of constants we intend to allow. This 
requires some elementary definitions from algebra. 
Definition A ring is an algebraic structure containing a set of elements S 
and two binary operations denoted by + and *. For each a, b ES, a + b 
and a*b are also in S. Also the following properties hold: 
(a + b) + c = a + (b + c) and (a*b)*c = a*(b*c) (associativity) 
a + b = b + a (commutativity) 
(a + b)*c = a*c + b*c and a*(b + c) = a*b + a*c (distributivity) 
a + 0 = 0 + a = a(O is the additive identity) 
a* 1 = 1 *a = a(l is the multiplicative identity) 
for each a E S there is an additive inverse denoted by - a such that a + 
(-a) = (-a) + a = 0. 
If multiplication is also commutative then the ring is called commutative. 
Definition Afield is a commutative ring such that for each element a E S 
(other than 0) there is a multiplicative inverse denoted by a - 1 which sat-
isfies the equation a *a - 1 = 1. 
The real numbers form a field under the regular operations of additon 
and multiplication. Similarly for the complex numbers. However the in-
tegers with the operations + and * do not form a field since only plus or 
minus one have multiplicative inverses. Another field is the set of integers 
modulo a prime as discussed in Chapter 9. They form a finite field con-
sisting of the integers (O, 1, ... , p -
1). 

480 
Lower Bound Theory 
Defmition An indeterminate over an algebraic system is a symbol which 
does not occur in S. The extension of S by the in determinates xi. ... , x n 
is the smallest commutative ring which contains all combinations of the 
elements of S and the indeterminates. Such an extension is denoted by 
S[x i. .•• , x n]. When an extension is made to a field which allows for 
quotients of combinations of elements of S and indeterminates then that 
is denoted by S(x i. ..• , x ,J. 
The elements in an extension S[x i. ••• , x n] can be viewed as polynomials 
in the variables x; with coefficients from the set S. The elements in an ex-
tension S(x i, .•• , x n) should be viewed as rational functions of the variables 
x; with coefficients which are from S. The indeterminates are independent 
in the sense that no one can be expressed by the others and hence two such 
polynomials or rational functions are equal only if one can be transformed 
into the other using the laws of the ring or field. 
The field of constants can make an important difference on the com-
plexity of the algorithms for some problems. For example if we wish to 
examine programs for computing x 2 + y 2 where the field is the reals, then 
two multiplications are required. However if the field is the complex num-
bers, then only one complex multiplication is needed, namely (x + iy)*(x 
-
iy). 
Theorem 10. 7 Every algorithm for computing the value of a general nth 
degree polynomial which uses only + , - , * requires n addition or sub-
tractions. 
Proof: 
Any straight line program which computes the value of anxn + 
+ . . . + a 0 can be transformed into a program to compute an + . . . + a o 
given some field of constants F and indeterminates (an, ... , a 0). This new 
program is produced by inserting the statement s -
1 at the beginning 
and then replacing every occurrence of x by s. We now prove by induction 
that an + . . . + a 0 requires n additions or subtractions. For n = 1 we 
need to compute a 1 + a 0 as an element in F[a i, ao]. If we disallow addi-
tions or subtractions then by the definition of extension only products of 
the a; multiplied by constants from the field can be produced. Thus a 1 + a o 
requires one addition. Now suppose we have computed a sum or difference 
of at least two terms where each term is possibly a product of elements 
from the vector a and possibly a field element. Without a loss of generality 
assume that an appears in one of these terms. If we substitute zero for an 
then this eliminates the need for this first addition or subtraction since one 
of the arguments is zero. We are now computing a n-1 + . . . + a o which 
by the induction hypotheses requires n - 1 additions or subtractions. Thus 
the theorem follows. 
D 

Techniques for Algebraic Problems 
481 
The basic idea of this proof is the substitution argument. Using the same 
technique one can derive a not much more complicated theorem which 
shows that Horner' s rule is optimal with respect to multiplications or 
divisions. 
Definition Suppose F and Gare two fields such that Fis contained in G 
and we are computing in G(a i. ... , an). The operationf op g where op is 
* or I is said to be inactive if one of the following hold: (i) g E F; (ii) f E F 
and the operation is multiplication; (iii)f E G andg E G. 
Any multiplication or division which is not inactive is called active. So 
for example operations such as x * x or 15 *a; are inactive while the opera-
tions x*a; or a 1*a2 or 15/a; are active. 
Definition Let a = (a o, .•• , a,,). Then p 1 (a), ... , p u(a) is linearly 
independent if there does not exist a nontrivial set of constants c i. ... , c,, 
such that E c ;p; = a constant. 
P(a, x) can be thought of as a general polynomial in the sense that it is a 
function not only of x, but of the inputs a. We can write P(a. x) as E(p; 
(a)xi) + r(x) where u of the p; are linearly independent. 
Theorem 10.8 [Borodin, Munro] If u active * or I are required to com-
pute P(a, x) then n active * or I are required to evaluate a general nth de-
gree polynomial. 
Proof: 
The proof proceeds by induction on u. Suppose u = 1. If there 
is no active* or I then it is only possible to form p;(a) + r(x) for some i. 
Now suppose (p;(a) + r i(x))*(pj(a) + r 2(x)) is the first active multiplica-
tion in a straight line program which computes P(a, x). Without loss of 
generality assume that p;(a) .,t. a constant. Then, in the straight line pro-
gram let p j(a) + r 2(x) be replaced by a constant d such that no illegal 
division by zero is caused. This can always be done for if P; is a linear 
combination of constants c; times a; and since there must be exist aj:c; -;t. 0, 
then by setting 
(10.4) 
it follows that p j(a) + r 2(x) = d. Now consider P(a, x) where the sub-
stitution of a; has been made. P can be rewritten into the form 
E P ;' (x) x; + r' (x) 
(10.5) 
o~,·~n 

#1 
482 
Lower Bound Theory 
Therefore by making the one replacement we can remove one active multi-
plication or division and we are now computing a new expression. If it 
can be shown that there are u -
1 linearly independent p j then by the 
induction hypothesis there are at least u -
1 remaining active * or I and 
the theorem follows. This is a technical lemma and so we will skip its 
presentation here. It can be found in the exercises. 
Corollary 10.l Homer's rule is an optimal algorithm with respect to the 
number of multiplications and divisions necessary to evaluate a polynomial. 
Proof: 
From the previous theorem and the result in the exercises that 
under substitution u -
1 linearly independent combinations remain and 
the fact that Homer's rule requires only n multiplications the theorem 
follows. 
D 
Another method of proof for deriving lower bounds for algebraic pro-
blems is to consider these problems in a matrix setting. Returning to poly-
nomial evaluation we can express this problem in the following way: com-
pute the 1 x (n + 1) by (n + 1) x 1 matrix product 
[l,x,x 2 , ••• ,x"] 
ao 
(10;6) 
which is the product of two vectors. Another problem is complex number 
multiplication. The product of (a + ib) * (c + id) = ac - bd + (be + 
ad)i can be written in terms of matrices as 
l
a -b] le] = lac - bdl 
b 
a 
d 
bc+ad 
(10. 7) 
In more general terms we wish to consider problems which can be formu-
lated as the product of a matrix times a vector 

Techniques for Algebraic Problems 
483 
[
au, ... , a,. l 
[x'] 
(lO.B) 
am i, ... ' a mn 
x n 
Definition Let F be a field and xi. ... , x n be indeterminates. Let Fm[x i. 
... , x n] stand for the m-dimensional space of vectors with components 
from Flx i. ... , x n] and Fm stand for the m-dimensional space of vectors 
with components from F. A set of vectors vi. ... , v k from Fm[x i. ••• , x n] 
is linearly independent modulo Fm if for u 1, ••• , u k in F the sum E(u ;V ;) 
i = 1, k in Fm implies the u; are all zero. If the v; are not linearly inde-
pendent then they are called linearly dependent modulo Fm. The row 
rank of a matrix A modulo F' is the number of linearly independent rows 
modulo F'. The column rank is the number of linearly independent 
columns. 
We now state the main theorem of this section. 
Theorem 10.9 Let A be an r x s matrix with elements from the exten-
sion field F[x i. •.. , x n] and y = [y 1, •• • y ,] a column vector containing 
s indeterminates. 
(i) 
if the row rank of A is v, then any computation of Ay requires at 
least v active multiplications; 
(ii) 
if the column rank of A is w, then any computation of Ay requires 
at least w active multiplications; 
(iii) If A contains a submatrix B of size v x w such that for any vec-
tors p E Fv, q E Fw, p TBq E F iff p = 0 or q = 0, then any com-
putation of Ay requires v + w -
1 multiplications. 
Proof: 
For a proof of part (i) see the paper by Winograd. For a proof of 
parts (ii) and (iii) see the papers by Fiduccia. Also see Aho, Hopcroft and 
Ullman. 
D 
Example 10.1 
Reconsider the problem of multiplying two 2 x 2 matrices 

-
484 
Lower Bound Theory 
[: :J [; ~] 
["' + bg, af + bh] 
= 
ce + dg, cf+ dh 
which by definition seemingly requires 8 multiplications. We can rephrase 
this computation in terms of a matrix-vector product as follows 
a b 0 0 
e 
a - b 
0 0 0 
-
c 
d 0 0 
g 
0 
0 0 0 
0 0 a b 
f 
a+ b 
0 0 0 
0 0 c 
d 
h 
0 
0 0 0 
b 
b 
0 0 
0 0 0 
0 
-b -b 0 0 
0 0 0 
c - d 
+ 
+ 
-
0 
0 0 0 
0 0 0 
0 
0 
0 0 0 
0 0 0 
-
c + d 
0 0 
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 
0 
0 
+ 
+ 
0 0 -c -c 
a + c 0 a + c 0 
0 0 
c 
c 
0 
0 
0 
0 
-
0 
0 
0 
0 
0 
0 0 
0 
e 
0 b + d 0 b +d 
b + c 
0 0 -b-c 
g 
+ 
+ 
0 
0 
0 
0 
-b-c 0 0 
b + c 
f 
0 
0 
0 
0 
0 
0 0 
0 
h 

Techniques for Algebraic Problems 
485 
The first 2 x 2 matrix, say A, has been expanded as the 4 x 4 matrix 
This matrix is then further decomposed into a sum of 7 matrices, each of 
size 4 x 4. Both the row rank and the column rank of each matrix is one 
and hence by Theorem 10.11 we see that 7 multiplications are necessary. 
Example 10.2 Given two complex numbers a + ib and c + id, the prod-
uct (a + ib) * (c + id) = ac - bd + i(ad + be) can be described by the 
matrix-vector computation 
[a -b] [c] = [ac - bd] 
b a 
d 
be + cd 
(10.9) 
which seemingly requires 4 multiplications, but it can also be written as 
(10.10) 
The row and column rank of the first matrix is 2 while the row and column 
rank of the second matrix is 1. Thus 3 multiplications are necessary. The 
product can be computed as 
(i) 
a * (d -
c) 
(ii) (a+ b) *c 
(iii) b * (c + d) 
Then (ii) -
(iii) = ac - bd and (i) + (ii) = ad + be. 
Example 10.3 Equation 10.6 phrases the evaluation of an nth degree 
polynomials in terms of a matrix-vector product. The matrix has n linearly 
independent columns modulo the constant field F and thus by theorem 
10.11, n multiplications are necessary. 

486 
Lower Bound Theory 
Lower bounds on polynomials with preconditioning 
In this section we've already seen that any algorithm which evaluates a 
general nth degree polynomial requres n multiplications or divisions and 
n additions or subtractions. This assertion was based on the assumption 
that the input to any algorithm was both the value of x plus the coefficients 
of the polynomial. We might take another view and consider how well 
one can do if the coefficients of the polynomial are known in advance and 
functions of these coefficients can be computed without cost before evalua-
tion begins. This process of computing functions of the coefficients is re-
ferred to as preconditioning. 
Suppose we begin by considering the general 4th degree polynomial 
A(x) = a4x4 + a3x 3 + a2x2 + a1x + a0x 0 and the scheme 
y-(x+co)x+ci 
A(x) -
((y + x + ci)y + C3)C4 
Only three multiplications and five additions are required if we can deter-
mine the values of the c; in terms of the a;. Expanding A(x) in terms of x 
and the c; we get 
A(x) = C¥ 4 + (2coc4 + C4)X 3 + (co2 + 2c1 + CoC4 + c2c4)x 2 + 
(2coc 1C4+c1C4 + CoC2C4)X + (c 12C4+c1C2C4 + C3C4) 
and equating the above coefficients with the a; we get that 
C4 = a4; co= (a3/a4 -
1)/2 
b = az/a4 -
co(co + 1) 
Applying the above method to the polynomial A(x) = -
x 4 + 3x 3 
2x 2 + 2x + 1 yields the straight line program 
q-x-2 
r - q*x 
y-r-2 
s-y+x 
t-s+4 
u -
t*y 
v -
u + 3 
p -
-1 *v 
which evaluates to A(x) in just three multiplications. 

Techniques for Algebraic Problems 
487 
In fact the following can be shown; for any polynomial A(x) of degree 
n ~ 3 there exist real numbers c, d ;, e i for 0 :5 i :5 I n/2 l - 1 such 
that A(x) can be evaluated in L n/2 J + 2 multiplications and n additions 
by the following scheme 
y -
x + c; 
w -
y*y 
z -
(a,..y + do)y + eo(n even); 
z -
any+ eo(n odd) 
z -z(w -
d;) + e;, fori = 1,2, ... ,m; 
answer - z. 
Now that we have a scheme which reduces the number of required 
multiplications by about one half, it is natural to ask how close we have 
come to the optimal. The lower bound we are about to present follows 
from the fact that any straight line program can be put into a "normal 
form" involving a limited number of constants. We will restrict our argu-
ments here to programs without division, leaving the extension to interested 
readers. 
Lemma 10.2 (Motzkin 1954) For any straight line program with k mul-
tiplications and a single input variable x, there exists an equivalent pro-
gram using at most 2k constants. 
Proof: 
Let s ;, 0 :5 i :5 k denote the result of the ith multiplication. We 
can rewrite the program as 
So -
X 
s; -
L ;* R ;, 
1 :5 i :5 k 
A(x) -Lk+t 
where each L; and R; is a certain sum of a constant (which may accumu-
late other constants from the original program) and an earlier Sj (an Sj may 
appear several times in this sum). The first product sl -
(c 1 + m ix)*(c 2 
+ m 2x) can be replaced by sl -
mx(x + c), where m = m im 2 and c = 
m 1c 2 + m 2c 1, provided that later constants are suitably altered. 
D 
Lemma 10.3 (Belaga 1958) For any straight line program with k addi-
tion-subtractions and a single input variable x, there exists an equivalent 
program using at most k + 1 constants. 

488 
Lower Bound Theory 
Proof: 
Let s ;, 0 ::5 i ::5 k be the result of the kth addition-subtraction. 
As in the previous proof we can rewrite the program as 
So -
X 
s i -
c ;pi + d ;q ;, 
1 ::5 i ::5 k 
where each p; and q; is a product of earlier s 1• For k = 1, 2, ... replace 
s; bys; -
(c ;d; - 1) p; + q ;, simultaneously replacing subsequent references 
to s; by d ;s ;. 
D 
Theorem 10.10 (Motzkin, Belaga) A randomly selected polynomial of 
degree n has probability zero of being computable either with less then 
j(n + 0121 multiplications-divisions or with less than n addition-sub-
tractions. 
Proof sketch: If a given straight line program with the single input vari-
able x has only a "few" operations, then we may assume that it has at most 
n constants. Each time these constants are set they determine a set of co-
efficients of the polynomial computed by the last operation of the program. 
Given A(x) of degree n, the probability is zero that the program's n or 
fewer constants can be adjusted to align the computed polynomial with 
all n + 1 of the given polynomial coefficients. A formal proof here relies 
on showing that the subset of (n + 1)-dimensional space which can be so 
represented has Lebesque measure zero. It follows (because the set of 
straight line programs is enumerable if we identify programs differing 
only in their constants) that with only zero probability can the constants 
of any such short program be set so as to evaluate the polynomial. 
D 
The above theorem shows that the preconditioning method previously 
given comes very close to being optimal, but some room for improvement 
remains. 
10.4 SOME LOWER BOUNDS ON PARALLEL COMPUTATION 
In this section we will present just some of the recent lower bounds which 
have been developed under the assumption that a machine with many 
processors is available. We refer to this situation as parallel computation. 
The machine model which underlies all of the results to be presented here 
assumes that k independently programmable processors are available. 
Sometimes k is fixed, while other times the number may vary with the 

Some Lower Bounds on Parallel Computation 
489 
problem instance. Each processor can perform arithmetic and comparisons 
just like the single processor we have been assuming throughout this book. 
At each time period it is possible that all processors can be "working", 
but during this time period they can perform at most a constant number of 
operations. We will not consider whether the processors are working syn-
chronously or asynchronously. We only wish to note that more control is 
necessary when the processors can act at varying speeds. There is a mem-
ory that is shared by all of the processors. Moreover, the assumption is 
made that at any time t all k processors can access this memory simulta-
neously. In practice this turns out to be a very unrealistic assumption. 
However from the perspective of obtaining lower bounds it does not in-
validate the results. It only implies that any actual speed-up in computa-
tion time through the use of parallel processors will not be as great as the 
bounds presented here. 
A parallel algorithm is an algorithm which is run on a parallel-proces-
sor, i.e. a machine which permits more than one processor to function on 
the same problem at the same time. Some of the algorithms which are 
typically described for a single processor machine are naturally converted 
to a many processor machine. Modular arithmetic as discussed in Chapter 
9 is one such example. On the other hand many solutions to problems 
seem essentially sequential in nature and it looks as if no speed-up can be 
obtained by running such an algorithm on a parallel machine. Therefore 
many researchers have recently been investigating new algorithms which 
will best exploit the capabilities of a parallel processor. The complexity 
of a parallel algorithm is the worst case number of time periods needed 
for an algorithm to complete. Since at each time period k processors can 
be computing, the complexity of a parallel algorithm is usually less than 
for a one processor machine. 
Information theoretic arguments 
Consider the computation of xn where n = 2m. Information theory tells 
us that it is impossible to generate too much information about a problem 
in a given amount of time. For this problem that means that xn cannot be 
computed in fewer than pog n l steps, or in particular that x 2, x 4, x 8, ••• , 
x n requires m steps no matter how many processors are available. This 
result was first stated in more general terms by Kung. 
Theorem 10.11 Let A(x) = P(x)I Q(x) be a rational function, P and Q 
are relatively prime, and where n is the maximum of the degrees of P and 
Q. Then at least pog n l parallel time is needed to compute A(x). 

490 
Lower Bound Theory 
Proof The proof proceeds by induction on n. Suppose n = 1. Then A(x) 
has the form (ax + b)l(cx + d) where a, b, c, dare constants. Thus A(x) 
can be computed in a constant amount of time which is bounded below by 
llog n l . Suppose that the theorem is true for any rational function A(x) 
where the maximum degree n is less than 2 m. By the induction hypothesis 
this implies that only m steps were needed to compute A(x). At step m + 1 
A(x) can either be added to another rational function or multiplied by 
another rational functional, but in both cases the degree of the other argu-
ment can be no more than n. Therefore at time m + 1 the maximum de-
gree of any result can be at most 2n = 2 m+ 1• Since the pog 2m+ 11 = 
m + 1 the result follows. 
D 
Now lets turn our attention to the sorting problem. In Section 10.1 we 
observed that O(n log n) was a lower bound for sorting on a sequential 
machine. 
Theorem 10.12 Given n = 2 m unordered elements it takes at least pog n l 
parallel time to sort these values. 
Proof: 
Consider the comparison tree model defined for the sorting problem 
and presented in Section 10.1. There are n! external nodes corresponding 
to then! possible permutations of the input. On any level of this comparison 
tree imagine that as many processors as one would like are available to 
determine the relationships on that level. Since the tests made on a given 
level depend upon the results of tests made on the previous level, we con-
clude that no parallel processor can work faster than the number of levels 
in the tree. As there are at least pog n l levels the theorem follows. 
D 
Evaluating arithmetic expressions 
Theorem 10.13 [Munro and Paterson] Suppose the computation of an 
arithmetic expression requires n binary operations. Then the shortest 
parallel time which is needed to evaluate this expression using at most k 
processors is bounded below by (n + 1)/ k + log k -
1 for n sufficiently 
large. 
Proof: 
Let P min be the fewest number of parallel steps required by k pro-
cessors to evaluate an expression. At the last time period at most one pro-
cessor is needed to evaluate the final binary operator. Similarly, at the time 
period P min -
1 at most 2 processors are needed, at time P min -
2 at most 

Some Lower Bounds on Parallel Computation 
491 
4 processors and in general at time p min -
m at most 2 m processors are 
needed. During the time periods 1, 2, ... , P min -
m -
1 at most k pro-
cessors can be used. Therefore we get a bound on n, namely 
n ::5 1 + 2 + 2 2 + . . . + 2 m + ( p min -
m -
1 )k 
Solving for P min one gets 
(n -
2 m+ l + 1)/ k ::5 p min -
m -
1 
and letting k = 2 m and simplifying we get 
P min ~ (n + l)lk + log k - 1 
D 
It is interesting to review how close researchers have come to this lower 
bound. For expressions with n -
1 binary operators and k processors 
where every variable appears once and no division is allowed, Brent has 
given an algorithm which requires 2nlk + O(log n) parallel steps and if 
division is allowed then lOn/k + O(log n) parallel steps. Winograd has 
improved on these bounds somewhat by giving algorithms such that for 
expressions without division 3n/2k + O((log n) 2) parallel time is required 
and if division is allowed than Sn/2k + O((log n) 2). See the references for 
more details. 
More on sorting and searching 
Theorem 10.14 [Valiant] Given n unordered elements and k = n pro-
cessors, if MAX(n) is a lower bound on the worst case time needed to deter-
mine the maximum value in parallel time, then MAX(n) ~ log log n -
c, 
where c is a constant. 
Proof: 
Consider the information determined from the set of comparisons 
which can be made by time t for some parallel maximum finding algo-
rithm. Some of the elements have been shown to be smaller than other 
elements and so they have been eliminated. The others form a set S which 
contains the correct answer. If at time t two elements not in Sare compared 
then no progress is made decreasing set S. If an element in set S and one 
not in S are compared and if the larger element is in S then again no im-
provement has been made. Assume that the worst case holds which means 
that the only way to decrease the set S is to make comparisons between 
pairs of its elements. 

492 
Lower Bound Theory 
Imagine a graph where the nodes represent the values in the input and 
a directed edge from a to b implies that b is greater than a. A subset of 
the nodes is said to be stable if no pair from it is connected by an edge. 
Then the size of S at time t can be expressed as 
Sat time t 2: min( max(h: the graph contains a stable set of size h) 
or G is a graph with the size of S nodes and n edges) 
It has been shown by Turan in On the theory of graphs, Colloq. Math., 
1954 that the size of S at time t is 2: the size of S at time t -
1, squared 
divided by 2k + the size of S. We can solve this recurrence relation using 
the fact that initially the size of S equals n which shows that the size of S 
will be greater than one so long as t < log log n - c. 
D 
This lower bound on maximum finding may come as a surprise and a 
first reaction might be that is it unusually low. Even more surprising is the 
fact that Valiant has given an algorithm for finding the maximum which 
takes no more time than log log n + a constant. Though his algorithm 
assumes a great deal of overhead between each parallel step, this sort of 
result is of great interest. For more details see his paper as listed in the 
references. 
Now what can we say about sorting on a parallel computer. The in-
formation theoretic lower bound says that O(log n) is the best any par-
allel algorithm can do. An interesting method given by K. Batcher re-
quires O(n (log n)2) on a sequential machine, but on a parallel machine 
only O((log n)2) parallel steps are required because at each time unit all 
comparisons are independent. 

Some Lower Bounds on Parallel Computation 
493 
Procedure BATCHER(A, n) 
I /sorts the values A(l), ... , A(n) in-place, assuming n ~ 211 
t -
llog n l 
i - 2**(t -
1) 
//2•- 1 < n ~ 2 1// 
while i ~ 1 do 
q -
2**(t -
1); r -
O; d -
i 
L:j -
0 
whilej < n -
d and ((j and i) = r) do 
if A(j + 1) > A(j + d + 1) 
then temp - A(J + 1); A(j + 1) -
A(.i + d + 1) 
A(.i + d + 1) -
temp 
endif 
j -j + 1 
repeat 
if q .,t. i ther. d -
q - i; q -
q/2; r -
i; go to L 
else i - i/2 
endif 
repeat 
end BATCHER 
Algorithm 10.2 
Example 10.4 Suppose we take nine values and trace the algorithm as 
it sorts these values. The lines indicate comparisons and exchanges which 
are possibly made. 
135, 382, 154, 72, 341, 422, 174, 243, 120 
one exchange 
120, 382, 154, 72, 341, 422, 174, 243, 135 
I 
I I I 
I 
I I I 
no exchanges 
120, 382, 154, 72, 341, 422, 174, 243, 135 
I 
l 
one exchange 
120, 382, 154, 72, 135, 422, 174, 243, 341 
I 
I 
I 
I 
I 
I 
I 
I 
two exchanges 
120, 72, 154, 382, 135, 243, 174, 422, 341 
no exchange 
120, 72, 154, 382, 135, 243, 174, 422, 341 
I 
I 
I 
I 
two exchanges 

#1 
494 
Lower Bound Theory 
120, 72, 135, 243, 154, 382, 174, 422, 341 
L_I 
L__j 
L_J 
L__J 
72, 120, 135, 243, 154,382, 174,422, 341 
72, 120, 135, 243, 154, 382, 174,422, 341 
I 
I 
I I I 
I 
72, 120, 135, 174, 154, 341, 243, 422, 382 
L....--1 
L-1 
L-1 
L_J 
72, 120, 135, 154, 174, 243, 341, 382, 422 
one exchange 
no exchanges 
two exchanges 
three exchanges 
A proof that Hatcher's method does actually sort in all cases can be found 
in Knuth, volume III. Also there one can find an account of how to use 
Batcher' s algorithm on a parallel processor so that the time for transferring 
data as well as for performing logical operations remains bounded by 
O((log n)2). The value of Batcher's method or any other parallel algorithm 
must wait until these machines are built and tested. At this point it seems 
that merely counting logical operations is insufficient to produce a truly 
efficient algorithm for a parallel processor and it is likely that data move-
ment will also be an important parameter to measure to determine the 
real efficiency of any algorithm. 
REFERENCES AND SELECTED READINGS 
For a detailed account of lower bounds for sorting, merging and selection see 
Sections 5.3, 5.3.1, 5.3.2 and 5.3.3 in 
The Art of Computer Programming, volume III sorting and searching by Donald 
Knuth, Addison-Wesley, 1973. 
Another good source of material on lower bounds was circulated as a set of notes for 
a while 
Cs230 notes by Richard Karp, Univ. of Calfornia-Berkeley, Fall quarter 1972 and 
spring quarter 1971 
The sorting algorithm which requires the fewest known number of comparisons was 
originally presented in 
"A tournament problem" by L. Ford Jr. and S. Johnson, American Math Monthly, 
66, (1959), 387-389. 

References and Selected Readings 
495 
See also 
"The Ford-Johnson algorithm is not optimal" by Glenn K. Manacher, Dept. of 
Information Engineering, Univ. of Illinois, Chicago, Ill. 60680. 
The minimum comparison merging algorithm was presented in 
"A simple algorithm for merging two disjoint linearly ordered sets" by F. K. Hwang 
and S. Lin, SIAM J. Computing, 1 (1972), 31-39. 
The lower bound on the selection problem can be found in 
"Bounds for selection", by Laurent Hyafil, SIAM J. Computing, vol. 5, no. 1, March 
1976, 109-114. 
Other relevant papers containing lower bound results includes 
"Using comparison trees to derive lower bounds for selection problems" by Frank 
Fussenegger and Harold Gabow, Proc. 17th Found. of C. S., IEEE, October, 
1976, 178-182. 
"Bounds on the complexity of the longest common subsequence problem" by A. 
Aho, D. S. Hirschberg and J. D. Ullman, J. ACM, vol. 23, no. 1, January 1976, 
1-12. 
"On the optimality of some set algorithms" by E. M. Reingold, J. ACM, vol. 19, 
no. 4, October 1972, 649-659. 
In 
"Computing the maximum and the median" by E. M. Reingold Proc. 12th Symp. 
on Switching and Automata Theory, IEEE, October 1971, 216-218 
it is shown that the maximum of a set of n integers cannot be computed in fewer 
than n -
1 comparisons if comparisons of only linear functions of the integers are 
permitted, but in log n comparisons if exponential functions are allowed. 
In 
"Some results on the effect of arithmetics on comparison problems" by Nathan 
Friedman, Proc. 13th Symp. on Switching and Automata Theory, IEEE, October 
1972, 139-143 
it is shown that the O(n log n) bound for sorting holds even if comparisons between 
arbitrary functions and analytic functions on output are permitted. Other results of 
interest are included. 

#I 
496 
Lower Bound Theory 
The solution of exercise 17 can be found in 
"On the complexity of computations under varying sets of primitives" by David 
Dobkin and Richard Lipton, Automata theory and formal languages, Springer-
Verlag lecture notes in computer science 33. 
"A lower bound of (1/2)n 2 on linear search programs for the knapsack problem" 
by David Dobkin and Richard Lipton, Research report 70, Yale University, New 
Haven, Conn. 
Several related papers on lower bounds can be found in the single technical report 
"Excursions into geometry" by David Dobkin, Richard Lipton and Reiss, Research 
report 71, Yale University, New Haven, Conn. 
Many of the algebraic lower bounds can be found in the following two books 
The computation complexity of algebraic and numeric problems, by A. Borodin 
and I. Munro, American Elsevier, New York, 1975. 
The Design and Analysis of Computer Algorithms by A. Aho, J. E. Hopcroft and 
J. D. Ullman, Addison-Wesley, Reading, 1974. 
A proof of part (i) of Theorem 10.11 can be found in 
"On the number of multiplications necessary to compute certain functions" by 
S. Winograd, Comm. Pure and Applied Math., vol. 23, 1970, 165-179. 
A proof of part (ii) of Theorem 10.11 can be found in 
"On obtaining upper bounds on the complexity of matrix multiplication" by C. 
Fiduccia, Proc. IBM Symposium on complexity of computer computations, March 
1972. 
A proof of part (iii) of Theorem 10.11 can be found in 
"Fast matrix multiplication" by C. Fiduccia, Proc. 3rd Annual ACM symposium 
on theory of computing, (1971), 45-49. 
For readings on parallel computation see the following papers plus their references 
"The complexity of parallel evaluation of linear recurrences" by L. Hyafil and H. T. 
Kung, J. ACM, (24, 1) July 1977, 513-521. 

Exercises 
497 
"The parallel evaluation of general arithmetic expressions" by Richard P. Brent, 
J. ACM, (21, 2) April, 1974, 201-206. 
"On the parallel evaluation of certain arithmetic expressions" by S. Winograd, 
J. ACM, (22, 4), October, 1975, 477-492. 
"New algorithms and lower bounds for the parallel evaluation of certain rational 
expressions and recurrences" by H. T. Kung, J. ACM, (23, 2), April, 1976, 252-261. 
"Optimal algorithms for parallel polynomial evaluation" by I. Munro and M. 
Paterson, J. Comp. and Sys. Scis, vol. 7, 1973, 189-198. 
"Parallelism in comparison problems" by Leslie Valiant SIAM J. Comp., (4, 3), 
September, 1975, 348-355. 
and the book by Borodin and Munro cited above. 
EXERCISES 
1. Draw the comparison tree for sorting four elements. 
2. Draw the comparison tree for sorting four elements which is produced by the 
binary insertion method. 
3. When equality between keys is permitted there are thirteen possible permuta-
tions when sorting 3 elements. What are they? 
4. When keys are allowed to be equal a comparison can have one of three results: 
A(i) < A(j), A(i) = A(JJ, A(i) > A(j). Sorting algorithms can therefore be 
represented by extended ternary comparison trees. Draw an extended ternary 
tree for sorting 3 elements when equality is allowed. 
5. Let TE min(n) be the minimum number of comparisons needed to sort n items 
and to determine all equalities between them. It is clear that TE(n) ~ T(n) 
since then items could be distinct. Show that TE(n) = T(n). 
6. Find a comparison tree for sorting six elements which has all external nodes 
on levels 10 and 11. 
7. 
Stirling's approximation is nl - .J'2in (nle)ne 11(1 2n>. Show how this approxi-
mation is used to show that Jlog nil = n log n - nl(ln 2) + (l/2)log n + 
0(1). 

• 
• 
• 
• 
498 
Lower Bound Theory 
8. Prove that the closed form for BISORT(n) 
correct. 
n IJog n l - 2 pog n l + 1 is 
9. Show that log (n!) is approximately equal to n log n -
n log e + 0(1) by 
using the fact that the function log k is monotonic and bounded below by 
f :_ 1logxdx. 
ltok. 
10. Showthatthesum2k - 2k-I + 2k- 2 + ... + (-l)k20 = (2k+I + (-l)k)/3. 
11. Let m = om. Then by Stirling's approximation log("" er: n) = n((l + o:) log 
(1 + o:) -
o:log o:) - (l/2)1og n + 0(1). Show that as o: -
0 the difference 
between this formula and m + n - 1 gets arbitrarily large. 
12. Let F(n) be the minimum number of comparisons, in the worst case, needed 
to insert B(l) into the ordered set A(l) < A(2) < .. . . < A(n). Prove by in-
duction that F(n) ~ !log n + 1 l . 
13. A partial ordering is a binary relation, denotes by " s ", which satisfies (i) if 
x s y andy s z thenx s z; and (ii) ifx s y andy s x thenx = y. A total 
ordering is a partial ordering which satisfies (iii) for all x, y either x s y or 
y s x. How can a directed graph be used to model a partial ordering or a 
total ordering. 
14. Consider the problem of determining a lower bound for the problem of multi-
plying an m x n matrix A by an n x 1 vector. Show how to reexpress this 
problem using a different matrix formulation so that theorem 10.11 can be 
applied yielding the lower bound of mn multiplications . 
15. [Reingold] Let A(l:n) and B(l:n) each contain n unordered elements. Show 
that if comparisons between pairs of elements of A or B are not allowed, then 
O(n 2) operations are required to test if the elements of A are identical (though 
possibly a permutation) of the elements of B. 
16. 
In the derivation of the Ford-Johnson sorting algorithm, the sequence ti must 
be determined. Explain why t1 + tj-1 = 21. Then show how to derive the 
formula t1 = (2f+t + (- l)i)/3. 
17. [Dobkin and Lipton] 
A search program is a finite sequence of instructions of 
three types: (i) lf/(x) R 0 then go to Ll else go to L2 where R is either <, >, 
or = and x is a vector; (ii) accept, (iii) reject. The sum of subsets problem 
asks for a subset I of the integers 1, 2, ... , n for the inputs w i, ••• , w n such 

Exercises 
499 
that E(w ;) = b, where b is a given number. Consider search programs where 
the function f is restricted so that it can only make comparisons of the form 
Ew; = b 
iEi 
(10.11) 
Using the adversary technique Dobkin and Lipton have shown that 0(2 ") such 
operations are required to solve the sum of subsets problem (w 1, ••• , w n, b ). 
See if you can derive their proof. 
18. Let A be an n x n symmetric matrix, A(i, j) = A(j, i) for 1 s i, j s n. Show 
that if p is the number of nonzero entries of A(i, j), i < j then n + p multipli-
cations are sufficient to compute Ax. 
19. Show how an n x n matrix can be multiplied by two n x 1 vectors using 
(3n 2 + Sn)/2 multiplications. 
20. 
[W. Miller] (i) Let (N, R) denote the reflexive transitive closure of a directed 
graph (N, E). Thus <u, v > is an edge in R if there is a path from u to v 
using zero or more edges in E. Show that R is a partial order on N iff (N, E) is 
acyclic. (ii) Prove that (N, EU (u, v)) is acyclic iff (N, E) is acyclic and there 
is no path from v to u using edges in E. (iii) Prove that if (N, E) is acyclic 
and if u, v are distinct elements of N, then one of (N, EU (u, v)) or(N, U(E 
(v, u))) is acyclic. (iv) Show that it is natural to think of an oracle as con-
structing an acyclic digraph on the set N of players. Interpret (ii) and (iii) as 
rules governing how the oracle may resolve matches. 
21. 
[Valiant] 
Devise a parallel algorithm which produces the maximum of n un-
ordered elements in log log n + c parallel time, where c is a constant. 
22. 
[Valiant] For a number of processors k = .Jmn and for n s m, devise a 
parallel algorithm for merging two ordered sets of m and n elements which 
works in time 2 log log n + c, where c is a constant. 
23. 
[Valiant] 
Use the idea of mergesort and the fast merging algorithm in the 
previous exercise to devise a parallel sorting algorithm which takes at most 
2 log n log log n + O(log n) parallel time. 
24. 
Write an exponentiation procedure which computes xn using the low order to 
the high order bits of n. 
25. 
Determine how fast the inner product Ea ;b, of two vectors can be formed in 
parallel time. 

• 
• 
. 
, 
., 
500 
Lower Bound Theory 
26. 
Devise a parallel algorithm which computes the set of values x 2, x 3, ••• , xn 
which requires less than O(n) time. 
27. 
[Kung] 
Consider the recurrence relation y;+1 = (112)(y; + a/y;) i = 0, 
1, 2, ... , n - 1 for approximating a 112. Show that evaluating y n by any par-
allel algorithm requires O(n) parallel time. 
28 •. [Kung] Given the recurrencey; = y;-1b; + a;+i, i ~ 1, show that a speed-
up of at most (2/3)k + 113 is the best possible for evaluating y n. 
29. 
[Borodin Munro] This exercise completes the proof of Theorem 10.9. Let 
p 1 (a i, ••• a,), ... , p u(a i, ••• , a,) be u linearly independent functions of 
a1, ..... , a,. Let a1 = p(a2, ... , a,). Then show that there are at lest u -
1 
linearly independent p; = p; where a 1 is replaced by p. 
30. Devise a parallel algorithm which computes the value of an nth degree poly-
nomial in time O(log n ) . 
31. Devise a parallel algorithm which merges two ordered sets of n elements in 
O(log n) time. 
32. [W. Miller] 
Show that the inner product of two n-vectors can be computed 
in I n/21 multiplications if separate preconditioning of the vector elements is 
not counted . 

Chapter 11 
NP-HARD AND NP-COMPLETE 
PROBLEMS 
11.1 BASIC CONCEPTS 
This chapter contains what is perhaps the most important theoretical de-
velopment in algorithms research in the past decade. Its importance arises 
from the fact that the results have meaning for all researchers who are 
developing computer algorithms, not only computer scientists but electrical 
engineers, operations researchers, etc. Thus we believe that many people 
will tum immediately to this chapter. In recognition of this we have tried 
to make the chapter self-contained. Also, we have organized the later sec-
tions according to different areas of interest. 
There are however some basic ideas which one should be familiar with 
before reading on. The first is the idea of analyzing apriori the computing 
time of an algorithm by studying the frequency of execution of its state-
ments given various sets of data. A second notion is the concept of the 
order of magnitude of the time complexity of an algorithm and its expres-
sion by asymptotic notation. If T(n) is the time for an algorithm on n in-
puts, then, we write T(n) = O(j(n)) to mean that the time is bounded 
above by the function j(n), and T(n) = !l(g(n)) to mean that the time is 
bounded below by the function g(n). Precise definitions and greater elabo-
ration of these ideas can be found in Section 1.4. 
Another important idea is the distinction between problems whose solu-
tion is by a polynomial time algorithm (j(n) is a polynomial) and problems 
for which no polynomial time algorithm is known (g(n) is larger than any 
polynomial). It is an unexplained phenomenon that for many of the problems 
we know and study, the best algorithms for their solution have computing 
times which cluster into two groups. The first group consists of problems 
whose solution is bounded by a polynomial of small degree. Examples we 
have seen in this book include ordered searching which is O(log n), poly-
501 

• 
.. 
•1 
., 
502 
NP-Hard and NP-Complete Problems 
nomial evaluation is O(n), sorting is O(n log n), and matrix multiplication 
which is O(n 2·81). 
The second group contains problems whose best known algorithms are 
nonpolynomial. Examples we have seen include the traveling salesperson 
and the knapsack problem for which the best algorithms given in this text 
have a complexity O(n22n) and 0(2n12) respectively. In the quest to develop 
efficient algorithms, no one has been able to develop a polynomial time 
algorithm for any problem in the second group. This is very important 
because algorithms whose computing time is greater than polynomial 
(typically the time is exponential) very quickly require such vast amounts 
of time to execute that even moderate size problems cannot be solved. (See 
Section 1.4 for more details.) 
The theory of NP-completeness which we present here does not provide 
a method of obtaining polynomial time algorithms for problems in the 
second group. Nor does it say that algorithms of this complexity do not 
exist. Instead, what we shall do is show that many of the problems for 
which there is no known polynomial time algorithm are computationally 
related. In fact, we shall establish two classes of problems. These will be 
given the names NP-hard and NP-complete. A problem which is NP-
complete will have the property that it can be solved in polynomial time iff 
all other NP-complete problems can also be solved in polynomial time. 
If an NP-hard problem can be solved in polynomial time then all NP-
complete problems can be solved in polynomial time. As we shall see all 
NP-complete problems are NP-hard but all NP-hard problems are not 
NP-complete. 
While one can define many distinct problem classes having the prop-
erties stated above for the NP-hard and NP-complete classes, the classes 
we study are related to nondeterministic computations (to be defined later). 
The relationship of these classes to nondeterministic computations together 
with the "apparent" power of nondeterminism leads to the "intuitive" 
(though as yet unproved) conclusion that no NP-complete or NP-hard 
problem is polynomially solvable. 
We shall see that the class of NP-hard problems (and the subclass of 
NP-complete problems) is very rich as it contains many interesting prob-
lems from a wide variety of disciplines. First, we formalize the preceding 
discussion of the classes. 
Nondeterministic Algorithms 
Up to now the notion of algorithm that we have been using has the property 
that the result of every operation is uniquely defined. Algorithms with this 

Basic Concepts 
503 
property are termed deterministic algorithms. Such algorithms agree with 
the way programs are executed on a computer. In a theoretical framework 
we can remove this restriction on the outcome of every operation. We can 
allow algorithms to contain operations whose outcome is not uniquely de-
fined but is limited to a specified set of possibilities. The machine executing 
such operations is allowed to choose any one of these outcomes subject to 
a termination condition to be defined later. This leads to the concept of a 
nondeterministic algorithm. To specify such algorithms we introduce one 
new function and two new statements into SPARKS: 
(i) choice (S) ... arbitrarily chooses one of the elements of set S 
(ii) failure 
... signals an unsuccessful completion 
(iii) success 
... signals a successful completion. 
The assignment statement X -
choice(l :n) could result in X being as-
signed any one of the integers in the range [l, n]. There is no rule specifying 
how this choice is to be made. The failure and success signals are used to 
define a computation of the algorithm. These statements are equivalent to 
a stop statement and cannot be used to effect a return. Whenever there is 
a set of choices that leads to a successful completion then one such set of 
choices is always made and the algorithm terminates successfully. A non-
deterministic algorithm terminates unsuccessfulZv if and only if there exists 
no set of choices leading to a success signal. The computing times for choice, 
success, and failure are taken to be 0(1). A machine capable of executing 
a nondeterministic algorithm in this way is called a nondeterministic 
machine. While nondeterministic machines (as defined here) do not exist 
in practice, we shall see that they will provide strong intuitive reasons to 
conclude that certain problems cannot be solved by "fast" deterministic 
algorithms. 
Example 11.1 
Consider the problem of searching for an element x in a 
given set of elements A(l:n), n 2: 1. We are required to determine an 
index j such that A (j) = x or j = 0 if x is not in A. A nondeterministic 
algorithm for this is 
j -
choice(l:n) 
if A(j) = x then print(j); success endif 
print('O'); failure 
From the way a nondeterministic computation is defined, it follows that 
the number 'O' can be output if and only if there is no j such that A (j) = x. 

• 
., 
• 
• 
504 
NP-Hard and NP-Complete Problems 
The above algorithm is of nondeterministic complexity 0(1). Note that 
since A is not ordered, every deterministic search algorithm is of complexity 
O(n). 
D 
Example 11.2 
[Sorting] 
Let A (i), 1 :s i :s n be an unsorted set of posi-
tive integers. The nondeterministic algorithm NSORT(A, n) sorts the num-
bers into nondecreasing order and then outputs them in this order. An 
auxiliary array B(l:n) is used for convenience. Line 1 initializes B to zero 
though any value different from all the A(i) will do. In the loop of lines 
2-6 each A (i) is assigned to a position in B. Line 3 nondeterministically 
determines this position. Line 4 ascertains that B(j) has not already been 
used. Thus, the order of the numbers in B is some permutation of the 
initial order in A. Lines 7 to 9 verify that B is sorted in nondecreasing 
order. A successful completion is achieved iff the numbers are output in 
nondecreasing order. Since there is always a set of choices at line 3 for 
such an output order, algorithm NSORT is a sorting algorithm. Its com-
plexity is O(n). Recall that all deterministic sorting algorithms must have 
a complexity O(n log n). 
D 
procedure NSORT(A, n) 
I I sort n positive integers/ I 
integer A(n), B(n), n, i,j 
1 
B -
0 I /initialize B to zero/ I 
2 
for i-lto ndo 
3 
j -
choice(l:n) 
4 
if B (}) ¢. 0 then failure endif 
5 
B(j) -
A(i) 
6 
repeat 
7 
for i -
1 to n - 1 do 
I /verify order/ I 
8 
if B(i) > B(i + 1) then failure endif 
9 
repeat 
10 
print(B) 
11 
success 
12 
end NSORT 
Algorithm 11.1 Nondeterministic sorting 
A deterministic interpretation of a nondeterministic algorithm can be 
made by allowing unbounded parallelism in computation. Each time a 
choice is to be made, the algorithm makes several copies of itself. One copy 

Basic Concepts 
505 
is made for each of the possible choices. Thus, many copies are executing 
at the same time. The first copy to reach a successful completion termi-
nates all other computations. If a copy reaches a failure completion then 
only that copy of the algorithm terminates. Recall that the success and 
failure signals are equivalent to stop statements in deterministic algorithms. 
They may not be used in place of return statements. While this interpre-
tation may enable one to better understand nondeterministic algorithms, 
it is important to remember that a nondeterministic machine does not 
make any copies of an algorithm every time a choice is to be made. Instead, 
it has the ability to select a "correct" element from the set of allowable 
choices (if such an element exists) every time a choice is to be made. A 
"correct" element is defined relative to a shortest sequence of choices that 
leads to a successful termination. In case there is no sequence of choices 
leading to a successful termination, we shall assume that the algorithm 
terminates in one unit of time with output "unsuccessful computation." 
Whenever successful termination is possible, a nondeterministic machine 
makes a sequence of choices which is a shortest sequence leading to a suc-
cessful termination. Since, the machine we are defining is fictitious, it is 
not necessary for us to concern ourselves with how the machine can make 
a correct choice at each step. 
It is possible to construct nondeterministic algorithms for which many 
different choice sequences lead to a successful completion. Procedure 
NSORT of Example 11.2 is one such algorithm. If the numbers A(i) are 
not distinct then many different permutations will result in a sorted se-
quence. If NSORT were written to output the permutation used rather 
than the A (i)'s in sorted order then its output would not be uniquely de-
fined. We shall concern ourselves only with those nondeterministic algo-
rithms that generate a unique output. In particular we shall consider only 
nondeterministic decision algorithms. Such algorithms generate only a zero 
or one as their output. A binary decision is made. A successful completion 
is made iff the output is '1'. A 'O' is output iff there is no sequence of 
choices leading to a successful completion. The output statement is im-
plicit in the signals success and failure. No explicit output statements 
are permitted in a decision algorithm. Clearly, our earlier definition of a 
nondeterministic computation implies that the output from a decision algo-
rithm is uniquely defined by the input parameters and the algorithm 
specification. 
While the idea of a decision algorithm may appear very restrictive at this 
time, many optimization problems can be recast into decision problems 
with the property that the decision problem can be solved in polynomial 
time iff the corresponding optimization problem can. In other cases, we 

• 
• 
•, 
• 
506 
NP-Hard and NP-Complete Problems 
can at least make the statement that if the decision problem cannot be 
solved in polynomial time then the optimization problem cannot either. 
Example 11.3 [Max Clique] A maximal complete subgraph of a graph G 
= (V, E) is a clique. The size of the clique is the number of vertices in it. 
The max clique problem is to determine the size of a largest clique in G. 
The corresponding decision problem is to determine if G has a clique of 
size at least k for some given k. Let DCLIQUE( G, k) be a deterministic 
decision algorithm for the clique decision problem. If the number of ver-
tices in G is n, the size of a max clique in G can be found by making 
several applications of DCLIQUE. DCLIQUE is used once for each k, 
k = n, n -
1, n -
2, ... until the output from DCLIQUE is 1. If the 
time complexity of DCLIQUE is f(n) then the size of a max clique can 
be found in time n*f (n ). Also, if the size of a max clique can be determined 
in time g (n) then the decision problem may be solved in time g (n). Hence, 
the max clique problem can be solved in polynomial time iff the clique 
decision problem can be solved in polynomial time. 
D 
Example 11.4 
[Oil-Knapsack] The knapsack decision problem is to 
determine if there is a 0/ 1 assignment of values to X;, 1 :s i :s n such that 
E p;X; ~Rand E w;x; :s M. Risa given number. The p/s and w/s are 
nonnegative numbers. Clearly, if the knapsack decision problem cannot be 
solved in deterministic polynomial time then the optimization problem 
cannot either. 
D 
Before proceeding further, it is necessary to arrive at a uniform parameter, 
n, to measure complexity. We shall assume that n is the length of the 
input to the algorithm. We shall also assume that all inputs are integer. 
Rational inputs can be provided by specifying pairs of integers. Generally, 
the length of an input is measured assuming a binary representation. I.e., if 
the number 10 is to be input then, in binary it is represented as 1010. Its 
length is 4. In general, a positive integer k has a length of L log2 kj + 1 
bits when represented in binary. The length of the binary representation 
of 0 is 1. The size or length, n, of the input to an algorithm is the sum of 
the lengths of the individual numbers being input. In case the input is 
given using a different representation (say radix r), then the length of a 
positive number k is Llog, kj + 1. Thus, in decimal notation, r = 10 
and the number 100 has a length log10 100 + 1 = 3 digits. Since log, k = 
log2 k /log2 r, the length of any input using radix r(r > 1) representation 
is c(r)· n where n is the length using a binary representation and c(r) is a 
number which is fixed for a given r . 

Basic Concepts 
507 
When inputs are given using the radix r = 1, we shall say the input is in 
unary form. In unary form, the number 5 is input as 11111. Thus, the 
length of a positive integer k is k. It is important to observe that the length 
of a unary input is exponentially related to the length of the corresponding 
r-ary input for radix r, r > 1. 
Example 11.5 [Max Clique] The input to the max clique decision problem 
may be provided as a sequence of edges and an integer k. Each edge in E( G) 
is a pair of numbers (i, j). The size of the input for each edge (i, j) is 
L log2 iJ 
+ L log2 jJ + 2 if a binary representation is assumed. The 
input size of any instance is 
n = 
E 
(Llog2 iJ + Llog2jJ + 2) + Llog2 kj + 1. 
li.i)f E(G) 
i<j 
Note that if G has only one connected component then n ~ I VI. Thus, if 
this decision problem cannot be solved by an algorithm of complexity p(n) 
for some polynomial p( ) then it cannot be solved by an algorithm of 
complexity p( I VI). 
D 
Example 11.6 
[0/1 Knapsack] 
Assuming p;, W;, Mand R are all in-
tegers, the input size for the knapsack decision problem is 
Note that m ~ n. If the input is given in unary notation then the input 
size sis E p; + E w; + M + R. Note that the knapsack decision and 
optimization problems can be solved in time p(s) for some polynomial p( ) 
(see the dynamic programming algorithm). However, there is no known 
algorithm with complexity O(p(n)) for some polynomial p( ). 
D 
We are now ready to formally define the complexity of a nondeterministic 
algorithm. 
Definition The time re1uired by a nondeterministic algorithm performing 
on any given input is the minimum number of steps needed to reach a suc-
cessful completion if there exists a sequence of choices leading to such a 
completion. In case successful completion is not possible then the time re-
quired is 0(1). A nondeterministic algorithm is of complexity O(fin)) if for 
all inputs of size, n, n ~ n 0, that result in a successful completion the time 
required is at most c-f(n) for some constants c and n0 • 

• 
• 
• 
• 
• 
508 
NP-Hard and NP-Complete Problems 
In the above definition we assume that each computation step is of a 
fixed cost. In word oriented computers this is guaranteed by the finiteness 
of each word. When each step is not of a fixed cost it is necessary to con-
sider the cost of individual instructions. Thus, the additon of two m bit 
numbers takes O(m) time, their multiplication takes O(m 2) time (using clas-
sical multiplication) etc. To see the necessity of this consider procedure 
SUM (Algorithm 11.2). This is a deterministic algorithm for the sum of sub-
sets decision problem. It uses an M + 1 bit word S. The i'th bit in Sis zero 
iff no subset of the integers A(j), 1 :s j :s n sums to i. Bit 0 of Sis always 
1 and the bits are numbered 0, 1, 2, ... , M right to left. The function 
SHIFT shifts the bits in S to the left by A(i) bits. The total number of steps 
for this algorithm is only O(n). However, each step moves M + 1 bits of 
data and would really take O(M) time on a conventional computer. As-
suming one unit of time is needed for each basic operation for a fixed word 
size, the true complexity is O(nM) and not O(n). 
procedure SUM(A, n, M) 
integer A(n), S, n, M 
S -
1 I IS is an M + 1 bit word. Bit zero is 11 I 
for i-1 to ndo 
S -
Sor SHIFT(S, A(i)) 
repeat 
if Mth bit in S = Othen print ('no subset sums to M) 
else print ('a subset sums to M) 
endif 
end SUM 
Algorithm 11.2 Deterministic sum of subsets 
The virtue of conceiving of nondeterministic algorithms is that often 
what would be very complex to write down deterministically is very easy to 
write nondeterministically. In fact, it is very easy to obtain polynomial 
time nondeterministic algorithms for many problems that can be deter· 
ministically solved by a systematic search of a solution space of exponential 
size. 
Example 11. 7 [Knapsack decision problem] Procedure DKP (Algo-
rithm 11.3) is a nondeterministic polynomial time algorithm for the knap-
sack decision problem. Lines 1 to 3 assign 0/1 values to X(i), 1 :s i :s n. 
Line 4 checks to see if this assignment is feasible and if the resulting profit 

Basic Concepts 
509 
is at least R. A successful termination is possible iff the answer to the deci-
sion problem is yes. The time complexity is O(n). If m is the input length 
using a binary representation, the time is O(m). 
D 
procedure DKP(P, W, n, M, R, X) 
integer P(n), W(n), R, X(n), n, M, i 
1 
for i -
l to n do 
2 
X(i) -
choice (0, 1) 
3 
repeat 
4 
if E ( W(i) *X(i)) > M or E (P(i) *X(i)) < R then failure 
tsi:sn 
tsi:sn 
else success 
5 
endif 
end DKP 
Algorithm 11.3 Nondeterministic Knapsack problem 
Example 11.8 [Max Clique] Procedure DCK (Algorithm 11.4) is a non-
deterministic algorithm for the clique decision problem. The algorithm 
begins by trying to form a set of k distinct vertices. Then it tests to see if 
these vertices form a complete subgraph. If G is given by its adjacency matrix 
and I VI = n, the input length mis n 2 + Llog 2 kJ + Llog2 nJ + 2. 
Lines 2 to 6 can easily be implemented to run in nondeterministic time O(n). 
The time for lines 7-10 i~ O(k 2). Hence the overall nondeterministic time is 
O(n + k2) = O(n2 ) = O(m). There is no known polynomial time determi-
nistic algorithm for this problem. 
D 
procedure DCK ( G, n, k) 
1 
S -
</> 
I IS is an initially empty set/ I 
2 
for i -
1 to k do 
I I select k distinct vertices/ I 
3 
t -
choice (l:n) 
4 
if t E S then failure endif 
5 
S -
S U t 
//add t to set SI I 
6 
repeat 
I I at this point S contains k distinct vertex indices/ I 
7 
for all pairs (i, j) such that i ES, j ES and i -:;e j do 
8 
if (i, j) is not an edge of the graph 
9 
then failure endif 
10 
repeat 
11 
success 
end DCK 
Algorithm 11.4 Nondeterministic clique 

• 
• 
• 
• 
., 
510 
NP-Hard and NP-Complete Problems 
Example 11.9 
[Satisfiability] Let x 1 , x2 , ••• , denote boolean variables 
(their value is either true or false). Let x; denote the negation of x ;. A literal 
is either a variable or its negation. A formula in the propositional calculus 
is an expression that can be constructed using literals and the operations 
and and or. Examples of such formulas are (x1 /\ x2 ) V (x3 /\ x4 ); (x3 V .X4 ) 
/\ (x 1 V :i 2). V denotes or and /\ denotes and. A formula is in conjunctive 
normal form (CNF) iff it is represented as /\ ~ ~ 1 c; where the c; are clauses 
each represented as V lij. The lij are literals. It is in disjunctive normal 
form (DNF) iff it is represented as Vt~ 1 C; and each clause c; is represented 
as A lij. Thus (x1 /\ x 2 ) V (x3 /\ x4 ) is in DNF while (x3 V .X4 ) /\ (x1 V .X2 ) 
is in CNF. The satisfiability problem is to determine if a formula is true 
for some assignment of truth values to the variables. CFN-satisfiability is 
the satisfiability problem for CNF formulas. 
It is easy to obtain a polynomial time nondeterministic algorithm that 
terminates successfully if and only if a given propositional formula E(x1 , 
... , xn) is satisfiable. Such an algorithm could proceed by simply choosing 
(nondeterministically) one of the 2n possible assignments of truth values 
to (x1 , 
•• ., Xn) and verifying that E(x1 , 
•• ., Xn) is true for that as· 
signment. 
Procedure EVAL (Algorithm 11.5) does this. The nondeterministic time 
required by the algorithm is O(n) to choose the value of (x1 , ••• , Xn) plus 
the time needed to deterministically evaluate E for that assignment. This 
time is proportional to the length of E. 
D 
procedure EVAL(E, n) 
I /Determine if the propositional formula E is satisfiable. The variables/ I 
I I are x ;, 1 :5 i :5 n/ I 
boolean x(n) 
for i -
1 to n do 
I I choose a truth value assignment/ I 
x; -
choice (true, false) 
repeat 
if E(x1 , ••• , xn) is true then success 
//satisfiable// 
else failure 
endif 
end EVAL 
Algorithm 11.5 Nondeterministic satisfiability 
The Classes NP-hard and NP-complete 
In measuring the complexity of an algorithm we shall use the input length 

Basic Concepts 
511 
as the parameter. An algorithm A is of polynomial complexity if there 
exists a polynomial p( ) such that the computing time of A is O(p(n)) for 
every input of size n. 
Definition P is the set of all decision problems solvable by a deterministic 
algorithm in polynomial time. NP is the set of all decision problems solv-
able by a nondeterministic algorithm in polynomial time. 
Since deterministic algorithms are just a special case of nondeterministic 
ones, we can conclude that P ~ NP. What we do not know, and what 
has become perhaps the most famous unsolved problem in computer science 
is whether P = NP or P -:;e NP. 
Is it possible that for all of the problems in NP there exist polynomial 
time deterministic algorithms which have remained undiscovered? This 
seems unlikely, at least because of the tremendous effort which has already 
been expended by so many people on these problems. Nevertheless, a 
proof that P -:;e NP is just as elusive and seems to require as yet undis-
covered techniques. But as with many famous unsolved problems, they 
serve to generate other useful results, and the P ~ NP question is no 
exception. 
In considering this problem S. Cook formulated the following question: 
Is there any single problem in NP such that if we showed it to be in P, then 
that would imply that P = NP. Cook answered his own question in the 
affirmative with the following theorem. 
Theorem 11.1 
(Cook) Satisfiability is in P if and only if P = NP. 
Proof: 
See Section 11.2 
0 
We are now ready to define the NP-hard and NP-complete classes of 
problems. First we define the notion of reducibility. 
Definition Let L 1 and Li be problems. L 1 reduces to Li (also written 
L1 ex Li) if and only if there is a way to solve L 1 by a deterministic poly-
nomial time algorithm using a deterministic algorithm that solves Li in 
polynomial time. 
This definition implies that if we have a polynomial time algorithm for 
Li then we can solve L1 in polynomial time. One may readily verify that 
QC is a transitive relation (i.e. if L1 QC L2 and L2 QC LJ then L1 QC LJ). 
Definition A problem L is NP-hard if and only if satisfiability reduces 

•, 
512 
NP-Hard and NP-Complete Problems 
to L (satisfiability oc L). A problem L is NP-complete if and only if L is 
NP-hard andL E NP. 
It is easy to see that there are NP-hard problems that are not NP-com-
plete. Only a decision problem can be NP-complete. However, an opti-
mization problem may be NP-hard. Furthermore if L 1 is a decision prob-
lem and Li an optimization problem, it is quite possible that L 1 oc Li. 
One may trivially show that the knapsack decision problem reduces to the 
knapsack optimization problem. For the clique problem one may easily 
show that the clique decision problem reduces to the clique optimization 
problem. In fact, we can also show that these optimization problems re· 
duce to their corresponding decision problems (see exercises). Yet, opti· 
mization problems cannot be NP-complete while decision problems can. 
There also exist NP-hard decision problems that are not NP-complete. 
Example 11.10 As an extreme example of an NP-hard decision problem 
that is not NP-complete consider the halting problem for deterministic 
algorithms. The halting problem is to determine for an arbitrary deter-
ministic algorithm A and an input I whether algorithm A with input I 
ever terminates (or enters an infinite loop). It is well known that this prob· 
lem is undecidable. Hence, there exists no algorithm (of any complexity) 
to solve this problem. So, it clearly cannot be in NP. To show satisfiability 
oc halting problem simply construct an algorithm A whose input is a 
propositional formula X. If X has n variables then A tries out all 2 n pos· 
sible truth assignments and verifies if X is satisfiable. If it is then A stops. 
If X is not satisfiable then A enters an infinite loop. Hence, A halts on 
input X iff X is satisfiable. If we had a polynomial time algorithm for the 
halting problem then we could solve the satisfiability problem in poly· 
nomial time using A and X as input to the algorithm for the halting 
problem. Hence, the halting problem is an NP-hard problem which is not 
in NP. 
D 
Definition Two problems L 1 and Li are said to be polynomially equivalent 
iff L1 oc Li and Li oc L1 . 
In order to show that a problem, Li is NP-hard it is adequate to show 
L 1 oc Li where L 1 is some problem already known to be NP-hard. Since 
oc is a transitive relation, it follows that if satisfiability oc L1 and L 1 oc Li 
then satisfiability oc Li. To show an NP-hard decision problem NP-com-
plete we have just to exhibit a polynomial time nondeterministic algorithm 
for it. Later sections will show many problems to be NP-hard. While we 
shall restrict ourselves to decision problems, it should be clear that the 

Cook's Theorem 
513 
corresponding optimization problems are also NP-hard. The NP-com-
pleteness proofs will be left as exercises (for those problems that are NP-
complete). 
11.2 COOK'S THEOREM 
Cook's theorem (Theorem 11.1) states that satisfiability is in P iff P 
NP. We shall now prove this important theorem. We have already seen 
that satisfiability is in NP (Example 11.9). Hence, if P = NP then satis-
fiability is in P. It remains to be shown that if satisfiability is in P then 
P = NP. In order to prove this latter statement, we shall show how to 
obtain from any polynomial time nondeterministic decision algorithm A 
and input I a formula Q(A, I) such that Q is satisfiable iff A has a suc-
cessful termination with input I. If the length of I is n and the time com-
plexity of A is p(n) for some polynomial p( ) then the length of Q will be 
O(p 3(n) log n) = O(p 4(n)). The time needed to construct Q will also be 
O(p 3(n) log n). A deterministic algorithm Z to determine the outcome of 
A on any input I may be easily obtained. Z simply computes Q and then 
uses a deterministic algorithm for the satisfiability problem to determine 
whether or not Q is satisfiable. If O(q(m)) is the time needed to deter-
mine if a formula of length m is satisfiable then the complexity of Z is 
O(p 3(n) log n + q(p 3(n) log n)). If satisfiability is in P then q(m) is a 
polynomial function of m and the complexity of Z becomes O(r(n)) for 
some polynomial r( ). Hence, if satisfiability is in P then for every non-
deterministic algorithm A in NP we can obtain a deterministic Zin P. So, 
the above construction will show that if satisfiability is in P then P = NP. 
Before going into the construction of Q from A and I. we shall make 
some simplifying assumptions on our nondeterministic machine model and 
on the form of A. These assumptions will not in any way alter the class of 
decision problems in NP or P. The simplifying assumptions are: 
i) The machine on which A is to be executed is word oriented. Each 
word is w bits long. Multiplication, addition, subtraction etc. between 
numbers one word long take one unit of time. In case numbers are 
longer than a word then the corresponding operations take at least 
as many units as the number of words making up the longest number. 
ii) A simple expression is an expression that contains at most one opera-
tor and all operands are simple variables (i.e., no array variables are 
used). Some sample simple expressions are -B, B + C, D or E, F. 

514 
NP-Hard and NP-Complete Problems 
• 
We shall assume that all assignment statements in A are of one of the 
following forms: 
•, 
a) (simple variable) -
(simple expression) 
b) (array variable) -
(simple variable) 
c) (simple variable) -
(array variable) 
d) (simple variable) -
choice (S) where S may be a finite set { S 1, 
S 2, ... , S k} or Smay be l:u. In the latter case the function chooses 
an integer in the range [/:u]. 
Indexing within an array is done using a simple integer variable and all 
index values are positive. Only one dimensional arrays are allowed. Clearly, 
all assignment statements not falling into one of the above categories may 
be replaced by a set of statements of these types. Hence, this restriction 
does not alter the class NP. 
iii) All variables in A are of type integer or boolean. 
iv) A contains no read or print statements. The only input to A is via 
its parameters. At the time A is invoked all variables (other than 
the parameters) have value zero (or false if boolean). 
v) 
A contains no constants. Oearly, all constants in any algorithm may 
be replaced by new variables. These new variables may be added to 
the parameter list of A and the constants associated with them can 
be part of the input. 
vi) In addition to simple assignment statements, A is allowed to contain 
only the following types of statements: 
a) go to k where k is an instruction number 
b) if c then go to a endif. c is a simple boolean variable (i.e., not an 
array) and a is an instruction number 
c) success, failure, end 
d) A may contain type declaration and dimension statments. These 
are not used during execution of A and so need not be translated 
into Q. The dimension information is used to allocate array space. 
It is assumed that successive elements in an array are assigned to 
consective words in memory. 
It is assumed that the instructions in A are numbered sequentially 
from 1 to I (if A has I instructions). Every statement in A has a num-
ber. The go to instructions in a) and b) use this numbering scheme 
to effect a branch. It should be easy to see how to rewrite 'while-

Cook's Theorem 
515 
repeat', 'repeat-until', 'case-endcase', 'for-repeat', etc. statements 
in terms of go to and if c then go to a endif statements. Also, note 
that the go to k statement can be replaced by the statement if true 
then go to k endif. So, this may also be eliminated. 
vii) Let p(n) be a polynomial such that A takes no more than p(n) time 
units on any input of length n. Because of the complexity assump-
tions of (i), A cannot change or use more than p( n) words of memory. 
We may assume that A uses some subset of the words indexed 1, 2, 
3, ... , p(n). This assumption does not restrict the class of decision 
problems in NP. To see this letj(l),f(2), ... ,f(k), 1 :5 k :5 p(n), be 
the distinct words used by A while working on input/. We can con-
struct another polynomial time nondeterministic algorithm A ' which 
uses 2p(n) words indexed 1, 2, ... , 2p(n) and solves the same deci-
sion problem as does A. A ' simulates the behavior of A. However, 
A' maps the addresses fil), f(2), ... , fik) onto the set {J, 2, 
... , k } . The mapping function used is determined dynamically and 
is stored as a table in words p(n) + 1 through 2Jf...n). If the entry at 
word ff... n) + i is j then A ' uses word i to hold the same value that 
A stored in word j. The simulation of A proceeds as follows: Let k 
be the number of distinct words referenced by A up to this time. Let 
j be a word referenced by A in the current step. A' searches its table 
to find word Jf...n) + i, 1 :5 i :5 k such that the contents of this word 
is j. If no such i exists then A' sets k -
k + 1, i -
k and word 
p(n) + k is given the value j. A' makes use of the word i to do 
whatever A would have done with word j. Clearly, A' and A solve 
the same decision problem. The complexity of A' is O(p 2(n)) as it 
takes A' /i..n) time to search its table and simulate a step of A. Since 
p 2 ( n) is also a polynomial in n, restricting our algorithms to use 
only consecutive words does not alter the classes P and NP. 
Formula Q will make use of several boolean variables. We state the 
semantics of two sets of variables used in Q: 
i) B(i,j, t), 1:5i:5p(n),1 :5j :5 w, 0 :5 t <p(n). 
B( i, j, t) represents the status of bit j of word i following t steps 
(or time units) of computation. The bits in a word are numbered 
from right to left. The rightmost bit is numbered 1. Q will be con-
structed such that in any truth assignment for which Q is true, 
B(i, j, t) is true iff the corresponding bit has value 1 following t 
steps of some successful computation of A on input/. 

516 
NP-Hard and NP-Complete Problems 
ii) S (j, t), 1 '5 j '5 l, 1 '5 t '5 p (n). 
Recall that I is the number of instructions in A. S(j, t) represents 
the instruction to be executed at time t. Q will be constructed such 
that in any truth assignment for which Q is true, S(j, t) is true iff 
the instruction executed by A at time t is instruction j. 
Q will be made up of six subformulas C, D, E, F, G and H. Q = 
C /\ D /\ E /\ F /\ G /\ H. These subformulas will make the following 
assertions: 
C: 
The initial status of the ]i.n) words represents the input I. All 
non-input variables are zero. 
D: Instruction 1 is the first instruction to execute. 
E: 
At the end of the i'th step, there can be only one next instruction 
to execute. Hence, for any fixed i, exactly one of the S(j, i), 1 '5 
j '5 I can be true. 
F: 
If S(j, i) is true then S(j, i + 1) is also true if instruction j is a 
success, failure or end statement. S(j + 1, i + 1) is true if j is an 
assignment statement. If j is a go to k statement then S(k, i + 1) 
is true. The last possibility for j is the If c then a endlf statement. 
In this case S( a, i + 1) is true if c is true and S (j + 1, i + 1) is 
true if c is false. 
G: If the instruction executed at step t is not an assignment statement 
then the B(i, j, t)s are unchanged. If this instruction is an assign-
ment and the variable on the left hand side is X, then only X may 
change. This change is determined by the right hand side of the 
instruction. 
H: The instruction to be executed at time Ji_ n) is a success instruction. 
Hence the computation terminates successfully. 
Oearly, if C through H make the above assertions, then Q = C /\ D /\ 
E /\ F /\ G /\ His satisfiable iff there is a successful computation of A on 
input I. We now give the formulas C through H. While presenting these 
formulas we shall also indicate how each may be transformed into CNF. 
This transformation will increase the length of Q by an amount indepen-
dent of n (but dependent on w and /). This will enable us to show that 
CNF-satisfiability is NP-complete. 
1. Formula C describes the input I. We have: 
C = 
A 
T(i,j, 0) 
l:Si:Sp(n) 
l:Sj:Sw 

Cook's Theorem 
517 
T(i,j, 0) isB(i,j, 0) ifthe input calls for bitB(i,j, O)(i.e. bitj ofwordi) 
to be 1. T(i,j, 0) isB(i,j, O)otherwise. Thus, if there is no inputthen 
C = 
A B(i,j, 0). 
Ls;is;p(n) 
ls,jsw 
Clearly, C is uniquely determined by I and is in CNF. Also, C is satis-
fiable only by a truth assignment representing the initial values of all 
variables in A. 
2. 
D = S(l, 1) A S(2, 1) A S(3, 1) A ... AS(/, 1). 
Clearly, Dis satisfiable only by the assignmentS(l, 1) = true andS(i, 1) 
= false, 2 :5 i :5 /. Using our interpretation of S(i, 1), this means that 
D is true iff instruction 1 is the first to be executed. Note that D is in 
CNF. 
3. E = 
A 
E,. 
I <ts;p(n) 
Each E, will assert that there is a unique instruction for step t. We may 
define E, to be: 
E, = (S(l, t) v S(2, t) v ... v S(/, t)) A ( 
A 
I s;js;/ 
l:sks;/ 
j#-k 
(S(j, t) v S(k, t)) 
OnemayverifythatE 1 istrueiffexactlyoneoftheS(j, t)s, 1 :5j :5 /is 
true. Also, note thatE is in CNF. 
4. F = 
A 
F;,. 
ls;i:s/ 
• 
ls;1<p(n) 
Each Fu asserts that either instruction i is not the one to be executed at 
time t, or if it is then the instruction to be executed at time t + 1 is cor-
rectly determined by instruction i. Formally, we have 
F;,, = S(i, t) v L 
whereL is defined as follows; 
i) 
if instruction i is success, failure or end then L is S (i, t + 1). Hence 
the program cannot leave such an instruction. 

•. 
-
i 
518 
NP-Hard and NP-Complete Problems 
ii) ifinstruction i is go to k then L is S (k, t + 1). 
iii) if instruction i is if X then go to k endif and variable X is repre-
sented by wordj then L is ((B (i, 1, t -
1) I\ S (k, t + 1)) V (/J (i, 1, 
t -
1) I\ S(i + 1, t + 1))). This assumes that bit 1 of Xis 1 iff Xis 
true. 
iv) ifinstruction i is not any of the above thenL isS(i + 1, t + 1). 
The Fi.ts defined in cases (i), (ii) and (iv) above are in CNF. TheF;, 1 in 
case (iii) may be transformed into CNF using the boolean identity a V 
(b I\ c) V (d I\ e) = (a V b V d) I\ (a V c V d) I\ (a V b V e) I\ (a V c V e). 
5. G = 
A 
G;,. 
Isis/ 
· 
ISt<p(n) 
Each G ;,, asserts that at time t either (i) instruction i is not executed or 
(ii) it is and the status of the p (n) words after step t is correct with re-
spect to the status before step t and the changes resulting from instruc-
tion i. Formally, we have 
G;. 1 = S(i, t) v M 
whereM is defined as follows: 
i) if instruction i is a go to, if-then go to-endif, success, failure, 
or end statement then M asserts that the status of the p (n) words is 
unchanged. I.e., B (k, j, t -
1) = B (k, j, t ), 1 :s k :s p (n) and 
1 :sj :s w. 
M = 
A 
((B(k,j, t - 1)1\B(k,j, t))V(/J(k,j, t - 1)1\/J(kj, t)) 
lsksp(n) 
ISjSw 
In this case, G ;,, may be rewritten as 
G;, = 
A 
(S(i,t)V(B(k,j,t-1)/\B(k,j,t)) 
' 
1Sk5p(n) 
ls;;jS::w 
v (/J(k,j, t - 1) A!J(k,j, t))) 
Each clause in G i.t is of the form z V (x I\ s) V (.f I\ s) where z is 
S(i, t), x represents a B( ,, t -
1) ands a B( ,, t). Note that z V 
(x I\ s) V (.f I\ s) is equivalent to (x Vs V z) I\ (.f Vs V z). Hence, 
G ;, 1 may be transformed into CNF easily. 
---·-----

Cook's Theorem 
519 
ii) if i is an assignment statement of type a) then M depends on the 
operator (if any) on the right hand side. We shall first describe the 
form of M for the case when instruction i is of the type Y -
V + Z. 
Let Y, V and Z be respectively represented in words y, v and z. 
We shall make the simplifying assumption that all numbers are 
non-negative. The exercises examine the case when negative num-
bers are allowed and l's complement arithmetic is being used. In 
order to get a formula asserting that the bits B(y,j, t), 1 :5 j :5 w 
represent the sum of B(v, j, t -
1) and B(z, j, t -
1) 1 :5 j :5 w, 
we shall have to make use of w additional bits C(j, t), 1 :5 j :5 w. 
C(j, t) will represent the carry from the addition of the bits B( v, j, 
t -
1), B(z, j, t -
1) and C(j -
1, t), 1 < j :5 w. C(l, t) is the 
carry from the addition of B(v, 1, t -
1) and B(z, 1, t -
1). Recall 
that a bit is 1 iff the corresponding variable is true. Performing a 
bit wise addition of V and Z, we obtain C(l, t) = B(v, 1, t -
1) /\ 
B(z, 1, t -
1) and B(y, 1, t) = B(v, 1, t -
1) (£) B(z, 1, t -
1) 
where (£) is the exclusive or operation (a (£) b is true iff exactly 
one of a and b is true). Note that a (£) b = (a V b) /\ (a /\ b ) = 
(a V b) /\ (a V h ). Hence, the right hand side of the expression for 
B(y, 1, t) may be transformed into CNF using this identity. For 
the other bits of Y, one may verify that 
B(y, j, t) = B(v, j, t -
1) (£) (B(z, j, t -
1) (£) C(j - 1, t)) 
and 
C(j, t) = (B(v,j, t -
1) /\ B(z, j, t -
1)) v (B(v, j, t -
1) 
/\ C(j - 1, t)) v (B(z,j, t -
1) /\ C(j - 1, t)). 
Finally, we require that C(w, t) = false. (i.e. there is no overflow). 
Let M' be the and of all the equations for B(y, j, t) and C(j, t), 
1 :5 j :5 w. M is given by 
M = ( 
/\ 
((B(k,j, t -
1) /\ B(k,j, t)) 
I .,k.,p(n) 
k ;Cy 
} 
l:fj:Sw 
w 
v (B(k, j, t -
1) "B(k,j, t))) "M' 
G;. 1 may be converted into CNF using the idea of 5 (i). This trans-
formation will increase the length of G;.1 by a constant factor in de-

520 
NP-Hard and NP-Complete Problems 
pendent of n. We leave it to the reader to figure out what Mis when 
instruction i is either of the form Y -
V; Y -
V@Zfor@one of - , 
!,*, <, >, :S, =,etc. 
When i is an assignment statement of types b) or c) then it neces-
sary to select the correct array element. Consider an instruction of 
type b): R(m) -
X. In this case the formula M may be written as: 
M = W /\ ( /\ M) 
I sjsu 
where u is the dimension of R. Note that because of restriction (vii) on 
the algorithm A, u :5 p(n)· W asserts that 1 :5 m :5 u. The specifica-
tion of W is left as an exercise. Each Mj asserts that either m -:;e j or 
m = j and only the jth element of R changes. Let us assume that the 
values of X and m are respectively stored in words x and m and that 
R(l:u) is stored in words a, a + l, ... , a + u -
1. Mj is given by: 
Mj = 
v 
T(m, k, t -
1) v Z 
lsksw 
where T is B if the k'th bit in the binary representation of j is 0 and T 
is iJ otherwise. Z is defined as 
Z = 
/\ 
((B(r, k, t -
1) /\ B(r, k, t)) V (B(r, k, t -
1) 
lsksw 
I srsp(n) 
r;itcx+j-1 
"iJ(r, k, t -
1))) 
A ((B(a + j -
1, k, t)/\ B(x, k, t -
1)) 
lsksw 
V (B(a + j -
1, k, t) /\ B(x, k, t -
1))) 
Note that the number of literals in M is O(p 2(n). Since j is w bits 
long it can represent only numbers smaller than 2 w. Hence, for u ~ 2 w 
we need a different indexing scheme. A simple generalization is to 
allow multiprecision arithmetic. The index variable j could use as 
many words as needed. The number of words used would depend on u. 
At most log (p(n)) words are needed. This calls for a slight change in 
M1 but the number of literals in M remains O(p 2(n)). There is no need 
to explicitly incorporate multiprecision arithmetic as by giving the 

Cook's Theorem 
521 
program access to individual words in a multi precision index j we can 
require the program to simulate multiprecision arithmetic. 
When i is an instruction of type c) the form of M is similar to that 
obtained for instructions of type b). Next, we describe how to construct 
M for the case i is of the form Y -
choice (S) where S is either a set 
of the form S = {Si, S 2, ••• , S d or S is of the form r:u. Assume Y 
is represented by wordy. Is S is a set then we define 
M = v M 1• 
I s;js;k 
M1 asserts that Y is S1• This is easily done by choosing M1 = a 1 /\ a 2 /\ 
· · · /\aw where a 1 = B(y, [, t) if bit I is 1 in S1 and a; = fJ(y, [, t) if 
bit I is zero in S 1• If S is of the form r :u then M is just the formula 
that asserts r ::5 Y ::5 u. This is left as an exercise. In both cases, Gu 
may be transformed into CNF increasing the length of Gu by at most 
a constant amount. 
6. 
Let i1, i2, ... , ik be the statement numbers corresponding to the sue· 
cess statements in A. His given by: 
One may readily verify that Q = C /\ D /\ E /\ F /\ G /\ H is satis· 
fiable iff the computation of algorithm A with input I terminates success· 
fully. Further, Q may be transformed into CNF as described above. For-
mula C contains wp (n) literals, D contains / literals, E contains 0(/2p(n)) 
literals, F contains 0(/p(n)) literals, G contains O(lwp 3(n)) literals and H 
contains at most I literals. The total number of literals appearing in Q is 
0(/wp 3(n)) = O(p 3(n)) as lw is constant. Since, there are O(wp 2(n) + 
lp(n)) distinct literals in Q, each literal can be written down using O(log 
(wp 2(n) + lp(n))) = O(log n) bits. The length of Q is therefore O(p 3(n) 
log n) = O(p 4(n)) as p(n) is at least n. The time to construct Q from A 
and I is also O(p 3(n) log n). 
The above construction, shows that every problem in NP reduces to 
satisfiability and also to CNF-satisfiability. Hence, if either of these two 
problems is in P then NP ~ P and so P = NP. Also, since satisfiability 
is in NP, the construction of a CNF formula Q shows that satisfiability ex 
CNF-satisfiability. This together with the knowledge that CNF-satisfiability 
is in NP, implies that CNF-satisfiability is NP-complete. Note that satis-
fiability is also NP-complete as satisfiability ex satisfiability and satisfia-
bility is in NP. 

. 
, 
522 
NP-Hard and NP-Complete Problems 
11.3 NP-HARD GRAPH PROBLEMS 
The strategy we shall adopt to show that a problem L 2 is NP-hard is: 
i) 
Pick a problem L 1 already known to be NP-hard. 
ii) Show how to obtain (in polynomial deterministic time) an instance 
I' of L 2 from any instance I of L 1 such that from the solution of I' 
we can determine (in polynomial deterministic time) the solution to 
instance I to L 1. 
iii) Conclude from (ii) thatL 1 oc L 2 • 
iv) Conclude from (i), (iii) and the transitivity of oc that Li is NP-hard. 
For the first few proofs we shall go through all the above steps. Later 
proofs will explicitly deal only with steps (i) and (ii). An NP-hard decision 
problem L 2 can be shown NP-complete by exhibiting a polynomial time 
nondeterministic algorithm for L 2• All the NP-hard decision problems we 
shall deal with here are also NP-complete. The construction of polynomial 
time nondeterministic algorithms for these problems is left as an exercise. 
Clique Decision Problem (CDP) 
The clique decision problem was introduced in Section 11.1. We shall show 
in Theorem 11.2 that CNF-satisfiability oc CDP. Using this result, the tran-
sitivity of oc and the knowledge that satisfiability oc CNF-satisfiability 
(Section 11.2) we can readily establish that satisfiability oc CDP. Hence, 
CDP is NP-hard. Since, CDP E NP, CDP is also NP-complete . 
Theorem 11.2 CNF-satisfiability oc clique decision problem (CDP) 
Proof: Let F = A is;sk C; be a propositional formula in CNF. Let X;, 1 :5 
i :5 n be the variables in F. We shall show how to construct from F a 
graph G = (V, E) such that G will have a clique of size at least k iff Fis 
satisfiable. If the length of F is m, then G will be obtainable from F in 
O(m) time. Hence, if we have a polynomial time algorithm for CDP, then 
we can obtain a polynomial time algorithm for CNF-satisfiability using this 
construction. 
For any F, G = (V, E) is defined as follows: V = { (a, i) I a is a literal 
in clause C;}; E = {((a, i), (o,j))li -:;e j and a -:;e 8}. A sample construc-
tion is given in Example 11.11. 
If F is satisfiable then there is a set of truth values for X;, 1 :5 i :5 n 
such that each clause is true with this assignment. Thus, with this assign-
ment there is at least one literal a in each C; such that a is true. Let S = 
{ (a, i) I a is true in C;} be a set containing exactly one (a, i) for each i. 

NP-Hard Graph Problems 
523 
S forms a clique in G of size k. Similarly, if G has a clique K = ( V' , E') 
of size at least k then let S = { (a, i) I (a, i) E V' } . Clearly, IS I = k as 
G has no clique of size more than k. Furthermore, if S' = {al (a, i) E S 
for some i } then S ' cannot contain both a literal o and its complement 8 
as there is no edge connecting ( o, i) and ( 8, j) in G. Hence by setting 
X; = true if X; E S' and X; = false if X; E S' and choosing arbitrary truth 
values for variables not in S', we can satisfy all clauses in F. Hence, F is 
satisfiable iff G has a clique of size at least k. 
D 
Example 11.11 Consider F = (x 1 v x 2 v x 3) /\ (x 1 v x2 v x3). The con-
struction of Theorem 11.2 yields the graph: 
<x 3 ,I> 
Figure 11.1 A sample graph and satisfiability 
This graph contains six cliques of size two. Consider the clique with 
vertices { (x 1, 1), (x 2, 2)}. By setting x 1 = true and x 2 = true (i.e. x 2 = 
false) F is satisfied. x 3 may be set either to true or false. 
D 
Node Cover Decision Problem 
A set S ~ Vis a node cover for a graph G = (V, E) iff all edges in E are 
incident to at least one vertex in S. The size of the cover, IS I , is the num-
ber of vertices in S. 
Example 11.12 Consider the graph: 
3 
Figure 11.2 A sample graph and node cover 

524 
NP-Hard and NP-Complete Problems 
S = {2, 4} is a node cover of size 2. S = {l, 3, 5} is a node cover of 
size 3. 
D 
In the node cover decision problem (NCDP) we are given a graph G and 
an integer k. We are required to determine if G has a node cover of size 
at most k. 
Theorem 11.3 Clique decision problem (CDP) ex node cover decision 
problem (NCDP) 
Proof: Let G = (V, E) and k define an instance of CDP. Assume that 
IV I = n. We shall construct a graph G' such that G' has a node cover 
of size at most n -
k iff G has a clique of size at least k. Graph G' is 
given by G' = (V, E) where£= {(u, v)lu EV, v E Vand (u, v) ¢ E}. 
Now, we shall show that G has a clique of size at least k iff G' has a 
node cover of size at most n - k. Let K be any clique in G. Since there 
are no edges in E connecting vertices in K, the remaining n -
I K I ver-
tices in G' must cover all edges in E. Similarly, if S is a node cover of G' 
then V - S must form a complete subgraph in G. 
Since G' can be obtained from G in polynomial time, CDP can be solved 
in polynomial deterministic time if we have a polynomial time deterministic 
algorithm for NCDP. 
D 
Note that since CNF-satisfiability ex CDP, CDP ex NCDP and ex is tran-
sitive, it follows that NCDP is NP-hard. 
Chromatic Number Decision Problem (CN) 
A coloring of a graph G = (V, E) is a functionf:V -
{ 1, 2, ... , k} de-
fined for all i E V. If (u, v) E E then f(u) .,t. f( v ). The chromatic number 
decision problem (CN) is to determine if G has a coloring for a given k. 
Example 11.13 A possible 2-coloring of the graph of Figure 11.2 is: 
f(l) = f(3) = f(S) = 1 andf(2) = f(4) = 2. Clearly, this graph has no 
1-coloring. 
D 
In proving CN to be NP-hard we shall make use of the NP-hard prob-
lem SA TY. This is the CNF satisfiability problem with the restriction that 
each clause has at most three literals. The reduction CNF-satisfiability ex 
SA TY is left as an exercise. 

NP-Hard Graph Problems 
525 
Theorem 11.4 Satisfiability with at most three literals per clause (SATY) 
ex chromatic number ( CN) 
Proof: Let F be a CNF formula having at most three literals per clause 
and having r clauses. Let X;, 1 ~ i ~ n be then variables in F. We may 
assume n <::: 4. If n < 4 then we can determine if Fis satisfiable by trying 
out all eight possible truth value assignments to x 1 , x 2 and x 3 • We shall 
construct, in polynomial time, a graph G that is n + 1 colorable iff F is 
satisfiable. The graph G = (V, E) is defined by: 
and 
E = {(x 1,.f 1), 1 ~ i ~ n} U {(y;.Y)li ;Cj} U {(y;. x)li ;ej} 
U {(y;,.t)li ;ej} U {(x;. C)lx;¢Cj} U {.t;. C)l.t;¢Cj} 
To see that G is n + 1 colorable iff F is satisfiable, we first observe that 
they ;'s form a complete subgraph on n vertices. Hence, each y; must be 
assigned a distinct color. Without loss of generality we may assume that 
in any coloring of G y; is given the color i. Since y; is also connected to 
all the x / s and .f / s except x; and .f ;, the color i can be assigned to only 
x; and .f ;. However (x ;, x ;) E E and so a new color n + 1, is needed for 
one of these vertices. The vertex that is assigned the new color, n + 1, 
will be called the false vertex. The other vertex is a true vertex. The only 
way to color G using n + 1 colors is to assign color n + 1 to one of { x ;, .f;} 
for each i, 1 ~ i ~ n. 
Under what conditions can the remaining vertices be colored using no 
new colors? Since n <::: 4 and each clause has at most three literals, each 
C; is adjacent to a pair of vertices xj, .fj for at least onej. Consequently, no 
C; may be assigned the color n + 1. Also, no C; may be assigned a color 
corresponding to an x j or .f j not in clause C ;. The last two statements imply 
that the only colors that can be assigned to C; correspond to vertices x j or .f j 
that are in clause C; and are true vertices. Hence, G is n + 1 colorable iff 
there is a true vertex corresponding to each C ;. So, G is n + 1 colorable 
iff F is satisfiable. 
D 

526 
NP-Hard and NP-Complete Problems 
Directed Hamiltonian Cycle (DHC) 
A directed Hamiltonian cycle in a directed graph G = ( V, E) is a directed 
cycle of length n = I VI. So, the cycle goes through every vertex exactly 
once and then returns to the starting vertex. The DHC problem is to deter-
mine if G has a directed Hamiltonian cycle. 
Example 11.14 1, 2, 3, 4, 5, 1 is a directed Hamiltonian cycle in the graph 
of Figure 11.3. 
If the edge (5, 1) is deleted from this graph then it has no directed 
Hamiltonian cycle. 
3 
Figure 11.3 A sample graph and Hamiltonian cycle 
Theorem 11.5 CNF-satisfiability ex directed hamiltonian cycle (DHC). 
Proof: Let F be a propositional formula in CNF. We shall show how to 
construct a directed graph G such that F is satisfiable iff G has a directed 
Hamiltonian cycle. Since this construction can be carried out in time poly-
nomial in the size of F, it will follow that CNF-satisfiability ex DHC. Under-
standing of the construction of G is greatly facilitated by the use of an 
example. The example we shall use is F = C 1 A C 2 A C 3 A C 4 where 
C1 = X1 v X2 v X4 v X5 
C2 = X1 v X2 v X3 
C3 = X1 v X3 v X5 
C4 = x 1 V x 2 V .X 3 V x 4 V i 5 
Assume that F has r clauses C 1 , C 2 , ••• , C, and n variables x 1 , x 2 , 
... , x n• Draw an array with r rows and 2n columns. Row i will denote 
clause C;. Each variable x; will be represented by two adjacent columns, 
one for each of the literals X; and x;. Figure 11.4 shows the array for the 

NP-Hard Graph Problems 
527 
example formula. Insert a 0 into column x; and row C j iff x; is a literal 
in C j· Insert a (!)into column .f; and row C j iff .f; is a literal in C j· Between 
each pair of columns x; and .t; introduce two vertices u; and v ;: u; at the 
top and v; at the bottom of the column. For each i, draw two chains of edges 
upwards from v; to u; one connecting together all 0s in column x; and 
the other connecting all 0s in column .f; (see Figure 11.4). Now, draw 
edges (u;, V;+1), 1 s i < n. Introduce a box rn at the right end of each 
row C;, 1 sis r. Draw the edges <un, ITJ> and <0, v1). Draw edges 
<[]].Ii+ 1I),1 s i < r(seeFigure11.4). 
To complete the graph we shall replace each 0 and II] by a subgraph. 
Each (!)is replaced by the subgraph of Figure 11.S(a) (of course, unique 
vertex labelings are needed for each copy of the subgraph). Each box []] 
is replaced by the subgraph of Figure 11.6. In this subgraph A; is an en· 
trance node and B; an exit node. The edges ( ITJ, I i + 1 I) referred to 
earlier are really (B;, A;+1). Edge (un, [!]> is (u ,,, A 1) and <0. v1) is 
(B" v 1). j; is the number of literals in clause C ;. In the subgraph of Figure 
11.6 an edge 
R. -t;'>-R. 
'·" 
\:./ 
1,a +I 
indicates a connection to a(!) subgraph in row C;. R;, 0 is connected to the 
"1" vertex of the (!)and Ri,a + 1(or R;, 1 if a = j;) is entered from the "3" 
vertex. Thus in the (!)subgraph 
of Figure 11.S(b) w 1andw3 are the "l" and "3" vertices respectively. The 
incoming edge is (R;:i. w1) and the outgoing edge is (w3, R;,2). This com-
pletes the construction of G. 
If F is satisfiable then let S be an assignment of truth values for which 
Fis true. A Hamiltonian cycle for G can start at v 1, go to u 1 then to v 2 , 
then u 2 , then v 3 , then u 3 , ••• , un. In going from v; up to u; this cycle will 
use the column corresponding to x; if x; is true in S. Otherwise it will go 
up the column corresponding to i;. From un this cycle will go to A 1 and 
then through R 1,1, R1•2 , R1,3 , ••• , Ri.j ,.B 1 to A 2 • • • to v 1 • In going from 
R i,a to R;, 0 + 1in any subgraph UJ a diversion will be made to a (!) subgraph 

528 
NP-Hard and NP-Complete Problems 
in row i iff the vertices of that 0 subgraph are not already on the path 
from v I to R;,a. Note that if c i has i j literals then the construction of m 
allows a diversion to at most ij -
1 0 subgraphs. This is adequate as at 
least one 0 subgraph must already have been traversed in row C; (as at 
least one such subgraph must correspond to a true literal). So, if Fis satis· 
fiable then G has a directed Hamiltonian cycle. It remains to show that 
if G has a directed Hamiltonian cycle then F is satisfiable. This may be 
seen by starting at vertex v 1 on any Hamiltonian cycle for G. Because of 
the construction of the 0 and m subgraphs, such a cycle must proceed 
by going up exactly one column of each pair (x;. x;). In addition, this part 
of the cycle must traverse at least one 0 subgraph in each row. Hence the 
columns used in going from v; to u;. 1 s i s n define a truth assignment 
for which F is true. 
We conclude that F is satisfiable iff G has a Hamiltonian cycle. The 
theorem now follows from the observation that G may be obtained from F 
in polynomial time. 
D 
Figure 11.4 Array structure for formula in Theorem 11.5 
Traveling Salesperson Decision Problem (TSP) 
The traveling salesperson problem was introduced in Chapter 5. The cor-
responding decision problem is to determine if a complete directed graph 
G = (V, E) with edge costs, c(u, v), has a tour of cost at most M. 

NP·Hard Graph Problems 
529 
•s 
I [ 
w, 
w6 
•2 
W3 
W7 
3 
•e 
Figure 11.5 The (!)subgraph and its insertion into column 2 
Figure 11.6 The UJ subgraph 
Theorem 11.6 Directed Hamiltonian cycle (DHC) ex traveling salesper· 
son decision problem (TSP) 
Proof: From the directed graph G = (V. E) construct the complete di· 
rected graph G' = (V, E'), E = { (i, j) Ii ¢ j} and c(i, j) = 1 if (i, j) 

530 
NP-Hard and NP-Complete Problems 
EE; c(i, .i) = 2 if i ;e. .i and ( i, j) <t. E. Clearly, G' has a tour of cost at 
most n iff G has a directed Hamiltonian cycle. 
D 
AND/OR Graph Decision Problem (AOG) 
AND/OR graphs were introduced in Section 6.3. Let us assume that there 
is a cost associated with each edge in the graph. The cost of a solution graph 
Hof an AND/OR graph G is the sum of the costs of the edges in H. The 
AND/OR graph decision problem (AOG) is to determine if G has a solu-
tion graph of cost at most k, fork a given input. 
Example 11.15 Consider the directed graph of Figure 11. 7. The problem 
to be solved is P 1 • To do this, one may solve either nodes P 2 , P3 or P7 , as 
P 1 is an OR node. The cost incurred is then either 2, 2 or 8 (i.e., cost in 
addition to that of solving one of P 2 , P 3 or P 7). To solve P 2 , both P 4 and 
P 5 have to be solved, as P 2 is an AND node. The total cost to do this is 2. 
To solve P 3 , we may solve either P 5 or P 6 • The minimum cost to do this 
is 1. P 7 is free. In this example, then, the optimal way to solve P 1 is first 
solve P 6 , then P 3 and finally P 1 • The total cost for this solution is 3. 
D 
U 
=>AND node 
Figure 11. 7 AND/OR graph 
Theorem 11.7 CNF-satisfiability ex AND/OR graph decision problem 
Proof: Let P be a propositional formula in CNF. We show how to trans-
form a formula Pin CNF into an AND/OR graph such that the AND/OR 
graph so obtained has a certain minimum cost solution iff P is satisfiable. 
Let 
k 
P = A C ;. C; = v /.i, 
i= 1 

NP-Hard Graph Problems 
531 
where the l/s are literals. The variables of P, V(P) are Xu x 2, ••• , xn. 
The AND/OR graph will have nodes as follows: 
1. There is a special node, S, with no incoming arcs. This node repre-
sents the problem to be solved. 
2. s is an AND node with descendent nodes P, x I• x 2' ••• ' x n. 
3. 
Each node x; represents the corresponding variable x; in the formula P. 
Each X; is an OR node with two descendents denoted Tx; and Fx; re-
spectively. If Tx; is solved, then this will correspond to assigning a 
truth value of "true" to the variable x;. Solving node Fx; will corre-
spond to assigning a truth value of "false" to X;. 
4. 
The node P represents the formula P, and is an AND node. It has k 
descendents C 1 , C 2 , ••• , Ck· Node C; corresponds to the clause C; in 
the formula P. The nodes C; are OR nodes. 
5. 
Each node of type Tx; or Fx; has exactly one descendent node which 
is terminal (i.e., has no edges leaving it). These terminal nodes shall 
be denoted v 1 , v 2 , ••• , v 2n • 
To complete the construction of the AND/OR graph, the following edges 
and costs are added: 
1. 
From each node C; an edge (C;, Txj) is added if x j occurs in clause C ;. 
An edge (C;, Fxj )is added if Xj occurs in the clause C;. This is done 
for all variables x j appearing in the clause C ;. C; is designated an 0 R 
node. 
2. 
Edges from nodes of type Tx; or Fx; to their respective terminal nodes 
are assigned a weight or cost 1. 
3. 
All other edges have a cost 0. 
In order to solve S, each of the nodes P, x 1 , x 2 , ••• , x n must be solved. 
Solving nodes x 1 , x 2 , ••• , x n costs n. To solve P, we must solve all the 
nodes C 1 , C 2 , ••• , Ck. The cost of a node C; is at most 1. However, if one 
of its descendent nodes was solved while solving the nodes x 1 , x 2 , ••• , x n• 
then the additional cost to solve C; is 0, as the edges to its descendent 
nodes have cost 0 and one of its descendents has already been solved. I.e., 
a node C; can be solved at no cost if one of the literals occurring in the 
clause C; has been assigned a value "true." From this it follows that the. 
entire graph (i.e., node S) can be solved at a cost n if there is some assign-
ment of truth values to the x;'s such that at least one literal in each clause 
is true under that assignment, i.e., if the formula P is satisfiable. If P is 
not satisfiable, then the cost is more than n. 

532 
NP-Hard and NP-Complete Problems 
We have now shown how to construct an AND/OR graph from a for-
mula P such that the AND/OR graph so constructed has a solution of cost 
n iff P is satisfiable. Otherwise the cost is more than n. The construction 
clearly takes only polynomial time. This completes the proof. 
D 
Example 11.16 Consider the formula: 
Figure 11.8 shows the AND/OR graph obtained by applying the construc-
tion of Theorem 11. 7. 
The nodes Tx 1 , Tx 2 , Tx 3 can be solved at a total cost of 3. The node 
P then costs nothing extra. The node S can then be solved by solving all 
its descendent nodes and the nodes Tx 1, Tx 2 and Tx 3 • The total cost for 
this solution is 3 (which is n ). Assigning the truth value "true" to the vari-
ables of P results in P being "true." 
D 
AND nodes marked\....-/ 
All other nodes ore OR 
Figure 11.8 AND/OR graph for Example 11.16 
11.4 NP-HARD SCHEDULING PROBLEMS 
To prove the results of this section we shall need to use the NP-hard prob-
lem called partition. This problem requires us to decide whether a given 

NP-Hard Scheduling Problems 
533 
multiset A = {a 1, a 2, ••• , an} of n positive integers has a partition P 
such that E;,p a; = E;a a;. We can show this problem NP-hard by first 
showing the sum of subsets problem (Chapter 7) NP-hard. Recall that in 
the sum of subsets problem we have to determine if A = {a 1 , a 2 , ••• , an } 
has a subset S that sums to a given integer M. 
Theorem 11.8 Exact cover ex sum of subsets. 
Proof: The exact cover problem is shown NP-hard in the exercises. In this 
problem we are given a family of sets F = { S 1 , S 2 , •.• , S k} and are re-
quired to determine if there is a subset T £:: F of disjoint sets such that 
From any given instance of this problem construct the sum of subset prob-
lem A = {a 1, ... , ak} with aj = E 1,,;:sn E)k + l)i-l where Ej; = 1 ifu; E 
sj and Ej; = 0 otherwise and M = Eo:si<n (k + l)i = ((k + l)n -
l)/k. 
Clearly, F has an exact cover iff A = {a 1 , ... , a k} has a subset with 
sum M. Since A and M may be constructed from F in polynomial time, 
exact cover ex sum of subsets. 
D 
Theorem 11.9 Sum of subsets ex partition 
Proof: Let A = {a 1 , ... , an} and M define an instance of the sum of 
subsets problem. Construct the set B = {bu b 2 , ••• , hn+ 2} with h; = a;, 
1 ~ i ~ n, hn+I = M + 1 and hn+2 = (E1,;;,;n a;) + 1 - M. B has a 
partition iff A has a subset with sum M. Since B may be obtained from 
A and M in polynomial time, sum of subsets ex partition. 
D 
One may easily show Partition ex Oil-Knapsack and Partition ex Job 
sequencing with deadlines. Hence, these problems are also NP-hard. 
Scheduling Identical Processors 
Let P;. 1 ~ i ~ m be m identical processors (or machines). The P; could 
for example be line printers in a computer output room. Let];, 1 ~ i ~ n 
be n jobs. Job]; requires t; processing time. A schedule Sis an assignment 
of jobs to processors. For each job l;. S specifies the time intervals and 
the processor(s) on which this job is to be processed. A job cannot be 
processed by more than one processor at any given time. Letf; be the time 

.. 
#, 
534 
NP-Hard and NP-Complete Problems 
at which the processing of job l; is completed. The mean finish time (mft) 
of schedule S is: 
1 
MFT(S) = -
.E f 
n l:Si:Sn 
I 
Let W; be a weight associated with each job l;. The weighted mean finish 
time (wmft) of schedule Sis: 
1 
WMFT(S) = -
.E 
w;f;. 
n lsi:sn 
Let T; be the time at which P; finishes processing all jobs (or job seg-
ments) assigned to it. The finish time of Sis: 
FT(S) = max {T;}. 
l:si:sm 
S is a non-preemptive schedule ifI each job l; is processed continuously 
from start to finish on the same processor. In a preemptive schedule each 
job need not be processed continuously to completion on one processor. 
At this point it is worth noting the similarity between the optimal tape 
storage problem of Section 4.2 and non-preemptive schedules. Mean re-
trieval time, weighted mean retrieval time and maximum retrieval time re-
spectively correspond to mean finish time, weighted mean finish time and 
finish time. Minimum finish time schedules can therefore be obtained 
using the algorithm developed in Section 4.2. Obtaining minimum weighted 
mean finish time and minimum finish time non-preemptive schedules is 
NP-hard. 
Theorem 11.10 Partition ex minimum finish time non-preemptive schedule 
Proof: We shall prove this form = 2. The extension tom > 2 is trivial. 
Let a;, 1 s i s n be an instance of the partition problem. Define n jobs 
with processing requirements t; = a;, 1 s i s n. There is a non-preemptive 
schedule for this set of jobs on two processors with finish time at most 
E t /2 iff there is a partition of the a/s. 
D 
Theorem 11.11 Partition ex minimum WMFT non-preemptive schedule 
Proof: Once again we prove this for m = 2 only. The extension to m > 2 

NP-Hard Scheduling Problems 
535 
is trivial. Let a;, 1 ~ i ~ n define an instance of the partition problem. 
Construct a two processor scheduling problem with n jobs and w; = t; = 
a;, 1 ~ i ~ n. For this set of jobs there is a non-preemptive schedule S 
with weighted mean flow time at most 112 E a? + 114 (E aY iff the a/s 
have a partition. To see this let the weights and times of jobs on P 1 be 
(wi. f1), ... , (wk, {k) and on P 2 be (w1, t1), ... , (w1, t1). Assume this is 
the order in which the jobs are processed on their respective processors. 
Then, for this schedule S we have: 
n*WMFT(S) = w1i1 + w2(l1 + ii)+ ... + wk(l1 + ... + it) 
+ w1t1 + w2(t1 + t2) + · · · + w1(t1 + 
+ t~) 
1 
1 
-
1 
= 2 E w;2 + 2 (E w;)2 + 2 (E w; - E w;)2. 
Thus, n*WMFT(S) <::: (112) I; w r + (1/ 4) (I; wY. This value is obtainable 
iffthe w;'s (and so also the a;'s) have a partition. 
D 
Flow Shop Scheduling 
We shall use the flow shop terminology developed in Section 5.8. When 
m = 2, minimum finish time schedules can be obtained in O(n log n) time 
if n jobs are to be scheduled. When m = 3 obtaining minimum finish time 
schedules (whether preemptive or non-preemptive) is NP-hard. For the 
case of non-preemptive schedules this is easy to see (exercise 30). We shall 
prove the result for preemptive schedules. The proof we shall give is also 
valid for the non-preemptive case. However, a much simpler proof exists 
for the non-preemptive case. 
Theorem 11.12 Partition ex minimum finish time preemptive flow shop 
schedule (m > 2). 
Proof: We shall use only three processors. Let A = {a 1 , a 2 , ••• , an} de-
fine an instance of the partition problem. Construct the following preemp-
tive flow shop instance, FS, with n + 2 jobs, m = 3 machines and at most 
2 nonzero tasks per job: 
f1,n+I = T/2; f2,n+I = T; 
f3,n+1 = 0 
t 1,n+ 2 = 0; t 2.n+ 2 = T; 
t 3.n+ 2 = T/2 

536 
NP-Hard and NP-Complete Problems 
where 
n 
T =Ea;. 
I 
We now show that the above flow shop instance has a preemptive sched-
ule with finish time at most 2T iff A has a partition. 
(a) If A has a partition u then there is a non-preemptive schedule with 
finish time 2T. One such schedule is shown in Figure 11.9. 
(b) If A has no partition then all preemptive schedules for FS must have 
a finish time greater than 2T. This can be shown by contradiction. 
Assume that there is a preemptive schedule for FS with finish time 
at most 2T. We make the following observations regarding this 
schedule: 
(i) task t1,n+1 must finish by time T (as t 2,n+1 = T and cannot start 
until t 1,n + 1 finishes) 
(ii) task t 3,n+i cannot start before T units of time have elapsed as 
t2,n+2 = T. 
Observation (i) implies that only T 12 of the first T time units are free 
on processor one. Let V be the set of indices of tasks completed on proces-
sor 1 by time T (excluding task t 1,n+ 1). Then, 
E tu< T/2 
iEV 
' 
as A has no partition. Hence 
E t 3 ; > T/2. 
iff 
• 
lsisn 
The processing of jobs not included in V cannot commence on processor 
3 until after time T since their processor 1 processing is not completed 
until after T. This together with observation (ii) implies that the total 
amount of processing left for processor 3 at time T is 
t 3 n+i + E t 3 ; > T. 
' 
ifV 
' 
lsisn 
The schedule length must therefore be more than 2T. 
D 

NP-Hard Scheduling Problems 
537 
0 
T/2 
T 
3T/2 
2T 
Figure 11.9 A possible schedule 
Job Shop Scheduling 
A job shop, like a flow shop, has m different processors. The n jobs to 
be scheduled require the completion of several tasks. The time of the jth 
task for job J; is t k.i.J· Taskj is to be performed on processor Pk· The tasks 
for any job I; are to be carried out in the order 1, 2, 3, ... , etc. Task j 
cannot begin until taskj - 1 (ifj > 1) has been completed. Note that it is 
quite possible for a job to have many tasks that are to be performed on 
the same processor. In a non-preemptive schedule, a task once begun is 
processed without interruption until it is completed. The definitions of 
FT(S) and MFT(S) extend to this problem in a natural way. Obtaining 
either a minimum finish time preemptive or minimum finish time non· 
preemptive schedule is NP-hard even when m = 2. The proof for the 
nonpreemptive case is very simple (use partition). We shall present the 
proof for the preemptive case. This proof will also be valid for the non· 
preemptive case but will not be the simplest proof for this case. 
Theorem 11.13 Partition ex minimum finish time preemptive job shop 
schedule (m > 1). 
Proof: We shall use only two processors. Let A = {a 1> a 2 , ••• , a"} define 
an instance of the partition problem. Construct the following job shop 
instance JS, with n + 1 jobs and m = 2 processors. 
Jobs 1, ... , n: t 1.;, 1 = t 2,;.2 =a; for 1 :s i :s n 
Job n + 1: 
t2.n+l,I = t1,n+l,2 = ti.n+l,3 = t1,n+l,4 = T/2 
where 
n 
T=Ea 
J 
I 

538 
NP-Hard and NP-Complete Problems 
We show that the above job shop problem has a preemptive schedule 
with finish time at most 2T iff S has a partition. 
a) If A has a partition u then there is a schedule with finish time 2T 
(see Figure 11.10). 
b) If A has no partition then all schedules for JS must have a finish 
time greater than 2T. To see this assume that there is a schedule S 
for JS with finish time at most 2T. Then, job n + 1 must be sched-
uled as in Figure 11.10. Also, there can be no idle time on either 
P 1 or P2 • Let R be the set of jobs scheduled on P 1 in the interval 
[O, T/2]. Let R' be the subset of R representing jobs whose first 
task is completed on P 1 in this interval. Since the a;'s have no par-
tition, EjeR' t;,j,1 < T/2. Consequently, EjeR' t 2.j,2 < T/2. Since only 
the second tasks of jobs in R ' may be scheduled on P 2 in the inter-
val [T/2, T], it follows that there is some idle time on P2 in this 
interval. Hence, S must have finish time greater than 2T. 
D 
T/2 
T 
3T/2 
2T 
Figure 11.10 Another schedule 
11.5 NP-HARD CODE GENERATION PROBLEMS 
Code Generation With Common Subexpressions 
When arithmetic expressions have common subexpressions they may be 
represented by a directed acyclic graph (dag). Every internal node (node 
with nonzero out-degree) in the dag represents an operator. Assuming the 
expression contains only binary operators, each internal node, P, has out-
degree two. The two nodes adjacent from P will be called the left and right 
children of P respectively. The children of P are the roots of the dags for 
the left and right operands of P. Pis the parent of its children. In case the 
expression contains no common subexpressions, its dag representation is 
identical to the tree representation of Section 6.2. Figure 11.11 shows some 
expressions and their dag representations. 

NP-Hard Code Generation Problems 
539 
Definition: A leaf is a node with out-degree zero. A level one node is a 
node both of whose children are leaves. A shared node is a node with more 
than one parent. A leaf dag is a dag in which all shared nodes are leaves. 
A level one dag is a dag in which all shared nodes are level one nodes. 
a+(b+a*c) 
(a) 
(a+b)*(a+b+c) 
(b) 
Figure 11.11 
Expressions and their dags 
(a+b)*c/ ((a+b)*c-d) 
( c) 
Example 11.17 The dag of Figure 11.ll(a) is a leaf dag. Figure 11.ll(b) 
is a level one dag. Figure 11.ll(c) is neither a leaf dag nor a level one 
dag. 
D 
A leaf dag results from an arithmetic expression in which the only com-
mon subexpressions are simple variables or constants. A level one dag re-
sults from an expression in which the only common subexpressions are of 
the form a @ b where a and b are simple variables or constants and @ 
is an operator. 
The problem of generating optimal code for level one dags is NP-hard 
even when the machine for which code is being generated has only one 
register. Determining the minimum number of registers needed to evaluate 
a dag with no STOREs is also NP-hard. Note that both these problems 
can be solved in linear time when there are no common subexpressions 
(Section 6.2). 
Example 11.18 The optimal codes for the dag of Figure 11.ll(b) for one 
and two registers machines is given in Figure 11.12. 
The minimum number of registers needed to evaluate this dag without 
any STOREs is 2. 
D 

• 
540 
NP-Hard and NP-Complete Problems 
LOAD 
a,Rl 
LOAD 
a,Rl 
ADD 
Rl,b,Rl 
ADD 
Rl,b,Rl 
STORE 
Tl,Rl 
ADD 
Rl,c,R2 
ADD 
Rl,c,Rl 
MUL 
Rl,R2,Rl 
STORE 
T2,Rl 
LOAD 
Tl,Rl 
MUL 
Rl,T2,Rl 
(a) 
(b) 
Figure 11.12 Optimal codes for one and two register machines 
In order to prove the above statements we shall use the feedback node 
set (FNS) problem that is shown to be NP-hard in the exercises. 
FNS: Given a directed graph G = (V, E) and an integer k determine if 
there exists a subset V' of vertices V' ~ V and I Y' I :5; k such that the 
graph H = (V - V', E -
{ (u, v) I u E Y' or v E Y' }) obtained from G 
by deleting all vertices in V' and all edges incident to a vertex in V' con-
tains no directed cycles. 
We shall explicitly prove only that generating optimal code is NP-hard. 
Using the construction of this proof one can also show that determining 
the minimum number of registers needed to evaluate a dag with no STOREs 
is also NP-hard. The proof assumes that expressions may contain commu-
tative operators and that shared nodes may be computed only once. It is 
easily extended to allow recomputation of shared nodes. Using an idea due 
to Ravi Sethi, the proof is easily extended to the case when only noncom-
mutative operators are allowed (see Exercise 41). 
Theorem 11.14 FNS ex optimal code generation for level one dags on a 
one register machine. 
Proof: Let G, k be an instance of FNS. Let n be the number of vertices 
in G. We shall construct a dag A with the property that the optimal code 
for the expression corresponding to A has at most n + k LOADs iff G has 
a feedback node set of size at most R. 
The dag A consists of three kinds of nodes: leaf nodes, chain nodes and 
tree nodes. All chain and tree nodes are internal nodes representing com-
mutative operators (e.g., '+ '). Leaf nodes represent distinct variables. 
We shall used v to denote the out-degree of vertex v of G. Corresponding to 
each vertex v of G there is a directed chain of chain nodes v 1 , v 2 , ••• , vdv+ 1 
in A. Node v dv + 1 is the head node of the chain for v and is the parent of two 

NP-Hard Code Generation Problems 
541 
leaf nodes vL and VR (see Example 11.19 and Figure 11.13). v1 is the tail 
of the chain. From each of the chain nodes corresponding to vertex v, ex-
cept the head node, there is one directed edge to the head node of one of the 
chains corresponding to a vertex w such that ( v, w) is an edge in G. Each 
such edge goes to a distinct head. Note that as a result of the addition of 
these edges, each chain node now has out-degree two. Since each chain 
node represents a commutative operator, it does not matter which of its 
two children is regarded as the left child. 
At this point we have a dag in which the tail of every chain has in-degree 
zero. We now introduce tree nodes to combine all the heads together so 
that we are left with only one node (the root) with in-degree zero. Since 
G has n vertices, we need n -
1 tree nodes (note that every binary tree 
with n -
1 internal nodes has n external nodes). These n -
1 nodes are 
connected together to form a binary tree (any binary tree with n - 1 nodes 
will do). In place of the external nodes we connect the tails of then chains 
(see Figure ll.13(b)). This yields a dag A corresponding to an arithmetic 
expression. 
It is easy to see that every optimal code for A will have exactly n LOADs 
of leaf nodes. Also, there will be exactly one instruction of type @ for 
every chain node and tree node (we assume that a shared node is com-
puted only once). Hence, the only variable is the number of LOADs and 
STOREs of chain and tree nodes. If G has no directed cycles then its ver-
tices may be arranged in topological order (vertex u precedes vertex v in a 
topological ordering only if there is no directed path from u to v in G). 
Let v1, v2, ... , v. be a topological ordering of the vertices in G. The ex-
pression A can be computed using no LOADs of chain and tree nodes by 
first computing all nodes on the chain for v. and storing the result of the 
tail node. Next, all nodes on the chain for v n- 1 may be computed. In 
addition, we can compute any nodes on the path from the tail for v n-1 to 
the root for which both operands are available. Finally, one result needs 
to be stored. Next, the chain for v._ 2 may be computed. Again, we can 
compute all nodes on the path from this chain tail to the root for which 
both operands are available. Continuing in this way, the entire expression 
may be computed. 
If G contains at least one cycle: v i. vi. ... , v ;, v 1 then every code for A 
must contain at least one LOAD of a chain node on a chain for one of 
vi, v2, ... , v;. Further, if none of these vertices is on any other cycle then 
all their chain nodes may be computed using only one load of a chain 
node. This argument is readily generalized to show that if the size of a 
minimum feedback node set is p then every optimal code for A contains 
exactly n + p LOADs. The p LOADs correspond to a combination of tail 

542 
NP-Hard and NP-Complete Problems 
nodes corresponding to a minimum feedback node set and the siblings of 
these tail nodes. In case we had used non-commutative operators for chain 
nodes and made each successor on a chain the left child of its parent then 
the p LOADs will correspond to the tails of the chains of any minimum 
feedback set. Furthermore, if the optimal code contains p LOADs of chain 
nodes then G has a feedback node set of size p. 
D 
Example 11.19 Figure ll.13(b) shows the dag A corresponding to the 
graph G of Figure ll.13(a). {r, s} is a minimum feedback node set for G. 
The operator in each chain and tree node may be assumed to be • + '. 
Every code for A has a load corresponding to one of (p L, p R), (q L, q R), ... 
and (uL, uR). The expression A can be computed using only two additional 
LOADs by computing nodes in the order r4, si, qi, q1, Pi, Pi, c, U3, ui, 
u 1, ti, t 1, e, s 1, r3, ri, r1, d, b, a. Note that a LOAD is needed to compute 
s 1 and also to compute r 3 • 
D 
a) Graph G 
b) CorreapondinQ daQ A 
Figure 11.13 A graph and its corresponding dag. 

NP-Hard Code Generation Problems 
543 
Implementing Parallel Assignment Instructions 
A SP ARKS parallel assignment instruction has the format ( v i. v 2, •• ., v ,,) 
-
(e 1 , e2 , ••• , e,,) where the v;s are distinct variable names and 1:he e;s 
are expressions. The semantics of this statement is that the value of v1 is 
updated to be the value of the expression e;. 1 $ i $ n. The value of the 
expression e; is to be computed using the values the variables in e; have 
before this instruction is executed. 
Example 11.20 
(i) 
(A, B) -
(B, C) is equivalent to A - B; B -
C 
(ii) 
(A, B) -
(B, A) is equivalent to T - A; A -
B; B -
T 
(iii) 
(A, B) -
(A + B, A -
B) is equivalent to Tl -
A; 11 -
B; 
A -
Tl + 11; B -
Tl -
11 and also to Tl -
A ; A -
A + B; 
B -
Tl - B. 
D 
As the above example indicates, it may be necessary to store some of the 
v;s into temporary locations when executing a parallel assignment. These 
stores are needed only when some of the v;s appear in the expressions e; , 
1 $ j $ n. A variable v; is referenced by expression e; iff v; appears in e;. 
It should be clear that only referenced variables need to be copied into 
temporary locations. Further, Examples 11.20 (ii) and (iii) show that not all 
referenced variables need to be copied. 
An implementation of a parallel assignment statement is a sequence of 
instructions of types T; -
v; and v; -
e '; where e '; is obtained from e; 
by replacing all occurrences of a v; that has already been updated with a 
reference to the temporary location in which the old value of v; has been 
saved. Let R = (7(1), ... , T(n)) be a permutation of (1, 2, ... , n). R is a 
realization of an assignment statement. It specifies the order in which 
statements of type v; -
e '; appear in an implementation of a parallel 
assignment statement. The order is V7ol -
e 'TO); vT(l) -
e 'T(l); etc. The 
implementation also has statements of type ~ -
v; interspersed. Without 
loss of generality we may assume that the statement ~· -
v; (if it appears 
in the implementation) immediately precedes the statement v; -
e ';. 
Hence, a realization completely characterizes an implementation. The 
minimum number of instructions of type T; -
v, for any given realization 
is easy to determine. This number is the cost of the realization. The cost 
C(R) of a realization R is the number of v1 that are referenced by an eJ that 
corresponds to an instruction v; -
e '; that appears after the instruction 
vi -
eri· 

.. 
544 
NP-Hard and NP-Complete Problems 
Example 11.21 Consider the statement (A, B, C) -
(D, A + B, A - B) 
The 3! = 6 different realizations and their costs are: 
R 
C(R) 
1, 2, 3 
2 
1, 3, 2 
2 
2, 1, 3 
2 
2, 3, 1 
1 
3, l, 2 
1 
3, 2, 1 
0 
The realization 3, 2, 1 corresponding to the implementation C -A - B; 
B - A + B; A -
D needs no temporary stores (C(R) = 0). 
D 
An optimal realization for a parallel assignment statement is one with 
minimum cost. When the expressions e; are all variable names or constants, 
an optimal realization can be found in linear time (O(n}). When the e; 
are allowed to be expressions with operators then finding an optimal real-
ization is NP-Hard. We shall prove this latter statement using the feedback 
node set problem. 
Theorem 11.15 FNS ex minimum cost realization. 
Proof: Let G = (V, £) be any n vertex directed graph. Construct the 
parallel assignment statement P: (v1, v2, ... , vn) -
(e1, e2, ... , e.) where 
the v;'s correspond to then vertices in V and e; is the expression v;1 + v;2 
+ · · · + v~. { v;1, v;2, •.• , v~} is the set of vertices adjacent from V; (i.e. 
(v;, v;1) E E(G), 1 s ls j}. This construction requires at most O(n2 ) 
time. 
Let Ube any feedback node set for G. Let G' = (V', E') = (V -
U, E 
-
{ (x, y) Ix E U or y E U}) be the graph obtained by deleting vertex set 
U and all edges incident to vertices in U. From the definition of a feedback 
node set it follows that G' is acyclic. So, the vertices in V -
U may be 
arranged in a sequence Si, s2 , ••• , Sm where m = IV -
UI and£' con-
tains no edge (s1, S;) for any i, j, 1 s i < j s m. Hence, an implementa-
tion of P in which variables corresponding to vertices in U are first stored 
in temporary locations followed by the instructions v; -
e '; corresponding 
to v; E U, followed by the corresponding instructions for s1, s2 , ••• , Sm (in 
that order), will be a correct implementation. (e '; is e; with all occurrences 
of V; E U replaced by the corresponding temporary location). The realiza-
tion, R, corresponding to this implementation has C(R) = I U I· Hence, 

Some Simplified NP-Hard Problems 
545 
if G has a feedback node set of size at most k then P has an optimal real-
ization of cost at most k. 
Suppose P has a realization R of cost k. Let U be the set of k variables 
that have to be stored in temporary locations and let R = (q 1, q 2 , ••• , q. ). 
From the definition of C(R) it follows that no eq; references a vq1 with} < i 
unless Vqi E U. Hence, the deletion of vertices in U from G leaves G acyclic. 
Thus, U defines a feedback node set of size k for G. 
G has a feedback node set of size at most k iff P has a realization of 
cost at most k. Thus we can solve the feedback node set problem in poly-
nomial time if we have a polynomial time algorithm that determines a 
minimum cost realization. 
D 
11.6 SOME SIMPLIFIED NP-HARD PROBLEMS 
Once we have shown a problem L to be NP-hard we would be inclined to 
dismiss the possibility that L can be solved in deterministic polynomial 
time. At this point, however, one may naturally ask the question: Can a 
suitably restricted version (i.e., some subclass) of an NP-hard problem 
be solved in deterministic polynomial time? It should be easy to see that 
by placing enough restrictions on any NP-hard problem (or by defining 
a sufficiently small subclass) we can arrive at a polynomially solvable prob-
lem. As examples, consider the following: 
i) CNF-satisfiability with at most three literals per clause is NP-hard. 
If each clause is restricted to have at most two literals then CNF-
satisfiability is polynomially solvable. 
ii) Generating optimal code for a parallel assignment statement is 
NP-hard. However, if the expressions e; are restricted to be simple 
variables then optimal code can be generated in polynomial time. 
iii) Generating optimal code for level one dags is NP-hard but optimal 
code for trees can be generated in polynomial time. 
iv) Determining if a planar graph is three colorable is NP-hard. To 
determine if it is two colorable we only have to see if it is bipartite. 
Since it is very unlikely that NP-hard problems are polynomially solv-
able, it is important to determine the weakest restrictions under which we 
can solve a problem in polynomial time. 
To narrow the gap between subclasses for which polynomial time algo-
rithms are known and those for which such algorithms are not known, it is 
desirable to obtain as strong a set of restrictions under which a problem 
remains NP-hard or NP-complete. 
We state without proof the severest restrictions under which certain 

546 
NP-Hard and NP-Complete Problems 
problems are known to be NP-hard or NP-complete. We shall state these 
simplified or restricted problems as decision problems. For each problem 
we shall specify only the input and the decision to be made. 
Theorem 11.16 The following decision problems are NP-complete: 
1. 
Node Cover 
Input: 
An undirected graph G with node degree at most 3 and an 
integer k. 
Decision: 
Does G have a node cover of size at most k? 
2. Planar Node Cover 
Input: A planar undirected graph G with node degree at most 6 and 
an integer k. 
Decision: Does G have a node cover of size at most k? 
3. Colorability 
Input: A planar undirected graph G with node degree at most four. 
Decision: 
Is G 3-colorable? 
4. 
Undirected Hamiltonian Cycle 
Input: An undirected graph G with node degree at most three. 
Decision: Does G have a Hamiltonian cycle? 
S. 
Planar Undirected Hamiltonian Cycle 
Input: A planar undirected graph. 
Decision: Does G have a Hamiltonian cycle? 
6. 
Planar Directed Hamiltonian Path 
Input: A planar directed graph G with in-degree at most 3 and out· 
degree at most 4. 
Decision: Does G have a directed Hamiltonian path? 
7. 
Unary Input Partition 
Input: Positive integers a;, 1 s i s m, n, and B such that 
E a; = nB, B4 < a; < B2 , 1 s i s m and m = 3n. 
l'St'Sm 
Input is in unary notation. 
Decision: Is there a partition {A 1 , ••• , A.} of the a;'s such that each 
A; contains three elements and 

Some Simplified NP-Hard Problems 
547 
E a = B 
1 s i s n? 
a EA; 
' 
8. 
Unary Flow Shop 
Input: Task times in unary notation and an integer T. 
Decision: Is there a two processor non-preemptive schedule with 
mean finish time at most T? 
9. 
Simple Max Cut 
Input: A graph G = (V, E) and an integer k. 
Decision: Does V have a subset V1 such that there are at least k edges 
(u, v) E E with u E V1 and v E V 1? 
10. SAT2 
Input: A propositional formula Fin CNF. Each clause in F has at 
most two literals. An integer k. 
Decision: Can at least k clauses of F be satisfied? 
11. Minimum Edge Deletion Bipartite Subgraph 
Input: An undirected graph G and an integer k. 
Decision: 
Can G be made bipartite by the deletion of at most k 
edges? 
12. Minimum Node Deletion Bipartite Subgraph 
Input: An undirected graph G and an integer k. 
Decision: Can G be made bipartite by the deletion of at most k 
vertices. 
13. Minimum Cut Into Equal-Su.ed Subsets 
Input: An undirected graph G = (V, E), two distinguished vertices s 
and t and a positive integer W. 
Decision: Is there a partition v = V1 u Vi. V1 n V2 = cp, I Vil = 
I V2 I, s E V1 , t E V2 and I { (u, v) I u E V1 , v E V2 and (u, v) E E} I 
s W? 
14. Simple Optimal Linear Arrangement 
Input: An undirected graph G = (V, E) and an integer k. I VI = n. 
Decision: Is there a one to one functionf: V -
{ 1, 2, ... , n} such 
that 
E lf(u) - f(v)I s k 
(u.v)EE 

• 
548 
NP-Hard and NP-Complete Problems 
REFERENCES AND SELECTED READINGS 
A comprehensive treatment of NP-hard and NP-complete problems may be 
found in the book: 
Computers and intractability: A guide to the theory of NP-Completeness, by M. 
Garey and D. Johnson, Freeman and Co., San Francisco, 1979. 
Cook's theorem (Section 11.2) appears in: 
"The complexity of theorem-proving procedures," by S. A. Cook, Proc. of the 
Third ACM Symposium on Theory of Computing, 1971, pp. 151-158. 
The above paper also shows satisfiability oc clique. Cook's original proof is in 
terms of Turing machines. The proof given in the text was adapted by S. Sahni. We 
are grateful to R. Kain for pointing out an error in the original adaptation. J. Ull-
man has adapted Cook's proof to a somewhat different machine model. 
Karp showed the importance of the class of NP-complete problems by exhibiting 
21 problems that are NP-complete. His list of problems includes node cover, feed-
back arc set, feedback node set, Hamiltonian cycle, partition, sum of subsets, job 
sequencing with deadlines, max cut etc. Karp's work appears in: 
"Reducibility among combinatorial problems," by R. Karp, Complexity of Computer 
Computations, R. E. Miller and J. W. Thatcher, eds., Plenum Press, New York, 
1972, pp. 85-104. [exercises 6, 10, 11, 14, 19, 20, 21, 23, 24, 29, 37, 40]. 
Our proof satisfiability oc directed Hamiltonian cycle is from: 
"On reducibility among combinatorial problems," by P. Hermann, MIT MAC 
Report TR-113, December 1973. [exercises 8, 9] 
The proof satisfiability oc AND/OR Graphs is from: 
"Computationally related problems," by S. Sahni, SIAM Journal on Computing, 
3:4(1974), pp. 262-279. [exercises 18, 25, 35] 
This paper also contains reductions to many network flow, n-person game theory 
and optimization problems. Theorem 11.11 is due to Bruno, Coffman and Sethi. 
It appears in the following paper: 
"Scheduling independent tasks to reduce mean finishing-time," by J. Bruno, E. G. 
Coffman, Jr. and R. Sethi, Comm. ACM, 17:7, July 1974, pp. 382-387. 
The proof used in the text for Theorem 11.11 is due to S. Sahni and appears in: 

References and Selected Readings 
549 
"Algorithms for scheduling independent tasks," by S. Sahni, JACM, 23, 1976, pp. 
114-127. 
Theorems 11.12 and 11.13 are due to Gonzalez and Sahni. The reference is: 
"Flow shop and job shop schedules: complexity and approximation," by T. Gonzalez 
and S. Sahni, Op. Res., 26(1), pp. 36-52, 1978. 
The proof used in the text for Theorem 11.13 is due to D. Nassimi. Many other 
scheduling problems are known to be NP-hard. Some references are: 
"Machine scheduling problems," by A. Rinnooy Kan, Ph.D. thesis, Mathematical 
Centrum, Amsterdam, 1976. 
"Sequencing by enumerative methods," by J. K. Lenstra, Ph.D. thesis, Mathem-
atisch Centrum, Amsterdam, 1976. 
"Polynomial complete scheduling problems," by J. D. Ullman, JCSS, June 1975, 
pp. 384-393. 
Computer and Job Shop Scheduling Theory, by E.G. Coffman, J. Wiley, New York 
1976. 
"The Complexity of Flowshop and Jobshop Scheduling," by M. Garey, D. Johnson, 
and R. Sethi, Math. of Operations Research, 1:2(1976), pp. 117-129 [exercises 
30, 31]. 
"Complexity results for multiprocessor scheduling under resource constraints," by 
M. Garey and D. Johnson, SIAM Journal on Computing, 4:4(1975), pp. 397-411. 
"On the complexity of a timetable and multicommodity flow problems," by S. Even, 
SIAM Jr. on Computing, 5, 691-703 (1976). 
"Algorithms for minimizing mean flow time," by J. Bruno, E. G. Coffman, and 
R. Sethi, Proc. IFIP Congr. 74, 1974, pp. 504-510. 
"On the complexity of mean flow time scheduling," by R. Sethi, Math. of Op. Res., 
2(4), 320-330 (1977). 
"Open Shop Scheduling to Minimize Finish Time," by T. Gonzalez and S. Sahni, 
JACM, 23(4), pp. 665-679 (1976). 
"Optimization and approximation in deterministic sequencing and scheduling: 
a survey," by R. Graham, E. Lawler, J. Lenstra and A. Rinnooy Kan, Department 
of Operations Research, Mathematisch Centrum, Amsterdam, Report #BW 82177, 
1977. 
"Complexity of machine scheduling problems," by P. Brucker, J. Lenstra and A. 

550 
NP-Hard and NP-Complete Problems 
Rinnooy Kan, Math. Centrum, Amsterdam, Report #BW 43175, 1975. [exercises 
30, 31, 32, 33]. 
"Complexity of scheduling shops with no wait in process," by S. Sahni and Y. Cho, 
University of Minnesota, Technical Report #77-20, 1977 (to appear in Math. of 
Oper. Res.). 
"Preemptive shop scheduling of independent job with release times," by Y. Cho 
and S. Sahni, University of Minnesota, Technical Report #78-5, 1978. [exercise 36]. 
The proof of Theorem 11.14 is an adapation of a proof that appears in: 
"Code generation for expressions with common subexpressions," by A. Aho, S. 
Johnson and J. Ullman, JACM, 24(1), pp. 146-160 (1977). [exercise 41]. 
The fact that the code generation problem for one register machines is NP-hard 
was first proved by Bruno and Sethi in: 
"Code generation for a one-register machine," by J. Bruno and R. Sethi, I.ACM, 
23(3), pp. 502-510 (1976). 
The result of the above paper is stronger than Theorem 11.14 as it applies even 
to expressions containing no commutative operators. Theorem 11.15 is due to 
R. Sethi. The reference is: 
"A note on implementing parallel assignment instructions," by R. Sethi, Info. 
Proc. Let., 2, pp. 91-95 (1973). 
Further results on NP-Hard code generation problems appear in: 
"Complete register allocation problems," by R. Sethi, SIAM Jr. on Comp., 4(3), 
pp. 226-248 (1975). 
"Code generation for short/long address machines," by E. Robertson, University 
of Wisconsin, MRC report #1779, August 1977. 
The results stated in Section 11.6 may be found in: 
"Some simplified NP-Complete graph problems," by M. Garey, D. Johnson and L. 
Stockmeyer, Jr. Theo. Comp. Sci., 1, pp. 237-267 (1976). 
"The planar Hamiltonian circuit problem is NP-Complete," by M. Garey, D. 
Johnson and R. Tarjan, SIAM Jr. on Computing, 5(4), pp. 704-714, 1976. 
"The complexity of flowshop and jobshop scheduling," by M. Garey, D. Johnson 
and R. Sethi, Math. ofOper. Res., 1(2), pp. 117-129 (1976). 

References and Selected Readings 
551 
Other interesting papers on NP-hard and NP-complete problems are: 
"Some complexity results for the traveling salesman problem," by C. Papadimi-
triou and K. Steiglitz, Proc. Eighth Annual ACM Symposium on Theory of Com-
puting, May 1976, pp. 1-9. 
"On the computational complexity of combinatorial problems," by R. Karp, Net-
works, 5, 1975, pp. 45-68. 
"Polynomially complete fault detection problems," by 0. Ibarra and S. Sahni, 
IEEE Trans. Comp., 24(3), pp. 242-249(1975). [exercises 26, 27]. 
"Generalizing NP-Completeness to permit different input measures," by M. Garey 
and D. Johnson, Bell Laboratories, New Jersey, 1976. 
"Strong NP-Completeness results: motivation, examples and implications," by M. 
Garey and D. Johnson, Bell Laboratories, New Jersey, 1976. 
"Constructing optimal binary decision trees is NP-Complete," by L. Hyafil and 
R. Rivest, Info. Proc. Let., 5(1), pp. 15-17 (1976). 
"Assignment commands with array references," by P. Downey and R. Sethi, Proc. 
17th Annual Symp. on Found. of Comp., pp. 57-66 (1976) (to appear inJACM). 
"On the computational complexity of schema equivalence," by R. Constable, 
H. Hunt and S. Sahni, 8th Annual Princeton Conference on Information Sciences 
and Systems, pp. 15-20 (1974). 
"Complexity of trie index construction," by D. Comer and R. Sethi, JACM, 24(3), 
1977, pp. 428-440. 
"Combinatorial problems: reducibility and approximation," by S. Sahni and 
E. Horowitz, Operations Research, to appear. 
"Complexity of decision problems based on fmite two-person perfect-information 
games," by T. Schaefer, Proc. Eighth Annual ACM Symposium on Theory of 
Computing, May 1976, pp. 41-49. 
"Some polynomial and integer divisibility problems are NP-Hard," by D. Plaisted, 
Proc. 17th Annual Symp. on Found. of Comp., pp. 264-267 (1976). 
"Traversal marker placement problems are NP-Complete," by S. Maheshwari, 
University of Colorado, Computer Science Technical Report #CU-CS-092-76, May 
1976. 
"A note on reductions to directed HC," by J. Seiferas, 8th Annual Princeton Con-
ference on Information Sciences and Systems, pp. 24-28, 1974. 
"Two NP-complete problems in nonnegative integer programming," by G. Leuker, 
Princeton University Computer Science Laboratory, Technical Report TR-178, 1975. 
[exercise 44]. 

552 
NP-Hard and NP-Complete Problems 
"The complexity of satisfiability problems," by T. Schaefer, 10th ACM Symposium 
on Theory of Computing, pp. 216-226, 1978. 
EXERCISES 
1. Obtain a nondeterministic algorithm of complexity O(n) to determine whether 
or not there is a subset of then numbers a;, 1 :s; i :s; n that sums to M. 
2. 
(i) Show that the knapsack optimization problem reduces to the knapsack 
decision problem when all the p's, w's and Mare integer and the com-
plexity is measured as a function of input length. (Hint: if the input 
length is m then E p; :s; n2m where n is the number of objects. Use a 
binary search to determine the optimal solution value). 
(ii) Let DK be an algorithm for the knapsack decision problem. Let R be 
the value of an optimal solution to the knapsack optimization problem. 
Show how to obtain a 0/1 assignment for the x;, 1 :s; i :s; n such that 
E p;x; = Rand E W;X; :s; M by making n applications of DK. 
3. In conjunction with formula Gin the proof of Cook's theorem (Section 11.2), 
obtain M for the following cases for instruction i. Note that M can contain at 
most 0( p(n)) literals (as a function of n). Obtain Munder the assumption that 
negative numbers are represented in ones complement. Show how the corre-
sponding G;,,'s may be transformed into CNF. The length of G;, 1 must in-
crease by no more than a constant factor (say w2 ) during this transformation. 
i) Y-Z 
ii) Y-V-Z 
iii) Y- V + Z 
iv) Y- V*Z 
v) Y -
choice (0, 1) 
vi) 
Y -
choice (r:u) where rand u are variables. 
4. Show that the clique optimization problem reduces to the clique decision 
problem. 
5. Let SAT(E) be an algorithm to determine whether or not a propositional for-
mula E in CNF is satisfiable. Show that if E is satisfiable and has n variables 
x 1 , x2 , ••• , xn then using SAT(E) n times one can determine a truth value 
assignment for the x;'s for which Eis true. 
6. Let SATY be the problem of determining whether a propositional formula in 
CNF having at most three literals per clause is satisfiable. Show that CNF 
satisfiability oc SATY (Hint: Show how to write a clause with more than 
three literals as the and of several clauses each containing at most three lit-
erals. For this you will have to introduce some new variables. Any assignment 
that satisfies the original clause must satisfy all the new clauses created). 

Exercises 
553 
7. 
Let SAT3 be similar to SATY (Exercise 6) except that each clause has exactly 
three literals. Show that SATY oc SAT3. 
8. Let F be a propositional formula in CNF. Two literals x and y in Fare com-
patible iff they are not in the same clause and x ~ y. x and y are incompatible 
iff x and y are not compatible. Let SA TINC be the problem of determining if 
a formula F in which each literal is incompatible with at most three other 
literals is satisfiable. Show that SAT3 oc SA TINC. 
9. 
Let 3-NODE COVER be the node cover decision problem of Section 11.3 re-
stricted to graphs of degree 3. Show that SATINC oc 3-NODE COVER (see 
Exercise 8). 
10. 
[Feedback Node Set] 
(a) Let G = (V. £) be a directed graph. Let S s;;; V be a subset of vertices 
such that deletion of S and all edges incident to vertices in S results in 
a graph G' with no directed cycles. Such an S is a feedback node set. 
The size of S is the number of vertices in S. The feedback node set de-
cision problem (FNS) is to determine for a given input kif G has a feed-
back node set of size at most k. Show that node cover decision problem 
oc FNS. 
(b) Write a polynomial time nondeterministic algorithm for FNS. 
11. [Feedback Arc Set] 
Let G = (V. £) be a directed graph. S s;;; E is a feed-
back arc set of G iff every directed cycle in G contains an edge in S. The 
feedback arc set decision problem (FAS) is to determine if G has a feedback 
arc set of size at most k. Show that node cover decision problem oc FAS. 
(b) Write a polynomial time nondeterministic algorithm for FAS. 
12. The feedback node set optimization problem is to find a minimum feedback 
node set (see Exercise 10). Show that this problem reduces to FNS. 
13. Show that the feedback arc set minimization problem reduces to FAS (Exercise 
11). 
14. 
[Hamiltonian Cycle] 
Let UHC be the problem of determining if in any given 
undirected graph G there exists an undirected cycle going through each vertex 
exactly once and returning to the start vertex. Show that DHC oc UHC (DHC 
is defined in Section 11.3). 
15. Show UHC oc CNF satisfiability. 
16. 
Show DHC oc CNF satisfiability. 
17. [Hamiltonian Path] 
An i toj Hamiltonian path in a graph G is a path from 

554 
NP-Hard and NP-Complete 
vertex i to vertexj that includes each vertex exactly once. Show that UHC is 
reducible to the problem of determining if G has an i toj hamiltonian path. 
18. [Minimum Equivalent Graph] 
A directed graph G = (V. £) is an equivalent 
graph of the directed graph G' = (V. E') iff E s;;; E' and the transitive clo-
sures of G and G' are the same. G is a minimum equivalent graph iff IE I 
is minimum amongst all equivalent graphs of G'. The minimum equivalent 
graph decision problem (MEG) is to determine if G' has a minimum equiva-
lent graph with IE I ~ k where k is some given input. 
(a) Show that DHC oc MEG. 
(b) Write a nondeterministic polynomial time algorithm for MEG. 
19. 
[Oique Cover] 
The clique cover decision problem (CC) is to determine if G 
is the union of I or fewer cliques. Show that chromatic number decision 
problem oc CC. 
20. 
[Set Cover] 
Let F = {Sj} be a fmite family of sets. Let T s;;; F be a subset 
of F. T is a cover of F iff 
US;= US;. 
S;ET 
S;EF 
The set cover decision problem is to determine if F has a cover T containing 
no more than k sets. Show that the node cover decision problem is reducible 
to this problem. 
21. 
[Exact Cover] Let F = { Sj } be as above. T s;;; F is an exact cover of F iff T 
is a cover of F and the sets in Tare pairwise disjoint. Show that the chromatic 
number decision problem reduces to the problem of determining if F has an 
exact cover. 
22. 
Show that SAT3 oc EXACT COVER (see Exercise 21). 
23. 
[Hitting Set] 
Let F be as in Exercise 21. The hitting set problem is to deter-
mine if there exists a set H such that IH n Sjl = 1 for all Sj E F. Show that 
exact cover oc hitting set. 
24. [Tautology] 
A propositional formula is a tautology iff it is true for all pos-
sible truth assignments to its variables. The tautology problem is to determine 
whether or not a DNF formula is a tautology. 
(a) Show that CNF satisfiability oc DNF tautology. 
(b) Write a polynomial time nondeterministic algorithm TAUT(F) that ter-
minates successfully iff Fis not a tautology. 
25. [Minimum Boolean Form] Let the length of a propositional formula be equal 

Exercises 
555 
to the sum of the number of literals in each clause. Two formulas F and G on 
variables Xi, ... , Xn are equivalent if for all assignments to xi, ... , xn F is 
true iff G is true. Show that deciding if F has an equivalent formula of length 
no more than k is NP-Hard. (Hint: Show DNF tautology reduces to this 
problem). 
26. 
[Circuit Realization] Let C be a circuit made up of and, or and not gates. 
Let Xi, ... , xn be the inputs and f the output. Show that deciding if /(xi, 
... ,.x,,) = F(x 1, ••• , x,,) where Fis a propositional formula is NP-hard. 
27. 
Show that determining if C is a minimum circuit (i.e. has a minimum num-
ber of gates, see Exercise 26) realizing a formula Fis NP-hard. 
28. 
[Oil-knapsack] 
Show that Partition oc Oil-knapsack decision problem. 
29. 
[Job Sequencing] 
Show that the job sequencing with deadlines problem 
(Chapter 8) is NP-hard. 
30. 
Show that partition oc minimum finish time non-preemptive 3 processor flow 
shop schedule. Use only one job that has three nonzero tasks. All other jobs 
have only one nonzero task. 
31. 
Show that partition oc minimum finish time non-preemptive 2 processor job 
shop schedule. Use only one job that has three nonzero tasks. All other jobs 
have only one nonzero task. 
32. Let li , ... , Jn be n jobs. Job i has a processing time t; and a deadline d;. 
Job i is not available for processing until time r;. Show that deciding whether 
all n jobs can be processed on one machine without violating any deadline is 
NP-Hard. (Hint: Use partition). 
33. Let l;. 1 :s; i :s; n be n jobs as in the above problem. Assume r; = 0, 1 :s; i 
:s; n. Let/; be the finish time of J; in a one processor schedule S. The tardiness 
T; off; is max{O,f; - d;}. Let w;. 1 :s; i :s; n be nonnegative weights asso-
ciated with the J;'s. The total weighted tardiness is E w;T;. Show that finding 
a schedule minimizing E w ;T; is NP-hard. (Hint: Use partition). 
34. Let l;. 1 :s; i :s; n be n jobs. Job l; has a processing time oft;. Its processing 
cannot begin until time r;. Let w; be a weight associated with l;. Let/; be the 
finish time of l; in a one processor schedule S. Show that finding a one proces-
sor schedule that minimizes r; w;f; is NP-hard. 
35. [Quadratic Programming] 
Show that finding the maximum of a function 
f(x 1, .•• , Xn) subject to the linear constraints E1,;j,;n ai;Xj :Sh;, 1 :Si :Sn 

556 
NP-Hard and NP-Complete Problems 
and x; 2: 0, 1 s i s n is NP-hard. The function f is restricted to be of the 
form E c;x;2 + E d;x;. 
36. Show that the problem of obtaining optimal fmish time preemptive schedules 
for a two processer flow shop is NP-hard when jobs are released at two dif· 
ferent times R 1 and R 2. Jobs released at R i cannot be scheduled before R ;. 
37. Let G = (V, £)be a graph. Let w(i, j) be a weighting function for the edges 
of G. A cut of G is a subset S ~ V. The weight of a cut is 
E · w(i, j). 
iESj~S 
A max·cut is a cut of maximum weight. Show that the problem of deter· 
mining the weight of a max-cut is NP-hard. 
38. 
[Plant Location] Let S;, 1 s i s n be n possible sites at which plants may 
be located. At each site at most one plant can be located. If a plant is located 
at site Si then a fixed cost Fi is incurred. This is the cost of setting up the 
plant. A plant located at Si will have a maximum production capacity of Ci. 
There are n destinations, Di, 1 s i s m, to which products have to be 
shipped. The demand at D; is d;, 1 s i s m. The per unit cost of shipping 
a product from site i to destination j is cii. A destination may be supplied 
from many plants. Defme Yi = 0 if no plant is located site i and Yi = 1 other· 
wise. Let xii be the number of units of the product shipped from S; to Dj. 
Then, the total cost is 
All xii are non-negative integers. We may assume that E Cii 2: Edi. Show 
that finding y; and x ij so that the total cost is minimized is NP-hard. 
39. 
[Concentrator Location] This problem is very similar to the plant location 
problem (Exercise 38). The only difference is that each destination may be 
supplied by only 1 plant. When this restriction is imposed, the plant location 
problem becomes the concentrator location problem arising in computer net· 
work design. The destinations represent computer terminals. The plants repre-
sent the concentration of information from the terminals which it supplies. 
Show that the concentrator location problem is NP-hard under each of the 
following conditions: 
i) 
n = 2, C 1 = C 2, F 1 = F 2 (Hint: use Partition) 
ii) F;/Ci = F;+ilC;+i, 1 s i < n, d; = 1 (Hint: use Exact Cover) 

Exercises 
557 
40. 
[Steiner Trees] 
Let T be a tree and R a subset of the vertices in T. Let 
w(i, j) be the weight of edge (i, j) in T. If (i, j) is not an edge in T then 
w(i, j) = oo. A Steiner tree is a subtree of T that includes the vertex set R. 
It may include other vertices too. Its cost is the sum of the weights of the 
edges in it. Show that finding a minimum cost Steiner tree is NP-hard. 
41. a) How should the proof of Theorem 11.14 be modified to permit recompu-
tation of shared nodes. 
b) [Ravi Sethi] Modify the proof of Theorem 11.14 so that it holds for level 
1 dags representing expressions in which all operators are noncommuta-
tive. Hint: designate the sucessor vertex on a chain to be the left child of 
its predecessor vertex and use the following n + 1 node binary tree to 
connect together the tail nodes of the n chains: 
n • 1 Nodes 
Connections to Tail Nodes 
c) Show that optimal code generation is NP-hard for leaf dags on an in-
finite register machine. (Hint: Use FNS). 
42. Assume that Pis a parallel assignment statement (v 1 , ••• , vn) -
(e1 , ••• , en) 
where each e; is a simple variable and the v ;'s are distinct. For convenience, as-
sume that the distinct variables in P are v 1 , ••• , vm with m 2: n and that 
E = (i 1 , i2 , ... , in )is a set of indices such that eu = vu. Write an O(n) algo-
rithm to find an optimal realization for P. 
43. 
Let F = {S1} be a finite family of sets. Let T :s; F be a subfamily of F. The 

558 
44. 
#• 
NP-Hard and NP-Complete Problems 
size of T, I Tl, is the number of sets in T. Let S;, sj be two sets in T. S; and sj 
are disjoint iff S,. n S1 = ¢. T is a disjoint subset of F iff every pair of sets in 
Tare disjoint. The set packing problem is to determine a disjoint subfamily 
T of maximum size. Show that clique ex set packing. 
Show that the following decision problem is NP-complete. 
Input: Positive integers n; w;, 1 s i s n and M. 
Decision: Do there exist nonnegative integers x; 2: 0, 1 s i s n such that 
E W;X; = M 
l'!f:i'!f:n 

Chapter 12 
APPROXIMATION ALGORITHMS 
FOR NP-HARD PROBLEMS 
12.1 INTRODUCTION 
In the preceding chapter we saw strong evidence to support the claim that 
no NP-hard problem can be solved in polynomial time. Yet, many NP-hard 
optimization problems have great practical importance and it is desirable 
to solve large instances of these problems in a "reasonable" amount of time. 
The best known algorithms for NP-hard problems have a worst case com-
plexity that is exponential in the number of inputs. While the results of the 
last chapter may favor abandoning the quest for polynomial time algo-
rithms, there is still plenty of room for improvement in an exponential 
algorithm. We may look for algorithms with subexponential complexity, 
say 2 111< (for c > 1), 2v11 or n 10gn. In the exercises of Chapter 5 an 0(2n12 ) 
algorithm for the knapsack problem was developed. This algorithm can also 
be used for the partition, sum of subsets and exact cover problem. Tarjan 
and Trojanowski ("Finding a maximum independent set," SIAM Com-
puting, 6(3), pp. 537-546, 1977.) have obtained an 0(2nl3) algorithm for 
the max-clique, max-independent set and minimum node cover problems. 
The discovery of a subexponential algorithm for an NP-hard problem in-
creases the maximum problem size that can actually be solved. However, 
for large problem instances, even an 0(n 4 ) algorithm requires too much 
computational effort. Clearly, what is needed is an algorithm of low poly-
nomial complexity (say O(n) or O(n2)). 
The use of heuristics in an existing algorithm may enable it to quickly 
solve a large instance of a problem provided the heuristic "works" on that 
instance. This was clearly demonstrated in the chapters on backtracking and 
branch-and-bound. A heuristic, however, does not "work" equally effec-
tively on all problem instances. Exponential time algorithms, even coupled 
with heuristics will still show exponential behavior on some set of inputs. 
559 

560 
Approximation Algorithms for NP-Hard Problems 
If we are to produce an algorithm of low polynomial complexity to solve 
an NP-hard optimization problem, then it will be necessary to relax the 
meaning of solve. In this chapter we shall discuss two relaxations of the 
meaning of solve. In the first we shall remove the requirement that the 
algorithm that solves the optimization problem P must always generate an 
optimal solution. This requirement will be replaced by the requirement 
that the algorithm for Pmust always generate a feasible solution with value 
"close" to the value of an optimal solution. A feasible solution with value 
close to the value of an optimal solution is called an approximate solution. 
An approximation algorithm for Pis an algorithm that generates approxi-
mate solutions for P. 
While at first one may discount the virtue of an approximate solution, 
one should bear in mind that often, the data for the problem instance 
being solved is only known approximately. Hence, an approximate solution 
(provided its value is "sufficiently" close to that of an exact solution) may 
be no less meaningful than an exact solution. In the case of NP-hard prob-
lems approximate solutions have added importance as it may be true that 
exact solutions (i.e. optimal solutions) cannot be obtained in a feasible 
amount of computing time. An approximate solution may be all one can 
get using a reasonable amount of computing time. 
In the second relaxation we shall look for an algorithm for P that almost 
always generates optimal solutions. Algorithms with this property are called 
probabilistically good algorithms. These are considered in Section 12.6. In 
the remainder of this section we develop the terminology to be used in dis-
cussing approximation algorithms. 
Let P be a problem such as the knapsack or the traveling salesperson 
problem. Let /be an instance of problem P and let F*(I) be the value of 
an optimal solution to I. An approximation algorithm will in general pro-
duce a feasible solution to I whose value F(.J) is less than (greater than) 
F*(I) in case Pis a maximization (minimization) problem. Several cate-
gories of approximation algorithms may be defined. 
Let a be an algorithm which generates a feasible solution to every in-
stance I of a problem P. Let F*(I) be the value of an optimal solution to 
I and let F(I) be the value of the feasible solution generated by a. 
Definition a is an absolute approximation algori"':hm for problem p if 
and only if for every instance I of P, IF*(!) - F(J) I s k for some con-
stant k. 
Definition a is an f(n)-approximate algorithm if and only if for every 

Introduction 
561 
instance I of size n, IF*(!) -
F(I) I IF*(!) ~ f(n). It is assumed that 
F*(I) > O. 
Definition An E-approximate algorithm is an fin)-approximate algorithm 
for whichf(n) ~ E for some constant E. 
Note that for a maximization problem, IF*(J) -
F(J) I IF*(J) :::; 1 for 
every feasible solution to I. Hence, for maximization problems we will nor-
mally require E < 1 for an algorithm to be judged €-approximate. In the 
next few definitions we consider algorithms (t(E) with E an input to a. 
Definition 
(t(E) is an approximation scheme iff for every given E > 0 and 
problem instance /, (t(E) generates a feasible solution such that I F*(J) 
F(I)i IF*(!) ~ E. Again, we assumeF*(/) > 0. 
Definition An approximation scheme is a polynomial time approximation 
scheme iff for every fixed E > 0 it has a computing time that is polynomial 
in the problem size. 
Definition An approximation scheme whose computing time is a poly-
nomial both in the problem size and in 1/ E is a fully polynomial time ap-
proximation scheme. 
Oearly, the most desirable kind of approximation algorithm is an abso-
lute approximation algorithm. Unfortunately, for most NP-hard problems 
it can be shown that fast algorithms of this type exist only if P = NP. Sur-
prisingly, this statement is true even for the existence of fin)-approximate 
algorithms for certain NP-hard problems. 
Example 12.1 Consider the knapsack instance n = 3, M = 100, {p1, p2, 
p3} = {20, 10, 19} and { w1, w2, w3} = {65, 20, 35}. (x1, x2, x3) = 
(1, 1, 1) is not a feasible solution as E w;x; > M. The solution (x1, x2, x3) 
= (1, 0, 1) is an optimal solution. Its value E p;x;is 39. Hence, F*(I) = 
39 for this instance. The solution (x1, x2, X3) = (1, 1, O) is suboptimal. 
Its value is E p;x; = 30. This is a candidate for a possible output from 
an approximation algorithm. In fact, every feasible solution (in this case 
all three element 011 vectors other than (1, 1, 1) are feasible) is a candidate 
for output by an approximation algorithm. If the solution (1, 1, O) is gen-
erated by an approximation algorithm on this instance then F(I) = 30. 
I F*(J) -
F(I) I = 9 and I F*(J) -
F(I) I I F*(I) = 0.3. 
D 

562 
Approximation Algorithms for NP-Hard Problems 
Example 12.2 Consider the following approximation algorithm for the 
0/1 knapsack problem: consider the objects in nonincreasing order of 
pi!Wi. If object itits then set Xi= 1 otherwise set Xi= 0. When this algo-
rithm is used on the instance of Example 12.1, the objects are considered 
in the order 1, 3, 2. The result is (x1, x2, x3) = (1, 0, 1). The optimal 
solution is obtained. Now, consider the following instance: n = 2, (p1, p2) 
= (2, r), (w1, w2) = (1, r) and M = r. When r > 1, the optimal solution 
is (x1, x2) = (0, 1). Its value, F*(l), is r. The solution generated by the 
approximation algorithm is (x1, x2) = (1, 0). Its value, F(I), is 2. Hence, 
I F*(J) -
F(I) I = r -
2. Our approximation algorithm is not an abso-
lute approximation algorithm as there exists no constant k such that I F*(J) 
- F(I) I s k for all instances I. Furthermore, note that IF*(/) - F(J) I I 
F*(I) = 1 - 2/ r. This approaches 1 as r becomes large. I F*(J) - F(I) I I 
F*(I) s 1 for every feasible solution to every knapsack instance. Since 
the above algorithm always generates a feasible solution it is a I-approxi-
mate algorithm. It is, however, not an €-approximate algorithm for any E, 
E < 1. 
D 
Corresponding to the notions of absolute approximation algorithm and 
f(n)-approximate algorithm, we may define approximation problems in 
the obvious way. So, we can speak of k-absolute approximate problems 
and f(n)-approximate problems. The .5-approximate knapsack problem 
is to find any 0/1 feasible solution with IF*(/) - F(I)i IF*(!) s .5. 
As we shall see, approximation algorithms are usually just heuristics 
or rules that on the surface look like they might solve the optimization 
problem exactly. However, they do not. Instead, they only guarantee to 
generate feasible solutions with value within some constant or some factor 
of the optimal value. Being heuristic in nature, these algorithms are very 
much dependent on the individual problem being solved. 
12.2 ABSOLUTE APPROXIMATIONS 
Planar Graph Coloring 
There are very few NP-hard optimization problems for which polynomial 
time absolute approximation algorithms are known. One problem is that 
of determining the minimum number of colors needed to color a planar 
graph G = ( V, E). It is known that every planar graph is four colorable. 
One may easily determine if a graph is 0, 1 or 2 colorable. It is zero color-
able iff V = ¢. It is 1 colorable iff E = ¢. G is two colorable iff it is bi 
partite (see Exercise 6.41). Determining if a planar graph is three colorable 

Absolute Approximations 
563 
is NP-hard. However, all planar graphs are four colorable. An absolute 
approximation algorithm with IF*(/) - F(I) I :S 1 is easy to obtain. Algo-
rithm 12.1 is such an algorithm. It finds an exact answer when the graph 
can be colored using at most two colors. Since we can determine whether 
or not a graph is bipartite in time O(I YI + 1£1), the complexity of the 
algorithm is O(I YI + 1£1). 
procedure ACOLOR<. V, E) 
I I determine an approximation to the mm1mum number of colors/ I 
//needed to color the planar graph G = (Y, £)// 
case 
: Y = ¢: return (0) 
: E = ¢: return (1) 
: G is bipartite: return (2) 
: else: return (4) 
endcase 
endACOLOR 
Algorithm 12.1 Approximate coloring 
Maximum Programs Stored Problem 
Assume that we have n programs and two storage devices (say disks or 
tapes). We shall assume the devices are disks. Our discussion applies to 
any kind of storage device. Let l; be the amount of storage needed to store 
the ith program. Let L be the storage capacity of each disk. Determining 
the maximum number of these n programs that can be stored on the two 
disks (without splitting a program over the disks) is NP-hard. 
Theorem 12.1 Partition a Maximum Programs Stored. 
Proof: Let { a1, a2, ... , an} define an instance of the partition problem. 
We may assume Ea;= 2T. Define an instance of the maximum programs 
stored problem as follows: L = T and /; = a;, 1 :S i :S n. Oearly, {a;, 
... , an} has a partition iff all n programs can be stored on the two 
disks. 
D 
By considering programs in order of nondecreasing storage requirement 
/;, we can obtain a polynomial time absolute approximation algorithm. 
Procedure PSTORE assumes 11 :S Ii :S · • • :S ln and assigns programs 

564 
Approximation Algorithms for NP-Hard Problems 
to disk 1 so long as enough space remains on this tape. Then it begins 
assigning programs to disk 2. In addition to the time needed to initially 
sort the programs into nondecreasing order of /;, O(n) time is needed to 
obtain the storage assignment. 
procedure PSTORE(l, n, L) 
//assume[; s l;+1, 1 s i < nl / 
i- 1 
forj- lto2do 
sum -
0 I I amount of disk j already assigned/ I 
while sum + [; s L do 
print ('store program', i, 'on dis/C, j) 
sum -
sum+ [; 
i - i + 1 
if i > n then return endif 
repeat 
repeat 
end PSTORE 
Algorithm 12.2 Approximation algorithm to store programs 
Example 12.3 Let L = 10, n = 4 and (/1, Ii, /J, /4) = (2, 4, 5, 6). Pro-
cedure PSTORE will store programs 1 and 2 on disk 1 and only program 3 
on disk 2. An optimal storage scheme stores all four programs. One way 
to do this is to store programs 1 and 4 on disk 1 and the other two on 
disk 2. 
D 
Theorem 12.2 Let I be any instance of the maximum programs stored 
problem. Let F*(I) be the maximum number of programs that can be 
stored on two disks of length L each. Let F(I) be the number of programs 
stored using procedure PSTORE. Then, I F*(J) - F(I) I s 1. 
Proof: Assume that k programs are stored when Algorithm 12.2 is used. 
Then, F(I) = k. Consider the program storage problem when only one 
disk of capacity 2L is available. In this case, considering programs in order 
of nondecreasing storage requirement maximizes the number of programs 
stored. Assume that p programs get stored when this strategy is used on a 
single disk of length 2L. Clearly, p ;;::: F*(I) and E~/; s 2L. Letj be the 
largest index such that D, l; s L. It is easy to verify that j s p and that 
PSTORE assigns the firstj programs to disk 1. Also, 

Absolute Approximations 
565 
p-1 
p 
E /; s 
E /; s L. 
i=J+l 
i=j+2 
Hence, PSTORE assigns at least programs j + l, j + 2, ... , p -
1 to 
disk 2. So, F(I) ~ p - 1 and I F*(J) -
F(I) I s 1. 
D 
Algorithm PSTORE may be extended in the obvious way to obtain a 
k - 1 absolute approximation algorithm for the case of k disks. 
NP-hard Absolute Approximations 
The absolute approximation algorithms for the planar graph coloring and 
the maximum program storage problems are very simple and straightfor-
ward. Thus, one may expect that polynomial time absolute approximation 
algorithms exist for most other NP-hard problems. Unfortunately, for the 
majority of NP-hard problems one can provide very simple proofs to show 
that a polynomial time absolute approximation algorithm exists iff a poly-
nomial time exact algorithm does. Let us look at some sample proofs. 
Theorem 12.3 The absolute approximate knapsack problem is NP-hard. 
Proof: We shall show that the 0/1 knapsack problem with integer profits 
reduces to the absolute approximate knapsack problem. The theorem then 
follows from the observation that the knapsack problem with integer profits 
is NP-hard. Assume there is a polynomial time algorithm a that guaran-
tees feasible solutions such that I F*(J) -
F(I) I s k for every instance I 
and a fixed k. Let (p ;, w ;), 1 s i s n and M define an instance of the 
knapsack problem. Assume the p; are integer. Let I' be the instance de-
fined by ((k + l)p;, w;), 1 s is n and M. Clearly, I and I' have the 
same set of feasible solutions. Further, F*(I') = (k + l)F*(J) and /and 
I' have the same optimal solutions. Also, since all the p; are integer, it 
follows that all feasible solutions to I' either have value F*(I') or have 
value at most F*(I') -
(k + 1). If F(I') is the value of the solution gen-
erated by a for instance I' then F*(I') - F(I') is either 0 or at least k + 1. 
Hence if F*(I') -
F(l') s k then F*(I') = F(I' ). So, a can be used to 
obtain an optimal solution for I' and hence I. Since the length of I' is at 
most (log k)*(length of /), it follows that using the above construction 
we can obtain a polynomial time algorithm for the knapsack problem with 
integer profits. 
D 
Example 12.4 Consider the knapsack instance n = 3, M = 100, (p1, p2, 

566 
Approximation Algorithms for NP-Hard Problems 
p3) = (1, 2, 3) and (wi, w2, w3) = (SO, 60, 30). The feasible solutions 
are (1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 0, 1) and (0, 1, 1). The values of these 
solutions are 1, 2, 3, 4 and 5 respectively. If we multiply the p's by 5 then 
(fti. P2, jJ3) = (5, 10, 15). The feasible solutions are unchanged. Their 
values are now 5, 10, 15, 20 and 25 respectively. If we had an absolute 
approximation algorithm for k = 4 then, this algorithm will have to output 
the solution (0, 1, 1) as no other solution is within 4 of the optimal solution 
value. 
D 
Now, consider the problem of obtaining a maximum clique of an un-
directed graph. The following theorem shows that obtaining a polynomial 
time absolute approximation algorithm for this problem is as hard as ob-
taining a polynomial time algorithm for the exact problem. 
Theorem 12.4 Max clique oc absolute approximation max clique. 
Proof: Assume that the algorithm for the absolute approximation problem 
finds solutions such that IF* (I) - F(I) I s k. From any given graph G = 
( V, E), we construct another graph G' = ( V', E') such that G' consists 
of k + 1 copies of G connected together such that there is an edge between 
every two vertices in distinct copies of G. I.e., if V = {Vi, v2, ... , vn} 
then 
and 
k+i 
V' 
U {Vi;, v2;, ... , v/} 
i=l 
( 
k+i 
~ 
E' = 
~i {(v/, v/)l(vp. v,) E Ev U {(v,1, vf)li ;itj}. 
Oearly, the maximum clique size in G is q iff the maximum clique size 
in G' is (k + 1) q. Further, any clique in G' which is within k of the op-
timal clique size in G' must contain a sub-clique of size q which is a clique 
of size q in G. Hence, we can obtain a maximum clique for G from a 
k-absolute approximate maximum clique for G'. 
D 
Example 12.5 Figure 12.l(b) shows the graph G' that results when the 
construction of Theorem 12.4 is applied to the graph of Figure 12.l(a). 
We have assumed k = l. The graph of Figure 12.l(a) has two cliques. 

t-Approximations 
567 
One consists of the vertex set {1, 2} and the other {2, 3, 4}. Thus, an 
absolute approximation algorithm for k = 1 could output either of the 
two as solution cliques. In the graph of Figure 12.l(b), however, the two 
cliques are { 1, 2, 1 ', 2'} and {2, 3, 4, 2 ', 3 ', 4' }. Only the latter may 
be output. Hence, an absolute approximation algorithm with k = 1 will 
output the maximum clique. 
D 
(a) 
(b) 
Figure 12.1 Graphs for Example 12.5 
12.3 €-APPROXIMATIONS 
Scheduling Independent Tasks 
Obtaining minimum finish time schedules on m, m ::::: 2 identical processors 
is NP-hard. There exists a very simple scheduling rule that generates 
schedules with a finish time very close to that of an optimal schedule. An 
instance I of the scheduling problem is defined by a set of n task times, 
t ;, 1 :S i :S n, and m, the number of processors. The scheduling rule we 
are about to describe is known as the LPT (longest processing time) rule. 
An LPT schedule is a schedule that results from this rule. 

568 
Approximation Algorithms for NP-Hard Problems 
Definition An LPT schedule is one that is the result of an algorithm 
which, whenever a processor becomes free, assigns to that processor a task 
whose time is the largest of those tasks not yet assigned. Ties are broken 
in an arbitrary manner. 
Example 12.6 Let m = 3, n = 6 and (t1, t 2, t 3 , t 4, t 5, t 6) = (8, 7, 6, 5, 
4, 3). In an LPT schedule tasks 1, 2 and 3 are assigned to processors 1, 
2 and 3 respectively. Tasks 4, 5 and 6 are respectively assigned to proces-
sors 3, 2 and 1. Figure 12.2 shows this LPT schedule. The finish time is 
11. Since, E t/3 = 11, the schedule is also optimal. 
6 
7 
8 
11 
Figure 12.2 LPT schedule for Example 12.6 
Example 12.7 Let m = 3, n = 7 and (t., t 2, t3 , t 4, t 5, t 6 , t 7) = (5, 5, 
4, 4, 3, 3, 3). Figure 12.3(a) shows the LPT schedule. This has a finish 
time of 11. Figure 12.3(b) shows an optimal schedule. Its finish time is 9. 
Hence, for this instance I F*(J) - F(J)I I F*(I) = (11 - 9)/9 = 219. D 
It is possible to implement the LPT rule so that at most O(n log n) 
time is needed to generate an LPT schedule for n tasks on m processors. 
An exercise examines this. The preceding examples show that while the 
LPT rule may generate optimal schedules for some problem instances, it 
does not do so for all instances. How bad can LPT schedules be relative 
to optimal schedules? This question is answered by the following theorem. 
Theorem 12.5 [Graham] Let F*(I) be the finish time of an optimal m 
processor schedule for instance I of the task scheduling problem. Let F(I) 
be the finish time of an LPT schedule for the same instance. Then, 
I F*(J) -
F(I) I 
F*(I) 
1 
1 
<----
-
3 
3m 

t-Approximations 
569 
4 
5 
8 
II 
(a) LPT Schedule 
(bl Optimal Schedule 
Figure 12.3 LPT and optimal schedules for Example 12. 7 
Proof: The theorem is clearly true for m = 1. So, assume m ;;::: 2. Assume 
that for some m, m > l, there exists a set of tasks for which the theorem 
is not true. Then, let Ct1t t 2, ... , tn) define an instance I with the fewest 
number of tasks for which the theorem is violated. We may assume t 1 ;;::: 
t2 ;;::: · · · ;;::: tn and that an LPT schedule is obtained by assigning tasks in 
the order l, 2, 3, ... , n. 
Let S be the LPT schedule obtained by assigning these n tasks in this 
order. Let F(I) be its finish time. Let k be the index of a task with latest 
completion time. Then, k = n. To see this, suppose k < n. Then, the 
finish time j of the LPT schedule for tasks l, 2, ... , k is also F(I). The 
finish time, f*, of an optimal schedule for these k tasks is no more than 
F*(I). Hence, If* - ii If*;;::: IF*(/) - F(I)I IF*(!) > 113 -
11(3m). (The 
latter inequality follows from the assumption on /.) If* - fl If* > 113 -
11(3m) contradicts the assumption that I is the smallest m processor in-
stance for which the theorem does not hold. Hence, k = n. 
Now, we show that in no optimal schedule for I can more than two tasks 
be assigned to any processor. Hence, n :S 2m. Since task n has the latest 
completion time in the LPT schedule for [, it follows that this task is started 

• 
•• 
•· 
570 
Approximation Algorithms for NP-Hard Problems 
at time F(I) -
tn in this schedule. Further, no processor can have any 
idle time until this time. Hence, we obtain: 
So, 
Since, 
we can conclude that 
or 
1 n-1 
FU) -
t n ~ -
E t j 
m 
1 
A 
1~ 
m-1 
F(I) ~ -
'-' t; + -- tn. 
m 1 
m 
1 
n 
F*(I) ~-Et;, 
m 1 
F(I) -
F*(I) ~ m -
l tn 
m 
I F*(J) - F(J)I < m -
1 _t_n_ 
F*(I) 
-
m 
F*(I) 
But, from the assumption on I, the left hand side of the above inequality 
is greater than 1/3 - 1/(3m). So, 
1 
1 
m - 1 
tn 
-
- -- <------
3 
3m 
m 
F*(I) 
or 
m -
1 < 3(m - l)tn!F*(I) 
or 
F*(I) < 3tn. 

t-Approximations 
571 
Hence, in an optimal schedule for /, no more than two tasks can be 
assigned to any processor. When the optimal schedule contains at most 
two tasks on any processor then it may be shown that the LPT schedule 
is also optimal. We leave this part of the proof as an exercise. Hence, 
/ F*(I) -
F(I) / I F*(I) = 0 for this case. This contradicts the assumption 
on I. So, there can be no I that violates the theorem. 
D 
Theorem 12.5 establishes the LPT rule as a (113 -
l/(3m))-approxi-
mate rule for task scheduling. As remarked earlier, this rule can be imple-
mented to have complexity O(n log n). The following example shows that 
113 -
l/(3m) is a tight bound on the worst case performance of the LPT 
rule. 
Example 12.8 Let n = 2m + l, t; = 2m -
LU+ 1)/2j, i = l, 2, ... , 
2m and t 2m+i = m. Figure 12.4(a) shows the LPT schedule. This has a 
finish time of 4m - 1. Figure 12.4(b) shows an optimal schedule. Its finish 
timeis3m. Hence, IF*(/) - F(I)l!F*(I) = 113 - l/(3m). 
D 
Pm-2 
~~~-+--~-+--++~~ 
pm-I 
t-?-7-:;~~*""~~~~ 
Pm 
(al L PT Schedule 
Pm-2 ____ .................... -+-........ 
pm-I 
1-+.+--.~.,.<-r~ 
Pm 
(bl Optimal Schedule 
Figure 12.4 Schedules for Example 12.8 
For LPT schedules, the worst case error bound of 113 -
l/(3m) is not 
very indicative of the expected closeness of LPT finish times to optimal 
finish times. When m = 10, the worst case error bound is .3. Two experi-
ments were conducted ("An application of bin-packing to multiprocessor 
scheduling," by E. Coffman, M. Garey and D. Johnson, SIAM Computing, 

572 
Approximation Algorithms for NP-Hard Problems 
7(1), pp. 1-17, 1978.) to see what kind of error one might expect on a ran-
dom problem for m = 10. In the first experiment, 30 tasks with task times 
chosen according to a uniform distribution between 0 and 1 were generated. 
F*(I) was estimated to be Efl t/10 and F(I) was the length of the LPT 
schedule generated. The experiment was repeated ten times and the average 
value of IF*(!) -
F(I) I /.F*(J) computed. This value was 0.074. In the 
second experiment task times were chosen according to a normal distribution. 
The average I F*(J) -
F(I) I I F*(I) was 0 .023 this time. These figures 
are probably a little inflated as E1° t ;110 is probably an underestimation 
of the true F*(I). 
Efficient €-approximate algorithms exist for many scheduling problems. 
The references at the end of this chapter point to some of the better known 
€-approximate scheduling algorithms. Some of these algorithms are also 
discussed in the exercises. 
Bin Packing 
In this problem we are given n objects which have to be placed in bins of 
equal capacity L. Object i requires l; units of bin capacity. The objective is 
to determine the minimum number of bins needed to accommodate all 
n objects. No object may be placed partly in one bin and partly in another. 
Example 12.9 Let L = 10, n = 6 and (lit 12, 13 , 14 , 15 , 16) = (5, 6, 3, 7, 
5, 4). Figure 12.5 shows a packing of the 6 objects using only three bins. 
Numbers in bins are object indices. It is easy to see that at least 3 bins are 
needed. Ill 
Figure 12.5 Optimal packing for Example 12. 9 
The bin packing problem may be regarded as a variation of the sched-
uling problem considered earlier. The bins represent processors and L is 
the time by which all tasks must be completed. l 1 is the processing require-
ment of task i. The problem is to determine the minimum number of 
processors needed to accomplish this. An alternative interpretation is to 
regard the bins as tapes. L is the length of a tape and l; the tape length 
needed to store program i. The problem is to determine the minimum 

t-Approximations 
573 
number of tapes needed to store all n programs. Clearly, many interpre-
tations exist for this problem. 
Theorem 12.6 The bin packing problem is NP-hard. 
Proof: To see this consider the partition problem. Let { ai. a2, ... , an} 
be an instance of the partition problem. Define an instance of the bin 
packing problem as follows: /; = a;, 1 s i s n and L = r; a;/2. Clearly, 
the minimum number of bins needed is 2 iff there is a partition for 
{a1,a2,····an}. 
D 
One can devise many simple heuristics for the bin packing problem. 
These will not, in general, obtain optimal packings. They will, however, 
obtain packings that use only a "small" fraction of bins more than an opti-
mal packing. Four simple heuristics are: 
I. First Fit (FF) 
Index the bins 1, 2, 3, .... All bins are initially filled to level zero. 
Objects are considered for packing in the order 1, 2, ... , n. To pack ob-
ject i, find the least index j such that bin j is filled to a level r, r s L -
I;. 
Pack i into bin j. Bin j is now filled to level r + I;. 
II. Best Fit (BF) 
The initial conditions on the bins and objects are the same as for FF. 
When object i is being considered, find the least j such that bin j is filled 
to a level r, rs L - [;and r is as large as possible. Pack i into bin j. Bin j 
is now filled to level r + /;. 
III. First Fit Decreasing (FFD) 
Reorder the objects so that /; :2::. I;+ i, 1 s i < n. Now use First Fit to 
pack the objects. 
IV. Best Fit Decreasin3 (BFD) 
Reorder the objects so that I; :2::. Ii+ i, 1 s i < n. Now use Best Fit to 
pack the objects. 
Example 12.10 Consider the problem instance of Example 12. 9. Figure 
12.6 shows the packings resulting when each of the above four packing 
rules is used. For FFD and BFD the six objects are considered in the order 

574 
Approximation Algorithms for NP-Hard Problems 
(4, 2, 1, 5, 6, 3). As is evident from the figure, FFD and BFD do better 
than either FF or BF on this instance. While FFD and BFD obtain optimal 
packings on this instance, they do not in general obtain such a packing. D 
2 
3 
4 
(al First Fit 
Iii~ 
I 
2 
3 
4 
(bl Best Fit Ill 
I 
2 
3 
(cl First Fit Decreasing and Best Fit Decreasing 
Figure 12.6 Packings resulting from the four heuristics 
Theorem 12. 7 Let I be an instance of the bin packing problem and let 
F*(I) be the minimum number of bins needed for this instance. The 
packing generated by either FF or BF uses no more than (17 /10) F*(I) + 2 
bins. The packing generated by either FFD or BFD uses no more than 
(11/9) F *(I) + 4 bins. These bounds are the best possible bounds for the 
respective algorithms. 
Proof: The proof of this theorem is rather long and complex. It may be 
found in the paper: "Worst-Case Performance Bounds For Simple One-
Dimensional Packing Algorithms," by Johnson, Demers, Ullman, Garey 
and Graham, SIAM Jr. On Computing, 3(4), pp. 299-325 (1974). 
D 

t-Approximations 
575 
NP-hard €-Approximation Problems 
As in the case of absolute approximations, there exist many NP-hard 
optimization problems for which the corresponding €-approximation prob-
lems are also NP-hard. Let us look at some of these. To begin, consider 
the traveling salesperson problem. 
Theorem 12.8 Hamiltonian cycle ex €-approximate traveling salesperson. 
Proof: 
Let G(N,A) be any graph. Construct the complete graph G 1(V, E) 
such that V = N and E = { (u, v) I u, v E V and u -;e v }. Define the edge 
weighting function w to be 
w(u, ') ~ t 
if(u, v) EA 
otherwise 
Let n = I NI. Fork> 1, the traveling salesperson problem on G 1 has a 
solution of length n if and only if G has a Hamiltonian cycle. Otherwise, 
all solutions to G 1 have length :2::. k + n -
1. If we choose k :2::. (1 + E)n, 
then the only solutions approximating a solution with value n (if there was 
a Hamiltonian cycle in G 1) also have length n. Consequently, if the €-ap-
proximate solution has length s (1 + E)n then it must be of length n. If it 
has length >(1 + E)n then Ghas no Hamiltonian cycle. 
D 
Another NP-hard €-approximation problem is the 0/1 integer program-
ming problem. In the optimization version of this problem we are provided 
with a linear optimization function f(x) = E p;x; + po. We are required 
to find a 0/1 vector (xi. x 2, ••• , xn) such that f(x) is optimized (either 
maximized or minimized) subject to the constraints that E llijXj s b;, 
1 s i s k. k is the number of constraints. Note that the Oil-knapsack 
problem is a special case of the 0/1 integer programming problem just 
described. Hence, the integer programming problem is also NP-hard. We 
shall now show that the corresponding €-approximation problem is NP-hard 
for all €, € > 0. This is true even when there is only one constraint (i.e., 
k = 1). 
Theorem 12.9 Partition oc €-approximate integer programming. 

576 
Approximation Algorithms for NP-Hard Problems 
Proof: Let (a., a2, ••• , an) be an instance of the partition problem. Con-
struct the following 0/1 integer program: 
minimize 1 + k(m - E aiXi) 
subject to E aiXi ::S m 
Xi= 0 or 1, 
l:Si:Sn 
m = E a;/2 
The value of an optimal solution is 1 iff the ai s have a partition. If they 
don't then every optimal solution has a value at least 1 + k. Suppose there 
is a p-0lynomial time €-approximate algorithm for the 0/1 integer program· 
ming problem for some€, € > O. Then, by choosing k > € and using the 
above construction, this approximation algorithm can be used to solve, 
in polynomial time, the partition problem. The given partition instance has 
a partition iff the €-approximate algorithm generates a solution with value 1. 
All other solutions have value F(I) such that I F*(I) -
F(I) I I F*(I) ~ 
k > €. 
D 
As a final example of an €-approximation problem that is NP-Hard for 
all €, € > 0, consider the quadratic assignment problem. In one interpre-
tation this problem is concerned with optimally locating m plants. There 
are n possible sites for these plants, n ~ m. At most one plant may be 
located in any of these n sites. We shall use x i,1t, 1 ::S i ::S n, 1 ::S k ::S m as 
mn 011 variables. x i.1t = 1 iff plant k is to be located at site i. The location 
of the plants is to be chosen so as to minimize the total cost of transporting 
goods between plants. Let d1t.1 be the amount of goods to be transported 
from plant k to plant/. du = 0, 1 s; k ::S m. Let Ci.J be the cost of trans-
porting one unit of the goods from site i to site j. cu = 0, 1 ::S i ::S n. The 
quadratic assignment problem has the following mathematical formulation: 
n 
m 
minimize f(x) = E E CiJd1t.1Xi.1tXJ.1 
iJ~I lt,l=l 
subject to (a) 
(b) 
{c) 
m 
E Xi.It ::S l, 1 ::S i ::S n 
It=! 
n 
E Xi.ft = 1, 1 ::S k ::S m 
i=I 
Xi.ft = 0, 1 for all i, k 
CiJ, d1t.1 ~ 0, 1 ::S i, j ::S n, 1 ::S k, I ::S m 

E·Approxlmations 
577 
Condition {a) ensures that at most one plant is located at any site. Con-
dition (b) ensures that every plant is located at exactly one site. f(x) is the 
total transportation cost. 
Example 12.11 Assume two plants are to be located (m 
2) and there 
are three possible sites (n = 3). Assume 
[du 
d1~ 
[ 0 4J 
di1 diJ = 
10 0 
and 
[
Cu 
C12 C1'] 
[o 9 3] 
C21 
C22 
C23 
5 0 10 
C31 
C32 
C33 
2 6 
0 
If plant 1 is located at site 1 and plant 2 at site 2 then the transportation 
cost f(x) is 9*4 + 5*10 = 86. If plant 1 is located at site 3 and plant 2 
at site 1 then the cost f(x) is 2*4 + 3*10 = 38. The optimal locations are 
plant 1 at site 1 and plant 2 at site 3. The cost f(x) is 3*4 + 2*10 = 
32. 
D 
Theorem 12.10 Hamiltonian cycle ex €-approximate quadratic assignment. 
Proof: Let G(N,A) be an undirected graph with m = INI. The following 
quadratic assignment instance is constructed from G: 
n = m 
c .. = {01 
,,, 
i = (j mod m) + 1, 1 ::5 i, j ::5 m. 
otherwise 
if ( k, I) E A, 1 ::5 k, I ::5 m. 
otherwise 
The total cost, f(-y ), of an assignment, -y, of plants to locations is 

578 
Approximation Algorithms for NP-Hard Problems 
i;r=i c;JdrUrr(j) where} = (i mod m) + 1 and r{i) is the index of the plant 
assigned to location i. If G has a Hamiltonian cycle i1, ii, ... , im i1 then 
the assignment -y(j) = ij has a cost f(-y) = m. In case G has no Hamil-
tonian cycle then at least one of the values dru>. r(imod m+l) must be w and so 
the cost becomes ~ m + w -
1. Choosing w > (1 + E)m results in opti-
mal solutions with a value of m if G has a Hamiltonian cycle and value 
>(1 + E)m if G has no Hamiltonian cycle. Thus, from an €-approximate 
solution, it can be determined whether or not G has a Hamiltonian 
cycle. 
D 
Many other €-approximation problems are known to be NP-hard. Some 
of these are examined in the exercises. While the three problems just dis-
cussed were NP-hard for €, € > 0, it is quite possible for an €-approxima-
tion problem to be NP-hard only for€ in some range, say, 0 < E s r. For 
€ > r there may exist simple polynomial time approximation algorithms. 
12.4 POLYNOMIAL TIME APPROXIMATION SCHEMES 
Scheduling Independent Tasks 
We have seen that the LPT rule leads to a {1/3 -
l/{3m))-approximate 
algorithm for the problem of obtaining an m processor schedule for n tasks. 
A polynomial time approximation scheme is also known for this problem. 
This scheme relies on the following scheduling rule: (i) Let k be some 
specified and fixed integer. (ii) Obtain an optimal schedule for the k longest 
tasks. (iii) Schedule the remaining n -
k tasks using the LPT rule. 
Example 12.12 Let m = 2; n = 6; (t1, ti, tJ, t4, t 5, t6) = (8, 6, 5, 4, 
4, 1) and k = 4. The four longest tasks have task times 8, 6, 5 and 4 re-
spectively. An optimal schedule for these has finish time 12 (Figure 12. 7{a)). 
When the remaining two tasks are scheduled using the LPT rule, the 
schedule of Figure 12.7(b) results. This has finish time 15. Figure 12.7(c) 
shows an optimal schedule. This has finish time 14. 
D 
14 
~ 
~ 
(a) optimal for 4 tasks 
(bl completed schedule (cl overall optimal 
Figure 12. 7 Using the approximation scheme with k = 4 

Polynomial Time Approximation Schemes 
579 
Theorem 12.11 [Graham] Let I be an m processor instance of the sched-
uling problem. Let F*(l) be the finish time of an optimal schedule for I 
and let F(l) be the length of the schedule generated by the above scheduling 
rule. Then, 
I F*(J) - F(J)I 
< 
1 - 1/m 
F*(I) 
-
1 + Lklmj 
Proof: Let r be the finish time of an optimal schedule for the k longest 
tasks. If F(I) = r then, F*(I) = F(I) and the theorem is proved. So, 
assume F(I) > r. Let t ;, 1 s i s n be the task times of the n tasks of I. 
Without loss of generality, we may assume t; ~ ti+ 1, 1 s i < n and n > k. 
Also, we may assume n > m. Let j, j > k be such that task j has finish 
time F(I). Then, no processor may be idle in the interval [O, F(I) -
t;]. 
Since h+1 ~ t;, it follows that no processor is idle in the interval [O, F(I) 
-
tk+d· Hence, 
and so, 
or 
n 
E t; ~ m(F(l) -
tk+1) + tk+t 
i~ 1 
1~ 
A 
m-1 
F*(I) ~ -
'-' t; ~ F(I) - -- h+1 
m 1 
m 
I F*(J) -
F(I) I 
m -
1 
S --- tk+I• 
m 
Since t; ~ tk+t, 1 s is k + 1 and at least one processor must execute 
at least 1 + L kl mj of these k + 1 tasks, it follows that: 
F*(I) ~ (1 + Lklmj )h+1. 
Combining these two inequalities, we obtain 
I F*(~*CJ{(I) I s ((m - 1)/ m)l(l + L kl mj ) = 
1 -
11 m 
D 
1 + LklmJ 
Using the result of Theorem 12.11, we can construct a polynomial time 
€-approximation scheme for the scheduling problem. This scheme has € as 

580 
Approximation Algorithms for NP-Hard Problems 
an input variable. For any input € it computes an integer k such that € ::5 
(1 - l/m)/(1 + L klm J ). This defines the k to be used in the scheduling 
rule described above. Solving for k, we obtain that any integer k, k > 
(m - 1)/ € -
m will guarantee €-approximate schedules. The time required 
to obtain such schedules, however, depends mainly on the time needed to ob-
tain an optimal schedule fork tasks on m machines. Using a branch-and-
bound algorithm, this time is O(m k). The time needed to arrange the tasks 
such that t; ~ t;+i and also to obtain the LPT schedule for the remaining 
n -
k tasks is O(n log n). Hence the total time needed by the €-approxi-
mate scheme is O(n log n + mk) = O(n log n + m«'" - lll.-rttl ). Since this 
time is not polynomial in 1/ € (it is exponential in 1/ €), this approximation 
scheme is not a fully polynomial time approximation scheme. It is a poly-
nomial time approximation scheme (for any fixed m) as the computing 
time is polynomial in the number of tasks n. 
0/1 Knapsack 
The 0/1 knapsack heuristic proposed in Example 12.2 does not result in 
an €-approximate algorithm for any€, 0 < € < 1. Suppose we try out the 
heuristic described by procedure €-APPROX (Algorithm 12.3). In this pro-
cedure P and W are the sets of profits and weights respectively. It is as-
sumed that p;lw; ~ p;+1lw;+1, 1 ::5 i < n. Mis the knapsack capacity 
and k a nonnegative integer. In the loop of lines 2-5, all E 7~oe;) different 
subsets, I, consisting of at most k of the n objects are generated. If the cur-
rently generated subset I is such that E;EI w; > M it is discarded (as it is 
infeasible). Otherwise, the space remaining in the knapsack (i.e., M -
E;EI 
w ;) is filled using the heuristic described in Example 12.2. This heuristic is 
stated more formally as procedure L (Algorithm 12.4). 
line procedure E-APPROX(P, W, M, n, k) 
I I (i) the size of a combination is the number of objects in it;/ I 
I I (ii) the weight of a combination is the sum of the weights of/ I 
I !the objects in that combination;/ I 
I !(iii) k is a nonnegative integer which defines the order of the// 
I I algorithm/ I 
1 
PMAX - 0; 
2 
for all combinations I of size ::5 k and weight ::5 M do 
3 
Pi- EiEIPi 
4 
PMAX -
max(PMAX, P1 + L(l, P, W, M, n)) 
5 
repeat 
6 end €-APPROX 
Algorithm 12.3 Heuristic algorithm for knapsack problem 

Polynomial Time Approximation Schemes 
581 
procedure L(l, P, W, M, n) 
S -
O; i -
1; T - M - Ew W; 
//initialize// 
for i -
1 to n do 
if iit. land w; :5 Tthen S -
S + p; 
T- T-
W; 
endlf 
repeat 
return (S) 
endL 
Algorithm 12.4 Subalgorithm for procedure e-APPROX 
Example 12.13 Consider the knapsack problem instance with n = 8 ob-
jects, size of knapsack = M = 110, P = { 11, 21, 31, 33, 43, 53, 55, 65} 
and W = {1, 11, 21, 23, 33, 43, 45, 55}. 
The optimal solution is obtained by putting objects 1, 2, 3, 5 and 6 into 
the knapsack. This results in an optimal profit, P*, of 159 and a weight of 
109. 
We obtain the following approximations for different k: 
a) 
k = 0, PMAX is just the lower bound solution L(</>, P, W, M, n); 
PMAX = 139; x = {1, 1, 1, 1, 1, 0, 0, 0); w = E;X;W; = 89; 
(P* -
PMAX)/ P* = 20/159 = .126. 
b) k = 1, PMAX = 151; x = {1, 1, 1, l, 0, 0, 1, O); W = 101; (P* -
PMAX)/ P* = 8/159 = .05. 
c) 
k = 2, PMAX = P* = 159; x = (1, 1, 1, 0, l, 1, 0, 0); W = 109. 
The table of Figure 12.8 gives the details for k = 1. It is interesting to 
note that the combinations I = {1}, { 2}, { 3}, { 4}, { 5} need not be tried 
since for I = { </>} x 6 is the first x; which is 0 and so these combinations will 
yield the same PMAX as I = { </>}. This will be true for all combinations I 
that include only objects for which x;was 1 in the solution for I= { </> }. D 
Theorem 12.12 Let Jbe an instance of the knapsack problem. Let n, M, 
P and W be as defined for procedure e-APPROX. Let P* be the value of 
an optimal solution for J. Let PMAX be as defined by procedure e·APPROX 
on termination. Then, 
IP* -
PMAXI IP* < ll(k + 1). 
Proof: Let R be the set of objects included in the knapsack in some opti-
mal solution. So, E iER p; = P* and E iER w; :5 M. If the number of objects 
in R, I RI, is such that I RI :5 k then at some time in the execution of pro-

-
-
582 
Approximation Algorithms for NP-Hard Problems 
PMAX= 
max 
I 
PMAX 
P1 
R1 
L 
{PMAX, 
X optimal 
P1 + L} 
</> 
0 
11 
1 
128 
139 
(l,l,l,l,l,0,0,0) 
6 
139 
53 
43 
96 
149 
c1,1,1,1,o,1,o,o) 
7 
149 
55 
45 
9 
151 
(l,l,l,l,0,0,l,O) 
8 
151 
65 
55 
63 
151 
(l,l,l,l,0,0,1,0) 
*Note that rather than update x optimal it is easier to update the optimal I and recom-
pute x optimal at the end 
Figure 12.8 Expansion of Example 12.13 fork = 1 
cedure €·APPROX, I= Rand so PMAX = P*. So, assume I RI > k. 
Let (p;, w;), 1 s i s I RI be the profits and weights of the objects in R. 
Assume these have been indexed such that /Ji. ... , ft,. are the k largest 
profits in Rand that jJJwi :2:: ft;+1/ W;+i. k < i < JR J. From the first of 
these assumptions, it follows that ft1c+r :S P*l(k + 1), 1 :S ts J RI -
k. 
Since the loop of lines 2-5 tries out all combinations of size at most k, it 
follows that in some iteration, I corresponds to the set of k largest profits 
in R. Hence, P1 = Ew p; = Er=1 ft;. Consider the computation of line 4 
in this iteration. In the computation of L(I, P, W, M, n) let j be the least 
index such that j ~ I, w1 > Tand j E R. Thus, object j corresponds to one 
of the objects {ft,, w,), k < r :S IR I and j is not included in the knapsack 
by algorithm L. Let object j correspond to (ftm, Wm). 
At the time object j is considered, T < w 1 = w 
m• The amount of space 
filled by procedure L is M - E iEI w; -
T and this is larger than E ;:-1c~ 1 w; 
(as ET w; :S M). Since this amount of space is filled by considering ob-
jects in nondecreasing order of p / w ;, it follows that the profit S added by 
L is no less than 
m-1 
A 
E ft; + ...If:m- D. 
i=lc+l 
Wm 
where 
m-1 
D. = M -
T -
E w ;. 
1 

Polynomial Time Approximation Schemes 
583 
Also, 
From these two inequalities, we obtain: 
IRI 
P* = P 1 + E p; 
k+I 
< P1 + S +Pm 
Since, PMAX :2:: P1 + Sand Pm ::S P*!(k + 1), it follows that: 
I P* - PMAX I < ...E..m_ < _1_ 
P* 
P* -
k + 1 
This completes the proof. 
D 
The time required by Algorithm 12.3 is O(n*+ 1). To see this, note that 
the total number of subsets tried is 
k (n\ 
*(n) 
k 
n*+I -
1 
E . ) 
and E . 
::S E n; = 
1 
= O(n *). 
i= o z 
i= o z 
i= o 
n -
Subalgorithm L has complexity O(n). So, the total time is O(n*+ 1). 
Algorithm €·APPROX may be used as a polynomial time approximation 
scheme. For any given €, 0 < € < 1 we may choose k to be the least in-
teger greater than or equal to (1 I€) -
1. This will guarantee a fractional 
error in the solution vaue of at most €. The computing time is O(n 11'). 
While Theorem 12.12 provides an upper bound on IP* - PMAXl/P*, 
it does not say anything about how good this bound is. Nor does it say 
anything about the kind of performance we may expect in practice. Let 
us now address these two problems. 
Theorem 12.13 For every k there exist knapsack instances for which 
I (P* - PMAX)/ P* I gets as close to l!(k + 1) as desired. 

584 
Approximation Algorithms for NP-Hard Problems 
Proof: For any k, the simplest examples approaching the lower bound are 
obtained by setting: n = k + 2; w 1 = l; p 1 = 2; Pi, wi = q, 2 ::5 i ::5 
k + 2, q > 2, M = (k + 1) q. Then, P* = (k + 1) q. The PMAX given 
by E·APROX for this k is kq + 2 and so I ( P* -
PMAX)/ P* I = (1 -
2/q)l(k + 1). By choosing q increasingly large one can get as close to 
1/(k + 1) as desired. 
D 
Another upper bound on the value of I P* -
PMAX)/ P* I can be ob· 
tained from the proof of Theorem 12.12. We know that P* - PMAX < 
Pm and that P* ~ PMAX. Also since Pm is one of P1c+ 1, ••• , PIRI• it fol· 
lows that Pm ::5 p where p is the (k + l)·st largest p. Hence I (P* -
PMAX)/ P* I < min{ ll(k + 1), p!PMAX}. In most cases p!PMAX will 
be smaller than 1/(k + 1) and so will give a better estimate of closeness in 
cases where the optimal is not known. We note that p is easy to compute. 
The preceding discussion leads to the following theorem: 
Theorem 12.14 The deviation of the solution PMAX obtained from the 
€-approximate algorithm, from the true optimal P* is bounded by I (P* -
PMAX)/P* I < min{ 1/(k + 1), p/PMAX}. 
In order to get a feel for how the approximation scheme might perform 
in practice, a simulation was conducted. A sample of 600 knapsack in-
stances was used. This sample included problems with n = 15, 20, 25, 
30, ... , 60. For each problem size, 60 instances were generated. These 
60 instances included five from each of the following six distributions: 
I. random weights wi and random profits Pi• 1 ::5 wi, Pi ::5 100. 
II. 
random weights wiand random profits Pi• 1 ::5 W;, Pi ::5 1000. 
III. random weights Wi, 1 ::5 Wi ::5 100, Pi = Wi + 10. 
IV. 
random weights Wi, 1 ::5 Wi ::5 1000, p.- = w.- + 100. 
V. 
random profits Pi• 1 ::5 Pi ::5 100, wi = Pi+ 10. 
VI. 
random profits Pi• 1 ::5 Pi ::5 1000, wi = Pi+ 100. 
Random profits and weights were chosen from a uniform distribution 
over the given range. For each set of p's and w's, two Ms were used; M = 
2* max{ w;} and M = E w/2. This makes for a total of 600 problem in-
stances. Figure 12. 9 summarizes the results. The figure gives the number 
of problems for which ( P* -
PMAX)/ P* was in a particular range . 
. 5-APPROX is €-APPROX with. k = 1 and .33-APPROX is €-APPROX 
with k = 2. As is evident, the observed IP* -
PMAX I! P* values are 
much less than indicated by the worst case bound of Theorem 12.12. Figure 

Fully Polynomial Time Approximation Schemes 
585 
12.10, gives the result of a simulation for large n. Computing times are 
for a FORTRAN program run on an IBM 360/65 computer. 
(P* - PMAX)I P* 
*100 
Method 
0 (Optimal 
value) 
.1% .5% 
1% 
2% 
3% 
4% 
5% 10% 25% 
L(cp,P,S,M,n) 
239 
267 
341 
390 
443 
484 
511 
528 
583 
600 
.5-APPROX 
360 
404 
477 
527 
567 
585 
593 
598 
600 
.33-APPROX 
483 
527 
564 
581 
596 
600 
Figures give number of solutions that were within r percent of the true optimal 
solution value; r is the figure in the column head. 
Figure 12.9 Results of simulation for set of 600 problems 
Problem size n 
100 
200 
500 
1000 
2000 
3000 
4000 
5000 
Computing Time 
.25 
.9 
3.5 
14.6 
60.4 
98.3 
180. 
350. 
Estimated % difference 2.5% 
1.3% 
.5% 
.25% 
.12% 
.08% 
.06% 
.04% 
min{p/PMAX, .5}*100 
M = Ew;/2; w;,p; E [1,1000]; times in seconds 
Figure 12.10 Computing times using the .5-approximate algorithm 
12.5 FULLY POLYNOMIAL TIME APPROXIMATION SCHEMES 
The approximation algorithms and schemes we have seen so far are par· 
ticular to :he problem considered. There is no set of well defined techniques 
that one may use to obtain such algorithms. The heuristics used depended 
very much on the particular problem being solved. For the case of fully 
polynomial time approximation schemes, we can identify three underlying 
techniques. These techniques apply to a variety of optimization problems. 
We shall discuss these three techniques in terms of maximization problems. 

586 
Approximation Algorithms for NP-Hard Problems 
We shall assume the maximization problem to be of the form: 
n 
max E p·x· 
i=l 
I 
I 
n 
subject to .E aijxi ::5 bj, 
•=I 
1 ::5 j::5 m 
xi= 0 or 1 
(12.1) 
Without loss of generality, we will assume that Cljj ::5 bj, 1 ::5 i ::5 n and 
1 ::5 j ::5 m. 
If 1 ::5 k ::5 n, then the assignment xi = y i, will be said to be a feasible 
assignment iff there exists at least one feasible solution to (12.1) with x; = 
y ;, 1 ::5 i ::5 k. A completion of a feasible assignment x; = y; is any feasible 
solution to (12.1) with x; = y ;, 1 ::5 i ::5 k. Let x; = y; and x; = z ;, 1 ::5 
i ::5 k be two feasible assignments such that for at least one j, 1 ::5 j ::5 k, 
Yi ;t. ZJ· Let E PiYi = E PiZ;. We shall say that y1, ... , y *dominates z i. 
... , z * iff there exists a completion y i. ••. , y *• y * + i. ••• , y n such that 
E 7 
=I Pi Yi is greater than or equal to E I si sn p iZ i for all completions z h ••• ' 
z n of z i. ••• , z k· The approximation techniques to be discussed will apply 
to those problems that can be formulated as (12.1) and for which simple 
rules can be found to determine when one feasible assignment dominates 
another. Such rules exist for example for problems solvable by the dynamic 
programming technique. Some such problems are Oil-knapsack; job se-
quencing with deadlines; job sequencing to minimize finish time and job 
sequencing to minimize weighted mean finish time. 
One way to solve problems stated as above is to systematically generate 
all feasible assignments starting from the null assignment. Let S-il repre-
sent the set of all feasible assignments for x 1 , x 2 , ••• , xi. Then S-0> repre-
sents the null assignment and S-n> the set of all completions. The answer to 
our problem is an assignment in S-n> that maximizes the objective function. 
The solution approach is then to generate S-i+Il from S-il, 1 ::5 i < n. If an 
S Cil contains two feasible assignments y ;, ... , y i and z i. ••• , z i such that 
E PiYi = E pizi then use of the dominance rules enables us to discard or kill 
that assignment which is dominated. (In some cases the dominance rules may 

Fully Polynomial Time Approximation Schemes 
587 
permit the discarding or killing of a feasible assignment even when E p1y1 
;t. E p 1z1• This happens, for instance, in the knapsack problem (see Sec-
tion 5.5). Following the use of the dominance rules, it is the case that for 
each feasible assignment in s-n Ej= 1 p1x1 is distinct. However, despite this, 
it is possible for each S-il to contain twice as many feasible assignments as 
in SJ- 1>. This results in a worst case computing time that is exponential 
in n. Note that this solution approach is identical to the dynamic program-
ming solution methodology for the knapsack problem (Section 5.5) and 
also to the branch-and-bound algorithm later developed for this problem 
(Section 8.2). 
The approximation methods we are about to discuss are called rounding, 
interval partitioning and separation. These methods will restrict the num-
ber of distinct Ej= 1 p1x1 to be only a polynomial function of n. The error 
introduced will be within some prespecified bound. 
Rounding 
The aim of rounding is to start from a problem instance, I, formulated as 
in (12.1) and to transform it to another problem instance I' that is easier 
to solve. This transformation is carried out in such a way that the optimal 
solution value of I' is "close" to the optimal solution value of I. In par-
ticular, if we are provided with a bound, 1:, on the fractional difference 
between the exact and approximate solution values then we require that 
I F*(J) -
F*(l' )I F*(l) I ~ 1:, where F*(l) and F*(l') represent the 
optimal solution values of I and I' respectively. 
I' is obtained from /by changing the objective function to max E q;x;. 
Since I and I' have the same constraints, they have the same feasible 
solutions. Hence, if the p;'s and q;'s differ by only a "small" amount, the 
value of an optimal solution to/' will be close to the value of an optimal solu-
tion to!. 
For example, if the p; in I have the values: (p1, P2, p3, p4) = (1.1, 
2.1, 1001.6, 1002.3) then if we construct I' with (q1, q2, q3, q4) = (0, 0, 
1000, 1000) it is easy to see that the value of any solution in I is at most 
7.1 more than the value of the same solution in I'. This worst case differ-
ence is achieved only when x; = 1, 1 ~ i ~ 4 is a feasible solution for I 
(and hence also for /' ). Since, aiJ ~ b1, 1 ~ i ~ n and 1 ~ j ~ m, it 
follows that F*(l) ~ 1002.3 (as one feasible solution is x1 = x2 = x3 = 0 
and X4 = 1). But F*(I) - F*(I') ~ 7.1 and so (F*(I) - F*(l'))IF*(I) 

588 
Approximation Algorithms for NP-Hard Problems 
:S 0.007. Solving I using the procedure outlined above, the feasible assign-
ments in S Cil could have the following distinct profit values: 
S'o> 
{ O} 
5'1> 
{ 0, 1.1} 
5'2> {O, 1.1, 2.1, 3.2} 
5'3> {O, 1.1, 2.1, 3.2, 1001.6, 1002.7, 1003.7, 1004.8} 
5'4> {O, 1.1, 2.1, 3.2, 1001.6, 1002.3, 1002.7, 1003.4, 1003.7, 
1004.4, 1004.8, lOOS.S, 2003. 9, 200S, 2006, 2007.1} 
Thus, barring any elimination of feasible assignments resulting from the 
dominance rules or from any heuristic, the solution of I using the pro-
cedure outlined above would require the computation of Eo,,. rs. n I S'n I = 31 
feasible assignments. 
The feasible assignments for I' have the following values: 
5'0) { 0} 
5'1> { O} 
5'2> { O} 
5'3) { 0, 1000} 
5'4> 
{ 0, 1000, 2000} 
Note that E 7 = o IS Cil I is only 8. Hence I' can be solved in about one fourth 
the time needed for I. An inaccuracy of at most . 7% is introduced. 
Given the p;'s and an 1:, what should the q;'s be so that 
n 
(F*([) - F*(I'))IF*([) :S € and .E I scn1 :S u(n, lit:) 
1mO 
where u is a polynomial in n and 111:? Once we can figure this out we will 
have a fully polynomial approximation scheme for our problem since it is 
possible to go from su- 1> to sen in time proportional to O(SU- 1>). (See 
the knapsack algorithm of Section S.S.) 
Let LB be an estimate for F*(I) such that F*(I) ~ LB. Clearly, we 
may assume LB ~ max; {p; }. If 
n 
i~l IP; -
q;I s t:F*(I) 
then, it is clear that, (.F*([) -
F*(I '))/ F*(l) :S 1:. Define q; = p; -
rem(p;, (LB·t:)ln) where rem(a, b) is the remainder of alb, i.e., a -

Fully Polynomial Time Approximation Schemes 
589 
Lal bj b (e.g., rem(7, 6) = 116 and rem(2.2, 1.3) = .9). Since rem(p,., 
LB·dn) < LB·dn, it follows that E /p; -
q,./ < LB·€ s F*·f. Hence, 
if an optimal solution to I' is used as an optimal solution for I then the 
fractional error is less than 1:. 
In order to determine the time required to solve I' exactly, it is useful 
to introduce another problem /" with s;, 1 s is n as its objective func-
tion coefficients. Define s,. = L (p; · n)/(LB · t:)J , 1 s is n. It is easy to 
see thats,.= (q,.·n)l(LB·t:). Oearly, the su»s corresponding to the solu-
tions of I' and I" will have the same number of tuples. (r, t) is a tuple 
in an S Cil for I' iff ((r · n)/(LB · 1:), t), is a tuple in the SCi> for I" . Hence, 
the time needed to solve I' is the same as that needed to solve /". Since 
p; ::5 LB, it follows thats,. ::5 L n/ 1: J . Hence 
i 
1sco1 s 1 + .E s1 s 1 + iln!1:J 
1=1 
and so 
n-1 
n-1 
.E I scn1 ::5 n + .E il nlt:J 
= O(n31t:). 
1=0 
1=0 
Thus, if we can go from s(i-l) to sen in O(I S(i-1) I) time then I" and hence 
I' can be solved in O(n 311:) time. Moreover, the solution for I' will be 
an 1:-approximate solution for I and we would thus have a fully polynomial 
time approximation scheme. When using rounding, we will actually solve 
I" and use the resulting optimal solution as the solution to I. 
Example 12.14 Consider the 0/1 knapsack problem of Section S.S. While 
solving this problem by successively generating S co>, So>, ••• , S Cn> the 
feasible assignments for SCil may be represented by tuples of the form (r, t) 
where 
i 
i 
r = E p·x· and t = E W·X·. 
j=l 
J 
J 
J=l 
J 
J 
The dominance rule developed in Section S.S for this _problem is: (r i, t 1) 
dominates (r2 , t2 ) iff t1 ::5 t2 and r 1 ~ r2 • 
Let us solve the following instance of the 0/1 knapsack problem: n = S, 
M = 1112 and (p1, P2. p3, p4, Ps) = (w1, W2, W3, W4, Ws) = {1, 2, 10, 
100, 1000}. Since p; = W;, 1 ::5 i ::5 s, the tuples (r, t) in su>, 0 ::5 i ::5 s 

590 
Approximation Algorithms for NP-Hard Problems 
will have r = t. Consequently, it is necessary to retain only one of the two 
coordinates r, t. The sen obtained for this instance are: sco) = { 0 }; so> = 
{o 1}· s(2) = {O 1 2 3}· S(J) = {O 1 2 3 10 11 12 13}· sc4> = 
' 
' 
' ' ' 
' 
' ' ' ' 
' 
' 
' 
' 
{O, 1, 2, 3, 10, 11, 12, 13, 100, 101, 102, 103, 110, 111, 112, 113}; scs> = 
{ 0, 1, 2, 3, 10, 11, 12, 13, 100, 101, 102, 103, 110, 111, 112, 113, 1000, 
1001, 1002, 1003, 1010, 1011, 1012, 1013, 1100, 1101, 1102, 1103, 1110, 
1111, 1112}. 
The optimal solution has value E p;x; = 1112. 
Now, let us use rounding on the above problem instance to find an ap-
proximate solution with value at most 10% less than the optimal value. 
We thus have f = 1110. Also, we know that F*(I) ~ LB ~ max{p..} = 
1000. The problem I" to be solved is: n = 5, M = 1112, (s i, s 2, s 3, s 4, s s) 
= (0, 0, 0, 5, 50) and (Wi., W2, W3, W4, Ws) = (1, 2, 10, 100, 1000). 
Hence, sco> =so>= SC2) = SCJ) = {_(O, 0)}; SC4) = {(0, 0), (5, 100)}; SCS) = 
{ (0, 0), (5, 100), (50, 1000), (55, 1100) }. 
The optimal solution is (x1 , x 2 , x3 , x4 , x5 ) = (0, 0, 0, 1, 1). Its value 
in I" is 55 and in the original problem 1100. The error (F*(I) -
F(I))I 
F*(I) is therefore 12/1112 < 0.011 < €. At this time we see that the 
solution may be improved by setting either x 1 = 1 or x2 = 1 or x3 = 1. D 
Rounding as described in its full generality results in O(n3 IE) time ap-
proximation schemes. It is possible to specialize this technique to the spe-
cific problem being solved. In particular, we can obtain specialized and 
asymptotically faster polynomial time approximation schemes for the knap-
sack problem as well as for the problem of scheduling tasks on two proces-
sors to minimize finish time. The complexity of the resulting algorithms is 
O(n(log n + 1/€2 )). 
Let us investigate the specialized rounding scheme for the 0/1 knapsack 
problem. Let I be an instance of this problem and let f be the desired 
accuracy. Let P*(I) be the value of an optimal solution. First, a good 
estimate UB for P*(I) is obtained. This is done by ordering the n objects 
in/ such thatp/w; ~ p;+ 1/w;+i, 1 s i < n. Next, we find the largest} 
such that l:11w; s M. lfj = n, then the optimal solution is x; = 1, 1 s i 
s n and P*(J) = E p ;. So, assume} < n. Define UB = r.1;+ 1 p ;. We can 
show 1h UB s P*(I) < UB. The inequality P*(I) < UB follows from the 
ordering on p jw ;. The inequality 1h UB s P*(I) follows from the observa-
tion that 

Polynomial Time Approximation Schemes 
591 
Now, let o = UB*E 219. Divide the n objects into 2 classes BIG and 
SMALL. BIG includes all objects with Pi > f UB/3. SMALL includes all 
other objects. Let the number of objects in BIG be r. Replace each Pi in 
BIG by qi such that qi = Lp/oj . (This is the rounding step.) The knap-
sack problem is solved exactly using these r objects and the q ;' s. 
Let scr> be the set up tuples resulting from the dynamic programming 
algorithm. For each tuple (x, y) E S(r) fill the remaining space M -
y by 
considering the objects in SMALL in nondecreasing order of p/w;. Use 
the filling that has maximum value as the answer. 
Example 12.15 Consider the problem instance of Example 12.14. n = 5, 
(pi, p2, p3, p4, Ps) = (wi, W2, W3, W4, Ws,) = (1, 2, 10, 100, 1000), M = 
1112 and E = 
1/io. The objects are already in nonincreasing order of 
pjwi. For this instance, UB = Ej Pi = 1113. Hence, o = 3.7113 and 
f UB/3 = 37.1. SMALL, therefore, includes objects 1, 2 and 3. BIG = 
{ 4, 5}. q 4 = 
LP Jo J = 94 and q s = 
LP sf o J = 946. Solving the knap-
sack instance n = 2, M = 1112, (q 4, w 4) = (94, 100) and (q s, w s) = 
(946, 1000), we obtain: sc0> = { (0, 0) }; so> = { (0, 0), (94, 100)} and sc2> 
= { (0, 0), (94, 100), (946, 1000), (1040, 1100) }. Filling (0,0) from SMALL, 
we get the tuple (13, 13). Filling (94, 100), (946, 1000) and (1040, 1100) 
yields the tuples (107, 113), (959, 1013) and (1043, 1100) respectively. The 
answer is given by the tuple (1043, 1100). This corresponds to (xi, x 2, x 3, 
X4,Xs) = (1, 1, 0, 1, 1) and Ep;Xi = 1103. 
D 
An exercise explores a modification to the basic rounding scheme illus-
trated in the above example. This modification results in "better" solutions. 
Theorem 12.15 [Ibarra and Kim] The algorithm just described is an 
€-approximate algorithm for the Oil-knapsack problem. 
Proof: The proof may be found in the paper by Ibarra and Kim which is 
cited at end of this chapter. 
D 
The time needed to initially sort according to p/wi is O(n log n). UB 
can be computed in O(n) time. Since P*(I) ~ UB, there are at most 
UB/o = 9/€ 2 tuples in any Sil in the solution of BIG. The time to obtain 
S'> is therefore O(r/f 2 ) ~ O(nlE 2 ). Filling each tuple in scr> with objects 
from SMALL takes O(I SMALL I) time. I s(r) I~ 9/€ 2 and so the total time 
for this step is at most O(nlE 2). The total time for the algorithm is there-
fore O(n(log n + 11€ 2 )). A faster approximation scheme for the knapsack 
problem has been obtained by Lawler (see the references). His scheme also 
uses rounding. 

592 
Approximation Algorithms for NP-Hard Problems 
Interval Partitioning 
Unlike rounding, interval partitioning does not transform the original prob-
lem instance into one that is easier to solve. Instead, an attempt is made to 
solve the problem instance I by generating a restricted class of the feasible 
. 
t 
£ 
sco) 
s 0 > 
s(n) Let p b th 
. 
'"'1 
ass1gnmen s ior 
, 
, ... , 
. 
; e 
e maximum '-'J= 1 p1x1 
amongst all feasible assignments generated for sen . Then the profit interval 
(0, P1] is divided into subintervals each of size P;El(n -
1) (except pos-
sibly the last interval which may be a little smaller). All feasible assign-
ments in S(i) with Ej= 1 p1x1 in the same subinterval are regarded as having 
the same Ej.1 p1x1 and the dominance rules are used to discard all but 
one of them. The S Cil resulting from this elimination is used in the genera-
tion of sc1+ 1>. Since the number of subintervals for each S(i) is at most 
r n/€ l + 1, IS(ill :s r n/€ l + 1. Hence, ETIS(ill = O(n 21€). 
The error introduced in each feasible assignment due to this elimination 
in S c1> is less than the subinterval length. This error may however propa-
gate from so> up through sen>. However, the error is additive. Let F(I) be 
the value of the optimal generated using interval partitioning, and F*(J) 
the value of a true optimal. It follows that 
F*(J) - F(I) :s (€ ~g P.)l(n -
1). 
Since P1 :s F*(I), it follows that (F*(I) -
F(I))I F*(I) :s E, as desired. 
In many cases the algorithm may be speeded by starting with a good 
estimate, LB for F*(I) such that F*(I) ~ LB. The subinterval size is then 
LB·El(n -
1) rather than P1d(n -
1). When a feasible assignment with 
value greater than LB is discovered, the subinterval size can be chosen as 
described above. 
Example 12.16 Consider the same instance of the 0/1 knapsack problem 
as in Example 12.14. € = 1110 and F*(J) ~ LB ~ 1000. We can start 
with a subinterval size of LB·El(n -
1) = 1000/40 = 25. Since all tuples 
(p, t) in S(i) have p = t, only p will be explicitly retained. The intervals 
are [O, 25), (25, 50), ... etc. Using interval partitioning we obtain: sc0> = 
SCI) = s<2> = S(J) = {O}; sc4> = {O, 100}; scs) = {O, 100, 1000, 1100}. 
The best solution generated using interval partitioning is (xi. x 2, x 3, x 4, 
x s) = (0, 0, 0, 1, 1) and its value F(I) is 1100. (F*(J) -
F(J))IF*(I) = 
12/1112 < 0.011 < f. Again, the solution value may be improved by using 
a heuristic to change some of the x ;' s from 0 to 1. 
D 

Fully Polynomial Time Approximation Schemes 
593 
Separation 
Assume that in solving a problem instance I, we have obtained an sen with 
feasible solutions having the following Eis.;s,. p1x1 : 0, 3.9, 4.1, 7.8, 8.2, 
11.9, 12.1. Further assume that the interval size P,.d(n -
1) is 2. Then 
the subintervals are [O, 2), [2, 4), [4, 6), [6, 8), [8, 10), [10, 12) and [12, 14). 
Each value above falls in a different subinterval and so no feasible assign-
ments are eliminated. However, there are three pairs of assignments with 
values within P;El(n -
1). If the dominance rules are used for each pair, 
only 4 assignments will remain. The error introduced is at most P;El(n - 1). 
More formally, let ao, a1 , a2 , ••• , a, be the distinct values of E5_ 1 p1x1 in 
S(i). Let us assume a0 < a1 < a2 • • • < a,. We will construct a new set 
J from S(i) by making a left to right scan and retaining a tuple only if its 
value exceeds the value of the last tuple in J by more than P,.d(n -
1). 
This is described by the following algorithm: 
J -
assignment corresponding to a 0; XP - a 0 
forj -
1 tor do 
ifa1 > XP + P;d(n -
1) 
endif 
repeat 
then put assignment corresponding to a 1 into J 
XP -a1 
The preceding algorithm assumes that the assignment with less profit 
will dominate the one with more profit in case we regard both assignments 
as yielding the same profit E p1x1• In case the reverse is true the algorithm 
can start with a, and work downwards. The analysis for this strategy is 
the same as that for interval partitioning. The same comments regarding 
the use of a good estimate for F*(I) hold here too. 
Intuitively one may expect separation to always work better than interval 
partitioning. The following example illustrates that this need not be the 
case. However, empirical studies with one problem indicate interval par-
titioning to be inferior in practice. 
Example 12.17 Using separation on the data of Example 12.14 yields the 
same S(i) as obtained using interval partitioning. We have already seen 
an instance where separation performs better than interval partitioning. 
Now, we shall see an example where interval partitioning does better than 
separation. Assume that the subinterval size LB·El(n -
1) is 2. Then the 

594 
Approximation Algorithms for NP-Hard Problems 
intervals are [O, 2), [2, 4), [4, 6) · · · etc. Assume further that (p1 , p 2 , p3, 
p4, p 5) = (3, 1, 5.1, 5.1, 5.1). Then, following the use of interval par-
titioning we have: sco> = {O}; so> = {O, 3}; sc2> = {O, 3, 4}; sc3> = 
{O, 3, 4, 8.1}; sc4> = {O, 3, 4, 8.1, 13.2}; scs> = {O, 3, 4, 8.1, 13.2, 18.3}. 
Using separation with LB·El(n -
1) = 2 we have: sc0> = {O}; so> = 
{O, 3}; S2 = {O, 3}; SCJ) = {O, 3, 5.1, 8.1}; sc4> = {O, 3, 5.1, 8.1, 10.2, 
13.2}; scs> = {O, 3, 5.1, 8.1, 10.2, 13.2, 15.3, 18.3}. 
D 
In order to compare the relative performance of interval partitioning 
(I) and separation (S), a simulation was carried out. We used the job 
sequencing with deadlines problem as the test problem. Algorithms for I 
and S were programmed in FORTRAN and run on a CDC CYBER 74 com-
puter. Both algorithms were tested with f 
= 0.1. Three data sets were 
used: (pi = profit; ti = processing time needed; di = deadline). 
Data Set A: random profits pi E [1, 100], ti = pi and di = E7 t ;12. 
Data Set B: random Pi E [l, 100]; ti = Pi and random di E [ti, ti + 
25n] 
Data Set C: random Pi E [l, 100]; random ti E [l, 100] and random 
diE [ti, ti+ 25n]. 
The program had a capacity to solve all problems generating no more 
than 9000 tuples (i.e., E3 ISCill 
::5 9000). For each data set an attempt 
was made to run 10 problems of size 5, 15, 25, 35, 45, .... Figure 12.11 
summarizes the results. 
The exercises examine some of the other problems to which these tech-
niques apply. It is interesting to note that one may couple existing heu-
ristics to the approximation schemes that result from the above three tech-
niques. This is because of the similarity in solution procedures for the 
exact and approximate problems. In the approximation algorithms of 
Sections 12.2-12.4 it is usually not possible to use existing heuristics. 
At this point, one might well ask the question: What kind of NP-hard 
problems can have fully polynomial time approximation schemes? Qearly, 
no NP-hard €-approximation problem can have such a scheme unless 
P = NP. A stronger result may be proven. This stronger result is that the 
only NP-hard problems that can have fully polynomial time approximation 
schemes (unless P = NP) are those which are polynomially solvable if 
restricted to problem instances in which all numbers are bounded by a 
fixed polynomial in n. Examples of such problems are the knapsack and 
job sequencing with deadlines problems. 

Fully Polynomial Time Approximation Schemes 
595 
Data Set 
A 
B 
c 
Total number of problems solved 
80 
30 
30 
Number of optimal solutions generated 
S4 
20 
16 
by I 
Number of optimal solutions generated 
S3 
18 
14 
bys 
Average fracitonal error in nonoptimal 
.0025 
.0047 
.0040 
solutions by I 
Average fractional error in nonoptimal 
.0024 
.0047 
.0040 
solutions by S 
Number of I solutions better than S 
3 
7 
9 
Number of S solutions better than I 
1 
7 
6 
Figure 12.11 Relative performance of I and S 
Definition [Garey and Johnson] Let L be some problem. Let I be an in-
stance of L and let LENGTH(!) be the number of bits in the representation 
of I. Let MAX(/) be the magnitude of the largest number in/. Without 
loss of generality, we may assume that all numbers in I are integer. For 
some fixed polynomial p let LP be problem L restricted to those instances 
I for which MAX(!) ~ p(LENGTH(J)). Problem L is strongly NP-hard 
iff there exists a polynomial p such that LP is NP-hard. 
Examples of problems that are strongly NP-hard are: Hamiltonian cycle; 
node cover; feedback arc set; traveling salesperson, max-clique, etc. The 
0/1 knapsack problem is probably not strongly NP-hard (note that there 
is no known way to show that a problem is not strongly NP-hard) as when 
MAX(!) ~ p(LENGTH(J)) then I can be solved in time O(LENGTH(J) 2* 
p(LENGTH(J))) using the dynamic programming algorithm of Section S.S. 
Theorem 12.16 [Garey and Johnson] Let L be an optimization problem 
such that all feasible solutions to all possible instances have a value that is 
a positive integer. Fuiiher, assume that for all instances I of L, the optimal 
value F*(J) is bounded by a polynomial function p in the variables 
LENGTH(!) and MAX(!), i.e., 0 < F*(I) < p(LENGTH(J), MAX(!)) and 
F*(I) is an integer. If L has a fully polynomial time approximation scheme, 
then L has an exact algorithm of complexity a polynomial in LENGTH(!) 
and MAX(!). 

• 
596 
Approximation Algorithms for NP-Hard Problems 
Proof: Suppose L has a fully polynomial time approximation scheme. We 
shall show how to obtain optimal solutions to L in polynomial time. Let 
I be any instance of L. Define <= 
= 1/p(LENGTH(J), MAX(!)). With 
this<:, the approximation scheme is forced to generate an optimal solution. 
To see this, let F(l) be the value of the solution generated. Then, 
I F*(l) - F(l) I ::5 tF*(l) ::5 F*(l)I p(LENGTH(J), MAX(!)) < 1 
Since, by assumption all feasible solutions are integer valued, F*(l) 
F(l). Hence, with this <:, the approximation scheme becomes an exact 
algorithm. 
The complexity of the resulting exact algorithm is easy to obtain. Let 
q(LENGTH(J), 11 t) be a polynomial such that the complexity of the ap-
proximation scheme is O(q(LENGTH(J), lit)). The complexity of this 
scheme when <: is chosen as above is O(q(LENGTH(J), p(LENGTH(J), 
MAX(!))) which is O(q '(LENGTH(!), MAX(!))) for some polynomial q '.D 
When Theorem 12.16 is applied to integer valued problems that are NP-
hard in the strong sense, we see that no such problem can have a fully 
polynomial time approximation scheme unless P = NP. The above theorem 
also tells us something about the kind of exact algorithms obtainable for 
strongly NP-hard problems. A pseudo-polynomial time algorithm is one 
whose complexity is a polynomial in LENGTH(/) and MAX(!). The dynamic 
programming algorithm for the knapsack problem (Section 5.5) is a pseudo-
polynomial time algorithm. No strongly NP-hard problem can have a pseudo 
polynomial time algorithm unless P = NP. 
12.6 PROBABILISTICALLY GOOD ALGORITHMS 
The approximation algorithms of the preceding sections had the nice 
property that their worst case performance could be bounded by some con-
stants (kin the case of an absolute approximation and<: in the case of an 
<:-approximation). The requirement of bounded performance tends to 
categorize other algorithms that "usually work well" as being bad. Some 
algorithms with unbounded performance may in fact "almost always" 
either solve the problem exactly or generate a solution that is "exceedingly 
close" in value to the value of an optimal solution. Such algorithms are 
"good" in a probabilistic sense. If we pick a problem instance I at random 
then there is a very high probability that the algorithm will generate a very 
good approximate solution. In this section we shall consider two algorithms 
with this property. Both algorithms are for NP-hard problems. 

Probabilistically Good Algorithms 
597 
First, since we shall be carrying out a probabilistic analysis of the algo-
rithms we need to define a sample space of inputs. The sample space is 
set up by first defining a sample space Sn for each problem size n. Prob-
lem instances of size n are drawn from Sn. Then, the overall sample space 
is the infinite Cartesian product s l x s 2 x s 3 x . . . x s n • • • • An ele-
ment of the sample space is a sequence X = x1 , x2 , ••• , Xn , • • • such that 
X; is drawn from S;. 
Definition [Karp] 
An algorithm a solves a problem L almost every-
where (abbreviated a. e.) if, when X = x 1 , x2 , ••• , Xn, ••• is drawn from 
the sample space S1 x S2 x S3 x . . . x Sn, ... , the number of X; on 
which the algorithm fails to solve L is finite with probability 1. 
Since both the algorithms we shall be discussing are for NP-hard graph 
problems, we shall first describe the sample space for which the probabilis-
tic analysis will be carried out. Let p(n) be a function such that 0 :s p(n) 
:s 1 for all n ~ 0. A random n vertex graph is constructed by including 
edge (i,j), i .,t. j, with probability p(n). 
The first algorithm we shall consider is due to Posa. This is an algorithm 
to find a Hamiltonian cycle in an undirected graph. Informally, Posa's 
algorithm proceeds as follows. First, an arbitrary vertex (say vertex 1) is 
chosen as the start vertex. The algorithm maintains a simple path P starting 
from vertex 1 and ending at vertex k. Initially Pis a trivial path with k = 1, 
i.e., there are no edges in P. At each iteration of the algorithm an attempt 
is made to increase the length of P. This is done by considering an edge 
(k, j) incident to the end point k of P. When edge (k, j) is being considered, 
one of three possibilities exist: 
(i) [j = 1 and path P includes all the vertices of the graph] 
In this case a Hamiltonian cycle has been found and the algorithm 
terminates. 
(ii) U is not on the path P] 
In this case the length of path Pis increased by adding (k, j) to it. 
jbecomes the new end point of P. 
(iii) [j is already on path P] 
Now there is a unique edge e = (j, m) in P such that deletion of e 
and the inclusion of (k, j) to P results in a simple path. e is deleted 
and (k, j) added to P. Pis now a simple path with endpoint m. 
The algorithm is constrained so that case (iii) does not generate two 
paths of the same length having the same end point. With a proper choice 

598 
Approximation Algorithms for NP-Hard Problems 
of data representations, this algorithm can be implemented to run in time 
O(n2 ) where n is the number of vertices in the graph G. It is easy to see 
that this algorithm does not always find a Hamiltonian cycle in a graph 
that contains such a cycle. However, Posa has shown the following: 
Theorem 12.17 [Posa] If p(n) """ (a In n/n), a > 1 then the preceding 
algorithm finds a Hamiltonian cycle (a. e.). 
Proof: See the paper by Posa. 
D 
Example 12.18 Let us try out the above algorithm on the five vertex 
graph of Figure 12.12. The path P initially consists of vertex 1 only. Assume 
edge (1, 4) is chosen. This represents case (ii) and Pis expanded to {1, 4 }. 
Assume edge (4, S) is chosen next. Path P now becomes {1, 4, S}. Edge 
(1, S) is the only possibility for the next edge. This results in case (iii) and 
Pbecomes {1, S, 4}. Now assume edges (4, 3) and (3, 2) are considered. 
Pbecomes {1, S, 4, 3, 2}. If edge (1, 2) is next considered, a Hamiltonian 
cycle is found and the algorithm terminates. 
D 
The next probabilistically good algorithm we shall look at is for the 
maximum independent set problem. A subset of vertices N of graph G(V, E) 
is said to be independent iff no two vertices in N are adjacent in G. Algo-
rithm 12.S is a greedy algorithm to construct a maximum independent set. 
procedure INDEP(V, £) 
N-<J> 
while there is av E(V-N) and v not adjacent to any vertex in N do 
N-NU{v} 
repeat 
return (N) 
end INDEP 
Algorithm 12.5 Finding an independent set 
One can easily construct examples of n vertex graphs for which INDEP 
generates independent sets of size 1 when in fact a maximum independent 
set contains n -
1 vertices. However, for certain probability distributions 
it can be shown that INDEP generates good approximations almost every-
where. If F*(J) and F(J) represent the size of a maximum independent 
set and one generated by algorithm INDEP, respectively, then the following 
theorem is obtained: 

Theorem 12.18 [Karp] If p(n) 
f > 0 we have: 
References and Selected Readings 
599 
c, for some constanct c, then for every 
(F*(J) - F(J))/ F*(J) ::; .S + f 
(a.e.). 
Proof: See the paper by Karp. 
D 
Figure 12.12 Graph for Example 12.18 
Algorithm INDEP can easily be implemented to have polynomial com-
plexity. Some other NP-hard problems for which probabilistically good 
algorithms are known are: Euclidean traveling salesperson, minimal 
colorings of graphs, set covering, maximum weighted clique and partition. 
REFERENCES AND SELECTED READINGS 
Note: Exercise numbers at the end of a reference indicate that these exercises are 
based on work reported in this reference. The reference, however, contains more 
results than covered by the cited exercises. 
Our terminology for absolute, f(n) and e-approximation algorithms is taken from 
the paper: 
"Combinatorial problems: reducibility and approximation," by S. Sahni and E. 
Horowitz, Op. Res., 26(4), 1978. 
The terms approximation scheme, polynomial time approximation scheme and 
fully polynomial time approximation scheme were coined by Garey and Johnson 
and used in their lecture on approximation algorithms which was presented at the 
Symposium on Algorithms and Complexity, Carnegie Mellon Institute, Pittsburgh, 
1976. Sahni pointed out that for the 0/1 knapsack problem the corresponding 

600 
Approximation Algorithms for NP-Hard Problems 
absolute approximation problem is also NP-hard. The following paper, contains the 
remark on the knapsack problem: 
"Approximate algorithms for the Oil-knapsack problem," by S. Sahni, /ACM, 22, 
pp. 115-124, 1975. [exercise 18). 
This paper also contains the polynomial time approximation scheme for the 
Oil-knapsack problem discussed in § 12.4. Several other absolute approximation 
problems are shown NP-hard in: 
"A computer scientist looks at reliability computations," by A. Rosenthal, in 
Reliability and Fault Tree Analysis, edited by J. Fussel and N. Singpurwalla, 
SIAM, 1975. 
The analysis of the LPT rule of Section 12.3 is due to R. Graham and appears in: 
"Bounds on multiprocessor timing anomalies," by R. Graham, SIAM Jr. on Appl. 
Math., 17(2), pp. 416-429, 1969. 
This paper also contains the polynomial time approximation scheme for sched-
uling independent tasks that was discussed in § 12.4. e-approximate bin packing 
algorithms may be found in: 
"Performance bounds for simple one dimensional bin packing algorithms," by 
D. Johnson, A. Demers, J. Ullman, M. Garey and R. Graham, SIAM Jr. on 
Comput., 3(4), pp. 299-325, 1974. 
An excellent bibliography on approximation algorithms is: 
"Approximation algorithms for combinatorial problems: an annotated bibliog-
raphy," by M. Garey and D. Johnson, in Algorithms and Complexity: Recent 
Results and New Directions, J. Traub, ed., Academic Press, 1976. 
Polynomial time e-approximate algorithms for many scheduling problems may 
be found in the following papers: 
"Scheduling independent tasks to reduce mean finishing time," by J. Bruno, 
E. Coffman and R. Sethi, CACM. 17(7), 382-387, 1974. 
"Algorithms for minimizing mean flow time," 
by 1. Bruno, E. Coffman and 
R. Sethi, Proc. IFIP Congr. 74, North Holland Pub. Co., Amsterdam, 1974, pp. 
504-510. 
"A level algorithm for preemptive scheduling," by E. Horvath, S. Lam and R. 
Sethi, /ACM, 24(1), pp. 32-43, 1977. 

References and Selected Readings 
601 
"Bounds on LPT schedules on uniform processors," by T. Gonzalez, O. Ibarra 
and S. Sahni, SIAM Jr. on Computing, 6(1), pp. 155-166, 1977 [exercises 4-6]. 
"Job shop and flow shop schedules: complexity and approximation," by T. Gonzalez 
and S. Sahni, Oper. Res., 26(1), pp. 36-52, 1978. 
"Heuristic algorithms for scheduling independent tasks on nonidentical processors," 
by 0. Ibarra and C. Kirn, JACM, 24(2), pp. 280-289, 1977 [exercises 8-10). 
A 0.5-approximate algorithm for the Euclidean traveling salesperson problem 
appears in: 
"Worst-case analysis of a new heuristic for the traveling salesman problem," by 
N. Christofedes, Manag. Sci. Res. Report #388, Carnegie Mellon University, 1976. 
e-approximate algorithms for other NP-hard problems appear in: 
"Approximation algorithms for some routing problems," by G. Fredrickson, M. 
Hecht and C. Kim, Proc. 17th An. Symp. on Found. of Comp. Sci., Houston, 
Texas, pp. 216-227, 1976. 
"Location of bank accounts to optimize float: an analytic study of exact and ap-
proximate algorithms," by G. Comuejols, M. Fisher and G. Nemhauser, Manag. 
Sd. 23(8), pp. 789-810, 1977. 
"An analysis of approximations for maximizing submodular set functions-II," by 
M. Fisher, G. Nemhauser and L. Wolsey, CORE discussion paper #7629, Universite 
Catholique De Louvain, Belgium, 1976. 
"An analysis of approximations for finding a maximum weight hamiltonian cir-
cuit," by M. Fisher, G. Nemhauser and L. Wolsey, CORE, University of Louvain, 
Belgium, 1977. 
"Code generation for expressions with common subexpressions," by A. Aho, 
S. Johnson and J. Ullman, /ACM, 24(1), pp. 146-160, 1977. 
Somef(n)-approximate algorithms appear in: 
"Approximation algorithms for combinatorial problems," by D. Johnson, JCSS, 
9, pp. 256-278, 1974. [exercises 11-16]. 
The approximation algorithm MSA T2 (exercise 12) for the maximum satisfiability 
problem has also been studied by K. Lieberherr. The weight of a CNF formula F 
is defined to be w(F) = E; 2 - I c; I where I C; I is the number of literals in the ith 
clause of F. He shows that MSAT2 leaves at most L w(F)j clauses unsatisfied. 
This result together with a generalization of MSA T2 appears in the paper: 

602 
Approximation Algorithms for NP-Hard Problems 
"Interpretations of 2-satisfiable conjunctive normal forms," by K. Lieberherr, 
Florida State Univ., Tallahassee, to appear in /ACM. 
Lieberherr has also considered the notion of an optimal polynomial time approxi-
mation algorithm. A polynomial time approximation algorithm is optimal iff the 
problem of guaranteeing better solutions is NP-hard. MSAT2 (exercise 12) as well 
as some heuristics for other NP-hard problems are shown optimal in the following 
paper: 
"Optimal heuristics for combinatorial optimization problems," by K. Lieberherr, 
Florida State University, Tallahassee, 1978. 
Sahni and Gonzalez were the first to show the existence of NP-hard e·approxi-
mate problems. Their results appear in the paper: 
"P-complete approximation problems," by S. Sahni and T. Gonzalez, /ACM, 23, 
pp. 555-565, 1976. [exercises 20-26, 29) 
Garey and Johnson have shown that the e-approximate graph coloring problem 
is NP-Hard fore < 1. Their result appears in the paper: 
"The complexity of near optimal graph coloring," by M. Garey and D. Johnson, 
/ACM, 23, pp. 43-49, 1976. 
Some other NP-hard e-approximate problems appear in: 
"Traversal marker placement problems are NP-complete," by S. Maheshwari, 
University of Colorado, Technical report #CU-CS-092-76, 1976. 
"Code generation for short/long address machines," by E. Robertson, Univ. of 
Wisconsin, MRC Report #1779, 1977. 
A polynomial time approximation scheme for submodular set functions appears 
in: 
"Best algorithms for approximating the maximum of a submodular set function," 
by G. Nemhauser and L. Wolsey, CORE discussion paper #7636, Universite Catho· 
lique De Louvain, Belgium, 1976. 
An approximation scheme for scheduling tasks with precedence constraints ap· 
pears in: 
"Scheduling for maximum profits/minimum time," by O. Ibarra and C. Kim, 
Math. of Oper. Res., to appear. 

References and Selected Readings 
603 
Ibarra and Kim were the first to discover the existence of fully polynomial time 
approximation schemes for NP-hard problems. Their work appears in the paper: 
"Fast approximation algorithms for the knapsack and sum of subsets problems," 
by 0. Ibarra and C. Kim, /ACM, 22, pp. 463-468, 1975. [exercises 27-28) 
This paper develops the O(n(log n + l/e 2)) algorithm for the 0/1 knapsack 
problem. An approximation scheme for the integer knapsack problem is also de-
veloped. E. Lawler has improved upon these schemes. He has obtained O(n log(l/ e) 
+ l/e 4 ) and O(n + l/e 3 ) schemes for the knapsack and sum of subset problems. 
Lawler's work appears in the paper: 
"Fast approximation algorithms for knapsack problems," by E. Lawler, Proc. 18th 
Ann. Symp. on Foundatz'ons of Computer Science, Rhode Island, pp. 206-213, 
1977. 
Fully polynomial time approximation schemes for many scheduling problems 
appear in the papers: 
"Algorithms for scheduling independent tasks," by S. Sahni, /ACM, 23, pp. 114-
127, 1976. [exercises 30, 31, 33-37, 39-41). 
"Exact and approximate algorithms for scheduling nonidentical processors," by 
E. Horowitz and S. Sahni, /ACM, 23, pp. 317-327, 1976. [exercises 31and38) 
Our discussion of the general techniques: rounding, interval partitioning and 
separation is from the paper: 
"General techniques for combinatorial approximation," by S. Sahni, Oper. Res., 
25(6), pp. 920-936, 1977. 
The notion of strongly NP-hard is due to Garey and Johnson. Theorem 12.16 
is also due to them and appears in: 
""Strong" NP-Completeness results: motivation, examples and implications," by 
M. Garey and D. Johnson, Bell Laboratories Report, Murray Hill, 1976. 
The discussion on probabilistically good algorithms is based on the following 
papers: 
"The probabilistic analysis of some combinatorial search algorithms," by R. Karp, 
University of California, Berkeley, Memo No. ERL-M581, April 1976. 
"The fast approximate solution of hard combinatorial problems," by R. Karp, 

604 
Approximation Algorithms for NP-Hard Problems 
Proc. Sixth Southeastern Conj. on Combinatorics, Graph Theory, and Computing, 
Winnipeg, 1975. 
"Hamiltonian circuits in random graphs," by L. Posa, Discrete Mathematics, 14, 
pp. 359-364, 1976. 
"Probabilistic analysis of partitioning algorithms for the traveling salesman prob-
lem in the plane," by R. Karp, Math. of Oper. Res., 2(3), pp. 209-224, 1977. 
The following paper contains a "good" algorithm for the general traveling sales-
person problem. The algorithms have worked well on all problem instances tested. 
However, no statistical or probabilistic analysis has been made. 
"An effective heuristic algorithm for the traveling salesman problem," by S. Lin 
and P. Kernighan, Operations Research, 21(2), 1973, 498-516. 
Analysis of other probabilistically good algorithms appear in: 
"Fast Probabilistic algorithms for hamiltonian circuits and matchings," by D. 
Angluin and L. Valiant, Proc. 9th Annual Symp. on Theo. of Computing, pp. 
30-41, 1977. 
"Analysis of the expected performance of algorithms for the partition problem," by 
C. Kim, Technical Report, University of Maryland, 1976. 
"Maximization problems on graphs with edge weights chosen from a normal distri-
bution," by G. Lueker Proc. 10th Annual Sy mp. on Theo. of Computing, pp. 13-18, 
1978. 
EXERCISES 
1. 
The following NP-hard problems were defined in either Chapter 11 or 12. For 
each of these defined in the exercises, the exercise number appears in paren-
thesis. For each of these problems, clearly state the corresponding absolute 
approximation problem. (Some of the problems listed below were defined as 
decision problems. For these, there correspond obvious optimization prob-
lems that are also NP-hard. The absolute approximation problem is to be 
defined relative to the corresponding optimization problem.) Also, show that 
the corresponding absolute approximation problem is NP-hard. 
i) Node Cover 
ii) 
Set Cover (ex. 11.20) 
iii) 
Set Packing (ex. 11.43) 
iv) 
Feedback Node Set 
v) 
Feedback Arc Set (ex. 11.11) 

Exercises 
605 
vi) Chromatic Number 
vii) Clique Cover (ex. 11.19) 
viii) Max-Independent Set (see Section 12.6) 
ix) Nonpreemptive scheduling of independent tasks to minimize finish time 
on m > 1 processors (Section 12.3) 
x) Flow shop scheduling to minimize finish time (m > 2) 
xi) Job shop scheduling to minimize finish time (m > 1) 
2. 
Obtain an O(n log n) algorithm that implements the LPT scheduling rule. 
3. 
Show that LPT schedules are optimal for all task sets that have an optimal 
schedule in which no more than two tasks are assigned to any processor. 
4. 
A uniform processor system is a set of m <!: 1 processors. Processor i operates 
at a speed s;, s; > 0. If task i requires t; units of processing then, it may be 
completed in t ;Is; units of real time on processor p;. When s; = 1, 1 s is m 
we have a system of identical processors (section 12.3). An MLPT schedule is 
defined to be any schedule obtained by assigning tasks to processors in order 
of nonincreasing processing times. When a task is being considered for assign-
ment to a processor, it is assigned to that processor on which its finishing 
time will be earliest. Ties are broken by assigning the task to the processor 
with least index. 
a) Let m = 3, s1 = 1, s2 = 2 and s3 = 3. Let the number of tasks n be 6. 
(t1, t2, t3 , t 4, t 5, t6) = (9, 6, 3, 3, 2, 2). Obtain the MLPT schedule for 
this set of tasks. Is this an optimal schedule? If not obtain an optimal 
schedule. 
b) Show that there exists a two processor system and a set /for which I F*(J) 
-
F(I) I I F*(I) > 1/3 -
l/(3m). F(I) is the finish time of the MLPT 
schedule. Note that 1/3 -
l/(3m) is the bound for LPT schedules on 
identical processors. 
c) 
Write an algorithm to obtain MLPT schedules. What is the time com-
plexity of your algorithm? 
5. 
Let I be any instance of the uniform processor scheduling problem. Let F(J) 
and F*(J) respectively be the finish times of MLPT and optimal schedules. 
Show that F(J)/ F*(J) s 2m/(m + 1) (see exercise 4). 
6. 
For a uniform processor system (see exercises 4 and 5) show that when m = 2, 
F(l)I F*(J) s (1 + v'i7)/4. Show that this is the best possible bound for 
m = 2. 
7. 
Let P1, ••• , Pm be a set of processors. Let tiJ• t;J > 0 be the time needed to 
process task i if its processing is carried out on processor Pj, 1 s is n, 1 s 
j s 
m. For a uniform processor system, t;JI t;,k = ski sj where sk and sj are 

606 
Approximation Algorithms for NP-Hard Problems 
the speeds of Pk and P1 respectively. In a system of nonidentical processors, 
such a relation need not exist. As an example, consider n = 2, m = 2 and 
[
tn t1i] = [1 21 . 
ti1 
tii 
3 2J 
If task 1 is processed on Pi and task 2 on P1, then the finish time is 3. If 
task 1 is processed on P1 and task 2 on Pi, the finish time is 2. Show that 
if a schedule is constructed by assigning task i to processor j such that t;J :5 
t i.k. 1 :5 k :s; m then F(I) I F*(I) :5 m. F(I) and F*(I) are respectively the 
finish times of the schedule constructed and of an optimal schedule. Show that 
this bound is best possible for this algorithm. 
8. 
For the scheduling problem of Exercise 7, define procedure A as: 
procedwe A 
fJ - 0, 1 :5 j :5 m 
fori - 1 ton do 
k -
leastj such that/1 + t;.J ~ft+ tu, 1 :s; l ~ m 
fk -fk + t;.k 
print ('schedule task', i , 'on processor', k) 
repeat 
end A 
Algorithm 12.6 Scheduling 
fj is the current finish time on processor j. So, F(J) 
max 1 { fj }. Show that 
F(I) I F*(I) :5 m and this bound is best possible. 
9. 
In the above exercise, first order the tasks so that min1 { t i.J} <!: min1{ ti+ 1.1}. 
1 :5 i < n. Then use algorithm A. Show that F(I)I F*(I) :5 m and this 
bound is best possible. 
10. 
Show that the results of exercise 8 hold even if the initial ordering is such that 
max1{ti.J} <!: max1{t;+1,J}, 1 :5 i < n. 
11. 
The satisfiability problem was introduced in chapter 11. Define maximum 
satisfiability to be the problem of determining a maximum subset of clauses 
that can be satisifed simultaneously. If a formula has p clauses, then all p 
clauses can be simultaneously satisfied iff the formula is satisfiable. For pro· 
cedure MSAT, show that for every instance /, I F*(J) -
F(I) I I F*(I) :5 
1/(k + 1). k is the minimum number of literals in any clause of I. Show that 
this bound is best possible for this algorithm. 

Exercises 
607 
procedwe MSAT (I) 
//approximation algorithm for maximum satisfiability. I is a formula./ I 
I /Let x ;, 1 :5 i :5 n be the variables in I and let C ;, 1 :5 i :5 p be the/ I 
I I clauses .I I 
CL -
cf> 
//set of clauses simultaneously satisfiable// 
LEFT -
{ C; I 1 ::: i::: p} I /remaining clauses/ I 
UT - {x;, x; I 1 :5 i :5 n} //set of all literals// 
while LIT contains a literal occurring In a clause In LEFT do 
let y be a literal in LIT that is in the most clauses of LEFT. 
let R be the subset of clauses in LEFT that contain y 
CL -
CL U R;LEFT-LEFT - R 
UT - UT - {y, ji} 
repeat 
return (CL) 
end MSAT 
Algorithm 12. 7 Procedure for Exercise 11 
12. 
Show that if procedure MSA T2 is used for the maximum satisfiability problem 
of Exercise 11 then, I F*(J) -
F(I) I I F*(I) :S 112k where k, fr and F* are 
as in Exercise 11. 
procedure MSA12 (/) 
I I same function as MSA TI I 
w(i) -
2 - I c; I, 1 :S i :S p 
//weighting function IC k I 
number of/ I 
//literals in C ;// 
CL -
</>;LEFT - { C; I 1 s i s p} 
UT-{x;,x;I 1::: i$. n} 
while LIT contains a literal occurring in a clause in LEFT do 
let y E LIT be such that y occurs in a clause in LEFT 
let R be the subset of clauses in LEFT containing y 
let S be the subset of clauses in LEFT containing y 
if Ec;eR w(i) ~ Ec;es w(i)then CL -
CL U R 
enclH 
LIT- LIT - {y, ji} 
repeat 
return (CL) 
end MSAT2 
LEFT -
LEFT -
R 
w(i) -
2*w(i)foreach C;E S 
else CL -
CL U S 
LEFT -
LEFT -
S 
w(i) -
2*w(i)for each C;E R 
Algorithm 12.8 Procedure for Exercise 12 

608 
Approximation Algorithms for NP-Hard Problems 
13. Consider the set cover problem of Exercise 11.20. Show that if procedure 
SET_COVER is used for the optimization version of this problem then 
k 
F(l)I F*(J) s E (1/j) 
I 
where k is the maximum number of elements in any set. Show that this bound 
is best possible. 
procedure SET _co VER(F) 
I IS;, 1 s i s m are the sets in F. IS d is the number of elements in S ;./I 
11 I U S;I = nl / 
G -
US;; R; -
S;, 1 s is m 
CO V -
</> 
I I elements covered/ I 
T -
cf> 
I I cover being constructed/ I 
while GOV ;1! G do 
letRjbe such that IRjl ~ IRql, 1 sq s m 
GOV- COVU Ri; T- TU Si 
R; -
R; - Rj, 1 s is m 
repeat 
return (T) 
end SET _COVER 
Algorithm 12.9 Procedure for Exercise 13 
14. 
Consider a modified set cover problem (MSC) in which we are required to find 
a cover Tsuch that Ese Tl SJ is minimum. 
(a) Show that exact cover aMSC (see Exercise 11.21) 
(b) Show that procedure MSC is not an E·approximate algorithm for this 
problem for any E, E > 0. 
procedure MSC (F) 
I /same variables as in SET_COVER// 
T- cp;LEFT- {Sd 1 sis m}; G -
US; 
while G ;1! cf> do 
let S j be a set in LEFT such that 
Is i -
GI I Is j n GI s Is q -
GI I Is q n GI for alls q E LEFT 
T -
T U Sj; G -
G -
Si; LEFT -
LEFT -
Si 
repeat 
return (T) 
end MSC 
Algorithm 12.10 Procedure for Exercise 14 

Exercises 
609 
15. 
Consider the following heuristic for the max clique problem: i) delete from G 
a vertex that is not connected to every other vertex ii) repeat (i) until the re-
maining graph is a clique. Show that this heuristic does not result in an 
E·approximate algorithm for the max clique problem for any E, 0 < E < 1. 
16. 
For the max-clique problem, consider the following heuristic: (i) S -
</>, (ii) 
add to S a vertex not in S that is connected to all vertices in S. If there is no 
such vertex then stop with S the approximate max clique, otherwise repeat (ii). 
Show that the algorithm resulting from this heuristic is not an E-approximate 
algorithm for the max-clique problem for any E, E < 1. 
17. 
Show that procedure COLOR is not an E·approximate coloring algorithm for 
the minimum colorability problem for any E, E > 0. 
procedme COLOR ( G) 
I /G = (V, E) is a graph with I VI = n vertices. COL(i) is the color to use// 
I /for vertex i, 1 :$ i :$ nl I 
i -
1 I I next color to use/ I 
j -
0 //number of vertices colored/ I 
while j ;1! ndo 
S -
</> 
I /vertices colored with color ii I 
while there is an uncolored vertex, v, not adjacent to a vertex in S do 
COL(v) - i;S - SU {v};j-j + 1 
repeat 
i -
i + 1 
repeat 
return (COL) 
end COLOR 
Algorithm 12.11 Procedure for Exercise 17 
18. Show that if line 4 of Algorithm 12.3 is changed to PMAX -
max { PMAX, 
L(I. P, W. M, n)} and line 1 of procedure L replaced by the line 
S- O; 
i- 1; 
T-M 
then the resulting algorithm is not E-approximate for any E, 0 < E < 1. Note 
that the new heuristic constrains I to be outside the knapsack. The original 
heuristic constrains I to be inside the knapsack. 
19. 
Show that procedure INDEP of Section 12.6 is not an E-approximate algo-
rithm for the maximum independent set problem for any E, 0 < E < 1. 

610 
Approximation Algorithms for NP-Hard Problems 
20. 
Consider any tour for the traveling salesperson problem. Let city ii be the 
starting point. Assume the n cities appear in the tour in the order i 1, i 2, i 3, 
... , in, in+I = i1. Let l(i;, i;+d be the length of edge (i;, i;+1). The arrival 
time Yk at city ik is 
k-1 
Yk = .E l(i;. i;+ 1), 1 < ks n + 1 
J= I 
The mean arrival time Y is 
l n+I 
Y= -
E Y 
n k=2 
k 
Show that the f-approximate minimum mean arrival time problem is NP-hard 
for all E, E > 0. 
21. 
Let Y k and Y be as in Exercise 20. The variance, u, in arrival times is 
Show that the E·approximate minimum variance time problem is NP-Hard 
for all E, E > 0. 
22. 
An edge disjoint cycle cover of an undirected graph G is a set of edge disjoint 
cycles such that every vertex is included in at least one cycle. The size of such 
a cycle cover is the number of cycles in it. 
(a) Show that finding a minimum cycle cover of this type is NP-hard. 
(b) 
Show that the E-approximation version of this problem is NP-hard for 
all f, f > o. 
23. 
Show that if the cycles in Exercise 22 are constrained to be vertex disjoint then 
the problem remains NP-Hard. Show that the f-approximate version is NP-
hard for all E, E > 0. 
24. 
Consider the partitioning problem: 
Let G = (V, E) be an undirected graph. Letf: E -
Z be an edge weighting 
function and let w: V -
Z be a vertex weighting function. Let k be a fixed 
integer, k ~ 2. The problem is to obtain k disjoint sets S 1, ••. , S k such that: 
(a) 
US; = V 
(b) S;nS;=</> for i;t!j 
(c) 
Ejes; ..;,(i) s. W; 
1 ~ i~ k 

(d) 
k 
E 
E f ( u, v) is maximized 
i=I (u,v)EE 
U,V ES; 
Exercises 
611 
Wis a number which may vary from instance to instance. This partitioning 
problem finds application in the minimization of the cost of interpage refer-
ences between subroutines of a program. Show that the E-approximate version 
of this problem is NP-hard for all t, 0 < t < 1. 
2.5. 
Let G = ( V, E) be an undirected graph. Assume that the vertices represent 
documents. The edges are weighted such that w(i, j) is the dissimilarity be· 
tween documents i and j. It is desired to partition the vertices into k <!: 3 
disjoint clusters such that 
k 
E 
E 
w(u, v) 
i=I (u,v)EE 
u,v E Ci 
is minimized. C; is the set of documents in cluster i. Show that the E-approxi-
mate version of this problem is NP-hard for all E, E > 0. Note that k is a fixed 
integer provided with each problem instance and may be different for different 
instances. 
26. 
In one interpretation of the generalized assignment problem, we have m 
agents who have to perform n tasks. If agent i is assigned to perform task j 
then a cost ciJ is incurred. When agent i performs task j, r 9 units of his re· 
sources are used. Agent i has a total of b; units of resource. The objective 
is to find an assignment of agents to tasks such that the total cost of the 
assignment is minimized and such that no agent requires more than his total 
available resource to complete the tasks he is assigned to. Only one agent 
may be assigned to a task. 
Using XiJ to be a 0/1 variable such that XiJ = 1 if agent i is assigned to 
task j and x 9 = 0 otherwise, the generalized assignment problem may be 
formulated mathematically as: 
m 
minimize E 
i= 1 
II 
subject to E 
j= I 
ls ism 
E 
xiJ = 1, 
1 s js n 
i= I 
x 9· = 0or1, for all iandj 

612 
Approximation Algorithms for NP-Hard Problems 
The constraints E xii = 1 ensure that exactly one agent is assigned to each 
task. Many other interpretations are possible for this problem. 
Show that the corresponding E-approximation problem is NP-hard for all E, 
f > 0. 
C.onsider the O(n(log n + l/E 2 )) rounding algorithm for the 0/1 knapsack 
problem. Let s<rl be the final set of tuples in the solution of BIG. Show that 
no more than (9/E 2)/q; objects with rounded profit value q; can contribute 
to any tuple in S Cr>. From this, conclude that BIG can have at most (9/ E 2)1 q; 
objects with rounded profit value q ;. Hence, r :S E (9/ E 2)/ q; where q; is in the 
range [3/E, 9/E 2 ]. Now, show that the time needed to obtain scrJ is 0(811E 4 
In (3/ E)). Use the relation 
28. 
Write a SPARKS algorithm for the O(n(log n + 1/E2 )) rounding scheme 
discussed in § 12.5. When solving BIG use three tuples (P, Q, W) such that 
P = E p;x;. Q = E q;x; and W = E w;x;. Tuple (P1 , Q1 , W1) dominates 
(P2. Q2, W2 ) iff Q, <!: Qi and W1 :S W2• In case Q1 = Qi and W1 = W2 
then an additional dominance criteria may be used. In this case the tuple 
(P1, Q1 , W1) dominates (P2, Q2, W2 ) iff P1 > P2. Otherwise, (P2, Q2, W2 ) 
dominates (P1 , Q1 , W1 ). Show that your algorithm is of time complexity 
O(n(log n + 1/E 2)). 
29. 
Show that if we change the optimization function of Exercise 25 to maximize 
E 
w(u, v) 
uE Ci 
vfCi 
(u.v)EE 
then there is a polynomial time E-approximation algorithm for some E, 0 < 
f < 1. 
30. 
Use separation to obtain a fully polynomial time approximation scheme for 
the independent task scheduling problem when m = 2 (see Section 12.4). 
31. Do Exercise 30 for the case when the two processors operate at speeds s 1 and 
s 2, s 1 ;1! s 2. See Exercise 4. 
32. 
Do Exercise 30 for the case when the two processors are nonidentical (see 
Exercise 5). 
33. 
Use separation to obtain a fully polynomial time approximation algorithm for 
the job sequencing with deadlines problem. 

Exercises 
613 
34. 
Use separation to obtain a fully polynomial time approximation scheme for 
the problem of obtaining two processor schedules with minimum mean 
weighted finish time (see Section 11.4). Assume that the two processors are 
identical. 
35. 
Do Exercise 34 for the case when a minimum mean finish time schedule that 
has minimum finish time amongst all minimum mean finish time schedules is 
desired. Again, assume two identical processors. 
36. 
Do Exercise 30 using rounding. 
37. 
Do Exercise 31 using rounding. 
38. 
Do Exercise 32 using rounding. 
39. 
Do Exercise 33 using rounding. 
40. 
Do Exercise 34 using rounding. 
41. 
Do Exercise 35 using rounding. 
42. 
Show that the following problems are strongly NP~hard 
i) 
Max Oique 
ii) Set Cover 
iii) Node Cover 
iv) Set Packing 
v) Feedback Node Set 
vi) Feedback Arc Set 
vii) Chromatic Number 
viii) Oique Cover 

APPENDIX A: SPARKS 
This section is meant for people who do most of their programming in 
FORTRAN. FORTRAN has the distinction of being essentially the 
earliest higher level programming language, developed about 1957 by 
a group at IBM. Since then it and its derivatives have become established 
as the primary language for scientific and engineering computation. But, 
with our greater understanding of the process of creating programs has 
come a realization of the deficiencies of FORTRAN. Creating a program 
is properly thought of as taking a real world problem and translating 
it into a computer solution. Concepts in the real world such as a geneology 
tree or a queue of airplanes must be translated into computer concepts. 
A language is good if it enables one to describe these abstractions of 
the real world in a natural way. Perhaps because of its very early 
development, FORTRAN lacks many such features. In this appendix 
we explore the idea of writing a preprocessor for FORTRAN which 
inexpensively adds some of these missing features. 
A preprocessor is a program which translates statements written in 
a language X into FORTRAN. In our case Xis called SPARKS. Such 
a program is normally called a compiler so why give it the special name 
preprocessor? A preprocessor is distinguished from a compiler in the 
following way: the source and target language have many statements 
in common . 
. Such a translator has many c:dvantages. Most importantly it preserves 
a close connection with FORTRAN. Despite FORTRAN's many negative 
attributes, it has several practical pluses: I) it is almost always available 
and compilers are often good, 2) there is a language standard which 
allows a degree of portability not obtainable with other languages, 3) 
there are extensive subroutine libraries, and 4) there is a large labor 
force familiar with it. These reasons give FORTRAN a strong hold 
in the industrial marketplace. A structured FORTRAN translator pre-
serves these virtues while it augments the language with improved 
syntactical constructs and other useful features. 
Another consideration is that at many installations a nicely structured 
language is unavailable. In this event a translator provides a simple 
means for supplementing an existing FORTRAN capability. The transla-
tor to be described here can be obtained by writing to the address 
given at the end of this appendix. 
614 

Sparks 
615 
In order to see the difference between FORTRAN and SPARKS 
consider writing a program which searches for X in the sorted array 
of integers A (N), N ::5 100. The output is the integer J which is either 
zero if X is not found or A (J) = X, I ::5 J ::5 N. The method used here 
is the well known binary search algorithm. The FORTRAN version 
looks something like this: 
SUBROUTINE BINS (A,N,X,J) 
IMPLICIT INTEGER (A - Z) 
DIMENSION A(IOO) 
BOT= I 
TOP= N 
J = 0 
JOO 
IF (BOT. GT. TOP) RETURN 
MID= (BOT+ TOP)/2 
IF (X. GE. A (MID)) GO TO JOI 
TOP= MID - I 
GO TO 100 
101 
IF (X. EQ. A (MID)) GO TO 102 
BOT= MID+ I 
GO TO JOO 
102 
J =MID 
RETURN 
END 
This may not be the "best" way to write this program, but it is a 
reasonable attempt. Now we write this algorithm in SPARKS. 
SUBROUTINE BINS (A,N,X,J) 
IMPLICIT INTEGER (A - Z) 
DIMENSION A(JOO) 
BOT= I: TOP= N; J = 0 
WHILE BOT. LE. TOP DO 
MID= (BOT+ TOP)/2 
CASE 
: X. LT. A(MID): TOP = MID -
I 
: X. GT. A(MID): BOT= MID+ I 
:ELSE: J =MID; RETURN 
ENDCASE 
REPEAT 
RETURN 
END 
The difference between these two algorithms may not be dramatic, 
but it is significant. The WHILE and CASE statements allow the algorithm 
to be described in a more natural way. The program can be read from 
top to bottom without your eyes constantly jumping up and down the 
page. When such improvements are consistently adopted in a large 
software project, the resulting code is bound to be easier to comprehend. 
We begin by defining precisely the SPARKS language. A distinction 
is made between FORTRAN statements and SPARKS statements. The 

616 
Appendix A 
latter are recognized by certain keywords and/ or delimiters. All other 
statements are regarded as FORTRAN and are passed directly to the 
FORTRAN compiler without alteration. Thus, SPARKS is compatible 
with FORTRAN and a FORTRAN program is a SPARKS program. 
SPARKS statements cause the translator to produce ANSI FORTRAN 
statements which accomplish the equivalent computation. Hence, the 
local compiler ultimately defines the semantics of all SPARKS statements. 
The reserved words and special symbols are: 
BY 
ENDIF 
REPEAT 
CASE 
EOJ 
UNTIL 
~ 
CYCLE 
EXIT 
WHILE 
DO 
FOR 
TO 
ELSE 
IF 
THEN 
ENDCASE 
LOOP 
Reserved words must always be surrounded by blanks. Reserved means 
they cannot be used by the programmer as variables. 
We now define the SPARKS statements by giving their FORTRAN 
equivalents. In the following any reference to the term "statements" 
is meant to include both SPARKS and FORTRAN statements. There 
are six basic SPARKS statements, two which improve the testing of 
cases and four which improve the description of looping. 
IF cond THEN 
SI 
ELSE 
S2 
ENDIF 
100 
101 
IF(.NOT. (cond)) GO TO 100 
SI 
GO TO 101 
S2 
CONTINUE 
S 1 and S 2 are arbitrary size groups of statements. Cond must be a 
legal FORTRAN conditional. The ELSE clause is optional but the ENDIF 
is required and it always terminates the innermost IF. 
CASE 
: condl: S 1 
: cond2 : S2 
: condn: Sn 
: ELSE: Sn+i 
ENDCASE 
IF(.NOT. (condl)) GO TO 101 
SI 
GO TO 100 
101 
IF(.NOT. (cond2)) GO TO 102 
S2 
GO TO 100 
102 
100 + n - 1 IF(.NOT. (condn)) GO TO 100 + n 
Sn 
GO TO 100 
100 + n 
CONTINUE 
Sn+I 
100 
CONTINUE 

Sparks 
617 
S 1 ,S2 , ••• ,Sn+i are arbitrary size groups of statements. Condi, cond2, 
... , condn are legal FORTRAN conditionals. The symbol ELSE sur-
rounded by colons designates that sn+l will be automatically executed 
if all previous conditions are false. This part of the case statement 
is optional. 
The four looping statements are: 
WHILE cond DO 
s 
REPEAT 
IOO 
IF(.NOT. (cond)) GO TO IOI 
s 
GO TO IOO 
lOI 
CONTINUE 
S is an arbitrary group of statements and cond a legal FORTRAN 
conditional. 
LOOP s 
UNTIL cond REPEAT 
100 
CONTINUE 
s 
IF(.NOT. (cond)) GO TO IOO 
Sand cond are the same as for the while statement immediately preceding. 
LOOP s 
REPEAT 
IOO 
IOI 
CONTINUE 
s 
GO TO IOO 
CONTINUE 
S is an arbitrary size group of statements. 
FOR vble = exp! TO exp2 BY exp3 DO 
s 
REPEAT 
This has a translation of: 
vble =exp! 
GO TO IOO 
102 vble = vble + exp3 
100 IF ((vble - (exp2))*(exp3) .GT. 0) GO TO 101 
s 
GO TO 102 
lOI 
CONTINUE 
The three expressions exp!, exp2, exp3 are allowed to be arbitrary 
FORTRAN arithmetic expressions of any type. Similarly vble may be 
of any type. However, the comparison test is made against integer 
zero. Since exp2 and exp3 are re-evaluated each time through the loop, 
care must be taken in its use. 

618 
Appendix A 
EXIT is a SPARKS statement which causes a transfer of control 
to the first statement outside of the innermost LOOP-REPEAT statement 
which contains it. One example of its use is: 
LOOP 
IOO 
CONTINUE 
S1 
SI 
IF cond THEN EXIT 
IF(.NOT. (cond)) GO TO I02 
ENDIF 
GO TO IOI 
S2 
I02 
CONTINUE 
REPEAT 
S2 
GO TO IOO 
IOI 
CONTINUE 
A generalization of this statement allows EXIT to be used within any 
of the four SPARKS looping statements: WHILE, LOOP, LOOP-UNTIL 
and FOR. When executed, EXIT branches to the statement immediately 
following the innermost looping statement which contains it. 
The statement CYCLE is also used within any SPARKS looping 
statement. Its execution causes a branch to the end of the innermost 
loop which contains it. A test may be made and if passed the next 
iteration is taken. An example of the use of EXIT and CYCLE follow. 
LOOP 
IOO 
SI 
CASE 
: cond I : EXIT 
: cond2 : CYCLE 
ENDCASE 
I03 
S2 
REPEAT 
I04 
IOI 
I02 
CONTINUE 
S1 
IF(.NOT. (condI) GO TO I03 
GO TO I02 
IF(.NOT. (cond2)) GO TO I04 
GO TO IOI 
CONTINUE 
S2 
GO TO 100 
CONTINUE 
EOJ or end of job must appear at the end of the entire SP ARKS 
program. As a statement, it must appear somewhere in columns 7 through 
72 and surrounded by blanks. 
ENDIF is used to terminate the IF and ENDCASE to terminate the 
CASE statement. REPEAT terminates the looping statements WHILE, 
LOOP and FOR. 
Labels follow the FORTRAN convention of being numeric and in 
columns one to five 
The use of doubleslash is as a delimiter for comments. Thus one 
can write 
./'This is a comment./' 
and all characters within the double slashes will be ignored. Comments 
are restricted to one line and FORTRAN comments are allowed. 

Sparks 
619 
The semi-colon can be used to include more than one statement on 
a single line For example, beginning in column one the statement 
99999 A= B + C; C = D + E; X =A 
would be legal in SPARKS. To include a semicolon in a hollerith field 
it should be followed by a second semicolon. This will be deleted in 
the resulting FORTRAN. 
We are now ready to describe the operation of the translator. Two 
design approaches are feasible. The first is a table-driven method which 
scans a program and recognizes keywords. This approach is essentially 
the way a compiler works in that it requires a scanner, a symbol table 
(though limited), very limited parsing and the generation of object 
(FORTRAN) code. A second approach is to write a general macro 
preprocessor and then to define each SPARKS statement as a new macro. 
Such a processor is usually small and allows the user to easily define 
new constructs. However, these processors tend to be slower than the 
approach of direct translation. Moreover, it is hard to build in the 
appropriate error detection and recovery facilities which are sorely needed 
if SPARKS is to be used seriously. Therefore, we have chosen the 
first approach. Figure A. I contains a flow description of the translator. 
find token 
case 
case 
colon 
cycle 
else 
end case 
end if 
eaj 
exit 
for 
FORTRAN 
if 
loop 
repeat 
until 
while 
end 
Figure A.1: Overview of SPARKS Translator 
translate 
and 
print 
FORTRAN 
close 
output 
file 
The main processing loop consists of determining the next statement 
and branching within a large CASE. This does whatever translation 
into FORTRAN is necessary. When EOJ is found the loop is broken 
and the program is concluded. 

• 
620 
Appendix A 
The SPARKS translator was first written in SPARKS. The original 
version was hand translated into FORTRAN to produce our first running 
system. Since that time it has been used by a variety of people and 
classes. Thus it is running far better than the original version. Neverthe-
less, the translator has not been proved correct and so it must be used 
with caution . 
Extensions 
Below is a list of possible extensions for SPARKS. Some are relatively 
easy to implement, while others require a great deal of effort. 
E. l Special cases of the CASE statement 
CASE SGN : exp : 
CASE: integer variable: 
: .EQ.O: SI 
: I : SI 
: .LT.O : S2 
and 
: 2 : S 2 
: .GT.O: S 3 
ENDCASE 
:n:S 0 
ENDCASE 
The first gets translated into the FORTRAN arithmetic IF statement. 
The second form is translated into a FORTRAN computed go to. 
E.2 A simple form of the FOR statement would look like 
LOOP exp TIMES 
s 
REPEAT 
where exp is an expression which evaluates to a non-negative 
integer. The statements meaning can be described by the SPARKS 
for statement: 
FOR ITEMP = I TO exp DO 
s 
REPEAT 
An internal integer variable ITEMP must be created. 
E.3 If F appears in column one then all subsequent cards are assumed 
to be pure FORTRAN. They are passed directly to the output 
until an F is encountered in column one. 
E.4 Add the capability of profiling a program by determining the number 
of executions of each loop during a single execution and the value 
of conditional expressions. 
HINT: For each subroutine declare a set of variables which can 
be inserted after encountering a WHILE, LOOP, REPEAT, FOR, 
THEN or ELSE statement. At the end of each subroutine a write 
statement prints the values of these counters. 
E.5 Add the multiple replacement statement so that 
A=B=C=D+E 
is translated into 
C = D + E; B = C; A= B 

E.6 Add the vector replacement statement so that 
(A,B,C) = (X + Y, 10,2*E) 
produces A= X + Y; B = IO; C = 2*E 
E. 7 Add an array "fill" statement so that 
NAME(*) -
exp I ,exp2,exp3 
gets translated into 
Sparks 
621 
NAME(l) =exp!; NAME(2) = exp2; NAME(3) = exp3 
E.8 Introduce appropriate syntax and reasonable conventions so that 
SPARKs programs can be recursive. 
HINT: Mutually recursive programs are gathered together in a 
module, MODULE (X(A,B,C)(IOO)) whose name is X, whose 
parameters are A,B,C and whose stack size should be 100. 
E.9 Add a character string capability to SPARKS. 
E.10 Add an internal procedure capability to aid the programmer m 
doing top-down program refinement. 
E.11 Attach sequence numbers to the resulting FORTRAN output which 
relates each statement back to the original SPARKS statement 
which generated it. This is particularly helpful for debugging. 
E.12 Along with the indented SPARKS source print a number which 
represents the level of nesting of each statement. 
E. 13 Generalize the EXIT statement so that upon its execution it can 
be assigned a value, e.g., 
LOOP 
SI 
IF condl THEN EXIT: expl : ENDIF 
S2 
IF cond2 THEN EXIT : exp2 : ENDIF 
S3 
REPEAT 
will assign either expl or exp2 as the value of the variable EXIT. 
E.14 Supply a simplified read and write statement. For example, allow 
for hollerith strings to be included within quotes and translated 
to the nH x 1 ... x 0 format. 
All further questions about the definition of SP ARKS should be addressed 
to: 
Chairman, SPARKS Users Group 
Computer Science, Powell Hall 
University of Southern California 
Los Angeles, California 90007 
To receive a complete ANSI FORTRAN version of SPARKS send $20.00 
(for postage and handling) to Dr. Ellis Horowitz at the above address. 

• 
INDEX 
Absolute approximation, S60, S62-S67 
Ackermann's function, 77 
Adelman, 460 
Adversary arguments, 469-478 
Aho, A., 40, 4S7, 49S, SSO, 601 
Albana, A., 360 
Algorithm, definition, 1-4 
Alpha beta pruning, 298 
Alpha cutoff, 297 
And/or graph, 286-290, S30-S32 
Anderson, J., 310 
Angluin, D., 604 
Approximation algorithms, SS9-613 
Approximation scheme, S61, S79-S96 
Assignment problem, 364, 410 
Asymptotic notation, 27-32 
Backtracking, 323-369 
estimating efficiency, 333-337 
Balancing principle, 436 
Batcher, K., 492-494 
Baudet, G., 310 
Baumert, L., 323, 3S9 
Belaga, 488 
Bellman, R., 416 
Bentley, J., 141 
Bestfit, S 73 
Bestfit decreasing, S73 
Beta cutoff, 298 
Bin packing, S72-S74 
Binary insertion sort, 466 
Binary merge tree, 169-170 
Binary merging, 472 
Binary search, 100-107, 463 
Binary search trees, 60 
Binary trees, S6-61, 249-262 
complete, S6-61 
full, S6-61 
inorder traversal, 249-262 
postorder traversal, 2Sl 
preorder traversal, 2S 1 
skewed, S6-61 
threaded, 313-314 
triple order traversal, 314-31S 
Bipartite graph, S47 
Bitner, J., 362 
Blum, M., 140 
Borodin, A., 40, 4S6, 496, 497, SOO 
Bounding functions, 323, 324, 334 
Branch-and-bound, 329, 3S8, 370-421 
Breadth first search, 263-268 
Breadth first solution tree, 289-290 
Brent, R., 491, 497 
Brown, M., 94 
Brown, W., 4S6 
Brucker, P., S49 
Bruijn, N., 41 
Bruno, J., S48, SSO, 600 
Brute force method, 323 
Bunch, J., 141 
Burstau, R., 41 
Cartwright, W., lSO 
Chang, L., 189 
Characteristic vector, 71 
Cheriton, D., 190 
Children, in a tree, S4 
Chinese Remainder Theorem, 444 
Cho, Y., SSO 
Christofedes, N., 601 
Chromatic number, 343, S24-S2S, S46, 
S62-S63 
Circuit realization, SSS 
Clique, 368, S06, S07, S09, S22, S66, S9S, 609 
Clique cover, SS4 
Clustering, 611 
CNF-satisfiability, SlO 
Code optimization, 270-286, S38-S4S 
Coffman, E. G., S48, S49, S71, 600 
Cohen, J., 139, 141 
Coin changing, 191 
Collins, G., 4S6 
Colorability, 343-347, S46, 609 
Comer, D., SSl 
Concentrator location, SS6 
Conjunctive normal form, SlO 
Constable, R., SSl 
Control abstraction, 1S3 
622 

Cook, S., Sl1, S13, S48 
Cook's Theorem, S13-S21 
Cooley, J., 432, 433, 4SS 
Cornuejols, G., 601 
Cycle cover, 610 
Dahl, 0., 3, 94 
Darlington, J., 41 
Decision algorithm, SOS 
Decision problem, SOS 
Deep alpha beta pruning, 299 
Degree, of a tree, S4 
Demers, A., S74, 600 
Deo, N., 40 
Depth first search, 268-269, 30S-309 
Diffie, 460 
Dijkstra, E., 3, 4, 190 
Disjoint set union, 70-79 
Disjunctive normal form, SlO 
Divide-and-conquer, 98-lSl 
Dobkin, D., 141, 496, 498 
Dodd, M., 91-92 
Doffey, T., 310 
Dominance rule, 222, 414, S86-S87 
Downey, P., SSl 
Doyle, J., 94 
Dreyfus, S., 238, 240 
D-search, 318, 328, 330, 370 
Dunstan, F., 191 
Dwyer, B., 309 
Dynamic programming, 198-247 
E-approximate, S61, S67-S78 
Edmonds, J., 191 
Eight queens problem, 324-339 
Euclid's algorithm, 442-443 
Euler, L., 444 
Evaluation, 426-431, 447-4S2 
Even, S., 41, S49 
Exact cover, SS4 
Exponential time, 29 
External path length, 104 
Four queens, 370-371 
Fifteen puzzle, 373-378 
f (n)-approximate, S60-S61 
Feedback arc set, SS3, S9S 
Feedback node set, S40, SS3 
Fiduccia, C., 141, 4S6, 496 
Field, 479 
FIFO search, 370 
First fit, S73 
First fit decreasing, S73 
Fischer, P., 139, 141 
Fischer, M., 601 
Index 
623 
Flow shops, 234-238, S3S-S36, S47, SSS, 
SS6 
Floyd, R., 140, 238, 310 
Ford, L., 411, 467, 494 
Forest, S4 
Fourier primes, 440-4S9 
Fourier transform, 431-440 
Frankowski, E., lSO 
Fredrickson, G., 601 
Freedman, N., 49S 
Fulkerson, D., 411 
Fussel, J., 600 
Fussenegger, F., 49S 
Gabow, H., 49S 
Gale, D., 191 
Game trees, 290-302 
Garey, M., S48, S49, SSO, SSl, S71, S74, 
S9S, S99, 600, 602, 603 
Garfinkel, R., 362, 412, 41S, 416 
Gass, S., 363, 411 
Generalized assignment problem, 611 
GF(p), Galois field, 441 
Glover, K., 141 
Godbole, S., 240 
Goldner, A., 311 
Golomb, S., 323, 3S9 
Gonzalez, T., 97, S49, 601, 602 
Graham, R., S49, S68, S74, S79, 600 
Graham, S., 311 
Graphs 
definitions, 79-82 
adjacency list, 79-82 
adjacency matrix, 79-82 
and/or, 286-290, S30-S32 
biconnected components, 302-309 
bipartite, 317 
connected, 79-82 
cycle, 79-82 
in-degree, 79-82 
isomorphism, 368 
multistage, 203-208 
out-degree, 79-82 
path, 79-82 
planar, 343 
search, 263-269 
shortest paths, 183-188, 198-202, 208-211 
spanning trees, 174-183, 188 
strongly connected, 79-82 
traversal, 263-269 
Greedy method, 1S2-197 
Greenberg, H., 362 
Hall, M., 362 
Halting problem, S12 

624 
Index 
Hamiltonian cycle, 348-3SO, S26-S28, S46, 
SS3, S9S, S97-S98 
Hamiltonian path, S46, SS3 
Harary, F., 17S 
Hashing, 82-93 
Heaps, 61-70 
Heapsort, 69-70 
Hearn, A., 4S6 
Hecht, M., 240, 41S, 417 
Hegerich, R., 362 
Height, of a tree, S4 
Heindel, L., 4S6 
Hellman, 460 
Hermann, P., S48 
Herschberg, D., 49S 
Heuristic search, 421 
Hirschberg, D., 49S 
Hi-Q, 366-367 
Hitting set, SS4 
Hoare, C., 3, 121, 140 
Hopcroft, J., 40, 139-141, 311, 4S7, 496 
Homer's rule, 426-428, 4S8, 480-482 
Horowitz, E., 239, 309, 311, 3S9, 362, 415, 
4S6, SSl, S99, 603 
Horvath, E., 600 
Huffman codes, 173-174 
Huffman, D., 189 
Hunt, H., SSl 
Hwang, F., 140, 471, 49S 
Hyafil, L., 478, 49S, 496, SSl 
lbaraki, T., 417 
Ibarra, 0., SSl, S91, 601, 602, 603 
lgnall, E., 416 
Independent set, S98-S99 
Ingargiola, G., 416 
Insertion sort, 118 
Integer programming, S7S-S76 
Internal node, 170 
Internal path length, 104 
Interpolation, 428-431, 4S3-4SS 
Interval partitioning, S99 
Job sequencing with deadlines, 384-387, S33 
Job shops, S37-S38, SSS 
Johnson, D. B., 94, 190, 311 
Johnson, D. S., S48, S49, SSO, SSl, S71, 
S74, S9S, S99, 600, 601, 602, 603 
Johnson, S., 240, 467, 494, SSO, 601 
Kain, R., S48 
Karel, C., 416 
Karp, R., 240, 41S, 417, 494, S48, SSl, S97, 
S99, 603, 604 
Kernighan, P., 3, 14, 604 
Kerr, L., 139, 140 
Kim, C., S91, 601, 602, 603, 604 
Knapsack problem, 1S7-160, 198 
0/1, 192, 199-202, 219-228, 3S0-3S9, 
390-403, S06, S07, S08, S33, 
S61-S62, S6S, S80-S8S, S89-S91, S9S 
Knuth, D., 40, 41, 86, 93, 94, 140, 218, 
239, 300-302, 309, 310, 362, 4S7, 494 
Kohler, W., 417 
Korsch, J., 189, 416 
Kruskal, J., 190, 196 
Kruskal's method, 179-183 
Kung, H., 489, 496, 497, SOO 
Lageweg, B., 416 
Lagrange, interpolation, 428 
Lam, S., 600 
Lame, G., 443 
Lawler, E., 41, 191, 239, 41S, S49, 603 
LC-search, 371-373 
Leaf node, S4 
Lehmer, D., 323 
Lenstra, J., 416, S49 
Level, of a node, S4 
Lewis, P., 433, 4SS 
Lieberherr, K., 601, 602 
LIFO search, 370 
Lin, S., 140, 471, 49S, 604 
Lindstrom, G., 309 
Linear arrangement, S47 
Linear searching, 463, 464 
Lipson, J., 440, 4SS, 4S6 
Lipton, R., 141, 496, 498 
Little, J., 416 
Lower bounds, 461-SOO 
LPT schedule, S67-S72 
Loyd, S., 373 
Lueker, G., SSl, 604 
Lum, V., 91-92 
Magazine, M., 188 
Maheshwari, S., SSl, 602 
Manacher, G., 468, 49S 
Manna, Z., 41 
Marsten, R., 41S 
Matrix multiplication, 137-139, 484, 48S 
Matrix product chains, 242-243 
Max-cut, S47, SS7 
Mead, C., 41 
Median, 127 
Median of medians, 131 
Melhorn, K., 239 
Merge insertion sorting, 467-468 
Mergesort, 113-121 

Merging, lower bound, 470-473 
Merging, optimal patterns, 169-174, 198 
Miller, R., S48 
Miller, W., 499, SOO 
Minimax, 296 
Minimum boolean form, SS4 
Minimum equivalent graph, SS4 
Mitten, L., 41S 
Modular arithmetic, 440-447 
Moenck, R., 4S6 
Moore, J., 239 
Moorty, K., 416 
Morin, T., 41S 
Moses, J., 4S6 
Motzkin, T., 488 
Munro, I., 40, 490, 496, 497, SOO 
Musinski, J., 141 
Nakata, I., 310 
Nassimi, D., S49 
Nemhauser, G., 188, 238, 239, 362, 41S, 
416, 601, 602 
Newborn, M., 310 
Newton, I., 426 
Nicholson, P., 4SS 
Nievergelt, J., 40 
Nijenhuis, A., 41, 362 
Nilsson, N., 310, 41S, 417 
Nim, 290-293 
Nodes, live or dead, 329 
Node cover, 193, S23-S24, S46, SS3, S9S 
Nondeterministic algorithm, S02-S10 
Nondeterministic machine, S03 
NP-complete, S01-SS8 
NP-hard, S01-SS8 
NP-hard approximations, S6S-S67 
Omega notation, 31 
On-line algorithm, 79 
Optimal assignment, 193 
Optimal binary search trees, 211-219 
Optimization measure, 1S2 
Oracles, 469-478 
Orsini, R., 360 
0-notation, 27 
Pan, A., 191 
Pan, V., 139 
Papadimitriou, C., SSl 
Parallel algorithms, 488-494 
Parent, in a tree, S4 
Partition, S33, S46 
Paterson, M., 490, 497 
Pentominoes, 366 
Index 
625 
Plaisted, D., SSl 
Plant location, SS6 
Plauger, 3, 14 
Pohl, I., 140 
Pollard, J., 4SS 
Polynomials, 423-436 
Posa, L., S97, S98, 604 
Postage stamp problem, 366 
Postfix, 318-319 
Postorder traversal, 333 
Pratt, V., 140 
Preconditioning, 486-488 
Prim, R., 190, 19S-196 
Prim 's method, 176-179 
Principle of optimality, 199 
Probabilistically good algorithms, S60, 
S96-S99 
Probert, R., 139, 141 
Pseudo polynomial algorithm, S96 
Quadratic assignment, S76-S78 
Quadratic programming, SSS 
Queues, 48-S3 
Quicksort, 121-127 
Rader, C., 455 
Rearrangement, 334 
Recursion, 13, 17-24 
Reducibility, S 11 
Redziejowski, R., 310 
Reingold, E., 40, 362, 49S, 498 
Reiss, 496 
Reliability design, 228-230 
Resource allocation, 203 
Retrieval time 
expected, 194 
maximum, 194 
mean, 1S3 
Ring, 479 
Rinnooykan, A., 416, S49, SSO 
Rivest, R., 94, 140, 141, 460, SSl 
Robertson, E., SSO, 602 
Robson, J., 309 
Root of unity, 433 
Rosenthal, A., 311, 600 
Roth, M., 139, 141 
Rounding, S87-S91 
Sahni, S., 239, 309, 311, 3S9, 362, 41S, S48, 
S49, SSO, SSl, S99, 600, 601, 602, 603 
Satisfiability, SlO, S47, 606-607 
Schaefer, T., SS1, SS2 
Schonage, 97, 447, 4S6 
Schrage, L., 416 
Schreier, J., 474 

626 
Index 
Schwartz, S., 190 
Searching, 503 
Sedgewick, R., 140 
Seiferas, J., 551 
Selection problem, 127-136, 147-151, 469, 
476-478 
Separation, 593-594 
Sequencing with deadlines, 161-168 
Set cover, 193, 554, 607 
Set packing, 558 
Sethi, R., 309, 540, 548, 550, 551, 557, 600 
Shamir, A., 460 
Shamos, M., 141 
Shortest paths 
all pairs, 208-211 
single source, 183-188, 198-202 
Singpurwalla, N., 600 
Slagle, J., 310 
Spanning tree 
breadth first, 267 
depth first, 269 
minimum, 174-183 
shortest path, 188 
SPARKS, 4-14, 614-621 
Spira, P., 191, 239 
SPT schedule, 189 
Stacks, 48-51 
State space descriptions, 328, 475, 476 
Steiglitz, K., 417, 551 
Steiner trees, 557 
Stockmeyer, L., 550 
Straight line program, 478, 479 
Strassen, V., 137-140, 447, 456 
Strongly NP-hard, 595 
Subexponential algorithm, 559 
Sum of subsets, 325, 327, 339-343, 508, 533 
Sutherland, I., 41 
Sweeny, D., 416 
Symbol table, 59, 82 
Tapes, storage on, 153-156, 563-565 
Tarjan, R., 78, 94, 140, 190, 310, 311, 550, 
559 
Tautology, 554 
Thatcher, J., 548 
Theta notation, 27 
Tournament, 474 
Transitive closure, 240, 241, 267, 269 
Traveling salesperson, 197, 231-234, 350, 
403-412, 528-530, 575, 595, 610 
Trees 
binary search, 60 
binary, 56-61 
comparison, 462-469 
dynamic, 329 
k-ary, 60 
permutation, 326 
state space, 328 
static, 329 
traversal, 262-263 
Trojanowski, A., 559 
Trotter, L., 188 
Tukey, J., 432, 455 
Ullman, J., 40, 309, 457, 495, 496, 548, 549, 
550, 574, 600, 601 
Ullman, Z., 239 
Unary input, 507 
Valiant, L., 491, 497, 604 
Venkatesh, N., 261, 358-361 
Vuillemin, J., 94 
Waldinger, R., 41 
Walker, R., 323, 359 
Wegman, M., 311 
Weighted path length, 170 
Welch, P., 433, 455 
Welsh, D., 191 
Wells, M., 40 
Wilf, H., 41, 362 
Winograd, S., 455, 491, 496, 497 
Wolsey, L., 601, 602 
Wood, D., 415 
Yao, A., 94, 190 
Yuen, P., 91-92 

ABOUT THE BOOK 
This outstanding book has organized a collection of algorithms, ;malysis techniques, and 
theory in a way that is feasible for use by both undergraduate and graduate students. The 
work shows the reader how to devise algorithms, how to expres!> them, how to decide 
between two alternative algorithms for a given problem, how to validate and analyze 
them, and how to test and improve a program. Each chapter is devoted to a fundamental 
design method. A series of problems is considered, and efficient solutions are derived in 
the form of algorithms by using the specific design methods. The computing time and 
space requirements for each algorithm are analyzed, enabling the reader to master the 
important twin skills of design and analysis. Chapters on NP-hard and NP-complete 
problems, the determination of lower bounds, and the dt>velopment and analysis of 
heuristic or approximation algorithms are also included. 
What reviewers have said: 
"Very useful for undergraduate and graduate students in computer science and mathematics as 
well as a reference work for a computer professional." ---Choice. 
"Throughout the book, the authors have taken pains to make the algorithmic developmerus seem 
muuml and well mvtivated with many illusirmive examples given. I certainly recommend the book 
as a possible text for a first course." ---Computing Reviews. 
ABOUT THE AUTHORS 
Ellis Horowitz received his B.S. degree from Brooklyn College and his Ph.D. in 
computer science from the University of Wisconsin. He was on the faculty there and at 
Cornell University before assuming his present post as professor of computer science and 
electrical engineering at the University of Southern California. He is a past chaim1an of 
the Computer Science Department at USC. 
Sartaj Sahni is a professor of computer science at the University of Minnesota.He 
received his B. Tech.(electrical engineering) degree from the Indian Institute of Technol-
ogy, Kanpur, and the M.S. and Ph.D. degrees in computer science from Cornell 
University. 
Other books of interest. 
FUNDAMENTALS OF DATA STRUCTURES 
Ellis Horowitz and Sartaj Sahni 
This widely acclaimed book covers topics of a data structures course. All algorithms are 
expressed in the SPARKS programming language. lSBN 0-914894-20-X. 
"A fine choice for advanced undergraduate courses on data structures." ---Computing Reviews. 
"A valuable addition to a Computer Science collection." --Choice. 
FUNDAMENTALS OF DATA STRUCTURES IN PASCAL 
Ellis Horowitz and Sartaj Sahni 
In this excellent new book based on Fundamentals of Data Structures by the same 
authors, all algorithms are expressed in Pascal. The outstanding benefits of the original 
book are now available with a choice of algurithms in either the SPARKS or the Pascal 
programming language. ISBN 0-914894-94-3. 
ISBN 0-91.4894-22-6 


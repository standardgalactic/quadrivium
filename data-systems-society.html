<h3
id="pp_i_i_data_systems_and_society">01.0_pp_i_i_Data_Systems_and_Society</h3>
<p>Title: Data, Systems, and Society: Harnessing AI for Societal Good by
Munther A. Dahleh</p>
<p>Munther A. Dahleh, a renowned professor of Electrical Engineering and
Computer Science at the Massachusetts Institute of Technology (MIT),
presents a comprehensive blueprint in his book titled “Data, Systems,
and Society: Harnessing AI for Societal Good.” This work underscores the
importance of interdisciplinary collaboration—bringing together
expertise from academia, industry, and government—to tackle complex
societal challenges using data science, artificial intelligence (AI),
and other advanced methodologies.</p>
<p>The book’s central argument is that traditional discipline boundaries
must be transcended to address high-value societal issues effectively.
Dahleh demonstrates how statistics, data science, information systems,
and social behavior intersect across various domains through real-life
examples. These illustrations range from optimizing transportation
networks to making critical healthcare decisions during pandemics,
understanding media influence on elections, and analyzing the role of AI
in societal transformations like revolutions.</p>
<p>Dahleh’s approach emphasizes several key concepts:</p>
<ol type="1">
<li><p><strong>Robustness</strong>: Ensuring systems can withstand
unexpected perturbations or errors without failing. In data-driven
solutions, robustness ensures reliability and resilience against
potential disruptions.</p></li>
<li><p><strong>Causality</strong>: Understanding cause-and-effect
relationships in data-driven decision-making processes is crucial for
developing accurate models and reliable predictions.</p></li>
<li><p><strong>Privacy</strong>: With the increased use of data, privacy
becomes a paramount concern. Dahleh discusses strategies to protect
individual privacy while leveraging data for societal good.</p></li>
<li><p><strong>Ethics</strong>: Ethical considerations are woven
throughout the book, addressing concerns like algorithmic bias and
fairness in AI systems.</p></li>
</ol>
<p>Beyond technical aspects, Dahleh shares insights from his experience
at MIT’s Institute for Data, Systems, and Society (IDSS), discussing
lessons learned about transdisciplinary education and unintended
consequences of AI and algorithmic systems. He advocates for a holistic
approach that integrates diverse perspectives to create innovative
solutions addressing pressing societal challenges.</p>
<p>The book is notably accessible, written in clear, non-technical
language, making it suitable for students, professionals, and
policymakers alike who are interested in the intersection of data,
systems, and society. It serves as a valuable resource for anyone
looking to understand how AI can be harnessed responsibly for societal
good.</p>
<p>Endorsements from notable scholars like Michael I. Jordan (University
of California, Berkeley) and Matthew O. Jackson (Stanford University)
attest to the book’s significance in today’s data-driven world. These
endorsements underscore Dahleh’s unique position as a bridge between
engineering principles and societal impacts, making this book an
essential read for anyone invested in the future of AI and its role in
shaping our society.</p>
<h3 id="pp_ii_ii_reviews">02.0_pp_ii_ii_Reviews</h3>
<p>“Data, Systems, and Society: Harnessing AI for Societal Good” by
Munther A. Dahleh, a professor at the Massachusetts Institute of
Technology (MIT), presents a comprehensive blueprint for leveraging data
science, artificial intelligence (AI), and systems thinking to address
complex societal challenges.</p>
<ol type="1">
<li><p><strong>Interdisciplinary Approach</strong>: The book advocates
for a transdisciplinary approach that combines various fields such as
statistics, data science, information and decision systems, social and
institutional behavior, among others. This holistic method is crucial to
tackle multifaceted problems of high societal value
effectively.</p></li>
<li><p><strong>Intersection of Disciplines</strong>: Dahleh illustrates
how different disciplines intersect in various domains. For instance,
optimizing transportation involves understanding human behavior (social
science), data-driven decision making (systems and statistics), and
technological infrastructure (computer science and
engineering).</p></li>
<li><p><strong>Real-World Applications</strong>: The book uses real-life
examples to explain complex concepts. These include optimizing urban
transportation systems, making critical healthcare decisions during
pandemics, analyzing the impact of media on elections and revolutions,
among others.</p></li>
<li><p><strong>Key Concepts</strong>: Dahleh emphasizes crucial concepts
like robustness (the ability to perform under different conditions),
causality (understanding cause-and-effect relationships), privacy
(protecting sensitive information), and ethics (ensuring AI is used
responsibly).</p></li>
<li><p><strong>Lessons Learned</strong>: He shares insights gained from
his experience at the MIT Institute for Data, Systems, and Society
(IDSS) and other similar initiatives worldwide. This includes lessons
about the unintended consequences of AI and algorithmic systems and the
importance of transdisciplinary education in this field.</p></li>
<li><p><strong>Author’s Expertise</strong>: Dahleh brings extensive
expertise to the table, having made significant contributions to
decision-making under uncertainty, influencing domains like
transportation systems, power grids, and social networks. His accolades
include multiple IEEE CSS George S. Axelby Outstanding Paper Awards and
the Eckman Award for outstanding control engineers under 35.</p></li>
</ol>
<p>The book is not overly technical, making it accessible to a wide
audience—from budding data scientists and professionals in various
fields to policymakers and institution builders interested in leveraging
AI for societal good. It’s a call to action for creating innovative,
holistic approaches using data-driven methods to solve pressing societal
challenges.</p>
<h3
id="pp_iii_iii_data_systems_and_society">03.0_pp_iii_iii_Data_Systems_and_Society</h3>
<p>“Data, Systems, and Society: Harnessing AI for Societal Good” by
Munther A. Dahleh presents a comprehensive blueprint for leveraging data
science, artificial intelligence (AI), and systems engineering to
address complex societal challenges. The book underscores the necessity
of transdisciplinary collaborations among academia, industry, and
government.</p>
<ol type="1">
<li><p><strong>Interdisciplinary Approach</strong>: Dahleh emphasizes
that tackling high-impact societal problems requires an
interdisciplinary approach. He illustrates how statistics, data science,
information systems, decision sciences, social sciences, and
institutional behavior intersect across multiple domains like
transportation, healthcare, media, and more.</p></li>
<li><p><strong>Real-World Applications</strong>: The book uses numerous
real-life examples to explain key concepts. For instance, it discusses
optimizing urban transportation systems, making critical healthcare
decisions during a pandemic, understanding the impact of media on
elections and revolutions. These scenarios help in understanding the
practical implications of data-driven methodologies.</p></li>
<li><p><strong>Crucial Concepts</strong>: Dahleh incorporates essential
concepts integral to responsible AI development and deployment:</p>
<ul>
<li><strong>Robustness</strong>: Ensuring models perform well under
varying conditions, not just in ideal lab settings.</li>
<li><strong>Causality</strong>: Understanding cause-and-effect
relationships rather than mere correlations to guide
decision-making.</li>
<li><strong>Privacy</strong>: Protecting sensitive information while
harnessing data’s potential.</li>
<li><strong>Ethics</strong>: Considering the moral implications and
societal impact of AI systems.</li>
</ul></li>
<li><p><strong>Lessons Learned</strong>: Dahleh shares valuable insights
from his experience at MIT’s Institute for Data, Systems, and Society
(IDSS) and other similar initiatives:</p>
<ul>
<li><strong>Transdisciplinary Education</strong>: Highlighting the
importance of educating professionals who can navigate across various
disciplines.</li>
<li><strong>Unintended Consequences of AI</strong>: Warned about
potential pitfalls and unforeseen impacts of AI systems, stressing the
need for careful system design and monitoring.</li>
</ul></li>
<li><p><strong>Author Credentials</strong>: Munther A. Dahleh, the
author, is a renowned figure in the field of control theory, economics,
and statistics. His accolades include multiple IEEE CSS George S. Axelby
Outstanding Paper Awards and the 1993 Eckman Award for young control
engineers, underscoring his expertise and influence in this
domain.</p></li>
</ol>
<p>The book is intended for researchers, professionals, and institutions
aiming to develop data-driven solutions for societal benefit. It
provides a roadmap, drawing on Dahleh’s extensive experience, to create
impactful, ethical, and responsible AI systems that can positively
influence our world.</p>
<h3 id="pp_iv_iv_copyright_page">04.0_pp_iv_iv_Copyright_page</h3>
<p>“Data, Systems, and Society: Harnessing AI for Societal Good” by
Munther A. Dahleh presents a comprehensive blueprint for leveraging data
science, artificial intelligence (AI), and systems thinking to address
complex societal challenges.</p>
<ol type="1">
<li><p><strong>Transdisciplinary Approach</strong>: The book emphasizes
the need for collaboration across various domains such as academia,
industry, and government. Dahleh underscores that tackling high-value
societal problems requires an interdisciplinary approach that integrates
statistics, data science, information systems, and social
behaviors.</p></li>
<li><p><strong>Intersection of Fields</strong>: Dahleh illustrates how
different fields intersect in real-life scenarios. For instance, he
explains how AI can optimize transportation networks, aid healthcare
decisions during pandemics, or influence election outcomes through media
impact analysis.</p></li>
<li><p><strong>Core Concepts</strong>: Key concepts integral to this
field are explored including robustness (the ability of a system to
perform well under varying conditions), causality (establishing
cause-and-effect relationships in data), privacy (ensuring individual
data protection), and ethics (addressing moral implications of
AI).</p></li>
<li><p><strong>Lessons Learned</strong>: Dahleh shares insights gleaned
from his experience at the MIT Institute for Data, Systems, and Society
(IDSS) and other similar initiatives. These lessons cover both
educational aspects - such as the importance of transdisciplinary
education - and practical challenges like unforeseen consequences
arising from AI and algorithmic systems.</p></li>
<li><p><strong>Author’s Expertise</strong>: As a renowned professor in
control theory, electrical engineering, computer science, and founding
director of IDSS, Dahleh brings extensive expertise to the book. His
contributions to the field of decisions under uncertainty have
significantly impacted various sectors like transportation systems,
power grids, and social networks.</p></li>
<li><p><strong>Endorsements</strong>: The book has received high praise
from leading figures in the field, including Michael I. Jordan and
Matthew O. Jackson, who highlight its relevance for students, data
scientists, policymakers, and institution builders alike.</p></li>
</ol>
<p>In essence, “Data, Systems, and Society” is a guide that equips
readers with the knowledge to apply advanced analytical tools -
primarily AI and data science - to solve pressing societal issues while
being mindful of ethical considerations and potential pitfalls. It’s an
essential read for anyone interested in the intersection of technology
and social impact.</p>
<h3 id="pp_v_viii_contents">05.0_pp_v_viii_Contents</h3>
<p>Title: A Summary and Explanation of Key Themes from “Data &amp;
Goliath” by Bruce Schneier</p>
<ol type="1">
<li><p><strong>The Pitfalls, Promises, and Challenges of
Data</strong></p>
<ul>
<li><p><strong>Seminal Challenge</strong>: Data has become an integral
part of modern life, shaping decisions in various sectors including
politics, economy, and society. However, the massive collection,
storage, and analysis of personal data pose significant challenges
related to privacy, security, and ethical use.</p></li>
<li><p><strong>Bigger Doesn’t Always Mean Better</strong>: Just because
we can collect vast amounts of data doesn’t guarantee that it will lead
to better outcomes or decisions. In fact, the complexity introduced by
big data can often complicate matters rather than simplify
them.</p></li>
<li><p><strong>Statistics: A Definition</strong>: Schneier introduces
statistics as a mathematical discipline concerned with collecting,
analyzing, and interpreting data to draw conclusions or make
predictions. It’s crucial in understanding the implications of
data.</p></li>
<li><p><strong>Timescales and Shortfalls</strong>: Data’s utility can be
limited by its timescale – it might not capture rapid changes or
long-term trends accurately. Additionally, data collection methods may
introduce shortfalls, leading to incomplete or biased
information.</p></li>
<li><p><strong>Causality</strong>: A fundamental challenge in data
analysis is establishing causation versus correlation. Just because two
events occur together doesn’t mean one caused the other.</p></li>
</ul></li>
<li><p><strong>A Confluence of Fields: Some Historical
Perspective</strong></p>
<ul>
<li><p><strong>Computing and Big Data Evolution</strong>: Schneier
outlines four revolutions in computing that have led to our current era
of big data: mainframe computers, personal computers, smartphones/cell
towers co-evolution, and the rise of embedded systems.</p></li>
<li><p><strong>The Founding Event of AI &amp; Neural Networks</strong>:
The Turing Test (1950) marked the foundational moment for artificial
intelligence. Despite advancements, neural networks are still debated in
terms of their capability to learn everything.</p></li>
</ul></li>
<li><p><strong>Who - and What - Should Drive
Decision-Making?</strong></p>
<ul>
<li><p><strong>Algorithmic Logic vs Justice</strong>: The book explores
tensions between the logic-driven decisions made by algorithms (which
may lack ethical nuance) and human justice, which considers context and
individual circumstances.</p></li>
<li><p><strong>Persisting Challenges of Bias</strong>: Despite best
efforts, biases can still permeate data and algorithms, leading to
unfair outcomes. Alternative training methods present their own
challenges, such as the risk of overfitting or reduced
accuracy.</p></li>
</ul></li>
<li><p><strong>A Data-and-Society Reckoning</strong></p>
<ul>
<li><p><strong>Arab Spring &amp; Social Media Effects</strong>: The Arab
Spring uprisings exemplify both positive (mobilizing power) and negative
(surveillance and manipulation) impacts of social media on
society.</p></li>
<li><p><strong>When AI Undercuts Democratic Principles</strong>:
Schneier discusses instances where AI and data-driven systems undermine
democratic processes, such as by manipulating information or targeting
vulnerable groups.</p></li>
<li><p><strong>More Questions than Answers</strong>: The book raises
more questions about the future of data and technology than it provides
answers, underscoring the complexity and ongoing debate in this
field.</p></li>
</ul></li>
<li><p><strong>Omnipresent and Omnivorous Computing</strong></p>
<ul>
<li><strong>The Arab Spring Case Study</strong>: This section uses the
Arab Spring as a case study to explore how pervasive computing
(ubiquitous devices and sensors) can both empower individuals and be
exploited by authorities for surveillance and control.</li>
</ul></li>
<li><p><strong>Certain and Uncertain Effects of Social
Media</strong></p>
<ul>
<li>The book discusses the double-edged sword of social media: while it
facilitates rapid information sharing and social connections, it also
introduces uncertainties like misinformation spread, echo chambers, and
psychological effects on users.</li>
</ul></li>
<li><p><strong>A Data-and-Society Reckoning</strong></p>
<ul>
<li>The book concludes by calling for a societal reckoning with the
implications of data-driven technology. It emphasizes the need for
balancing technological advancement with ethical considerations, privacy
protection, and democratic values.</li>
</ul></li>
</ol>
<p>Title: “A Transdiscipline Is Born” (Chapter 57)</p>
<p>This chapter discusses the birth and development of a new
transdisciplinary field, referred to as IDSS (Institute for Data,
Systems, and Society), at MIT. The primary goal of this transdiscipline
is to bridge the gap between data, systems, and society to tackle
complex, real-world problems.</p>
<ol type="1">
<li><p><strong>Why Disciplines Matter</strong>: Disciplinary knowledge
provides a foundation for understanding specific areas of study.
However, these disciplines often operate in silos, leading to fragmented
solutions for interconnected issues.</p></li>
<li><p><strong>Challenges of the Disciplinary Model</strong>: The
traditional disciplinary approach has limitations when addressing
complex problems due to its rigid boundaries and specialized language.
It can hinder collaboration and lead to suboptimal outcomes.</p></li>
<li><p><strong>Disciplinarity: Multi, Inter, and Trans</strong>: The
author introduces different types of discipline - multi (several
interconnected disciplines working together), inter (two or more
disciplines merging), and trans (transcending traditional boundaries).
IDSS aims to be a transdiscipline that integrates data science, systems
engineering, and social sciences.</p></li>
<li><p><strong>Early Culture Clashes over Data</strong>: The chapter
highlights initial conflicts arising from different perspectives on data
within various disciplines. These clashes emphasize the need for a
shared understanding of data’s role in decision-making
processes.</p></li>
<li><p><strong>Facing Existing Disciplinary Headwinds</strong>: The
establishment of IDSS faced resistance from existing disciplines, as it
challenged traditional ways of conducting research and teaching.
Overcoming these headwinds required building bridges between
computational and social sciences.</p></li>
<li><p><strong>A Better Framework for Data-to-Decision Collaborations:
The IDSS Triangle</strong>: This framework unites data science, systems
engineering, and social sciences to address complex societal challenges
effectively. It provides a holistic approach that considers both
technical aspects (data) and human elements (society).</p></li>
<li><p><strong>Data for the People</strong>: Emphasizes the importance
of making data accessible and understandable to diverse audiences,
fostering informed decision-making at all levels.</p></li>
<li><p><strong>Responding to a Pandemic in Real-Time: Case in
Point</strong>: The chapter uses the COVID-19 pandemic as an example of
how IDSS principles can be applied to create rapid responses, including
a “moonshot” for testing and vaccination programs, leveraging data about
contagion networks, and informing public policies.</p></li>
</ol>
<p>This transdiscipline approach aims to address complex problems by
integrating diverse knowledge areas, breaking down disciplinary
barriers, and focusing on real-world impact.</p>
<p>Title: “Personal Reflections on the Journey” (Chapter 8) from an
unspecified source, likely a book or research paper about academic
entrepreneurship, startups within academia, and leadership.</p>
<ol type="1">
<li><p><strong>Startups within Academia (Page 119):</strong> This
section delves into the concept of academic entrepreneurship—the process
by which faculty members or researchers in universities develop and
commercialize their research findings. The author emphasizes that these
ventures, often referred to as “startups within academia,” require a
unique blend of scientific expertise and business acumen. They can lead
to significant societal benefits through innovation and economic growth
while also providing academic institutions with new revenue
streams.</p></li>
<li><p><strong>Intellectual Strength Is Key (Page 120):</strong> The
author highlights the importance of intellectual strength as a critical
factor for success in this context. This refers to a researcher’s deep
understanding and mastery of their field, which is essential for
generating valuable, cutting-edge ideas. Intellectual strength enables
these individuals to identify promising research areas, design novel
solutions, and navigate the complex landscape of technology transfer and
commercialization.</p></li>
<li><p><strong>Additional Building Blocks and Obstacles (Page
121):</strong> Here, the author identifies several key elements
necessary for launching successful academic startups:</p>
<ul>
<li><p><strong>Team Complementarity:</strong> Combining scientific
expertise with complementary skills such as business acumen, marketing,
and project management is vital.</p></li>
<li><p><strong>Institutional Support:</strong> Universities play a
significant role in providing resources (e.g., infrastructure, funding,
mentorship) to foster these startups’ growth. However, institutional
culture can sometimes hinder their progress due to bureaucratic
obstacles or lack of understanding about the commercialization
process.</p></li>
<li><p><strong>External Partnerships:</strong> Collaboration with
industry partners, investors, and other stakeholders can offer valuable
resources, market insights, and credibility.</p></li>
</ul></li>
<li><p><strong>Administrative Duties Need Not Be Limiting (Page
123):</strong> The author acknowledges that academic leaders often
juggle multiple roles—research, teaching, service, and administration.
However, they argue that effective time management strategies can help
mitigate the burden of administrative tasks without compromising a
leader’s primary responsibilities. Prioritizing tasks, delegating when
possible, and leveraging technology for administrative support are
suggested ways to manage this challenge.</p></li>
<li><p><strong>Returning to Ethics (Page 123):</strong> The author
revisits the topic of ethics in academic entrepreneurship. While
pursuing commercial success, it’s crucial to maintain integrity and
adhere to ethical guidelines. This includes respecting intellectual
property rights, avoiding conflicts of interest, and ensuring that
research findings are accurately represented during technology
transfer.</p></li>
<li><p><strong>We’re Only Human, After All (Page 124):</strong> The
author acknowledges the inherent limitations and fallibility of human
beings involved in this process. Despite their best efforts, individuals
may make mistakes or face unforeseen challenges. Recognizing these
constraints can foster humility, resilience, and continuous
learning—essential qualities for navigating the complex journey of
academic entrepreneurship.</p></li>
<li><p><strong>Acknowledgments (Page 127):</strong> This section
typically expresses gratitude towards individuals, organizations, or
entities that provided support during the research or writing
process.</p></li>
<li><p><strong>Bibliography (Page 137):</strong> A comprehensive list of
sources cited throughout the document, allowing readers to explore
further related literature on the topic.</p></li>
<li><p><strong>Index (Page 141):</strong> An alphabetical listing of key
terms, concepts, and names mentioned in the text, facilitating quick
reference for readers.</p></li>
</ol>
<h3 id="pp_ix_xii_preface">06.0_pp_ix_xii_Preface</h3>
<p>The preface of this book discusses the author’s transition from
academic electrical engineering and applied mathematics to focusing on
real-world data science applications, particularly in mitigating traffic
congestion. This shift occurred due to increasing recognition that
vehicular congestion is not just an infrastructure issue but also a
complex problem involving data and networked decisions.</p>
<p>The author highlights that traditional solutions like building more
or wider roads are less effective than creating a robust, real-time
database of traffic behaviors and their outcomes. The crux lies in using
data science methods to quantify variables such as optimal driving
speeds, individual and collective effects of driving decisions, and the
potential impact of providing private and public information to drivers
to reduce congestion.</p>
<p>This transition also led the author to explore broader,
interconnected systems like power grids, transportation networks,
financial and social networks, and human behavior. Consequently, he
reassessed his research focus towards addressing societal problems of
high value through innovative, holistic data-driven methodologies.</p>
<p>The author then introduces the MIT Institute for Data, Systems, and
Society (IDSS), an initiative he helped establish to tackle significant
societal challenges using transdisciplinary collaborations between
academia and industry. The IDSS represents a global community of
researchers, scholars, and practitioners working towards common goals
with diverse disciplines.</p>
<p>Finally, the author discusses his use of “we” in the narrative,
clarifying that these ideas and examples stem from years of collective
rumination, conversations, explorations, and innovations among a global
community. This usage aims to express a sense of shared purpose and
mutual responsibility towards solving urgent challenges benefiting
various communities, societies, and the planet.</p>
<h3
id="pp_1_18_the_pitfalls_promises_and_challenges_of_data">07.0_pp_1_18_The_Pitfalls_Promises_and_Challenges_of_Data</h3>
<p>The text discusses the evolving role of data science in scientific
inquiry, highlighting its increasing importance due to advancements in
technology and computational power. However, it also points out a
significant gap in systematically utilizing big data to tackle pressing
societal issues across various domains.</p>
<p>The narrative centers around an initiative at the Massachusetts
Institute of Technology (MIT) and similar efforts elsewhere, aiming to
bridge this gap by developing a transdiscipline called Data, Systems,
and Society. This new field employs pioneering technologies to address
complex challenges, with the ultimate goal of inspiring academics,
practitioners, students, and funders worldwide to use data science for
societal benefit.</p>
<p>One case study provided is the Kaiser Family Foundation’s (KFF)
analysis of COVID-19 vaccination equity in the United States during
early 2021. The report revealed alarming disparities, with a significant
underrepresentation of racial and ethnic minorities among those
receiving vaccinations, despite their proportionate representation in
infections, deaths, and population statistics. Furthermore,
race/ethnicity data was missing for more than half (54%) of individuals
who had received at least one dose.</p>
<p>The text emphasizes that simply collecting vast amounts of data does
not ensure valuable insights or accurate results. Flawed datasets due to
poor sampling, inconsistent collection, narrow focus, and manipulation
can lead to misperceptions, mistaken conclusions, and ineffective
policies. The sheer volume of available data, often unlabeled or lacking
crucial details, further complicates matters by making it challenging
for humans—and even advanced AI/ML systems—to draw meaningful insights
from all the information.</p>
<p>In summary, the text underscores three key points:</p>
<ol type="1">
<li>The transformative impact of big data on scientific inquiry and
societal challenges, but also the absence of systematic approaches to
leverage this data for good.</li>
<li>A real-world example of data inequity using COVID-19 vaccination
data from KFF, highlighting racial disparities and incomplete or missing
information.</li>
<li>The limitations of big data: despite its scale, it doesn’t
automatically yield accurate, useful insights; flawed datasets can
mislead; and overwhelming volumes of data make it difficult to extract
valuable, actionable knowledge without appropriate methods and
tools.</li>
</ol>
<p>Title: The Challenges and Importance of Causality in Data
Analysis</p>
<p>Causality, as opposed to correlation, refers to a relationship
between variables where one is the effect of the other. Understanding
causality is crucial in data analysis and decision-making processes
across various fields such as drug design, recommendation systems,
economic policy, and more.</p>
<p>In everyday contexts, people generally understand that correlation
does not imply causation. However, it’s still common to misinterpret one
for the other, especially in data science.</p>
<p>A classic illustration of this distinction involves ice cream
consumption and sunburns. If we only look at correlations, it might seem
that consuming ice cream causes sunburns (or vice versa), due to similar
patterns of increase during summer months. However, the true cause is
spending time at the beach, a confounding factor that explains both
phenomena.</p>
<p>Recommendation systems serve as an excellent example of extracting
causal information from observational data. Consider a movie-rating
platform like Netflix, where users rate movies they’ve watched.
Determining if a customer (A) is interested in a specific movie (X)
based on their past behavior can be challenging due to sparse rating
data.</p>
<p>Nearest neighbor methods often tackle this problem by finding another
user (B), who has rated movies similarly to user A and also liked movie
X. The assumption here is that if B likes X, then A might like it too,
based on their similarity. This method leverages the concept of
proximity to infer causality.</p>
<p>However, this approach comes with caveats. First, finding a highly
similar individual (B) can be difficult due to the uniqueness of human
preferences. This has led to the development of synthetic control
groups, which are collections of individuals that together approximate
the behavior of user A.</p>
<p>The second challenge is more profound: the absence of a rating for
movie X from user A might not be random. It could indicate a lack of
interest in movie X. In statistical terms, this introduces a confounder
- an unmeasured or unaccounted variable that affects both the treatment
(rating movie X) and outcome (having an interest in movie X), leading to
biased results if not properly addressed.</p>
<p>In summary, while correlation can reveal interesting patterns, it
does not necessarily imply causation. Understanding causality is vital
for accurate data interpretation and effective decision-making.
Techniques like nearest neighbor methods, synthetic control groups, and
careful consideration of confounding factors are some ways to navigate
this complexity in data analysis.</p>
<p>The passage discusses the challenges of establishing causality,
particularly in the context of drug trials and observational data
analysis.</p>
<ol type="1">
<li><p>Randomized Clinical Trials (RCTs): These are crucial for
determining whether a drug has a measurable effect. In RCTs, two
randomly selected groups are created: one receives the drug, while the
other gets a placebo. The average outcome of each group is compared to
determine the Average Treatment Effect (ATE), which provides a reliable
assessment of the drug’s impact, controlling for potential confounding
variables. Despite these robust methods, RCTs only provide
population-level insights and cannot predict individual responses
accurately.</p></li>
<li><p>Personalized Treatment Effect: This refers to how a drug
interacts with an individual’s unique characteristics, health
conditions, and other factors. While RCTs can contain information for
assessing this effect, it requires deeper data analysis beyond the broad
conclusions drawn at the population level. Understanding personalized
treatment effects is vital for tailored medical interventions and
optimizing healthcare outcomes.</p></li>
<li><p>Causality from Observational Data: This is a significant
challenge in statistics as it’s often impractical or expensive to run
experiments that demonstrate causality. In health research, such as gene
therapy or curing diseases like Lyme, live subjects can’t always be used
due to ethical considerations and the rarity of events. Therefore, we
must enhance our ability to analyze mechanisms using models in research
and development.</p></li>
<li><p>Alternative Approaches for Determining Causality: When dealing
with observational data that doesn’t lend itself to RCTs, alternative
methods are necessary. These include constructing instrumental variables
or synthetic control groups to limit dependence on confounders. Abadie’s
article “Using Synthetic Controls” provides more detail on this
topic.</p></li>
<li><p>Interconnected Systems: The passage also highlights the
complexities arising from interconnections among subsystems, using
aviation as an example. Despite our familiarity with individual
components (like a plane’s autopilot), understanding the entire
system—including global control systems coordinating flights—is crucial.
Interconnectedness can lead to cascaded failures, as seen in the 2016
Atlanta snowfall example where delays at one airport (due to unsuitable
infrastructure for local weather conditions) led to projected delays at
another (LAX).</p></li>
</ol>
<p>In summary, while RCTs are powerful tools for establishing causality
in drug trials and other fields, they have limitations. Observational
data analysis presents its own set of challenges, especially when trying
to discern individual-level effects. Understanding complex
interconnected systems requires acknowledging these dependencies and
their potential cascading impacts. Enhancing our analytical
methods—including the use of synthetic controls for observational
data—is essential to navigate these challenges.</p>
<p>The passage discusses two main themes: the tension between privacy
and data utilization, and the issues of bias and fairness in data
collection and use.</p>
<ol type="1">
<li><p>Privacy Concerns: The author introduces Clearview AI, a company
that scraped billions of images from public web sources to create a
facial recognition database. This revelation sparked controversy due to
its scale, the fact that individuals were unaware their images had been
collected, and the wide range of entities (including major corporations
and a billionaire) that accessed this technology. The example
underscores how extensive data collection can infringe on personal
privacy, raising questions about whether society should accept these
privacy losses as necessary for solving complex challenges or strive to
reclaim lost privacy. Public opinion is divided: while many Americans
are willing to share sensitive data for potential benefits, others
prioritize privacy and express concerns about the risks associated with
data collection by both private companies and government
entities.</p></li>
<li><p>Data Bias and Fairness: The text also delves into historical and
ongoing issues of bias in data use. A notable example is the
introduction of racial categorizations into the U.S. Census in the 19th
century, which was based on a dubious genetic argument. This
categorization reinforced societal structures and policies that favored
certain groups over others, perpetuating systemic biases in areas like
voting rights, housing, healthcare, biometrics, policing, and crime
prevention. For instance, practices such as redlining have historically
prevented Black individuals from acquiring property in more valuable
neighborhoods, contributing to wealth disparities between Black and
White Americans.</p></li>
</ol>
<p>More recently, biased surveillance data collection in predominantly
Black neighborhoods has created a similar self-reinforcing cycle.
Initial assumptions about higher crime rates led to increased
surveillance, resulting in more arrests, which then provided
justification for even more intensive monitoring. This dynamic has
perpetuated stereotypes and supported policies that disproportionately
affect Black communities. The author suggests that these issues
highlight the critical need for addressing bias in data collection and
algorithmic fairness to avoid exacerbating existing societal
inequalities.</p>
<p>In summary, the passage explores the complex relationship between
data utilization and privacy concerns, revealing a public divided on
this issue. It also emphasizes the historical and ongoing problems of
biased data use, illustrating how such practices can perpetuate systemic
discrimination. The author suggests that finding a balance between
leveraging data for societal benefits and protecting individual privacy
while ensuring fairness in algorithmic decision-making is crucial to
avoid exacerbating existing social issues.</p>
<p>The text discusses several key themes surrounding data collection,
bias in machine learning (ML) algorithms, and societal implications.</p>
<ol type="1">
<li><p><strong>Counterfactual Fairness</strong>: The 2017 paper by
Kusner et al. highlights how biased data can lead to unfair outcomes. In
the context of crime prediction, higher arrest rates (often correlated
with racial bias) can result in algorithms predicting more future crimes
in certain areas, justifying increased police presence. This, in turn,
may diminish public safety as it perpetuates a cycle of over-policing in
marginalized communities.</p></li>
<li><p><strong>Fairness Methods</strong>: The authors discuss various
approaches to mitigate bias in ML algorithms. “Racially unaware” methods
attempt to ignore racial information from datasets, but this approach
often falls short due to systemic biases embedded within data.
Statistical Parity (SP) is another method that aims to make algorithmic
decisions equivalent across different groups, but it’s inconsistent in
detecting bias in complex real-world systems.</p></li>
<li><p><strong>Monetization of Data</strong>: The text emphasizes the
widespread practice among businesses to collect vast amounts of data for
profit maximization, with companies like Facebook and Amazon reaping
substantial earnings from user data. While advocating against
restricting data monetization per se, it asserts that focusing solely on
profit generation undermines the full potential of big data.</p></li>
<li><p><strong>Societal Benefits</strong>: The authors argue for a shift
in perspective - using big data not just to boost corporate profits but
also to address societal challenges, such as improving climate models
for small-scale farmers or enhancing public health responses during
pandemics. This necessitates building trust through transparent, ethical
data collection practices that respect individual privacy and
consent.</p></li>
<li><p><strong>Income Inequality Debate</strong>: The discussion also
touches on income inequality, referencing Thomas Piketty’s influential
work “Capital in the 21st Century.” Piketty argues that sluggish
economic growth periods tend to widen wealth gaps due to higher returns
on inherited capital compared to earned income. His proposed solutions
involve increased taxation for the wealthy and substantial social
investments, all underpinned by rigorous data analysis.</p></li>
<li><p><strong>Critique of Piketty’s Methodology</strong>: Gerald Auten
and David Splinter challenge Piketty’s methodology in measuring income
inequality trends. They argue that his team might have overestimated the
wealth of top earners, leading to an inflated perception of inequality
levels.</p></li>
</ol>
<p>In essence, this text underscores the critical need for fair and
ethical data practices in both corporate and societal contexts. It
advocates for a shift away from purely profit-driven data collection
towards more equitable and beneficial applications that consider broader
societal needs and respect individual rights.</p>
<p>The text discusses a significant ongoing debate among economists
regarding the measurement and estimation of income inequality, primarily
focusing on the methodological differences between two prominent
research groups led by Thomas Piketty, Emmanuel Saez, and Gabriel Zucman
(PSZ) on one side, and Andrew Auten and Luke Splinter (AS) on the
other.</p>
<p>The PSZ team uses a method that ranks incomes based on total amounts
reported by tax filers, splitting jointly reported income evenly between
filers, and then compares the top 1% to the bottom 50%. This approach
tends to show a larger gap in income distribution.</p>
<p>On the other hand, AS uses an approach that integrates the number of
children in a family and normalizes income by the unit’s size (i.e.,
dividing by the square root of the number of people). They assign the
full, unnormalized income to its respective grouping when ranking. This
method places higher-income units into lower groups, leading to a higher
average income for the lower 99% and effectively reducing the calculated
income gap.</p>
<p>Both teams use similar data sets from institutions like the Internal
Revenue Service (IRS), Federal Reserve Board, Bureau of Economic
Analysis, and others. They also agree on the timeline of government
interventions affecting misreported income. However, their primary
disagreement lies in how they handle factors such as family size and
unreported/evaded incomes.</p>
<p>When it comes to unreported incomes (tax evasion), both teams rely on
frequency-of-evasion research but differ in how they allocate evaded
income: PSZ proportionally distribute evaded income based on reported
income, while AS allocate evaded income to randomly selected filers by
reported income group.</p>
<p>The text emphasizes that despite their shared data sources and
understanding of certain factors, these methodological differences lead
to significantly different estimates of income inequality. It
underscores the importance of transparency in research (as demonstrated
by PSZ making their data available for replication) and the complex
challenges in deriving accurate conclusions from economic data due to
issues like unreported incomes and varying methodologies.</p>
<p>The debate, according to the text, serves as a prime example of
robust scientific discourse within the field of Data, Systems, and
Society, highlighting the multifaceted nature of analyzing
societal-level problems, the necessity for consistent and defensible
methodology, and the power and potential of data-driven thinking in
understanding complex socio-economic challenges.</p>
<h3
id="pp_19_40_a_confluence_of_fields">08.0_pp_19_40_A_Confluence_of_Fields</h3>
<p>The text provides a historical perspective on the evolution of
computing technologies, particularly focusing on mainframe computers and
their impact on data science, machine learning (ML), and artificial
intelligence (AI). Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Analog Computers</strong>: The history of computing dates
back to ancient times with analog machines that processed data based on
physical inputs like light, temperature, or voltage. These were used for
tasks such as calculating astronomical positions, solving mathematical
equations, predicting tides, and even aiming naval artillery.</p></li>
<li><p><strong>Transition to Digital-Analog Vacuum Tube
Computers</strong>: In the mid-20th century, digital-analog vacuum tube
computers emerged, such as MIT’s Whirlwind I, which could handle complex
numerical calculations and were used by nuclear physicists and
structural engineers.</p></li>
<li><p><strong>Rise of Mainframe Computers</strong>: The second half of
the 20th century saw a shift from analog to digital-based mainframe
computers. These general-purpose machines, powered by vacuum tubes,
transistors, integrated circuits, and later microprocessors, were faster
and more versatile than their analog counterparts. Mainframes dominated
academic and industrial computing in the 1960s and 1970s, leading to
significant technological advancements:</p>
<ul>
<li><p><strong>Technological Advancements</strong>: Mainframes
introduced various technologies that are now commonplace, including
memory storage media, user interfaces, programming languages, discrete
transistors, integrated circuits, output devices (like printers), and
graphic displays.</p></li>
<li><p><strong>Impact on Problem Solving</strong>: Physicist Howard H.
Aiken humorously noted, “We’ll have to think up bigger problems if we
want to keep them [computers] busy.” Mainframes enabled large-scale
simulations of complex phenomena like weather fronts, financial systems,
transportation networks, and aerospace flight.</p></li>
</ul></li>
<li><p><strong>Impact on Specific Research</strong>: The author mentions
how mainframe computers influenced his research in control theory and
the design of control systems under uncertainty. His PhD thesis adopted
a computational perspective, marking a shift from traditional analytical
“closed-form” solutions.</p></li>
<li><p><strong>Implications for Modern Data Science, ML, and
AI</strong>: The text suggests that the evolution of computing power,
exemplified by mainframes, has laid the groundwork for modern data
science, machine learning, and artificial intelligence. It implies that
as computing capabilities have increased, so too have the complexity and
scale of problems these fields can address.</p></li>
</ol>
<p>In essence, the text emphasizes that contemporary advancements in
data science, ML, and AI are not isolated phenomena but rather the
result of a long-term evolution in computing technologies. Understanding
this historical context can help appreciate the immense potential for
future breakthroughs in these fields as computing power continues to
grow.</p>
<p>The text discusses the historical significance of mainframes and
their role in shaping modern computing, artificial intelligence (AI),
and machine learning (ML).</p>
<ol type="1">
<li><p><strong>Mainframe Era’s Impact</strong>: The advent of mainframes
initiated a paradigm shift in systems management and processing. This
revolution brought rapid advancements in computing, but also had
unintended negative consequences. For instance, mainframe-enabled
databases facilitated racialized data collection, contributing to
discriminatory policies in areas like housing, healthcare, and
research.</p></li>
<li><p><strong>Turing Test</strong>: In the mid-20th century, Alan
Turing’s work on mainframes led to philosophical and scientific debates
about machine intelligence. His Turing Test asked whether machines could
think, sparking discussions on the nature of intelligence and AI
potential. Turing’s exploration of predicate calculus and logic revealed
that formal systems have inherent limitations, but this didn’t stop the
development of AI technologies we use today.</p></li>
<li><p><strong>Birth of AI</strong>: The 1956 Dartmouth Summer Research
Project on Artificial Intelligence, led by John McCarthy, Marvin Minsky,
Nathaniel Rochester, and Claude Shannon, marked a significant milestone
in AI’s development. Despite initial disappointments, the project laid
essential groundwork for future AI advancements. Government involvement
through DARPA funding at MIT further accelerated progress in the
field.</p></li>
<li><p><strong>Neural Networks</strong>: Parallel to AI development, ML
researchers explored artificial neural networks (NNs), inspired by
Hebb’s 1949 neuronal model. The Perceptron, introduced by Frank
Rosenblatt in 1957, was a pivotal moment as it demonstrated pattern
recognition and decision-making capabilities of machines using simple
mathematical models.</p></li>
<li><p><strong>Deep Learning</strong>: Advancements in multilayer neural
networks led to the rise of Deep Neural Networks (DNNs), characterized
by multiple hidden layers allowing for complex data relationships
representation. This development enabled DNNs to excel in tasks such as
image and speech recognition, natural language processing, and
more.</p></li>
<li><p><strong>Reinforcement Learning</strong>: Arthur L. Samuel’s 1950
checkers-playing program marked a significant step in AI by employing
reinforcement learning (RL). Using a scoring mechanism to estimate
winning probabilities and updating them through self-play, Samuel’s work
was an early example of algorithms and heuristics mimicking human
decision-making processes.</p></li>
<li><p><strong>Machine Learning Progress</strong>: The progression of ML
using NNs and DNNs is closely tied to statistical learning theory,
considering aspects like predictive accuracy, data resilience, and
computational complexity. High-dimensional statistics and optimization
have been crucial in shaping the core of ML advancements.</p></li>
</ol>
<p>In summary, mainframes set the stage for modern computing
revolutions, while figures like Turing, McCarthy, Minsky, and others
propelled AI’s development through theoretical frameworks and practical
applications. Neural networks and reinforcement learning, parallel to
these efforts, contributed significantly to the current state of AI and
ML technologies.</p>
<p>The text discusses three pivotal moments in the evolution of
computing and artificial intelligence (AI), each marked by significant
advancements that propelled the field forward.</p>
<ol type="1">
<li><p><strong>First Revolution - The Emergence of AI and Machine
Learning (ML):</strong> This period was characterized by a multitude of
theoretical approaches to understanding and building AI, including
symbolic AI (rooted in logic and computation) and connectionism
(inspired by neuroscience). However, the progress was hampered by the
physical limitations of computational power and storage capacity. The
computational complexity of training and running ML algorithms posed
formidable challenges.</p></li>
<li><p><strong>Second Revolution - Personal Computers (PCs):</strong>
The advent of PCs in the late 1970s and early 1980s, such as the Apple
II mentioned in the text, democratized computation. They provided
researchers with substantial computational power for scientific
programming, thesis writing, and more. Although initially limited to
scientific communities, PCs revolutionized computing when connected to
the internet in the late 1990s. This integration led to a new era of
distributed computing, where multiple machines could share resources and
computational power across networks, facilitating collaborative
problem-solving and information exchange.</p></li>
<li><p><strong>Third Revolution - Mobile Computing and
Smartphones:</strong> The early mobile computers of the 1980s, though
rudimentary by today’s standards, ignited public interest in portable
computing. Hardware advancements like metal-oxide-silicon transistors,
mobile transceivers, base stations, routers, telecommunications
circuits, and radio transceivers paved the way for 2G, 3G, and 4G
wireless networks. This led to smartphones that are now integral to our
daily lives, offering unparalleled connectivity, interactivity,
portability, and user-specific applications.</p></li>
</ol>
<p>Each of these revolutions built upon the last, expanding
computational capabilities and fostering new ways for machines to
interact with data and users. The convergence of logic, computation, and
neuroscience in AI and ML was facilitated by increasingly powerful
hardware, enabling more complex algorithms and driving the modern
landscape of artificial intelligence we see today.</p>
<p>Title: The Evolution of Machine Learning (ML) and Artificial
Intelligence (AI), and the Rise of Embedded Systems</p>
<ol type="1">
<li><p>Early Cell Phones to Modern Smartphones:</p>
<ul>
<li>Initially, cell phones were used primarily for voice calls and basic
text messaging.</li>
<li>The advent of new communication protocols enabled sending images and
videos alongside voice and texts.</li>
<li>By 2010, smartphone capabilities expanded significantly, including
financial transactions via Venmo and real-time stock trading.</li>
<li>As of 2022, nearly 84% of the world’s population uses smartphones
predominantly for activities beyond voice calls, such as social media,
internet browsing, and various apps.</li>
</ul></li>
<li><p>Machine Learning (ML) Resurgence:</p>
<ul>
<li>ML experienced a slump for decades before bouncing back in the 1990s
due to increased data availability and enhanced computational
power.</li>
<li>The marriage of big data with advanced computation led to
breakthroughs in various fields, including robust learning and
optimization techniques for Deep Neural Networks (DNNs).</li>
<li>Research communities started integrating autonomous learning into
their modeling approaches, with control researchers focusing on
safety-critical systems and the optimization community tackling
non-convex, high-dimensional problems.</li>
</ul></li>
<li><p>Advancements in Decision Theory:</p>
<ul>
<li>1997 marked a significant milestone when IBM’s Deep Blue defeated
world chess champion Gary Kasparov using reinforcement learning
(RL).</li>
<li>RL involves deriving optimal decision strategies through iterative
improvement, building upon the theory of optimal control and dynamic
programming from the 1950s-60s.</li>
</ul></li>
<li><p>AI’s Evolution and Current Landscape:</p>
<ul>
<li>ML has become a subset of Artificial Intelligence (AI), which is
broadly defined as any system that imitates human intelligence in
learning about its environment and decision-making capabilities.</li>
<li>While traditional AI approaches rely on predicate logic (expert
systems), modern data-driven methods dominate ML, allowing for complex,
automated decision-making without explicit human intervention.</li>
<li>Current AI applications span natural language processing, computer
vision, robotics, and creative tasks such as generating art and
music.</li>
</ul></li>
<li><p>The Rise of Embedded Systems:</p>
<ul>
<li>Embedded systems consist of computational hardware, input/output
components, and specialized software in larger devices that may include
additional electronic, electrical, or mechanical parts.</li>
<li>These systems are found in an extensive range of products from
digital watches to autonomous vehicles, exemplifying the convergence of
sensing, communication, and control capabilities.</li>
<li>Society’s increasing comfort with and dependence on distributed and
mobile systems highlights both the promise and challenges of navigating
this new technological landscape.</li>
</ul></li>
</ol>
<p>The passage discusses the evolution, importance, and implications of
embedded systems, with a particular focus on their role in the
development of networked decision systems (like formations of unmanned
aerial vehicles or UAVs). Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Emergence and Importance of Embedded Systems</strong>:
Embedded systems are computer systems designed to perform dedicated
functions within larger systems. They have become ubiquitous, with 98%
of new CPUs produced in 2009 used for embedded applications. The global
embedded market was valued at approximately $34.63 billion in 2022,
projected to double by 2027.</p></li>
<li><p><strong>Networked Decision Systems (NDS)</strong>: These systems
are a convergence of sensing, communication, and decision-making
capabilities made possible by advancements in faster processors, smaller
computers, and embedded solutions. UAV formations are a prime example;
each UAV has local control but must also coordinate with others to avoid
collisions and follow commanded trajectories, necessitating information
exchange within the formation or through external nodes.</p></li>
<li><p><strong>Examples of NDS</strong>: Apart from UAVs, other examples
include distributed emergency response systems, interconnected
transportation and energy networks, and social media platforms—all
relying on similar principles of sensing, communication, and
decision-making across multiple nodes.</p></li>
<li><p><strong>Future Implications - AI Convergence</strong>: The text
highlights how the convergence of computation, communication, and
decision-making forms a foundation for what’s often termed Artificial
Intelligence (AI). With the proliferation of IoT devices (20 billion in
2019), society faces challenges in managing and making sense of vast
amounts of data generated.</p></li>
<li><p><strong>Vignette - AlphaGo</strong>: The passage uses a 2017 Go
match between human world champion Ke Jie and Google’s AI, AlphaGo, to
illustrate the surprising capabilities of AI. AlphaGo’s victory
showcased the power of combining advanced search algorithms, pre-learned
neural networks, and extensive self-play data, outpacing human
strategies.</p></li>
<li><p><strong>The Rise of Generative AI</strong>: In 2022, ChatGPT—a
large language model (LLM)—emerged as the first widely available LLM,
sparking a transformative wave in AI applications. Its conversational
abilities and training on vast datasets opened new avenues for research
and business but also raised concerns over potential existential risks
due to its capacity to influence narratives and understanding of human
existence.</p></li>
<li><p><strong>Hallucination and Limitations</strong>: One significant
issue with LLMs like ChatGPT is ‘hallucination,’ where the AI fabricates
entirely false information. This occurs because LLMs lack an inherent
understanding of confidence levels or margins of error, leading to
content generation beyond their capabilities despite being able to
acknowledge uncertainty.</p></li>
<li><p><strong>Generative AI Beyond Language Models</strong>: The text
also introduces diffusion modeling as another aspect of generative AI,
focusing on producing output resembling real content. Diffusion models
can generate large volumes of data from small initial sets using
principles of system theory and dynamics, offering benefits such as
transfer learning—applying knowledge from one domain to
another.</p></li>
</ol>
<p>In essence, the passage underscores the growing significance of
embedded systems and networked decision systems in modern technology,
particularly in shaping AI’s evolution and its far-reaching implications
for society. It also highlights recent advancements in generative AI,
emphasizing both its promise and the challenges it presents, such as
issues of accuracy and potential existential risks.</p>
<p>The text discusses two significant real-world problems that
Artificial Intelligence (AI) has helped solve, along with a historical
perspective on the development of computing technology.</p>
<ol type="1">
<li><p><strong>Autonomous Control Systems and Moral
Dilemmas:</strong></p>
<p>The first problem discussed is the application of AI in autonomous
control systems, such as airplane autopilots. While these systems have
been remarkably successful in managing landings and improving safety,
full autonomy remains a challenge. The scenario highlights the broader
context of human-machine shared decision-making, which is also
applicable to future passenger road vehicles transitioning from human
control to mixed (shared human-AI) and then fully autonomous AI
control.</p>
<p>A crucial question raised is how we ensure that these machines adhere
to predetermined moral and ethical boundaries when they eventually take
complete control. This underscores the need for careful consideration of
the risks versus benefits in such implementations, as well as the
development of robust frameworks for ethical AI
decision-making.</p></li>
<li><p><strong>Protein Folding Prediction:</strong></p>
<p>The second problem highlighted is the challenge of understanding
protein folding and its implications for human health. Proteins are
crucial to cellular function; each has a specific role determined by its
unique 3D structure, which arises from amino acid sequences. Incorrect
folding can lead to serious diseases like cancer, cystic fibrosis, and
Alzheimer’s.</p>
<p>Scientists have identified about 150,000 protein folding patterns
since the 1990s, but millions more remain undiscovered. This hinders the
development of effective therapies for these conditions. Enter
AlphaFold, an AI system developed by DeepMind that predicts 3D protein
structures from amino acid sequences. It uses an iterative learning loop
to improve its predictions over time, often matching experimental
measurements’ accuracy levels. The implications are potentially
revolutionary: predicting protein folding could lead to new treatments
for complex diseases and vaccine development.</p></li>
</ol>
<p><strong>Historical Perspective on Computing:</strong></p>
<p>The text also provides a historical overview of computing technology
developments:</p>
<ul>
<li><p><strong>1990s Processor Speed Revolution:</strong> During this
decade, there was a significant increase in processor clock speeds,
leading to faster, smaller, cheaper, and more energy-efficient
microprocessors from competitors like AMD and Intel.</p></li>
<li><p><strong>2000s Parallel Computing Era:</strong> When processing
speed hit practical heat dissipation limits, parallel computing gained
prominence. This approach involves using multiple cores or processors to
solve larger problems simultaneously rather than sequentially. It was
initially limited to scientific computing and system simulations but has
since become common in consumer devices like desktops, laptops, and
smartphones due to advancements in manufacturing and
programming.</p></li>
<li><p><strong>Modern Distributed Computing:</strong> The text concludes
with a note on the pervasiveness of distributed computing in today’s
world. This includes not only familiar platforms like Google, Facebook,
YouTube, and Twitter but also less visible yet critical infrastructure
such as cellular networks, aircraft controls, banking systems, weather
forecasting models, renewable energy systems (like windmills), and
travel reservation platforms. These diverse applications highlight the
integral role of computing in modern society.</p></li>
</ul>
<p>This text discusses the intersection of data science, artificial
intelligence (AI), and societal implications, particularly focusing on
the role of digital platforms and AI in shaping future experiences and
decision-making processes.</p>
<p>The author begins by referencing Niels Bohr’s quote about the
difficulty of predicting future events, especially with data-driven
methods. Despite this challenge, they project that companies like
Facebook and Netflix will continue to accumulate vast amounts of
personal data, which will be analyzed using increasingly sophisticated
AI algorithms. These systems will drive personalized experiences, manage
platform behaviors, and even influence significant decisions in sectors
like finance.</p>
<p>The author critiques a common approach that assumes advancements in
hardware (Moore’s Law) and software will automatically lead to solutions
for humanity’s challenges. They point out several complications: the
environmental impact of massive computational energy consumption,
particularly concerning in the context of climate change; potential
speed and energy limitations on AI growth; and the necessity to
scrutinize the knowledge acquired by machines and evaluate the
reliability of their decisions.</p>
<p>The text then introduces a historical case study - the Arab Spring -
to illustrate the profound societal impact of information technology,
particularly social media. It highlights how a YouTube video of Mohamed
Bouazizi’s self-immolation sparked unrest across several Middle Eastern
countries, facilitated by digital communication tools that allowed for
the rapid dissemination of information and coordination of protests.</p>
<p>A specific example is provided: the 2011 Egyptian Revolution, where a
Facebook page created by Wael Ghonim played a crucial role in mobilizing
protesters to Tahrir Square, leading to the downfall of President Hosni
Mubarak’s regime. This case demonstrates how social media can empower
citizens to organize and express dissent against oppressive
governments.</p>
<p>However, the author also cautions against overstating the causal
effect of social media on revolutionary outcomes. They reference
research by Sahar Khamis suggesting that increased social media use in
repressed countries did not necessarily lead to more domestic activism
but rather shifted it to diaspora communities where freedom of
expression was possible. This implies that authoritarian regimes may
adapt and find alternative means to control their populations.</p>
<p>In summary, the text underscores the significant role data science
and AI will likely play in shaping future personalized experiences and
critical decision-making processes. It also emphasizes the need for a
holistic, multidisciplinary approach to evaluate the broader societal
implications of these technologies. Historical examples like the Arab
Spring illustrate both the potential and limitations of digital tools in
driving societal change, highlighting the complex interplay between
technology, human behavior, and political structures.</p>
<p>The text discusses the impact of digital technology, particularly
social media and AI, on democratic principles and practices. It uses the
Cambridge Analytica-Facebook scandal as a primary case study to
illustrate these concerns.</p>
<ol type="1">
<li><p><strong>The Rise of Social Media and Data Revolution</strong>:
The advent of distributed computing architecture in the 1990s and the
proliferation of wireless, mobile networks led to an explosion of data
(often referred to as the ‘data deluge’) and a social media revolution.
This digital transformation has had profound effects on society over the
past two decades.</p></li>
<li><p><strong>Cambridge Analytica-Facebook Scandal</strong>: This
incident involved Cambridge Analytica, a political consulting firm,
gaining access to millions of Facebook users’ data without consent. This
data was then used to create detailed psychological profiles of these
individuals, which were subsequently leveraged for targeted political
advertising.</p>
<ul>
<li><p><strong>Psychological Profiling</strong>: By understanding users’
personalities and preferences, Cambridge Analytica could tailor ads to
sway their political views or biases. For instance, an anxious user
might see ads related to a terrorist act, ostensibly to influence their
voting behavior.</p></li>
<li><p><strong>Misuse of Data</strong>: The scandal raised serious
ethical concerns about the misuse of user data for political purposes.
It highlighted the delicate balance between social media platforms’ role
in facilitating communication and their responsibility to safeguard
users’ privacy and election integrity.</p></li>
</ul></li>
<li><p><strong>Unresolved Questions and Ethical Concerns</strong>: The
Cambridge Analytica-Facebook scandal brought to light several unanswered
questions about AI and machine learning (ML).</p>
<ul>
<li><p><strong>Predictive Accuracy of AI/ML</strong>: While AI can
combine behavior analysis, neuroscience, and psychology, the extent to
which it can accurately predict individual emotional responses to
information is disputed among researchers.</p></li>
<li><p><strong>Privacy Violations</strong>: There’s a significant
ethical issue around Facebook sharing sensitive user data without
explicit consent, violating users’ privacy rights.</p></li>
<li><p><strong>Causal Influence and Election Impact</strong>: The extent
to which Cambridge Analytica’s tactics influenced the election outcome
is challenging to determine. Voting behavior is often private, and it’s
debated whether such targeted ads significantly alter political
convictions.</p></li>
</ul></li>
<li><p><strong>Need for Regulation</strong>: Given these ethical
concerns and potential for manipulation, there are pressing questions
about the appropriate regulation of voter data use in political
campaigns. Should societies implement restrictions on how such data can
be used to influence voters?</p>
<ul>
<li><strong>Network Effects and Consensus Manipulation</strong>: The
ability of AI tools to manipulate information and individuals within
social networks, leading to unpredictable outcomes, also raises
questions about the appropriateness of unregulated AI use in shaping
public understanding and consensus.</li>
</ul></li>
</ol>
<p>In summary, the text underscores how digital technologies,
particularly when weaponized for political purposes, can undermine
democratic principles. It emphasizes the need for ethical considerations
and regulation to protect privacy, ensure election integrity, and
maintain a well-informed citizenry capable of making independent
decisions.</p>
<h3
id="pp_41_56_who_and_what_should_drive_decision-making">09.0_pp_41_56_Who_and_What_Should_Drive_Decision-Making</h3>
<p>Title: Who - and What - Should Drive Decision-Making? Harnessing Data
for the Good of Society</p>
<p>The chapter discusses the complex relationship between data, AI/ML
technologies, and decision-making, emphasizing the need for a balanced
approach that considers both logical precision and social justice. It
outlines various aspects to ponder as we navigate this evolving
landscape:</p>
<ol type="1">
<li><p><strong>Balance Between Mathematical Logic and Social
Justice</strong>: The authors stress that while AI and ML can generate
logically consistent outcomes, they are not inherently rational or
socially beneficial. We must critically evaluate these tools to ensure
they contribute positively to society rather than perpetuating biases
and inequalities.</p></li>
<li><p><strong>Addressing Persistent Biases</strong>: The chapter points
out that AI systems can reflect and amplify existing human biases,
potentially reinforcing systems of oppression and inequity. It’s crucial
to identify and mitigate these biases during the development process and
continuously monitor their effects post-deployment.</p></li>
<li><p><strong>Human Control and Accountability</strong>: A key question
raised is how much control humans should retain over AI/ML
decision-making systems, and what mechanisms should be in place for
accountability when things go wrong. The authors argue that we must not
abdicate responsibility entirely to machines.</p></li>
<li><p><strong>Ethics of Machines vs. Humans</strong>: They question
whether the same ethical standards applicable to human behavior should
also apply to AI/ML systems, or if new frameworks are needed given their
unique capabilities and limitations.</p></li>
<li><p><strong>Free Market as a Solution</strong>: The chapter
challenges the notion that unregulated markets will automatically
deliver societal good. It suggests that proactive intervention is
necessary to guide AI/ML development in ways that benefit everyone, not
just those with economic power.</p></li>
<li><p><strong>Examples of Promises and Pitfalls</strong>: Various
examples are given illustrating both the transformative potential (like
faster drug discovery or safer self-driving cars) and the risks (such as
biased chatbots or lethal autonomous vehicle accidents) of AI/ML
technologies.</p></li>
<li><p><strong>Distributed Decision Systems</strong>: The text
acknowledges that future decision-making will increasingly rely on
distributed agents (AI/ML systems) rather than humans alone. It
advocates for understanding and engaging deeply with these systems to
reap their benefits while mitigating potential negative
consequences.</p></li>
<li><p><strong>Human vs. Machine</strong>: The authors warn against
becoming passive in the face of superior machine intelligence, urging us
to maintain our critical thinking and agency. They reference Norbert
Wiener’s warning that future AI advancement will demand even greater
human intellectual effort rather than passive reliance on
machines.</p></li>
</ol>
<p>The chapter ultimately calls for a broad-based dialogue involving
various stakeholders - private individuals, academics, public servants,
advocacy groups, and corporate decision-makers - to shape the ethical,
regulatory, and operational frameworks governing AI/ML technologies.</p>
<p>The text discusses the challenges and ethical dilemmas surrounding
the use of artificial intelligence (AI) and machine learning (ML) in law
enforcement, predictive policing, facial recognition, and autonomous
vehicles.</p>
<ol type="1">
<li><p><strong>Bias in AI Systems</strong>: The text highlights how AI
systems, particularly those used in law enforcement, can perpetuate
racial biases due to the data they are trained on. For instance, if
arrest data from police departments is predominantly sourced from
neighborhoods with higher percentages of Black and Brown residents, AI
tools learn to direct more law enforcement resources to those areas,
exacerbating existing racial disparities in policing. Similarly, facial
recognition algorithms have been found to be less accurate for
individuals with darker skin tones due to being primarily trained on
predominantly white faces. This leads to the risk of wrongful arrests
and perpetuates racial bias.</p></li>
<li><p><strong>Limitations in Addressing Bias</strong>: Efforts to
mitigate these biases through alternative training methods have yielded
limited success, as shown by a study where Carnegie Mellon researchers
found that “well-trained” predictive policing algorithms still
underreported crime hotspots and overreported them in other areas. This
demonstrates the difficulty in achieving unbiased AI systems through
technical fixes alone.</p></li>
<li><p><strong>Ethical Decision-Making</strong>: The text also delves
into ethical dilemmas posed by autonomous vehicles, such as the decision
of whether to prioritize passenger safety or pedestrian safety in an
accident scenario. This conundrum raises questions about who should
determine the correct balance between individual safety and collective
wellbeing.</p></li>
<li><p><strong>Complexities in AI Decision-Making</strong>: The text
underscores that creating fair and equitable AI systems isn’t solely a
technical challenge but also involves understanding societal norms,
ethics, and confounding factors (variables not initially considered)
that can lead to biased outcomes. It argues for a broader perspective
that includes input from economists, sociologists, and other relevant
fields, in addition to computer science and programming
expertise.</p></li>
<li><p><strong>Malleable Ethics</strong>: Lastly, the text touches on
the evolving nature of societal ethics and how AI systems must adapt to
these changes. It points out that traditional, static ethical guidelines
may not suffice in the context of rapidly developing technologies like
autonomous vehicles.</p></li>
</ol>
<p>In summary, the text emphasizes the persistent challenges and ethical
dilemmas associated with AI systems’ deployment, particularly in
sensitive areas like law enforcement and autonomous decision-making. It
underscores the need for a multidisciplinary approach that combines
technical expertise with societal understanding to build fair and
equitable AI tools.</p>
<p>The text discusses the complexities and challenges surrounding
decision-making processes, particularly in relation to artificial
intelligence (AI) and machine learning (ML). It raises questions about
the standards we apply to AI versus humans, suggesting that our
expectations for AI are often unrealistically high.</p>
<ol type="1">
<li><p><strong>Human vs Machine Decision-Making</strong>: The text
points out an interesting paradox: while we don’t train human drivers to
calculate potential casualties in complex situations, we expect
autonomous vehicles to perform flawlessly. This discrepancy implies a
double standard and calls for examining our ethical framework regarding
AI.</p></li>
<li><p><strong>Dual Bifurcated Mindset</strong>: The author suggests
that society has different tolerance levels for errors based on the
application of AI. Autonomous vehicles are held to a much higher
standard than, say, loan approval systems or language models like LLMs
(Large Language Models). This inconsistency in expectations is
problematic and warrants further ethical exploration.</p></li>
<li><p><strong>Lack of Ethical Education</strong>: The text suggests
that ethical education could play a crucial role in fostering
responsible AI behavior. It poses the hypothetical question: Could such
education have deterred the creation and use of nuclear weapons, given
their devastating consequences?</p></li>
<li><p><strong>Outpacing Societal Understanding</strong>: The rapid
advancement of technology like AI and ML is outstripping our ability to
fully comprehend and address its societal implications. The author
argues for a more deliberate consideration of the ethical ramifications
of these technologies, learning from past experiences with media
regulation (like TV content appropriateness).</p></li>
<li><p><strong>Robustness in ML</strong>: The text highlights the
vulnerability of machine learning algorithms to small, intentional
changes that can lead to significant and unpredictable outcomes. This
phenomenon raises concerns about the reliability and robustness of these
models, especially in large language models (LLMs), which lack a
conscious understanding of their information generation
process.</p></li>
<li><p><strong>Causality and Fairness</strong>: The author stresses the
importance of causal analysis in achieving algorithmic fairness,
particularly in sensitive areas like police brutality. Using the
stop-and-frisk policy as an example, the text explains how influence
diagrams can be constructed to understand and quantify factors
contributing to discriminatory outcomes, even when controlling for
variables like suspicion of carrying a weapon.</p></li>
</ol>
<p>In essence, the text underscores the need for a more nuanced,
ethically-informed approach to AI decision-making. It argues that as we
continue to develop and deploy sophisticated AI systems, we must also
invest in understanding their implications and ensuring they align with
our societal values and expectations.</p>
<p>The text discusses several critical aspects related to the use of
data, AI systems, and privacy laws. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Causality and Confounding Variables</strong>: The
importance of careful analysis in establishing causal relationships is
emphasized. Influential diagrams may be misleading if they omit crucial
variables. For instance, a study on racial disparities in police stops
by Emma Pierson et al. uncovered a significant confounder: Black drivers
were less likely to be stopped after sunset, suggesting potential racial
bias.</p></li>
<li><p><strong>Data-driven Decision Making and Legal Landscape</strong>:
Large entities, including corporations, countries, states,
municipalities, and universities, increasingly rely on AI systems for
decision-making. However, regulating these systems and the data they use
presents a significant challenge for policymakers. The pervasive use of
sensors makes it unlikely that personal data can evade harvesting and
manipulation.</p></li>
<li><p><strong>Data Privacy and Ownership</strong>: Fundamental legal
frameworks need to address data privacy and ownership issues, resolving
conflicts between individuals and organizations. For example, Google’s
free search service raises questions about its right to access and use
user data without explicit consent or payment for privacy
protection.</p></li>
<li><p><strong>EU vs US Approaches to Data Privacy Regulation</strong>:
The European Union (EU) has been more proactive in regulating privacy
through laws like the General Data Protection Regulations (GDPR), which
imposes heavy penalties for mishandling personal data. Critics argue,
however, that such strict regulations disproportionately affect smaller
EU enterprises compared to US-based tech giants with broader profit
margins. This is referred to as the “Google Data Privacy Regulation”
critique, highlighting potential stifling of innovation in heavily
regulated zones.</p></li>
<li><p><strong>Bias in AI Systems</strong>: As AI becomes integral to
decision-making processes, it’s crucial to monitor and prevent biases
within these systems. Discovering systemic biases often occurs late in
the development cycle, especially when AI is produced through federated
learning. Legal challenges arise at the intersection of data use and AI
regulation for both developers and enforcement bodies.</p></li>
<li><p><strong>Differential Privacy</strong>: This concept, introduced
by Cynthia Dwork et al., involves adding calibrated noise to data within
algorithms to protect privacy. It ensures that an algorithm’s output
cannot be reverse-engineered or used to infer private information about
individual data points. Not all algorithms can feasibly adopt
differential privacy due to sensitivity constraints, but those that can,
like those calculating averages, offer a promising alternative to
traditional regulatory measures.</p></li>
</ol>
<p>In essence, the text underscores the complexities of using data and
AI systems, particularly concerning privacy, bias, and legal regulation.
It highlights the need for careful analysis, robust legal frameworks,
and innovative techniques like differential privacy to navigate these
challenges effectively.</p>
<p>The passage discusses the importance of human accountability in
decision-making processes, particularly as technology advances and
artificial intelligence (AI) becomes more integrated into various
sectors of society. It emphasizes that while AI can be a valuable tool
for tasks such as drug design, humans must maintain their understanding
of underlying concepts to fully leverage these tools effectively.</p>
<p>The text further explores the concept of ‘data set externalities,’
referring to the broader implications and consequences of data use. This
includes considering a nuanced definition of one’s data, suggesting that
data ownership and control are critical issues in today’s digital
age.</p>
<p>The authors then delve into the topic of market forces and data
monetization, using online advertising as a case study. They describe
how platforms like Netflix and Amazon use predictive models to influence
user behavior through personalized recommendations, which can lead to
narrow genre suggestions (in Netflix’s case) or targeted advertisements
(in Amazon’s).</p>
<p>The passage highlights that these recommendation systems can amplify
biases. For instance, research shows that ads for STEM programs are more
heavily targeted towards men rather than women, due to retailers
outbidding educational institutions for ad space on websites. This
results in fewer women seeing these ads, creating a vicious cycle where
less interest in STEM among women appears justified by the limited
exposure to such promotions.</p>
<p>The authors warn that while data-driven systems can be beneficial,
they also carry risks. The ability of AI and machine learning (ML) to
generate convincing, artificial scenarios adds a layer of complexity to
monitoring and regulating these systems. They underscore the necessity
for continued human oversight and involvement in decision-making
processes, especially when dealing with complex, interconnected systems
where the consequences of actions may not be immediately apparent.</p>
<p>In summary, this passage argues that as we increasingly rely on AI
and data science to make decisions, it’s crucial to remember the
importance of human accountability and understanding. It cautions
against over-reliance on automated systems, emphasizing the potential
for AI to inadvertently perpetuate biases and the need for ongoing human
involvement in decision-making processes. The authors also highlight the
significance of considering data set externalities and ensuring
individuals maintain control over their data.</p>
<p>The text discusses several key issues surrounding data, digital
advertising, and privacy within the context of STEM education and
broader society.</p>
<ol type="1">
<li><p><strong>Data as a Product</strong>: The central premise is that
in digital advertising, users are essentially the product, not the
consumers. Advertisers pay for access to user data, which reveals
consumer interests when they click on ads. This raises questions about
fair compensation and ownership of personal data.</p></li>
<li><p><strong>Data Marketplaces and Coordination</strong>: The idea is
floated that if users were to collectively refuse to engage with digital
ads (a “work stoppage”), it could potentially give them leverage to
demand payment from platforms for using their data. However, the text
points out the impracticality of achieving such coordinated action on a
global scale.</p></li>
<li><p><strong>Value of Data</strong>: The value of data is inherently
tied to its use, not its collection. This makes it challenging to
establish intrinsic value and complicates discussions about ownership
and privacy rights.</p></li>
<li><p><strong>Externalities in Data Markets</strong>: When data about
individuals is bought and sold, there are externalities or side effects
that can negatively impact third parties without being reflected in the
cost of goods or services. For instance, when multiple companies
purchase the same user data, each company’s value from that data
decreases due to competition.</p></li>
<li><p><strong>Genetic Data and Privacy</strong>: The text uses 23andMe
and Ancestry as examples of businesses where users pay for genetic
information about themselves. This data can be highly valuable but also
poses privacy risks, such as the potential for misuse by insurers or law
enforcement. The “Golden State Killer” case is cited as an example of
how such data can aid investigations, highlighting the complex interplay
between privacy and societal benefits.</p></li>
<li><p><strong>Decision-making and Society</strong>: Lastly, the text
touches on who should drive decision-making in data use. It suggests
that considerations should extend beyond individual users or companies
to include societal good. This implies a need for balanced policies that
protect privacy while also allowing beneficial uses of data, like in
criminal investigations or medical advancements.</p></li>
</ol>
<p>In essence, the text underlines the complexities and tensions around
data use in the digital age, particularly within educational and
commercial contexts, and questions how we can balance individual rights
with societal benefits.</p>
<h3
id="pp_57_72_a_transdiscipline_is_born">10.0_pp_57_72_A_Transdiscipline_Is_Born</h3>
<p>The text discusses the evolution of academic disciplines and the
emergence of transdisciplinary initiatives, using the MIT Initiative for
Data Science and Society (IDSS) as a case study.</p>
<p>Disciplines are defined by five key attributes: community, journals,
educational track, shared understanding, and criteria for evaluation.
They offer efficiency in higher education, providing reliable streams of
skilled professionals and structured knowledge progression. Disciplines
also support research by offering stable environments for exploration
and discovery, even when the immediate relevance or application to
everyday life is unclear.</p>
<p>However, disciplines have limitations. Their focus on specific tools,
methods, and phenomena can exclude a wide range of pertinent questions.
This is particularly evident in tackling complex, rapidly evolving
issues that span multiple disciplines, such as the impacts of digital
media and social networks. Collaboration across different fields can be
hindered by differences in language and methodology.</p>
<p>In response to these challenges, new forms of research have emerged:
multidisciplinary (where researchers from different disciplines work
together on a project) and interdisciplinary (where a problem is
addressed by drawing on multiple disciplines). The MIT IDSS,
specifically its ISOLAT initiative formed in response to the COVID-19
pandemic, exemplifies this shift.</p>
<p>ISOLAT was a volunteer collaboration that operated outside
traditional academic norms of peer review and publication cycles. It
focused on creating a data structure from heterogeneous datasets related
to the pandemic, making predictions about critical variables, and
understanding the effects of interventions. This initiative underscores
how transdisciplinary teams can provide timely, evidence-based guidance
for pressing societal challenges, even when members lack specific
expertise in the issue at hand.</p>
<p>The author argues that such dynamic, responsive initiatives are
becoming increasingly important as complex global problems demand
comprehensive solutions that transcend traditional disciplinary
boundaries.</p>
<p>The text discusses the evolution of disciplinary approaches in
research, distinguishing between multidisciplinary, interdisciplinary,
and transdisciplinary methods.</p>
<ol type="1">
<li><p><strong>Multidisciplinary</strong>: This approach involves
collaboration among multiple disciplines but each maintains its distinct
identity, methodology, and theoretical base. An example given is the
study of internet bullying, which incorporates aspects from social
interactions, psychology, and digital networks. Collaboration is
essential for tackling such complex issues, yet each discipline retains
its own practices and perspectives.</p></li>
<li><p><strong>Interdisciplinary</strong>: This method involves the
fusion of two or more traditional disciplines to address a problem that
none could solve alone. Molecular biology is cited as an example; it
emerged from the convergence of genetics, physics, and chemistry. This
interdisciplinary approach led to breakthroughs not achievable by any
single discipline in isolation.</p></li>
<li><p><strong>Transdisciplinary</strong>: Unlike multidisciplinary or
interdisciplinary, transdisciplinarity goes beyond simple interaction or
fusion of disciplines. It generates novel ways of thinking and problem
formulation at the intersection of Data, Systems, and Society (DSS). The
text suggests that mathematics is a prime example of this, as it
permeates various fields (like economics, computer science, management)
without losing its core identity, thereby enhancing the approachability
of disciplinary silos.</p></li>
</ol>
<p>The text further highlights early challenges in data-driven research,
particularly the cultural clashes between computer scientists and social
scientists. The rapid development of data analytics in the late 1990s
led to a mismatch in vocabulary, methods, and objectives among these
disciplines. Computer scientists were driving the field with algorithmic
processing and big data techniques, while social scientists were more
accustomed to nuanced, qualitative data analysis. This disparity often
resulted in misunderstandings and reluctance from social sciences to
embrace computational methods fully.</p>
<p>Despite these challenges, the importance of
interdisciplinary/multidisciplinary efforts became apparent due to the
complexity of real-world problems that transcended single disciplines.
The urgency of addressing these issues drove researchers to collaborate
despite the headwinds of disciplinary silos in academia, which typically
favor specialization over interdisciplinarity for factors like job
security, funding, and recognition.</p>
<p>In summary, while multidisciplinary and interdisciplinary approaches
involve collaboration or fusion among existing disciplines,
transdisciplinary research like DSS aims to generate novel mindsets and
problem-solving methods at the intersection of different domains. The
text emphasizes the need for such an approach in addressing complex
societal challenges, especially those involving data and technology,
despite the historical resistance from traditional academic
structures.</p>
<p>The text describes the emergence of an interdisciplinary field called
Integrated Design &amp; Management Systems Science (IDSS) at MIT. This
new discipline was born out of the need to tackle complex,
multidimensional real-world problems that transcend traditional academic
boundaries and require a holistic approach.</p>
<p>The driving forces behind IDSS are large-scale, interconnected
systems such as energy, transportation, finance, healthcare,
manufacturing, social networks, and others, which have rapidly evolved
due to advancements in smart sensors, high-speed communications, social
networking, and real-time decision platforms. However, these
developments outpaced society’s ability to understand and mitigate their
unintended negative consequences.</p>
<p>The creators of IDSS recognized that academia, industry, and
government were not effectively harnessing the power of these complex
systems to uphold universal human values like justice, privacy, and
sustainability. They believed this was primarily due to lack of
attention and understanding rather than malicious intent or capitalist
pressures.</p>
<p>To address these challenges, IDSS developed a framework known as the
“IDSS Triangle.” This model combines three types of data:
scientific/engineering data, institutional data (global in scope,
derived from organizational interventions), and social interaction data
(individual-level, made available through social media). The triangle is
used to analyze societal challenges that often involve interactions
among these three domains.</p>
<p>The IDSS Triangle aids in understanding complex problems by
emphasizing the need to investigate systemic operations, human-system
interactions, and relevant policies/regulations. For instance, when
dealing with issues like biases on digital platforms (like Facebook), it
requires examining the platform’s operation, user behavior, and
applicable rules.</p>
<p>The core principles of IDSS involve data collection, reliable
prediction, understanding system fragility, bias, resilience, and
security, addressing causality vs. correlations, ensuring
sustainability, defining interaction architectures, and proposing
ethical solutions promoting justice and privacy.</p>
<p>IDSS aims to foster human-centered and socially-centered data
science, focusing on enhancing human judgment rather than replacing it
with artificial intelligence. The ultimate goal is to use AI, Machine
Learning (ML), and similar technologies to augment human understanding
of complex systems, not replace it.</p>
<p>In essence, IDSS was established at MIT in 2014 to leverage data
about individuals and systems for societal benefit. It models and
predicts systemic behavior, designs systems, and advocates for social
welfare, sustainability, and resilience while considering equity and
systemic bias. The methodologies are rooted in statistics,
information/decision sciences, human behaviors, and institutional
interactions, involving domain experts as necessary to fully understand
the complex systems they study.</p>
<p>The passage discusses the emergence of a new transdiscipline,
referred to as DSS (Data Science for Society), which integrates
information, decision sciences, statistics, and principles from social
sciences, economics, and humanities. This transdiscipline is designed to
address complex societal challenges by leveraging data and systems,
recognizing their origins in human actions and their role in creating
solutions for societal needs.</p>
<p>The author introduces “The Triangle,” a conceptual framework
illustrating the dynamic interactions among physical and engineered
systems, individual social behavior, and social institutions. Data is
portrayed as the connective tissue that embodies all three nodes of this
triangle. This model acknowledges that data can be complex and
heterogeneous, often incomplete, with different time scales, and lacking
natural spatial or temporal stamps.</p>
<p>Probabilistic thinking, statistics, and decision-making under
uncertainty form the theoretical backbone of DSS, providing a framework
for modeling complex phenomena. The approach to each challenge varies
depending on its nature; for instance, developing an algorithmic
diagnostic tool for cancer would require different considerations than
creating a customer service tool in digital realms.</p>
<p>The author emphasizes that DSS should not be viewed as merely a
facilitator of sporadic interdisciplinary or multidisciplinary
collaborations but as a comprehensive transdiscipline embedded within
all relevant fields, including engineering, sciences, social sciences,
and management. The establishment of an Institute for Data Science and
Systems Studies (IDSS) at MIT exemplifies this vision, aiming to bring
together diverse domains under a shared theoretical framework and
practical toolkit.</p>
<p>The Triangle is crucial in shaping IDSS as a transdiscipline, guiding
researchers, educators, and policymakers towards a holistic
understanding of complex problems like inequality, misinformation,
economic development, climate change, technology policy, and pandemic
response. By providing a structured approach to these challenges, DSS
can foster more effective use of heterogeneous data in modeling systems
and devising solutions.</p>
<p>The author draws an analogy with mathematics’ role in science and
engineering, highlighting how mathematical departments not only forge
new theories but also impart fundamental principles and tools that are
vital to advances in various disciplines. This transdisciplinary
approach is essential for DSS, enabling it to bridge gaps between
different fields and foster a cohesive community of scholars and
practitioners dedicated to tackling society’s most pressing issues
through data-driven insights.</p>
<p>The passage discusses the emergence of a new transdiscipline called
Information Decision Systems Science (IDSS) and its interconnected
nature with various fields. The authors argue that IDSS cannot be
established as an independent discipline but rather should be integrated
within existing ones due to its inherent dependencies on other domains.
This relationship is depicted through a Venn diagram referred to as the
IDSS Ecosystem (Figure 4.2).</p>
<p>The text uses the COVID-19 pandemic as a case study to illustrate the
complexities and interdependencies of this transdiscipline, highlighting
what they term as ‘The Triangle’. The components of this triangle
include:</p>
<ol type="1">
<li><strong>Biological aspect</strong>: This refers to characteristics
of the virus like its size and binding ability which influence its
transmission rate (R0).</li>
<li><strong>Social data</strong>: Human behavior such as interaction
patterns that contribute to virus spread.</li>
<li><strong>Institutional data</strong>: Government policies and
interventions, including lockdowns, aimed at controlling the pandemic’s
impact.</li>
</ol>
<p>Addressing these challenges necessitates considering the interactions
between all three elements, demonstrating how IDSS integrates
scientific, social, and institutional aspects.</p>
<p>The narrative then shifts to discussing responses to the pandemic,
particularly vaccination efforts. The authors draw a parallel between
this global vaccination program and the Apollo moon landing missions of
the 20th century, emphasizing its magnitude and potential long-term
impact on medical advancements beyond COVID-19.</p>
<p>Rapid testing is also highlighted as another crucial intervention.
The scientific community developed and deployed these tests at an
unprecedented pace to help curb the virus’s spread by enabling the
quarantine of infected individuals who might not self-isolate due to
asymptomatic or mild symptoms.</p>
<p>Control theory, a component of IDSS, played a significant role in
devising optimal testing strategies. By estimating R0 (the basic
reproduction number) based on infection data, scientists could determine
the necessary testing rate to outpace viral spread within communities.
This approach was successfully implemented at MIT and subsequently
adopted by many other educational institutions during 2020’s fall
semester.</p>
<p>In essence, this passage underscores how IDSS embodies an
interdisciplinary approach essential for managing crises like pandemics,
combining elements of statistics, domain knowledge, human behavior, and
decision systems to form comprehensive solutions.</p>
<p>The text discusses a testing strategy implemented during the COVID-19
pandemic at an institution (presumably an educational one), focusing on
students, faculty, and staff.</p>
<ol type="1">
<li><p><strong>Testing Frequency</strong>: Students were tested three
times a week, while faculty and staff were tested once a week. This
frequency was determined by calculations considering the spread of
COVID-19 and student interactions.</p></li>
<li><p><strong>Quarantine Procedure</strong>: Those who tested positive
among students were isolated in a separate dormitory. Infected faculty
and staff had to wait for a negative test result or for more than 10
days after symptoms before returning to work, reflecting different
protocols for different groups.</p></li>
<li><p><strong>Effectiveness</strong>: Despite the challenges, this
strategy was remarkably effective in maintaining continued interaction
among students, faculty, and staff, indicating its success in balancing
public health and educational continuity.</p></li>
<li><p><strong>Development of “whentotest.org” App</strong>: The
institution’s Institute for Data Systems and Society (IDSS) collaborated
with the National Institutes of Health/Center for Integrated Biomedical
Informatics (NIH/CIMIT) to create this app. This tool aimed to simplify
the testing theory, catering to public needs during the pandemic by
offering straightforward guidance on when to get tested.</p></li>
<li><p><strong>Public Policies and IDSS Triangle</strong>: The concept
of the “IDSS Triangle” is introduced here, which encapsulates the
interconnectedness of Decision Support Systems (DSS) in policy-making.
This triangle involves three components: the science of contagion,
social and economic structures/behaviors, and trade-offs embedded in
policies like testing and quarantining.</p></li>
<li><p><strong>Impact of Policies</strong>: The article “Implicit
feedback policies for COVID-19: why ‘zero-COVID’ policies remain
elusive” by Jadbabaie, Shah, and Sarkar reveals that publishing data
about local infection rates can lead to increased self-restraint among
individuals. This underscores the importance of considering social
behaviors when designing public health policies.</p></li>
<li><p><strong>Unintended Consequences</strong>: The policy had
unforeseen consequences; it disproportionately affected under-resourced
communities. Policies aimed at controlling the spread of COVID-19
inadvertently exacerbated existing health and economic disparities,
highlighting the need for policies that consider fairness and
equity.</p></li>
</ol>
<p>The IDSS Triangle is crucial in evaluating policy impacts and
ensuring equitable responses to crises like the COVID-19 pandemic. It
emphasizes the necessity of a holistic approach when formulating public
health strategies, taking into account scientific data, societal
factors, and potential trade-offs.</p>
<h3
id="pp_73_88_complexity_versus_relevance">11.0_pp_73_88_Complexity_versus_Relevance</h3>
<p>Title: The Power of Abstractions: Complexity vs Relevance</p>
<p>This passage explores the concept of abstraction, its importance, and
its applications in various fields. It begins by discussing Christoph
Niemann’s perspective on abstraction in graphic design, where he
emphasizes that every idea requires a specific amount of information or
detail. He uses the example of a heart symbolizing love—too simplistic
(a red square) would be unrecognizable, while overly realistic could
obscure the intended message.</p>
<p>The essay then transitions to physics, illustrating how simplified
models or abstractions are instrumental in understanding complex
phenomena. Newton’s law of gravitation is given as an example—though it
lacks the complexity of space-time dynamics, it provides a powerful and
intuitive framework for predicting motion.</p>
<p>The author argues that abstractions are instinctive to human
cognition, confirmed by neuroscience. However, creating appropriate
abstractions for complex societal systems demands more conscious effort
and expertise. The author posits that there’s no systematic method for
deriving an abstraction; rather, it stems from our creative minds.</p>
<p>The text then delves into Information Theory, pioneered by Claude
Shannon in the 1930s and 1940s. Shannon aimed to model signal separation
amidst noise and message decoding from encrypted signals. He used
probability theory to quantify uncertainty in digital transmissions,
introducing the concept of ‘information’ as the freedom one has in
choosing any particular communication, reduced by language rules.</p>
<p>Shannon defined entropy, which measures the randomness within an
alphabet, crucial for determining the shortest codes (source coding)
representing messages from a source. By applying this to English words,
he showed how to drastically reduce digital signaling needs for reliable
transmission—a significant leap forward in communication efficiency.</p>
<p>Shannon’s work on channel coding provided optimal methods for
introducing just enough redundancy into signals to ensure complete
recovery at the receiving end. This abstraction played a pivotal role in
the digital communications revolution, influencing fields like computer
networks, AI, and machine learning.</p>
<p>In summary, this passage underscores the power of
abstractions—simplified yet meaningful representations of complex
realities—in various disciplines. Whether it’s graphic design, physics,
or information theory, striking the right balance between detail
(complexity) and essence (relevance) is key to effective communication
and understanding. The case studies of Niemann’s graphic design and
Shannon’s Information Theory demonstrate how abstractions enable better
comprehension, decision-making, and technological advancement.</p>
<p>The text discusses the concept of dynamic systems, focusing on their
complexity and the role of abstraction in understanding them. It
highlights that complex behaviors emerge from the interactions among
multiple components within a system, not just individual parts. This
principle applies to various domains, including biology (like human
health determined by multiple organs), engineering (aircraft performance
depending on all parts working together), finance (systemic failures
resulting from interbank relationships), and social sciences (viral
pandemics influenced by population interconnections).</p>
<p>A crucial aspect of these complex systems is the presence of feedback
mechanisms, which help maintain equilibrium—a state where the system
behaves predictably. For instance, an airplane’s control system
stabilizes its position, our bodies regulate sugar levels or internal
temperature through feedback loops, and market prices adjust to balance
demand and supply.</p>
<p>The text emphasizes that while studying individual components
provides valuable insights, understanding the overall system behavior is
essential due to interconnections and feedback mechanisms. It also notes
that real-world systems often deviate from idealized models,
necessitating interventions and approximations to manage
uncertainty.</p>
<p>In the context of robust control theory, researchers address such
uncertainties using feedback mechanisms that incorporate actual observed
outcomes. These strategies optimize robustness when real-world behaviors
differ from modeled expectations, as demonstrated in managing
aerodynamic unpredictability during turbulence or addressing the
challenges posed by asymptomatic carriers in a pandemic.</p>
<p>The narrative then shifts to illustrate this concept through a
vignette involving statistician Philippe Rigollet and information
theorist Guy Bresler from IDSS (Institute for Data, Systems, and
Society). They bridge the gap between statistics and computer science by
focusing on two fundamental factors affecting large-scale data analysis:
sample complexity (volume of data required for model learning) and
computational complexity (resources needed for learning process).</p>
<p>Historically, these aspects were studied separately - statisticians
focused on sample complexity in statistical learning theory, while
computer scientists assessed computational complexity. Rigollet and
Bresler’s work unifies these aspects via the mathematical concept of
discovering a ‘planted clique’ within a graph—an abstract model
representing signal (the hidden pattern we’re trying to find) amidst
noise (random data).</p>
<p>The planted clique problem is presented as a powerful abstraction.
Solving it involves finding a fully connected subgraph embedded in a
random graph, which mirrors the challenge of extracting meaningful
patterns from noisy data. The size of the clique represents ‘signal
strength’ relative to the ‘noise’ level. If the clique is large compared
to natural occurrences within the random graph (high signal-to-noise
ratio), efficient algorithms can locate it. Conversely, if the clique is
too small, separating it from the noise becomes virtually impossible,
reflecting limitations in what can be learned from available data.</p>
<p>In essence, this example demonstrates how mathematical abstractions
can help us understand and tackle complex statistical challenges
inherent in big data analysis, thereby bridging theoretical statistics
and computational complexity considerations.</p>
<p>The text discusses the concept of abstraction in the context of
complex systems, its application in various fields, and its significance
in policymaking.</p>
<ol type="1">
<li><p><strong>Complexity vs Relevance and Abstraction</strong>: The
author emphasizes that dealing with complex systems often requires
simplification through abstraction - finding the simplest model that
explains a system’s behavior in a specific context. This principle is
fundamental to mathematical system theory and statistics but less so in
humanities and social sciences.</p></li>
<li><p><strong>Application of Abstraction in Statistics</strong>: The
example given is about clique detection in random graphs, studied by
Rigollet and Bressler. They defined a regime where the size of the
clique is a function of the graph’s size, categorizing problems into
three types: statistically unlearnable, easily learnable with known
algorithms, and learnable but without simple algorithms. This work
provides a complete characterization of these complexity
trade-offs.</p></li>
<li><p><strong>Bresler’s Work on PCA</strong>: Bresler extended this
analysis to Principal Component Analysis (PCA), a common problem in
high-dimensional statistics. His interdisciplinary approach, combining
statistics, theory of computing, and information theory, is seen as
exemplary for the emerging 21st-century statistics. For deeper
understanding, the author recommends Bresler’s 2018 paper “Reducibility
and Computational Lower Bounds for Problems with Planted Sparse
Structure.”</p></li>
<li><p><strong>Abstraction in Policymaking</strong>: The text argues
that abstractions are crucial in policymaking due to the vast number of
details in complex systems, making it impossible to address every
element without losing sight of actionable solutions. It uses the 2008
subprime mortgage crisis as an example, stating that understanding such
crises requires modeling not just data but also underlying protocols,
policies, and incentives influencing human behaviors.</p></li>
<li><p><strong>Systemic Risks and Collaborative Modeling</strong>: The
author suggests that while individual data might be insufficient to
predict systemic risks like the 2008 financial crisis, collaborative
modeling could yield better insights into potential problems.
Interdependencies within complex global systems, as seen in the
financial crisis and the 2020 pandemic’s supply chain disruptions,
underscore this need.</p></li>
<li><p><strong>Traffic Congestion as a Complex System</strong>: The text
applies these ideas to traffic congestion, suggesting that mathematical
models (like partial differential equations describing traffic
densities) can help understand and predict it. Individual behaviors,
economic costs, control levers, and decision-makers all play roles in
this complex system. Abstraction could potentially aid in managing such
systems more effectively by focusing on key components and
interdependencies.</p></li>
</ol>
<p>In essence, the text underscores the importance of abstraction in
understanding and addressing complex systems across various domains,
from statistics to policymaking and beyond. It highlights how
simplification through abstraction can reveal crucial insights while
acknowledging the need for humanistic, ethical considerations in
non-engineering contexts.</p>
<p>The passage discusses the complexity of modeling public
transportation systems, focusing on traffic congestion as a multifaceted
issue. It highlights various factors influencing individual decisions
regarding transportation modes, such as personal needs, real-time
feedback from congestion, herding behaviors, and incentives. These
elements make creating unified models of collective impact challenging
and not universally agreed upon.</p>
<p>Municipalities and states aim to minimize the economic impacts of
congestion, which include lost job productivity, diminished property
values, displacement of available housing, business relocations, and
environmental degradation. They employ various control levers at local
levels like real-time traffic signals, speed limits, toll
incentives/penalties, and travel directives. At a regional scale,
authorities might construct alternative routes, restrict vehicle access,
or impose penalties for excess CO2 emissions.</p>
<p>The decision-making spectrum is broad, involving engineers (designing
infrastructure), individual travelers (optimizing personal objectives),
and local/regional authorities (establishing equitable policies).</p>
<p>GPS systems in vehicles can exacerbate congestion by directing many
drivers to the same alternate route. Newer GPS generations attempt to
mitigate this by considering driver preferences and suggesting
personalized alternative routes, presenting opportunities for
municipalities to use information design to combat congestion.
Information design aims to influence individual decisions without
compromising their objectives.</p>
<p>The research area of cyber-physical systems involves GPS digital
platforms and real-time dynamic controls for toll, traffic light, and
speed regulation. However, this typically does not consider
incentivizing individual behavior, unlike information design which
focuses on using private information to guide decisions towards
system-beneficial actions without undermining personal goals.</p>
<p>Creating comprehensive abstractions of complex systems like traffic
congestion is challenging due to the distinct perspectives of different
disciplines. Computer scientists or control theorists might create
multilayered models separating detailed operations from higher-level
decisions, while environmental scientists could examine long-term
effects of policies on the environment. Urban planners might use
aggregate models to study impacts on city expansion or migration, and
economists may evaluate policy effects on macroeconomic equilibrium.</p>
<p>Each approach provides valuable insights but risks ignoring crucial
components necessary for improving the overall system. The challenge
lies in combining all relevant analyses into one coherent model—an
endeavor central to the IDSS Triangle and ongoing research, including
the Social and Engineering Systems (SES) doctoral program.</p>
<p>The text discusses the concept of abstraction in various contexts,
particularly in the development of Intelligent Decision Support Systems
(IDSS) and its application in understanding complex societal
challenges.</p>
<ol type="1">
<li><p><strong>Abstraction as Causality</strong>: The text posits that
abstraction, in essence, is about causality - understanding how an
intervention or policy impacts a desired outcome. In pharmaceutical
development, for instance, simply relying on experimentation and
clinical trials without delving into the underlying biological
mechanisms (like liver functions in the case of hepatitis C) would be
ineffective at best and dangerous at worst.</p></li>
<li><p><strong>Abstraction in IDSS</strong>: The creation of IDSS is
rooted in a model that synthesizes data from scientific, economic,
engineering processes, institutional sources, and social interactions.
These domains often interact, and without considering these
interdependencies, data analysis can lead to erroneous conclusions,
especially regarding causality.</p></li>
<li><p><strong>Complexity vs Relevance</strong>: The text argues that
while it’s crucial to account for the complexity of real-world systems,
overly simplistic models may lose essential elements and yield
misleading results. A balance must be struck between model complexity
and practical relevance.</p></li>
<li><p><strong>Wisdom of Crowds and Herd Behavior</strong>: The concept
of “wisdom of crowds” suggests that large groups make better decisions
than a select few experts. However, this wisdom can be compromised by
herding behavior - the tendency for individuals to mimic others without
critical evaluation. This behavior is prevalent in various domains,
including consumer choices, voting, and financial markets, but it’s
challenging to model due to its emotional and psychological
underpinnings.</p></li>
<li><p><strong>Herd Decisions are Not Wholly Rational</strong>: The
decision-making process in herding scenarios involves a mix of rational
analysis (comparing features, considering expert reviews) and irrational
factors (emotional bias, network influence). This blend makes it hard to
quantify individual decisions accurately, let alone predict the
collective behavior of an entire electorate.</p></li>
<li><p><strong>Modeling Herd Behavior</strong>: The text highlights that
modeling herding behavior is complex due to its heterogeneous data
nature (varied resolution levels, time scales, sparsity, high
dimensionality) and the influence of individual actions on system
properties. For instance, a popular restaurant’s improved reputation
might not reflect actual improvements in cuisine, while a bank run can
directly impact a financial institution’s health.</p></li>
</ol>
<p>In summary, the text emphasizes the importance of abstraction in
understanding complex systems, acknowledging its necessity for
manageable analysis while warning against oversimplification. It also
underscores the challenges in modeling herding behavior, given its
rational and irrational components and the complexity of
individual-to-system influence.</p>
<p>This text discusses the concept of “wisdom of crowds” or herding
behavior in decision-making processes, particularly in the context of
social networks. Despite the complexities involved such as unknowable
simultaneous activities across various networks, researchers continue to
explore methods for harnessing collective intelligence.</p>
<p>The text introduces three key assumptions that simplify the study of
this phenomenon:</p>
<ol type="1">
<li>Binary Decision: The decision at hand is simplified to a binary
choice (e.g., choosing the best restaurant from several options).</li>
<li>Ground Truth Existence: There is an objective ‘best’ option or
ground truth, even if it’s not always known.</li>
<li>Network Information Observation: Each individual in the network can
observe all decisions made by others they are connected to, but not
their private signals (individual data or reasons leading to these
decisions).</li>
</ol>
<p>These assumptions form the basis of Condorcet’s Jury Theorem, which
suggests that when a sufficient number of individuals make independent,
unbiased decisions, the collective decision will be correct more often
than not. This theory provides a benchmark for information aggregation
in social networks.</p>
<p>However, observing only decisions, as opposed to private signals,
introduces new challenges. Even if everyone can see previous decisions
(creating what’s called a ‘star network’ where everyone relies on a
single expert), herding might still lead to incorrect decisions if the
initial expert makes a mistake. This is because observing decisions can
perpetuate a cascade of incorrect choices.</p>
<p>Interestingly, even when all prior decisions are visible, the
aggregate private signals (individual data or reasons) that led to those
decisions can get obscured. This means that while one can see what
others have decided, the underlying information that influenced these
decisions remains hidden.</p>
<p>Yet, if a non-trivial number of individuals possess increasingly
accurate private signals over time - a concept known as ‘unbounded
rationality’ – the herd can still trend towards the correct decision.
This is explored in studies about systemic risk (like in transportation
networks, power grids, and financial systems) and cultural forces like
opinion dynamics and consensus development in social networks.</p>
<p>In essence, while simplifying complex social network behaviors for
analysis, these models help understand how information aggregation
works, the potential pitfalls of herding behavior, and conditions under
which collective intelligence can thrive despite individual
limitations.</p>
<h3
id="pp_89_110_the_care_and_feeding_of_a_new_discipline_at_mit">12.0_pp_89_110_The_Care_and_Feeding_of_a_New_Discipline_at_MIT</h3>
<p>The text discusses the challenges and history behind the
establishment of a new interdisciplinary field, referred to as IDSS
(Institute for Data Science and Systems) at MIT.</p>
<ol type="1">
<li><p><strong>Interdisciplinary Challenges</strong>: The author
highlights several hurdles in sustaining multidisciplinary and
interdisciplinary work within academia:</p>
<ul>
<li>Lack of sense of community and coherent culture among diverse
participants</li>
<li>Absence of shared journals, conferences, core topics, common
knowledge, and administrative clarity typical of established
disciplines</li>
<li>Insufficiently shared multidisciplinary challenges and open problems
that drive long-term research efforts</li>
</ul></li>
</ol>
<p>These issues complicate evaluations for hiring, promotion, tenure,
and securing resources like funding, space, infrastructure, and
institutional support.</p>
<ol start="2" type="1">
<li><strong>IDSS History</strong>: The initiative to create IDSS was
sparked by then Dean of Engineering Ian Waitz in late 2013. This came
after a committee, chaired by Ron Rivest and involving the author,
recommended a new entity due to the increasing roles of data and systems
across various domains at MIT.</li>
</ol>
<ul>
<li><p><strong>Committee Formation</strong>: A large committee of 40
faculty and staff was assembled to outline the blueprints for this novel
entity. After extensive discussions, they agreed on four key areas:
vision, structure, education, and statistics.</p></li>
<li><p><strong>Consensus Building</strong>: Despite initial dissent from
a couple of committee members, consensus was reached on positioning
statistics and data science at the core of the new entity while
maintaining strong ties to the systems generating such data.</p></li>
</ul>
<ol start="3" type="1">
<li><p><strong>IDSS Antecedents and Foundations</strong>: The author
emphasizes that IDSS didn’t emerge as a break from MIT’s past but rather
as the culmination of several intersecting historical and current
academic initiatives at MIT.</p>
<ul>
<li><strong>Computational Revolution</strong>: Chapter 2 of the book
provides detailed historical perspectives on the computational
revolution, which laid the groundwork for the surge in machine learning
and AI—key elements underpinning IDSS’s transdisciplinary nature.</li>
</ul></li>
<li><p><strong>IDSS Culture</strong>: The author notes that while a
strong sense of culture wasn’t fully formed at IDSS’s inception, it
evolved over time as they contemplated its foundations and the
structures and programs designed to foster interdisciplinary
collaborations.</p></li>
</ol>
<p>The development of Decision Systems Science (DSS) as a
transdiscipline at MIT is rooted in several key historical milestones
across the 20th and early 21st centuries. This timeline showcases the
evolutionary steps that culminated in the establishment of IDSS
(Institute for Data, Systems, and Society), which focuses on addressing
societal problems through data and systems.</p>
<p><strong>1940s-1950s:</strong> The Servomechanism Lab at MIT played a
pivotal role in this era. J.W. Forrester initiated the Aircraft
Stability Analyzer project and led the Whirlwind computer project, which
created the first high-speed computer using magnetic core memory (a
forerunner to modern RAM). This period also saw Claude E. Shannon’s
introduction of information theory in his seminal 1949 book “A
Mathematical Theory of Communication.” Norbert Wiener’s work on
cybernetics, summarized in his book “Cybernetics,” provided a
theoretical basis for feedback systems applicable to various domains,
from technical and biological to social. Forrester also founded system
dynamics, integrating social dynamics with engineered systems.</p>
<p><strong>1960s:</strong> Modern control theory advanced significantly
at MIT’s Engineering Systems Laboratory (ESL), an evolution of the
Servomechanism Lab. Pivotal publications like “Analytical Design of
Linear Feedback Controls” by G.C. Newton, L.A. Gould, and J.F. Kaiser,
and “Optimal Control: An Introduction to the Theory and Its
Applications” by M. Athans and P.L. Falb, contributed to this progress.
Robert G. Gallager introduced low-density parity-check (LDPC) codes in
1963, which wouldn’t be directly applied in computing for another three
decades.</p>
<p><strong>1970s:</strong> Herman Chernoff joined MIT’s Mathematics
Department to develop statistics at the Institute but had limited
success in establishing a robust program before leaving in 1985. The ESL
evolved into LIDS (Laboratory for Information and Decision Systems) in
1978, expanding its focus to incorporate broader problems requiring
insights from information, decision, and control theories.</p>
<p><strong>1970s-1980s:</strong> Jerry Hausman, Frank Fisher, and Dan
McFadden helped establish modern econometric analysis during this
period. Their work included significant contributions like McFadden’s
“Conditional Logit Analysis of Qualitative Choice Behavior” on discrete
choice models and Hausman’s “Speciﬁcation Tests in Econometrics” on
speciﬁcation analysis. MIT also launched the Technology and Policy
Program to tackle policy questions stemming from new technologies.</p>
<p><strong>1980s-2000s:</strong> This period witnessed advancements in
robust control theory, intelligent control systems, and distributed
computation, with significant publications like “Parallel and
Distributed Computation” by D.P. Bertsekas and J.N. Tsitsiklis.
Computational methods for control and reinforcement learning were also
developed, documented in works such as “Neurodynamic Programming” and
“Control of Uncertain Systems: A Linear Programming Approach.”</p>
<p><strong>2000s:</strong> Foundations in convex and non-convex analysis
were laid out in “Sum of Squares: Theory and Applications” by P.A.
Parrilo and R.R. Thomas, while inference, machine learning (ML), and
graphical methods were advanced through works like D. Shah’s “Gossip
Algorithms.”</p>
<p><strong>2010s:</strong> Network research emerged as a crucial area of
study during this decade. Works like “Systemic Risk and Stability in
Financial Networks” by Daron Acemoglu, Asu Ozdaglar, and Alireza
Tahbaz-Salehi contributed significantly to network economics, cascades,
and systemic risk understanding. Alex “Sandy” Pentland’s “Social
Physics” pushed the boundaries of social interconnection knowledge.</p>
<p><strong>2015:</strong> MIT founded IDSS (now Institute for Data,
Systems, and Society) to integrate diverse scientific
disciplines—including science, technology, humanities, arts, and social
sciences—with data-driven research aimed at resolving societal
challenges. This initiative consolidated various advancements in
statistics, information, and decision sciences under one umbrella,
defining principles for addressing societal problems through data and
systems.</p>
<p><strong>2019:</strong> MIT established the Schwarzman College of
Computing to bolster computing studies and research across all
disciplines, further strengthening its commitment to interdisciplinary
collaboration and technological innovation.</p>
<p>In essence, DSS at MIT builds upon a legacy spanning systems theory,
information theory, econometrics, cybernetics, and network science. It
synthesizes these fields into an integrated approach to tackle complex
societal issues using data-driven methods and systems
thinking—effectively acting as “Cybernetics 2.0.”</p>
<p>The text discusses the establishment of the Statistics and Data
Science Center (SDSC) at MIT, a response to the unstructured nature of
statistics education across various departments within the institute.
Prior to SDSC’s creation, statistics was not centrally organized due to
MIT’s engineering-centric focus and the early development of
econometrics in multiple disciplines.</p>
<p>The SDSC was established with a unique approach: it wasn’t tied to
any single department. This allowed for an intersectional perspective on
statistics, suitable for dealing with complex, big data issues such as
large, noisy datasets, high-dimensional sparse data, multiscale
time-varying data sets, and more. The center’s focus on modern,
21st-century statistics aimed to improve model accuracy, robustness
assessment, and distinguish causation from correlation in the context of
machine learning tools and algorithmic methods.</p>
<p>The text also highlights the importance of integrating domain
knowledge into data science, especially as machine learning (ML) became
widespread but often without sufficient consideration for the specific
domain’s knowledge essential to understanding complex systems. The
example given is racial bias research: it requires not only data
collection and analysis but also deep understanding of sociology or
economics to identify potential biases in data collection processes,
quantify uncertainty, and ensure reliable results.</p>
<p>Failing to integrate domain expertise can lead to erroneous
conclusions and self-perpetuating cycles of inequities. This is
illustrated through the example of algorithmic bias in loan approval
systems or facial recognition tools, where a lack of understanding about
racial bias could lead to discriminatory outcomes.</p>
<p>Finally, the text mentions the development of a flagship academic
program, Social and Engineering Systems (SES), which emerged from IDSS’s
efforts to create compelling curricula that balance data science with
system-level inquiries, while being comprehensible beyond MIT’s
boundaries. This program aims to prepare future data scientists who
understand the nuances of their respective domains and can apply robust
data science methodologies effectively and responsibly.</p>
<p>The text describes the Institute for Data Systems and Society (IDSS)
at MIT, which was designed to tackle complex societal challenges through
a unique interdisciplinary approach. The SES (Science of Engineering
Systems) program is a core component of IDSS, focusing on three
educational pillars: Information Sciences, Twenty-first Century
Statistics, and Humanities &amp; Social Science Studies.</p>
<ol type="1">
<li><p><strong>Information Sciences</strong>: This pillar involves
probabilistic modeling, information theory, optimization, and decision
theory. It provides students with the computational and mathematical
tools necessary for data analysis and modeling.</p></li>
<li><p><strong>Twenty-first Century Statistics</strong>: Unlike
traditional statistics that focuses on mapping data to models, this
modern approach emphasizes probabilistic modeling in the context of
complex systems, large datasets, and high dimensionality. It introduces
concepts such as causal inference, machine learning, and big data
analytics.</p></li>
<li><p><strong>Humanities &amp; Social Science Studies</strong>: This
component integrates social science and humanities perspectives into
data-driven analysis. It covers areas like collective/organizational
behavior, ethics, and societal impacts, which are crucial for
understanding the broader implications of technological solutions to
societal problems.</p></li>
</ol>
<p>SES students specialize in a particular domain (like energy systems,
finance, healthcare) while gaining expertise across all three pillars.
This interdisciplinary approach enables them to formulate and address
complex, real-world problems ethically and effectively.</p>
<p>IDSS also offers various educational initiatives:</p>
<ul>
<li><p><strong>Interdisciplinary Doctoral Program in Statistics
(IDPS)</strong>: Provides advanced training in twenty-first century
statistics and data science for doctoral students across MIT’s
schools.</p></li>
<li><p><strong>Undergraduate Minor &amp; Online MicroMaster’s in
Statistics and Data Science</strong>: These offer foundational knowledge
in statistics and data science applicable to various fields, fulfilling
the growing demand for data scientists worldwide.</p></li>
<li><p><strong>Technology and Policy Program (TPP)</strong>: A Master of
Science program focusing on developing leaders who can create
responsible technology policies informed by technical understanding and
social context.</p></li>
</ul>
<p>IDSS’s commitment to online education, through programs like the
Statistics and Data Science MicroMasters, extends its reach globally. It
partners with institutions worldwide (like Aporta in Peru) to offer
customized, high-quality educational experiences that are both efficient
and cost-effective.</p>
<p>Overall, IDSS’s approach combines rigorous scientific methods with
societal context and ethical considerations, aiming to produce
well-rounded professionals capable of addressing complex societal
challenges through data-driven solutions.</p>
<p>The text describes the establishment and organization of the MIT
Institute for Data, Systems, and Society (IDSS), an interdisciplinary
program that focuses on data-driven policy research with considerations
for data privacy and ethics. Here are the key points:</p>
<ol type="1">
<li><p><strong>Vision and Organization</strong>: IDSS was designed as an
Institute at MIT, akin to a department but with faculty from various
departments across MIT’s five schools. This cross-disciplinary structure
allowed it to cover a broad range of topics and avoid dictating specific
teaching or research content to existing departments.</p></li>
<li><p><strong>Faculty Hiring</strong>: Unlike traditional departments
that focus on a core set of courses, topics, and faculty members, IDSS
adopted a non-traditional approach for hiring. The faculty was composed
of “core members” who held joint appointments in IDSS and their
respective home departments. This arrangement encouraged each department
to take ownership of developing the transdiscipline within their
domains.</p></li>
<li><p><strong>Teaching and Research</strong>: Instead of pulling
faculty away from their departments or expecting them to abandon their
disciplinary research, this model boosted the reach and rigor of
disciplinary work. IDSS faculty members, with expertise in statistics,
data science, information systems, decision science, and social
sciences, facilitated access to foundational education and research
activities for other departments’ faculty and students.</p></li>
<li><p><strong>Supporting Junior Faculty</strong>: To address concerns
about time division between IDSS and home departments potentially
affecting tenure prospects for junior faculty, senior IDSS faculty
placed emphasis on mentoring these colleagues in applying IDSS resources
to pioneering research within their home domains. MIT also played a role
by funding novel research in the early years.</p></li>
<li><p><strong>Student Recruitment</strong>: The focus was on nurturing
“trilingual” students - individuals who are experts in statistics and
information sciences, as well as having deep understanding of social and
institutional behavior alongside expertise in their chosen research
domains. This approach requires a deliberate recruitment process that
aligns candidates with the transdisciplinary vision.</p></li>
<li><p><strong>Building Transdiscipline Capacity</strong>: IDSS plays a
significant role in assisting departments in building capacity within
the emerging transdiscipline of data, systems, and society (DSS). This
includes jointly managing search committees to recruit candidates
aligned with each department’s vision and needs.</p></li>
</ol>
<p>In essence, IDSS was created to foster a new transdisciplinary
approach in an existing university structure. By integrating faculty
from various departments and designing flexible hiring practices, it
aims to infuse statistical, information-systems, and decision-systems
thinking into diverse disciplines, encouraging collaboration and
innovation across traditional academic boundaries.</p>
<p>The passage discusses the establishment and development of the
Initiative on Data Science + Society (IDSS) at MIT, highlighting its
unique transdisciplinary approach that transcends traditional academic
boundaries.</p>
<ol type="1">
<li><p><strong>Student Profile</strong>: IDSS students are identified
for their entrepreneurial spirit and willingness to tackle societal
challenges by combining skills in statistics, information science, and
social/institutional studies. They often work across various MIT
departments, fostering collaboration and versatility.</p></li>
<li><p><strong>Bridging Disciplinary Gaps</strong>: A significant aspect
of IDSS is bridging the gap between STEM (Science, Technology,
Engineering, Mathematics) fields like engineering and computer science,
and the social sciences/humanities. This is achieved by encouraging a
culture of collaboration and nurturing financial dependencies to support
interdisciplinary work.</p></li>
<li><p><strong>Supporting Research</strong>: IDSS dedicates resources to
student research, allowing for open-ended exploration of societal
problems without being restricted to traditional discipline-specific
research lines. This has led to the establishment of an
institutionalized transdisciplinary approach, making it easier for
researchers to secure independent funding.</p></li>
<li><p><strong>Postdoctoral Fellows</strong>: IDSS utilizes postdoctoral
fellows who work at the intersection of this emerging transdiscipline
and various specific domains. These fellows collaborate directly with
faculty members across different fields, contributing to a rich
interdisciplinary research environment.</p></li>
<li><p><strong>Publishing and Career Considerations</strong>: While
high-quality, domain-specific publications are crucial for academic
careers, IDSS also supports transdisciplinary journals to accommodate
the unique nature of its research. Despite these challenges, IDSS
graduates have found success in both academia and industry due to their
ability to tackle complex problems with novel tools.</p></li>
<li><p><strong>Physical Space</strong>: The physical space dedicated to
IDSS is vital for fostering collaboration and innovation, facilitating
spontaneous interactions among students, faculty, and postdocs, and
providing venues for events and seminars.</p></li>
<li><p><strong>Addressing Systemic Racism</strong>: A specific example
of IDSS’s impact is the Initiative on Combatting Systemic Racism
launched in 2021 by two faculty members. This initiative leverages
IDSS’s transdisciplinary nature to address systemic racism across
various societal domains, from labor markets and criminal justice to
education, healthcare, and financial services.</p></li>
</ol>
<p>In essence, the passage underscores how IDSS at MIT has successfully
created a space for interdisciplinary research that addresses pressing
societal issues, fosters innovative thinking, and prepares students for
diverse career paths while navigating challenges related to academic
publishing norms.</p>
<p>The passage discusses an initiative at the MIT Institute for Data,
Systems, and Society (IDSS) aimed at combating systemic racism. The
focus is on leveraging transdisciplinary collaboration between social
scientists and data science/computing experts to address racially
inequitable outcomes in areas like housing and healthcare.</p>
<ol type="1">
<li><p><strong>Transdisciplinary Approach</strong>: IDSS brings together
faculty from diverse disciplines, including the social sciences, data
science, and computing, to tackle complex issues of systemic racism.
They develop concept papers as starting points for broader
collaboration.</p></li>
<li><p><strong>Developing Policy Solutions</strong>: Beyond identifying
problems, IDSS aims to create proactive policy solutions that rectify
racially inequitable outcomes stemming from institutional biases. This
includes addressing the exacerbating effects of AI and algorithmic
decision-making on these disparities.</p></li>
<li><p><strong>Data Analysis for Equity</strong>: The research collects
and examines vast data sets across multiple domains to identify sources
of racial bias in data collection processes. These insights are used to
develop computational tools aimed at reducing racial disparities within
institutional structures.</p></li>
<li><p><strong>Student Engagement</strong>: IDSS students and postdocs
are involved in this initiative, tackling complex technical challenges
that demand sensitive and socially conscious solutions. They work on
debiasing data, eliminating racial indicators without losing valuable
information, and studying the persistence of historical segregation
patterns.</p></li>
<li><p><strong>Outreach and Engagement</strong>: IDSS actively engages
with historically underserved communities through programs like the
Minority Summer Research Program (MSRP) and adapted online educational
initiatives for students in Historically Black Colleges and Universities
(HBCUs).</p></li>
<li><p><strong>Case Studies</strong>: Two specific case studies are
mentioned:</p>
<ul>
<li><p><strong>Policing</strong>: A study by IDSS faculty Fotini
Christia, Devavrat Shah, Craig Watkins, and their SES students analyzed
22 million 911 records and over 400,000 police stops. They found that
calls related to people of color were more likely to result in police
actions compared to those involving white citizens, suggesting potential
bias in 911 call processes influencing police behavior.</p></li>
<li><p><strong>Housing Disparities</strong>: Research by Peko Hosio and
her SES student Aurora Zhang revealed a significant correlation between
neighborhood racial composition and eviction rates using data from the
Eviction Lab dataset. Even after adjustments, this association
persisted, highlighting persistent housing inequities.</p></li>
</ul></li>
</ol>
<p>In essence, IDSS is a multidisciplinary hub within MIT that combines
social science research with advanced data analysis techniques to
understand and combat systemic racism, particularly in areas like
policing and housing. They engage students, collaborate with
underrepresented communities, and publish rigorous research to inform
policy decisions towards a more equitable society.</p>
<p>The text discusses two main research areas within the broader context
of academic studies at MIT, focusing on economic variables like poverty
rate, median income, and median rent.</p>
<ol type="1">
<li><p><strong>Racial Disparities in Eviction</strong>: The first
research area explores the hidden mechanisms through which racial
disparities are embedded in eviction processes. This includes
investigating how systemic factors may lead to certain racial groups
being more susceptible to evictions, thereby perpetuating economic and
housing instability. The study also looks into whether causal inference
methods can be applied to understand and mitigate these disparities
through policy interventions. This research aims to shed light on the
complex relationship between neighborhood demographics and patterns of
eviction, contributing to a more nuanced understanding of this
issue.</p></li>
<li><p><strong>Online Bias and Offline Discrimination</strong>: The
second area of research examines the correlation between biased content
on social media and subsequent discriminatory behaviors in real life.
This investigation is driven by existing evidence showing that online
discrimination can lead to harmful offline actions. Researchers are
probing how the portrayal of different demographic groups on social
media might influence specific instances of offline discrimination. The
aim is to determine if an individual’s identity or attributes affect the
link between consuming biased content online and exhibiting
discriminatory behavior off-line. This line of inquiry contributes to
uncovering systemic racism across various sectors, including healthcare,
policing, housing, social media, and climate change.</p></li>
</ol>
<p>In essence, these studies aim to understand complex societal issues
by applying rigorous academic methods, with the ultimate goal of
informing policy decisions that could help reduce disparities and
promote fairness in various sectors of society.</p>
<h3
id="pp_111_118_people_programs_and_research">13.0_pp_111_118_People_Programs_and_Research</h3>
<p>The text discusses the formation and development of the MIT Institute
for Data, Systems, and Society (IDSS) from 2015 to 2024. IDSS was
created with the aim of tackling complex societal challenges through an
interdisciplinary approach, incorporating elements such as statistics,
stochastic modeling, information theory, systems and control theory,
optimization, economics, network science, and human behavior.</p>
<p>IDSS’s founders recognized that merely setting up programs was
insufficient; they needed to cultivate a specific culture and attract
the right kind of researchers and students. They envisioned a new breed
of doctoral students who could work fluently across disciplines like
statistics, information sciences, engineering, and social sciences,
while also mastering real-world domains such as media, transportation,
or energy markets. These PhD students were seen as crucial in
establishing the foundation for a new teaching and research community
within IDSS, energizing and broadening the faculty involved in this
novel discipline.</p>
<p>The text presents two examples of such IDSS doctoral students:
Minghao Qiu and Manon Revel.</p>
<p>Minghao Qiu, a SES (Science, Engineering, and Society) doctoral
student at IDSS, was inspired by the “Beijing Haze” incident in 2013.
This severe air pollution event prompted him to study air pollution,
energy, and public policy as an interconnected system generating
multiple adverse outcomes. Qiu’s research focus aligned with IDSS’s
mission of using data analysis to tackle complex societal issues. In his
first paper at MIT (published in 2020), he collaborated with Professor
Noelle Selin from Tsinghua and Carnegie Mellon universities, applying a
retrospective analysis to explore the policies potentially contributing
to the Beijing Haze of 2013. This study demonstrated how past policy
assessments could enhance predictions about future policy consequences
during their early stages.</p>
<p>Manon Revel, another IDSS SES doctoral student, grew up in Paris with
a passion for journalism and science. She studied engineering and
applied mathematics at École Centrale Paris while continuing her
journalistic work part-time. Upon joining MIT, she found the
transdisciplinary ecosystem within IDSS appealing and decided to pursue
a doctoral degree in 2017. Revel’s research interests lie at the
intersection of journalism, society, and data analysis, aiming to
understand the dynamics of these fields in today’s digital age.</p>
<p>The narrative emphasizes IDSS’s commitment to fostering an
interdisciplinary environment where students can combine their passions
and expertise to address complex societal challenges effectively using
data-driven methods.</p>
<p>The text describes a study conducted by Revel and her team from the
Institute for Data, Systems, and Society (IDSS) at MIT, focusing on the
impact of clickbait ads on reader trust. The research is an
interdisciplinary effort involving experts from civil and environmental
engineering, management, political science, and electrical engineering
and computer science.</p>
<p>The study defines clickbait as a headline or concise text designed to
entice clicks leading to content of questionable value or interest. They
used a Bayesian text classification program to identify clickbait in
nearly 1.5 million online ads from 2016-2019. The findings revealed that
over 80% of the ads were classified as clickbait.</p>
<p>Following this data collection, large-scale randomized experiments
were conducted. These experiments showed that a single exposure to
clickbait near legitimate content could undermine readers’ confidence in
both the specific content and the publication itself. This effect was
most pronounced for medium-familiarity publications (recognized by 25%
to 50% of the audience). However, for well-known sources like CNN or Fox
News, the impact was negligible.</p>
<p>Revel’s research aims to inform journalism publishers about the
potential harm of clickbait in the pursuit of short-term profits at the
expense of reader trust. She plans to extend this work to explore how
people process information related to voting decisions, as seen in her
2021 paper “Native advertising and the credibility of online
publishers.”</p>
<p>The text also highlights IDSS’s growth and interdisciplinary nature.
Over eight years, the student body has grown rapidly, with students from
various fields like engineering, political science, economics,
management, statistics, and mathematics. This diversity is attributed to
a hiring strategy focused on broadening internal domain engagement and
enhancing MIT’s statistical capabilities through recruitment of faculty
with non-traditional perspectives in key areas such as information
theory, high-dimensional learning, theoretical computer science,
optimization, econometrics, networks, and core statistics.</p>
<p>An example of IDSS’s interdisciplinary success is Associate Professor
Caroline Uhler, who combines expertise in algebraic statistics for
computational biology with machine learning (ML) tools. Her work has led
to breakthroughs in understanding cellular disease impacts at a granular
level and repurposing existing drugs for combating diseases like
COVID-19 using gene expression data. Uhler’s current research focuses on
applying ML techniques to characterize different types of cells,
including normal, fibrocystic, cancerous, and metastatic, by analyzing
vast gene expression vectors. She is overcoming challenges such as
detecting subtle changes in gene expressions and preserving the
regulatory network structure through novel, ML-based approaches.</p>
<p>The text discusses two main themes: Machine Learning (ML)
applications in genomics and Emery Brown’s work on using EEG data to
improve anesthesia.</p>
<ol type="1">
<li><p><strong>Machine Learning in Genomics</strong>: The section begins
by introducing the concept of using ML techniques for integrating
single-cell data, a process crucial in understanding cellular behavior
across different conditions or species. This is exemplified by the work
of Uhler and her collaborator G. Shivashankar.</p>
<ul>
<li><p><strong>Single-Cell Data Integration</strong>: This technique
aims to merge information from two populations with distinct probability
distributions, using a cost function that’s often biologically
motivated. The goal is to establish an efficient one-to-one association
between these populations.</p></li>
<li><p><strong>Intervention Program</strong>: Uhler’s intervention
program focuses on transforming cell distributions by utilizing
transcription factors (TFs). These are cells with specific genome
expressions capable of binding to other cells, aiming to discover an
input-output function. The challenge lies in identifying the correct TFs
among countless possibilities.</p></li>
<li><p><strong>Encoded Variable-Level Structures</strong>: A significant
aspect of this research involves recognizing encoded structures at
variable levels, which can simplify the search for new TFs from existing
options. This intersection of control theory and genomics holds
promising potential for streamlining complex searches.</p></li>
</ul>
<p>The author highlights that Uhler’s work epitomizes the Data Science
and Statistics (DSS) transdiscipline, combining network sciences,
probability, causality, and low-dimensional structures to tackle
previously unsolvable issues. This, combined with DSS’s commitment to
ethical guidelines, instills optimism for future advancements.</p></li>
<li><p><strong>Emery Brown’s Anesthesia Research</strong>: The text then
shifts focus to MIT Professor Emery Brown’s work on using EEG data to
enhance anesthesia delivery during surgery.</p>
<ul>
<li><p><strong>Propofol and Brain Dynamics</strong>: Brown’s research
reveals that propofol, a common anesthetic drug, significantly alters
brain dynamics, creating neurological patterns distinct from normal
sleep. This challenges the long-held belief that anesthesia merely ‘puts
patients to sleep.’</p></li>
<li><p><strong>Real-time EEG Monitoring</strong>: By continuously
monitoring EEG data during surgery, Brown’s approach suggests that
anesthesia equipment could precisely adjust drug levels based on patient
brain activity, potentially reducing side effects. This involves a
feedback system where medication levels are adjusted according to
measured brain activity.</p></li>
<li><p><strong>Implications and Challenges</strong>: Successfully
implementing this method requires high precision to account for
individual patient variations and accurately interpret various brain
states correlating with routine vital signs during surgery. It poses
significant real-time ML and control systems integration challenges,
classified as ‘safety critical’ due to the potential severe consequences
of failure.</p></li>
</ul>
<p>The author suggests that Brown’s work is a testament to the power of
integrating advanced ML tools, control systems, and patient safety
considerations for improving medical practices in the 21st
century—illustrating key aspects of the DSS discipline.</p></li>
</ol>
<h3
id="pp_119_126_personal_reflections_on_the_journey">14.0_pp_119_126_Personal_Reflections_on_the_Journey</h3>
<p>The text is a personal reflection on the journey of establishing an
interdisciplinary research institute (IDSS) within MIT, focusing on the
challenges, insights, and strategies encountered along the way.</p>
<ol type="1">
<li><p><strong>Academic Environment at MIT</strong>: The author
describes the competitive nature of MIT and its rigorous evaluation
process for new initiatives. This environment mirrors a human body’s
immune system resisting organ transplants - even when necessary for
growth, initial resistance is common until alignments are made to ensure
success.</p></li>
<li><p><strong>Committee Involvement</strong>: The author emphasizes the
importance of their active participation in the committee that outlined
IDSS’s blueprint. Through extensive discussions and diverse viewpoints,
they gained a deep understanding of community expectations, which
couldn’t be fully captured in the final report.</p></li>
<li><p><strong>Learning from Others</strong>: Recognizing similar
initiatives at other institutions, the author sought to learn from their
successes and setbacks, acknowledging each institution’s unique approach
tailored to specific needs.</p></li>
<li><p><strong>Intellectual Strength in Leadership</strong>: The author
asserts that intellectual strength is more critical than administrative
skills for leaders of a new academic unit. This intellectual prowess
should be collectively exhibited by the founding team, which must also
have sufficient overlap in interest and capacity to interact with
various experts from different domains.</p></li>
<li><p><strong>Managing Autonomous Teams</strong>: The author highlights
that managing such a diverse, autonomous team requires understanding and
coordinating their plans, even when it involves resisting the urge to
impose one’s viewpoint. Recognizing individual incentives and
facilitating engagement through funding, student recruitment,
collaborations, and structured communication is crucial for maintaining
momentum towards shared objectives.</p></li>
<li><p><strong>Staff Leadership</strong>: The author stresses the
importance of having the right staff leaders to bring faculty vision
into reality. Hiring these individuals, especially when they embrace a
startup mindset, is vital. An environment fostering this mentality
supports team cohesion and achieving desired outcomes.</p></li>
<li><p><strong>Competitive Landscape</strong>: The author acknowledges
that establishing an interdisciplinary institute quickly reveals a
competitive landscape with potential rivals. Addressing concerns from
other departments sensitively, without compromising the institute’s
unique value proposition and resources, is essential for navigating
these challenges.</p></li>
<li><p><strong>Vignette</strong>: A specific example illustrates
addressing faculty concerns by demonstrating how IDSS’s approach
differed from existing curricula (abstracting away details of physical
phenomena) to showcase its added value.</p></li>
</ol>
<p>In summary, this reflection underscores the importance of
intellectual strength in leadership, understanding and managing diverse
teams, navigating a competitive academic landscape, and learning from
both successes and failures when establishing an interdisciplinary
research institute within a prestigious, competitive institution like
MIT.</p>
<p>The text is a reflection on the author’s journey in establishing and
leading the Institute for Data, Systems, and Society (IDSS) at MIT,
focusing on three main themes: administrative duties, ethics in
technology, and the recognition of human complexity in
problem-solving.</p>
<ol type="1">
<li><p><strong>Administrative Duties</strong>: The author starts by
discussing how, despite initial reservations from colleagues about
taking on an administrative role, they found it to be enriching rather
than limiting. They managed to balance their passion for research and
teaching with the demands of leading IDSS. This experience broadened
their research interests and excitement for a wider range of academic
pursuits. The author suggests that administrative roles can, in fact,
expand one’s horizons in research, a phenomenon they’ve observed in
their colleagues as well.</p></li>
<li><p><strong>Ethics in Technology</strong>: The author reflects on the
rapid pace of technological advancement and its impact on society,
highlighting the lagging ethical understanding. They question whether
better training in ethical issues could have prevented past misuse of
technology (like nuclear weapons). They argue for a shift in perspective
where social and humanistic responsibilities are given equal weight to
economic values when designing systems. This critique is particularly
relevant to AI and data-driven technologies, which can perpetuate
discrimination and bias due to their utilitarian focus on efficiency and
cost-effectiveness over fairness and inclusivity. The author calls for a
paradigm shift in how we evaluate and regulate technology, considering
both short-term gains and long-term societal impacts.</p></li>
<li><p><strong>Human Complexity in Problem-Solving</strong>: Initially
focused on optimizing precise metrics for decision systems, the author
acknowledges a shift towards problems that integrate human behavior and
effects into their design. They note the complexity of humans compared
to physical systems—humans don’t always respond predictably to
instructions or share clear objectives. This recognition led them to
explore mechanism design and behavioral economics as ways to incorporate
human behavior into system designs, emphasizing its potential for
solving complex societal issues.</p></li>
</ol>
<p>In summary, the text is a personal account of leadership, ethical
considerations in technology, and the growing recognition of human
complexity in problem-solving within an academic context. The author
shares their evolving perspective on balancing administrative duties
with research and teaching, advocates for a more holistic approach to
technology ethics, and highlights the importance of considering human
behavior when designing systems to address real-world challenges.</p>
<p>The text discusses the importance of understanding causality,
particularly in the context of AI’s integration with human behavior
studies. The author emphasizes that human decision-making can be
influenced by bounded rationality and oversampling of significant past
events, leading to biased conclusions about cause and effect.</p>
<p>AI systems are proposed as a solution to mitigate these biases by
identifying deviations from what may seem like statistically significant
conclusions. The author asserts that while AI can provide valuable
insights, it should be complemented by domain-specific knowledge for the
most effective results. This is because specific nuances of a problem
often hold the key to progress.</p>
<p>The text further highlights the significance of understanding how
data is systemically created across various platforms. This includes not
just technical systems but also platforms that elicit human behavior and
data. By gaining insights into these processes, we can design better
information access systems for the public and potentially evolve market
standards to include more humanitarian objectives.</p>
<p>The author admires the forward-thinking approach of their colleagues
at IDSS (Institute for Data, Systems, and Society), who recognize
‘systems’ as fundamental components alongside data in addressing
societal challenges. Despite not foreseeing a rapid shift towards a new
humanitarian paradigm, the author anticipates many questions that
require collaboration between technical experts and humanists to answer
effectively.</p>
<p>This collaboration is already taking place within academia, though
significant sacrifices—both technically and from a humanistic
perspective—are required. The challenges are ill-defined and do not fit
neatly into any single academic discipline, necessitating the use of
broader perspectives in research. The author personally finds themselves
at such a crossroads and extends an invitation to their colleagues to
join them in this endeavor.</p>
<h3 id="pp_127_136_acknowledgments">15.0_pp_127_136_Acknowledgments</h3>
<p>The text is an acknowledgment section from an institutional
perspective, highlighting key individuals who significantly contributed
to the establishment and success of the Institute for Data Science and
Systems (IDSS) at Massachusetts Institute of Technology (MIT). Here’s a
detailed summary:</p>
<ol type="1">
<li><p><strong>Founding Committee</strong>: A 40-member committee was
instrumental in formulating IDSS’s vision, mission, and structure. Their
collective brainstorming and decision-making set the direction for the
institute.</p></li>
<li><p><strong>Leadership Team</strong>: This team oversaw various
aspects of IDSS’s development, including the PhD program (SES),
Statistics and Data Science Center (SDSC), adaptation of the Technology
Policy Program (TPP), faculty hiring, and broader campus initiatives.
Each member played a crucial role in shaping these areas.</p></li>
<li><p><strong>Ian Waitz</strong>: Serving as Dean of Engineering during
IDSS’s establishment, Ian Waitz provided essential support and
resources, enabling the creation of this new institute. His unwavering
commitment was vital to the project’s success.</p></li>
<li><p><strong>Michael Sipser</strong>: As Dean of Science at MIT,
Michael Sipser played a significant role in establishing SDSC within
IDSS. His expertise in statistics and strategic vision helped launch
IDSS effectively.</p></li>
<li><p><strong>Ali Jadbabaie</strong>: A colleague who significantly
influenced the creation of IDSS’s vision document. Ali provided valuable
insights based on his research and past initiatives, helping shape the
institute’s direction. He later joined MIT as part of the founding team,
contributing to various aspects such as academic program design and logo
selection.</p></li>
<li><p><strong>John Tsitsiklis</strong>: A brilliant problem-solver and
team player, John initially led the committee defining the SES PhD
program and later assumed leadership of LIDS (a research lab now part of
IDSS). He managed LIDS’s growth, fostered collaboration between LIDS and
IDSS, and provided candid feedback to the leadership team.</p></li>
<li><p><strong>Devavrat Shah</strong>: Under his guidance, IDSS unified
MIT’s statistics efforts, resulting in new academic offerings like a
minor in statistics and data science, an interdisciplinary PhD in
statistics, and online courses. His determination overcame long-standing
obstacles to achieve what MIT had pursued for decades.</p></li>
<li><p><strong>Alberto Abadie</strong>: As Associate Director
representing SHASS (School of Humanities, Arts, and Social Sciences),
Alberto promoted IDSS’s vision within his school, recruited faculty for
SDSC, and supported the SES PhD program. His research attracted students
to engage in compelling research endeavors.</p></li>
<li><p><strong>Noelle Selin</strong>: Director of TPP, Noelle infused
her passion for science policy into leading TPP while guiding other IDSS
units’ growth. She also led IDSS after the original author’s departure
in 2023.</p></li>
<li><p><strong>Jennifer Kratochwill</strong>: As Director of
Administration and Finance, Jennifer played a crucial role in shaping
IDSS’s inclusive culture. She fostered collaboration among diverse
community members (staff, students, postdocs, researchers, and faculty)
and ensured high-quality services, earning IDSS recognition as one of
MIT’s best units for staff support.</p></li>
<li><p><strong>Beth Milnes</strong>: Serving as IDSS’s academic
administrator, Beth was the driving force behind the flagship SES
program. She administered this innovative program since its inception,
understanding students’ perspectives and remaining dedicated to its
success – key reasons for its thriving status.</p></li>
</ol>
<p>These individuals, along with the broader IDSS community,
collectively built a successful data science institute at MIT by
leveraging their diverse expertise, commitment, and collaborative
spirit.</p>
<p>The text provided is an acknowledgment section from a book or report,
highlighting key individuals who significantly contributed to the
establishment and success of the Institute for Data, Systems, and
Society (IDSS) at MIT. Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p>Elizabeth Bruce: She was instrumental in creating an external
partnership program for IDSS. Her systematic approach to engaging
corporations and presenting clear value propositions helped establish
enduring partnerships, setting the framework for these
collaborations.</p></li>
<li><p>Steven Graves: As a colleague of the author, Steve played a
pivotal role in guiding the team during IDSS’s formation. His extensive
experience at MIT and leadership background were crucial assets as they
embarked on their pioneering journey.</p></li>
<li><p>Founding Team Transitions: A few years after IDSS’s inception,
some founding members transitioned to new roles. John and Devavrat
stepped down, Ali became the department head of Civil and Environmental
Engineering, and Fotini Christia succeeded Ali as the leader of SSRC and
SES program.</p></li>
<li><p>Fotini Christia: A computational social scientist, Fotini took on
a leadership role at IDSS. Under her direction, the institute launched
the Initiative on Combating Systemic Racism, which received significant
participation from MIT faculty, students, and postdocs. Her exceptional
interpersonal skills, strategic acumen, and ability to connect people
made her an invaluable asset to IDSS.</p></li>
<li><p>Ankur Moitra: After Devavrat’s departure, Ankur assumed the role
of leading SDSC. Known for his systematic approach, he enhanced SDSC
course offerings in statistics during the first two years of student
engagement, emphasizing mathematical rigor in applied work and shaping
the foundation of statistics at MIT that Devavrat had
established.</p></li>
<li><p>Annett (Peko) Hosoi: Peko played a pivotal leadership role during
the COVID-19 pandemic, when IDSS faced its first real-time challenge.
She demonstrated resilience by swiftly restructuring operations to
support data-driven decision-making, elevating IDSS’s standing within
MIT. Her work on the “when-to-test” app had broad implications and was
adopted by numerous institutions for safely reopening their
campuses.</p></li>
<li><p>Karene Chu: Chu is acknowledged for her exceptional efforts in
advancing the MicroMasters program to its current state, a significant
contribution of IDSS to academia.</p></li>
<li><p>MIT Professional Education and Great Learning Team: The
partnership with MIT Professional Education, under Bhaskar Pant’s
leadership, has been pivotal in advancing online education at IDSS.
Additionally, the collaboration with the Great Learning team has
delivered top-tier programs with global expert support, particularly
from Milind Kopikare and Mohan Lakhamraju.</p></li>
<li><p>Brescia Family and Aporta: The partnership with the Brescia
family through their social impact lab Aporta has been crucial in
expanding IDSS’s reach. Jaime Aroaz Medanic, Luz Fernandez, and Lucia
Gonzales are specifically acknowledged for initiating and nurturing this
partnership and supporting the MicroMasters program.</p></li>
<li><p>Additional Acknowledgments: The author expresses gratitude to
several key individuals who supported IDSS’s formation, including former
MIT President Rafael Reif, Provost Marty Schmidt, Anantha Chandrakasan
(later dean of engineering), Daron Acemoglu, Sanjay Sarma, and various
MIT department heads and deputy deans.</p></li>
</ol>
<p>The text emphasizes that IDSS’s success is a collective effort,
involving not just visionaries but also supporters and collaborators who
helped establish trust, garner support, and advance the mission of this
new academic unit within MIT.</p>
<p>The acknowledgments section of this text is a tribute to several
individuals who have significantly contributed to the establishment and
development of the Institute for Data, Systems, and Society (IDSS) at
MIT. Here’s a detailed breakdown:</p>
<ol type="1">
<li><p><strong>Champy</strong>: Mentioned as an advisor with background
in re-engineering, his advice and oversight were crucial during the
transition from ESD to IDSS.</p></li>
<li><p><strong>Mark Gorenberg</strong>: Recognized for his pivotal role
in IDSS’s integration into the College of Computing, as well as his
ongoing support and encouragement.</p></li>
<li><p><strong>Jennifer Chayes</strong>: Acknowledged for her continuous
support during her time at Microsoft Research and subsequently as part
of the visiting committee. Her insights, connections, and encouragement
were highly valued.</p></li>
<li><p><strong>Michael I. Jordan (UC Berkeley)</strong>: Recognized for
his consistent backing of IDSS’s overall initiative, particularly within
the statistics effort.</p></li>
<li><p><strong>Students, Friends, and Colleagues</strong>: The author
thanks his students and peers from research communities, notably Jeff
Shamma and John Doyle, for decades of influential discussions shaping
their careers.</p></li>
<li><p><strong>Dedicated Faculty Members</strong>: Unnamed faculty
members are thanked for their unwavering support and tireless efforts
towards IDSS’s success.</p></li>
<li><p><strong>Advisory Board Members</strong>: Numerous people have
served on advisory boards, and the author expresses gratitude for their
valuable contributions.</p></li>
<li><p><strong>Funders</strong>: Substantial financial support was
crucial for IDSS’s operation:</p>
<ul>
<li>Phyllis Hammer supported the SES program.</li>
<li>Tom Seibel provided funds for a faculty chair.</li>
<li>The Brescia family funded general operations and fellowships.</li>
</ul></li>
<li><p><strong>Alan Willsky</strong>: Former director of LIDS, his
vision, guidance, and support were instrumental during IDSS’s initial
stages.</p></li>
<li><p><strong>Sanjoy Mitter (late)</strong>: A colleague, mentor, and
friend, Mitter’s relentless pursuit of boundaries in academia and
commitment to high standards continue to influence IDSS’s
ethos.</p></li>
<li><p><strong>Moroccan Hosts</strong>: The author thanks Moustapha
Terrab, Mohamed Benkamoun, and Hicham Al Habti for their generosity and
hospitality during his sabbatical at the University of Mohammed VI
Polytechnique.</p></li>
<li><p><strong>Robert Thurston-Lighty</strong>: A key figure in shaping
this book, Robert provided insightful guidance to transform complex
academic concepts into accessible language, teaching the author about
engaging diverse audiences and maintaining focus on critical ideas. His
support during biweekly meetings amidst pandemic isolation was
crucial.</p></li>
<li><p><strong>Manuscript Reviewers</strong>: Several individuals
provided valuable feedback, including Fotini Christia, John Tsitsiklis,
Devavrat Shah, Manon Revel, and the author’s daughter Deema (a data
scientist at Google), who offered a practitioner’s perspective.</p></li>
<li><p><strong>Editor Lauren Cowles</strong>: Her dedication to
polishing the manuscript was instrumental in bringing this project to
fruition.</p></li>
<li><p><strong>Family and Friends</strong>: The author expresses deep
gratitude for their unwavering support, particularly his wife Jinane,
children Deema, Hilal, Yazeed, nephew Taher, sister Diana, and parents
Wisam Abushaqra (deceased) and Abdullah Dahleh (deceased). They have
been a constant source of encouragement during moments of
self-doubt.</p></li>
</ol>
<p>The passage is an excerpt from a book’s acknowledgment section, where
the author expresses deep gratitude and emotion towards their late
brother, Mohammed.</p>
<ol type="1">
<li><p><strong>Influence of the Brother</strong>: The author describes
Mohammed as “the most influential figure” in their formative years,
suggesting that his impact on the author’s life was profound and shaped
many aspects of their development.</p></li>
<li><p><strong>Premature Departure</strong>: Despite this significant
influence, Mohammed passed away prematurely - before his time, leaving a
void that is still felt by the author. The phrase “left us” emphasizes
the suddenness or unexpected nature of his death.</p></li>
<li><p><strong>Persistent Memory</strong>: Even after Mohammed’s
departure, his memory remains vivid in the author’s mind and heart. This
implies that despite the passage of time, Mohammed’s influence continues
to be a constant presence in the author’s life.</p></li>
<li><p><strong>Reflection on Decisions</strong>: The author mentions
that every step they take is accompanied by thoughts about how Mohammed
would have perceived or reacted to those actions. This suggests an
ongoing dialogue with his brother’s memory, using it as a moral compass
or source of guidance in decision-making.</p></li>
<li><p><strong>Dedication</strong>: The author dedicates this book
(presumably their own work) “to the three of them”. The ‘three’ likely
refers to Mohammed and possibly two other individuals, possibly parents
or siblings, who were significant in the author’s life but are not
explicitly named here. This act signifies deep respect and appreciation
for these individuals, acknowledging their influence on the author’s
work.</p></li>
</ol>
<p>In essence, this passage is a tribute to the late brother, Mohammed,
expressing profound grief, admiration, and ongoing connection. It
suggests that despite his absence, Mohammed’s legacy continues to shape
the author’s perspective and actions, serving as a source of inspiration
and wisdom.</p>

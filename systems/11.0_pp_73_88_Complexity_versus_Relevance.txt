5
COMPLEXITY VERSUS RELEVANCE
The Power of Abstractions
In the ﬁrst episode of the Netﬂix television series Abstract: The
Art of Design, illustrator Christoph Niemann talks about how he
achieves the right level of abstraction in his graphic design work.
"Each idea," he says, "requires a very speciﬁc amount of information.
Sometimes it's a lot - a lot of details, a lot of realism - and sometimes
it's really like just this one line, the one pixel. But each idea has one
moment on that scale."
To illustrate his point, Niemann cites the example of a heart as
an abstraction or symbol for the idea of love. If you depict it with the
ultimate simpliﬁcation of a red square, he says, "nobody knows what
you're talking about, so it totally falls ﬂat." If, on the other hand, you
use super-realism and draw a lifelike human heart, the last thing any-
body would ever think about is love. But somewhere in between those
two points on what he calls his "Abstract-O-Meter" is the two-
dimensional, graphic Valentine's Day shape we recognize as "just right
to transport this idea of the symbol for love." The result is an inﬁnitely
varied and complex phenomenon - love - distilled to a communicative
essence.
In physics, simple models are abstractions of reality. Newton's
gravitation law is an extremely pared-down representation of motion
that does not account for the dependence of space and time. While this
model has limitations at high speed, it is one of the most intuitive and
powerful tools of physics. The bottom line is that abstractions help us
build better and more intuitive understandings of real-world systems
and enable us to make very good decisions in a timely fashion.

Abstractions are common in our everyday lives because they are
instinctive, and neuroscience conﬁrms that such thinking is crucial to
human cognition. But when it comes to modeling, analyzing - and
hopefully improving - the complex systems that determine the physical,
social, and economic health of human societies, achieving the right level
of abstraction requires quite a bit more intention and expertise.
I contend that there is no systematic approach to deriving an abstrac-
tion; instead, in my view, it is a manifestation of the creativity of
our minds.
Information Theory and the Power of Abstraction
The creation of information theory as a discipline is a good
example of how abstractions help us solve difﬁcult problems in com-
plex systems. One of the core challenges tackled by the mathemat-
ician, electrical engineer, and cryptographer Claude Shannon in the
1930s and 1940s was to model the problem of separating, or
"smoothing," core signals from extraneous noise (e.g., in a ﬁre-
control communications network). Another problem was to extract
the correct meaning of a transmission from an encrypted set of signals
designed to confound everyone except the person or persons with the
key for decoding the message. Shannon used fundamental ideas from
probability theory to deﬁne the limits of uncertainty in those digital
transmissions. Shannon's primary goal was to formulate a theory for
transmitting digital data through channels affected by noise in order
to send a digital message through a noisy channel and reliably recover
it at the receiving end, either exactly or with a high probability
of accuracy.
Consider the following scenario in which we aim to transmit a
randomly generated sequence of binary signals with values 0, 1. Let's
assume that the channel ﬂips each binary signal 25% of the time. The
objective is to ensure nearly precise retrieval of any sequence at the
receiving end. Naturally, a straightforward solution exists - transmit
each binary signal multiple times. The receiver can employ a majority
rule to correctly identify the transmitted binary signal that, with high
probability, yields the correct answer. But there's a trade-off. This
approach results in a low transmission rate. In situations where high
transmission rates are crucial, such as real-time video streaming, the
approach introduces signiﬁcant latency.
74
/
Complexity versus Relevance: The Power of Abstractions

Shannon abstracted this challenging problem by deﬁning a
distinct notion of information.
Information,
as
modeled
in
Shannon's
1948
article
"A
Mathematical Theory of Communication," is not a function of what
you do say but rather what you could say. As such, it reﬂects the amount
of freedom a person has in choosing any particular communication
reduced by any limiting rules of the written language being represented.
Shannon used this approach to create a mathematical formalism
that quantiﬁed the amount of information contained in a given source of
messages. He hypothesized a random alphabet with a known distribution
as a source of messages from which a particular letter is picked with some
probability. In a similar vein, we can think of a collection of English
words, each picked with some frequency, as a message source.
In Shannon's hypothesis, the entropy of the source was shown to play a
very important role in characterizing the shortest codes representing
messages in the sources, a problem Shannon referred to as "source
coding." The concept of entropy is important in the hypothesis because
it measures the degree of randomness in a given alphabet. In a binary
alphabet with only two letters (say 0, 1), for example, the entropy is small
if one of these letters occurs with high probability. The maximum ran-
domness is when they both have equal probabilities.
When Shannon applied his methodology to the English lan-
guage, he showed how it could reduce by nearly ﬁve times the amount
of digital signaling necessary for a reliable transmission. (Note that the
concept of a source, as depicted by Shannon, is in itself an abstraction of
real messages in communication systems.) Once messages were coded,
they were transmitted through a noisy channel. In that scenario, the
redundancy approach I mentioned earlier would not be efﬁcient.
Shannon's work on channel coding described the optimal
approach to creating just enough redundancy in a signal to completely
recover it at the other end of transmission, giving mathematicians and
engineers one of their most powerful examples of abstraction. Although
coding methods in digital communication must account for many add-
itional factors such as transmission delays, it was Shannon's abstraction
that enabled the digital communications revolution. It paved the way
for such disciplines and technologies as computer networks, adaptive
and anticipatory systems, informatics, cybernetics, AI, and ML, among
many others. Shannon's theory of information also exempliﬁed a meth-
odology that enables us to effectively tackle problems in complex,
75
/
Information Theory and the Power of Abstraction

dynamic systems when faced with noisy, imperfect, incomplete, or
overwhelming amounts of data. Today, it is quite common to conceive
of levels of abstraction when designing the architecture of complex
systems, from the physical layer all the way to the application layer.
Systems, Interconnections, and Equilibria: More Abstraction
Scientists and engineers have made tremendous progress in dis-
covery and design by breaking systems down into smaller components,
such as the individual organs of the body or the engines and wings of a
plane. A challenge arises, however, when trying to understand the com-
plex behaviors that emerge from reassembling those smaller components.
Although studying the liver in isolation provides valuable insights, a
person's overall well-being is determined by the integration of multiple
organs. Similarly, individual banks may have distinct strategies, but sys-
temic failures result from the interactions among multiple ﬁnancial insti-
tutions. Even executing excellent designs for individual components of a
plane is not enough to ensure that it ﬂies well. The performance of a fully
assembled aircraft depends on the interconnected functionality of all the
parts. Social behaviors and viral pandemics also exemplify the inﬂuence of
interconnections in the overall outcomes of complex systems.
An essential aspect of interconnections is the presence of
feedback systems. In an airplane, a feedback system could measure the
position of the airplane and then, through some control design, adjust
the power of the thrusters in order to keep the plane in a steady position.
In our bodies, we have numerous feedback systems that adjust our sugar
level or keep our internal temperature constant.
Feedback systems, whether natural or designed, help maintain
equilibrium within a given context. Equilibrium represents a state where
a system behaves in a highly predictable manner. An airplane's control
system, for example, is designed to keep the plane stable (i.e., in equi-
librium) at a speciﬁc pitch angle. In infectious diseases, natural stabiliz-
ing feedback mechanisms might emerge when more people get sick,
causing them to stay home and reduce infection rates. In domains such
as economics, demand and supply are stabilized through pricing mech-
anisms. When demand increases, prices rise. Price hikes, in turn, reduce
demand and generate a more stable state in the market.
The general concept of equilibrium provides a simpliﬁed way to
think about interconnections and complexity in systems such as aircraft,
76
/
Complexity versus Relevance: The Power of Abstractions

markets, and food demand. We often consider factors that can push
systems away from equilibrium and intervene to mitigate these devi-
ations. In the speciﬁc case of COVID-19, natural feedback loops were
eliminated by the fact that we had so many asymptomatic carriers,
which in turn created the necessity for additional interventions above
and beyond self-isolation. Through intervention and feedback, we can
minimize the uncertainty caused by these deviations and work with
approximations and simpliﬁed models to better address complexity.
Within the domain of robust control and decision theory,
researchers
address
uncertainty
within
abstracted
models
using
feedback mechanisms that incorporate actual observed outcomes from
past decisions. Strategies can thus be crafted to optimize robustness
when real-world behaviors deviate from modeled expectations. In the
context of an airplane navigating severe turbulence, for example, the
aerodynamics may exhibit greater unpredictability than a model pre-
dicts. By skillfully designing a feedback control system, a researcher or
engineer can effectively manage and mitigate the impact of uncertain
effects. Such advanced methods in robust control theory offer valuable
insights that extend to broader societal decision-making challenges.
Vignette
Computation Meets Statistics: One rewarding aspect of the IDSS commu-
nity is the extent to which practical and theoretical research - and research-
ers from different domains - thrive in proximity to one another. This is
evident in the work of statistician Philippe Rigollet and information theor-
ist Guy Bresler, both IDSS faculty members. For a considerable period in
the recent past, these two domains were mostly disconnected and lacked a
clear bridge between them. This mattered because certain statistical chal-
lenges pose greater difﬁculties than others. While information theory offers
a framework for characterizing the information embedded in a noisy data
set, and thus provides a basis for evaluating what is learnable, it is often the
case that computational complexity imposes an additional constraint
on learning.
In the cases of both Rigollet and Bresler, their work spans that gap by
using insights from both domains to address the complexity that arises from
the presence of two fundamental factors: the volume of data required for
model learning (sample complexity) and the computational resources needed
77
/
Systems, Interconnections, and Equilibria: More Abstraction

for the learning process (computational complexity). Historically, statisti-
cians have focused primarily on questions related to sample complexity
(addressed in statistical learning theory literature), while computer scientists
have concentrated on assessing the computational complexity of problems
and their categorization. What was lacking was a unifying mathematical
concept that could harmonize these two aspects, enabling the derivation of
valuable insights into various statistical learning questions. This abstract
mathematical concept was subsequently identiﬁed as the pursuit of discover-
ing a hidden clique within a graph.
If you are given a randomly generated graph of nodes and links and you are
told that there is a clique (a fully connected subgraph) embedded in the graph,
how would you ﬁnd it? More important, if you are not a theoretical mathem-
atician, why should you care? It turns out that if you are interested in using
large data sets to predict the probable behavior of large and complex real-life
systems (e.g., the community spread of an airborne virus), successfully identi-
fying a planted clique has immense potential value.
The utilization of the planted clique as a tool for understanding the
relationship between statistical challenge and computational difﬁculty is a
unique and powerful abstraction. One can appreciate this by thinking of
the planted clique of a given size as a signal and the random graph as noise.
Separating this signal from the noise without additional information will
require an exhaustive search for all cliques, which would be prohibitively
time-consuming. But can some statistical information reduce this search?
Intuitively, if the clique is of a large size (e.g., larger than natural cliques in
the random graph), then it is likely that you can ﬁnd it quickly just by
starting with nodes of large degrees. That intuition turns out to be correct,
and we think of this condition as a large signal-to-noise ratio. Of course,
one has to quantify exactly what a large clique really means.
Alternatively, if the size of the clique is too small (e.g., smaller than
natural cliques in the random graph), then it appears to be impossible to
separate the signal from the noise, regardless of what algorithm you use.
This is the result of confusion between the embedded clique and the natural
ones, and it corresponds to the limit of what is learnable from data. Again,
one has to quantify what is meant by a small clique. My colleagues Rigollet
and Bressler addressed exactly this issue and were able to provide the
regime (in terms of the size of the clique as a function of the size of the
random graph) where one end represented the statistical limit of learning,
and the other end represented what is easily learnable. The regime in
between represented problems that are learnable but do not have a known
simple algorithm for ﬁnding the clique. This approach provided a complete
characterization of these complexity tradeoffs.
78
/
Complexity versus Relevance: The Power of Abstractions

Bresler examined this problem further and was intrigued - how far could
he stretch such an analysis to address common problems in high dimen-
sional statistics such as the celebrated principal component analysis (PCA)?
His transformative work at the intersection of three ﬁelds - statistics,
theory of computing, and information theory - is exactly what we envi-
sioned our young faculty would be doing to support the new statistics
emerging in the twenty-ﬁrst century. It's beyond the scope of this book to
describe fully how Bresler did this. If you are interested in a deeper dive,
I
highly
recommend
the
in-depth
2018
paper
"Reducibility
and
Computational
Lower
Bounds
for
Problems
with
Planted
Sparse
Structure," which describes how Bresler and his collaborators solved
this problem.
The Wider Landscape of Uncertainty and Imperfection
Although information theory has had a broad and enduring
impact on the digitization of society, its original application was focused
on a purely engineering problem. Today, our imperative is to intertwine
signiﬁcant engineering challenges with social science phenomena and
public and technology policy initiatives.
It is nearly impossible to solve a complex societal problem
without zeroing in on the most consequential subset of elements within
a given system. This simpliﬁcation process applies as much to devising
and adjusting effective public health policy in the midst of a global
pandemic as it does to the engineering and siting of a nuclear power
plant. Both problems comprise countless details - and you could devote
countless hours accounting for every element in those systems without
ever arriving at an actionable solution.
The pursuit of a workable abstraction - that is, the simplest
model that explains the behavior of a system in a particular context - is
a fundamental principle of mathematical system theory and statistics,
but it is historically less a feature of domains concerned with human and
institutional behaviors, with the exception of certain aspects of theoret-
ical economics. That's why domain experts from the humanities and
social sciences are essential to data-driven team efforts to solve our most
daunting challenges. In forging those collaborations, however, we on
the data science side must resist the temptation to force our nonengi-
neering colleagues to think like engineers. We must, if you will,
79
/
The Wider Landscape of Uncertainty and Imperfection

transition from a form of totalitarian (and utilitarian) thinking to
incorporate more humanistic, artistic, and ethical considerations.
The Role of Abstraction in Policymaking
Consider, for example, the subprime mortgage crisis and the Great
Recession of 2008. It was not possible to understand, predict, or mitigate
the systemic risk or cascaded failures of that global ﬁnancial-system crisis
solely from observations of past data. This rare event was heavily inﬂu-
enced by the ﬁnancial protocols, underlying government policies, and
numerous incentives that had been created to inﬂuence people's behaviors.
Data were available for some parts of this complex system - and
different data were available to different decision-makers in the system -
but few, if any, key actors had all the data needed to address the crisis
coherently before, during, and immediately after the crash. It is impos-
sible to gauge the extent to which an abstraction of the global ﬁnancial
system could have limited, or even prevented, some of the damage.
I suggest, however, that a more useful predictor of potential problems
may emerge from collaborative modeling.
Although most corporate leaders failed to anticipate the crisis,
many were cognizant of the potential for cascading failure within their
industries. When the CEO of Ford Motor Company requested emer-
gency government action to prevent General Motors from failing, the
motive was not a desire to preserve one of its ﬁercest rivals for the sake
of competition. Rather, Ford's leaders understood that if GM failed, so
would many of the intermediate suppliers on which Ford also depended.
GM's collapse could very well have put Ford at grave risk of failure, as
well. Such interdependencies within complex global systems underscore
the need for a more rigorous understanding of systemic issues (i.e.,
issues emerging from interconnections) in general. The fact that the
2020 global pandemic triggered massive supply chain and travel break-
downs reinforces the urgency of developing new tools for analyzing and
predicting large-scale systemic failures.
Wrapping Our Heads around Congestion
At the beginning of this book, I touched on trafﬁc delays as a
negative consequence of a complex system that has huge implications
for our physical, mental, and economic wellbeing - individually and
80
/
Complexity versus Relevance: The Power of Abstractions

collectively. Our day-to-day experiences of ever-worsening congestion
suggest that we have largely failed on a societal level to solve or even
lessen this problem. Might abstraction help us manage those systems
more effectively? A look at four key components of the overall chal-
lenge - individuals' behaviors, economic costs, control levers, and
decision-makers - may shed some light on the possibilities.
Within a network of connected roads, congestion can be fairly
well modeled mathematically using partial differential equations that
describe how trafﬁc densities change over time as a function of trafﬁc
input. We affect such networks when we drive to our jobs as well as the
various amenities available in our towns, cities, and regions. Some of
our travel is dictated by the clock (e.g., to and from work), while other
travel occurs on more ﬂexible schedules (e.g., recreational activities).
We may choose to travel by personal vehicle, ride-sharing, or various
modes of public transportation. Needs, incentives, feedback from real-
time congestion, and herding behaviors also inﬂuence our decisions.
Models of our collective impact on a transit system must account for all
these factors, and such models are very complicated and not collectively
agreed upon.
From a cost perspective, municipalities, and states strive to
minimize the short- and long-term economic impacts of congestion -
lost job productivity, diminished property values, displacement of avail-
able housing, business relocations, and environmental degradation, to
name a few. These harms occur on multiple time scales and with varying
degrees of interdependence. The potential control levers available to
authorities at local levels include real-time trafﬁc signals, speed limits,
toll incentives and penalties, and one- and two-way travel directives.
These levers operate at local levels. On a regional scale, authorities may
construct more alternative routes, deny access to certain types of
vehicles at certain times of day, or impose penalties for excess
Co2 emissions.
Complicating these factors is the wide spectrum of decision-
makers within the system. Engineers make infrastructure design deci-
sions to control ﬂow and maximize throughput. Individual travelers
make strategic personal decisions to satisfy their primary objectives
(e.g., travel from point A to point B in the shortest amount of time).
Any person's assessment of which actions will best serve that objective,
however, may vary from day to day and often may be counterproduc-
tive. Local and regional authorities, for their part, seek to establish
81
/
Wrapping Our Heads around Congestion

policies and incentives they hope will promote equitable access to
healthy, productive, and rewarding living conditions.
When Individual Knowledge Becomes a Systemic Problem
Embedded GPS systems in passenger vehicles represent a poten-
tially confounding factor in trafﬁc congestion. While those systems
provide valuable trafﬁc information to individual drivers, they can have
a negative impact by routing a critical mass of drivers toward the same
alternative route, resulting in new congestion points. Newer generations
of GPS systems aim to overcome this limitation by incorporating indi-
vidual driver preferences and providing personalized recommendations
for alternative routes. That technological advance opens up opportun-
ities for municipalities to utilize information design as a tool for battling
congestion in their regions. Information design, in that context, refers to
deliberately using the design and presentation of private information to
inﬂuence
the
decisions
and
behavior
of
multiple
individuals.
By leveraging that approach, municipalities and states can enhance their
control over trafﬁc congestion at the individual level.
The control and transportation communities named this
research area cyber-physical systems. The cyber component is created
from the GPS digital platform as well as sophisticated, real-time
dynamic controls for toll, trafﬁc light, and speed regulation. Those
components typically are concerned with the physical transportation
system
but
do
not
attempt
to
incentivize
individual
behavior.
Information design, by contrast, focuses on providing information that
will nudge people to behave in a way that is good for the overall system
while not compromising their individual objectives. This emerging and
dynamic area of research presents many challenges, however. Modeling
each of these complicated subsystems is extremely difﬁcult and merging
all the components into one overarching representation is nearly impos-
sible. Even if such a model could be created, the result would be so
unwieldy that it would have little utility for policymakers seeking to
design equitable systems that incentivize societally beneﬁcial behaviors.
No One-Abstraction-Fits-All Solution
Unfortunately, we lack a foolproof formula for creating com-
prehensive abstractions. On the problem of congestion, every discipline
82
/
Complexity versus Relevance: The Power of Abstractions

is likely to view the system through a distinct lens. A computer scientist
or control theorist might create a multilayered model that separates the
detailed operations of the transportation system from the higher-level
decisions discussed above. By combining the layers, the model could
theoretically describe the effects of people's interactions with the oper-
ations of the system. Across a short timescale, abstracted models of the
impact of tolls on driving behaviors have proven to be very effective in
managing congestion.
Looking at the same system, environmental scientists may
arrive at a different abstraction. They may examine the long-term effect
of a particular policy (e.g., aggregating models of trafﬁc and pollution
over decades) to assess the impact of a given policy on the environment.
Urban planners might use similar aggregate models to study the impact
on city expansion or migration. And economists may evaluate the
effects of adding a carbon tax on system users. Their primary objective
would likely be to understand how policies affect the equilibrium of the
macroeconomy such as highlighting how the carbon tax could alter
other related operations that may ultimately result in a loss of efﬁciency
or some other metric.
Each approach outlined above could provide insight into key
aspects of this complex system. At the same time, each discipline would
run the risk of ignoring some essential component that is key to improv-
ing the overall system. Inventing ways of combining all relevant ana-
lyses is the crux of successfully analyzing complex systems and making
effective policies. This is what we are attempting to model with the IDSS
Triangle and to implement in our ongoing research, including our
doctoral program in Social and Engineering Systems (SES). I'll talk more
about that and other IDSS programs in Chapter 7.
How Abstraction Shaped IDSS
For
systems
and
society,
one
important
implication
of
abstraction in all its forms is causality. A proposed policy or interven-
tion must have a causal effect on the desired outcome. In the context of
pharmaceutical development, for example, you would be inefﬁcient at
best, and dangerously erroneous at worst, to develop a vaccine for
hepatitis C solely based on experimentation and clinical trials while
foregoing a deeper dive into functions and mechanisms inside the
human liver.
83
/
How Abstraction Shaped IDSS

As noted in Chapter 4, the model that guides IDSS grew out of
the convergence of data from scientiﬁc, economic, and engineering
processes; data from institutional sources; and data from social inter-
actions. The societal challenges that continue to interest us - in domains
such as ﬁnance, energy, urbanization, social networks, and personal
and public health - typically hinge on the interactions among those
three heterogeneous data pools, and we seek to give appropriate weight
to all three. Without an explicit understanding of the interactions
between those components, data alone may result in vastly erroneous
conclusions. Causality is certainly one dimension that can suffer greatly
in the absence of such systemic dependencies. Our previous discussion
in Chapter 4 on COVID-19 is a primary example of this interaction and
the role of causality in assessing the impacts of interventions.
Thinking about causality played a role in leading us to structure
our academic program around the Triangle. In the context of a particu-
lar domain, we observe aspects of the three elements - systems, human
and social interactions, and institutional interactions - through hetero-
geneous data sets. Such data sets can be noisy - different resolution
levels, different time scales, sparse, high dimensional, or aggregate.
We need new statistical theories with which to assess causality, predic-
tion, or systemic effects. A certain level of abstraction is necessary to get
any quantiﬁable or testable outcomes. Such an abstraction takes us back
to modeling, which requires domain experts at the heart of the analysis.
It also carries with it a responsibility to stay engaged with each problem
we tackle so that we can adjust any components of our abstraction that
result in erroneous or counterproductive conclusions.
Our discussion earlier in this book about the abstraction of
transportation systems illustrates the challenge of staying engaged and
making adjustments to simpliﬁed models. To grasp the resilience of a
trafﬁc network -that is, understanding the minimum disruption capable
of triggering widespread trafﬁc cascades - engineers or researchers can
create simpliﬁed mathematical models of trafﬁc ﬂow based on models of
drivers' behaviors. They can use those models to analyze how the
equilibrium shifts when there is a loss of capacity on certain roads
(e.g., in the event of an accident). Their analyses may suggest real-time
interventions to avert equilibria with high congestion. In tandem with
any intervention, however, practitioners must evaluate the robustness of
their initial solution when drivers react to the intervention with strategic
behaviors that may counteract the effectiveness of the ﬁrst solution.
84
/
Complexity versus Relevance: The Power of Abstractions

Modeling the Wisdom of Crowds: Case in Point
In today's digitally interconnected sociocultural communities,
most of our individual decisions are inﬂuenced - and sometimes deter-
mined - by the advice and actions of others. Consumer purchases,
dining experiences, travel excursions, investments, and voting are
among numerous choices routinely affected by the opinions and experi-
ences of other individuals and groups. This phenomenon, ﬁrst charac-
terized as "the wisdom of crowds" by The New Yorker ﬁnancial writer
James Surowiecki in 2005, was embraced by many as one of the great
promises of the internet age.
The fundamental premise of the wisdom of crowds (sometimes
referred to as collective intelligence) is that large groups of people com-
municating with one another are smarter than an elite subset of presumed
subject experts. Surowiecki attributed this capability to activities such as
problem-solving, invention, decision-making, and prediction of future
trends or outcomes. Although the general premise is seductive, many
commentators - as well as Surowiecki himself - have identiﬁed numerous
phenomena that can distort or confound the wisdom of crowds.
One confounding phenomenon of particular interest in relation
to data science is herding. In daily life, herding behaviors are ubiqui-
tous, notoriously difﬁcult to model, and sometimes catastrophic.
A crowded restaurant, for example, can draw in passersby on the
assumption that the food must be good. An in-demand product can
attract additional buyers based solely on its popularity. On the more
disastrous end of the spectrum, a bank run can cascade into the collapse
of a ﬁnancial system. In each case, the momentum of herding behaviors
may quickly outpace or run contrary to underlying facts that should
inform the crowd's actions.
Herd Decisions Are Not Wholly Rational
When a typical consumer decides what laptop to buy, they com-
pare the features of various options, digest marketing material, sample
expert reviews, and perhaps most important, consider what others in their
social and professional networks buy. Their decision, in turn, affects what
the next person in their network decides to buy. In this way, the decision-
making process becomes a ﬂuid mix of rational thinking and psychological
and emotional bias that can be difﬁcult to quantify.
85
/
Herd Decisions Are Not Wholly Rational

The mix of rational and emotional decision-making becomes
even murkier when people decide how to cast their ballots for president.
On the rational side of the equation, voters consider competing candi-
dates' policy positions, past records, and analyses from pundits on a
variety of issues. On the psychological/emotional side, the opinions of
voters' contacts in social and professional networks hold sway along
with emotion-based appeals from campaigns and media inﬂuencers.
When a vote is ﬁnally cast, the typical voter is likely incapable of
accurately identifying to what extent each of those many factors inﬂu-
enced their ﬁnal decision. How much more complex, then, for a
researcher to predict or model the effect of herding across the
entire electorate.
Another important factor to consider when analyzing herding
scenarios is whether the actions of individuals within the herd inﬂuence
the underlying properties of the object or entity being acted upon. Our
instincts and experience help us understand this phenomenon in relation
to restaurants and bank runs. As a restaurant, for example, gains
popularity and grows more crowded, the cuisine may or may not
improve as a result. The herd cannot be directly credited with changing
the object (cuisine). In the case of a bank run, however, cascading
withdrawals by the herd can degrade the health of a bank until it fails.
Although both of these examples seem relatively straightfor-
ward, neither is simple to model for the purposes of analyzing or
predicting. The data any one individual collects about a product or an
institution is quite heterogeneous, possibly correlated, and potentially
erroneous. Alongside that, the inﬂuence of a person's network on their
ﬁnal decision will vary depending on the strength of the connections
across the network. Add to that the simultaneous and mostly unknow-
able activity across the networks of all the people in the ﬁrst person's
network, and you have a lot of factors you can't easily quantify. Finally,
the process of aggregating these various types of information may not
be rational, and each person's assessment of the riskiness of an action
will factor into the ﬁnal decision.
Extracting Wisdom from a Herd
Despite the complications described above, researchers con-
tinue to search for the wisdom of crowds in the midst of herding
phenomena. Engineering intuition suggests that we should abstract
86
/
Complexity versus Relevance: The Power of Abstractions

out as many lesser effects as possible and focus on one or two compon-
ents of the problem. One approach to simplifying relies on three
assumptions. First, assume that the decision at hand is binary and that
there is a ground truth (e.g., one restaurant can be judged objectively as
the best, all others less than the best). Second, assume that the data each
person collects (known as the private signal) has a greater than 50%
probability of being correct (i.e., the aggregate information on local
restaurants favors the best restaurant for a majority of people, but not
necessarily all). Third, assume that each individual's network is ﬁxed
and that any member of the network can observe all the decisions
already made by network members to whom they are connected.
That third assumption incorporates an understanding that each
individual observes the decisions of others in their network rather than
the private signals they receive (e.g., a person observes the decisions
made by their neighbors but not the information they used to make that
decision). The law of large numbers guarantees that observing the
independent private signals about the correct decision is sufﬁcient for
the correct aggregation to the right answer. That assertion - known as
the Condorcet's Jury Theorem - was set out by David Austen-Smith and
Jeffrey S. Banks in their 1996 paper "Information Aggregation,
Rationality, and the Condorcet Jury Theorem" to provide a benchmark
for the aggregation of information. But what about observing just the
decisions?
Although this is a relatively simpliﬁed view of real-world
herding behaviors, the approach yields some interesting insights.
A network with good characteristics should avoid a key weakness of
the so-called "star network" in which everyone is connected to a single
expert whose opinion is valued by everyone in the network. If the star
makes the wrong decision, herding will trend toward the wrong deci-
sion. Similarly, just observing the decisions that came before yours, you
still could fall victim to a cascading series of incorrect choices and
reinforce erroneous herding. A surprising aspect of this latter outcome
is that it can occur even if you are able to observe the decisions that
everyone made before you in the network. In other words, observing the
decisions of agents on the network obscures the underlying information
available in the aggregate private signals each agent receives.
It is possible, however, that if agents are able to observe all the
decisions others in the network made before making their decisions,
then the herd can trend toward the right decision. All it takes is a certain
87
/
Extracting Wisdom from a Herd

non-trivial number of agents to have private signals with increasing
accuracy for productive learning to occur over time - a phenomenon
known as unbounded rationality. The topic is thoroughly discussed by
Daron Acemoglu and coauthors in the 2011 article "Bayesian Learning
in Social Networks" as well as in Matthew O. Jackson's book "Social
and Economic Networks." This approach to abstraction has applica-
tions in studies of systemic risk - in transportation networks, power
grids, and ﬁnancial systems - as well as research into cultural forces
such as opinion dynamics, the evolution of social norms, and the
development of consensus in social networks.
88
/
Complexity versus Relevance: The Power of Abstractions


2
A CONFLUENCE OF FIELDS
Some Historical Perspective
I marvel - as do many of my colleagues - at the pace of
developments in data science, ML, and AI during the ﬁrst two decades
of the twenty-ﬁrst century. Given my own wonder at the speed of
progress, I certainly understand how the average person may feel that
all this dropped out of the sky fully formed. It didn't, of course, and my
hope is that a brief recap of key historical milestones in these ﬁelds will
help us better appreciate the interconnecting factors that produced the
current state of affairs. The history of developments can also enable us
to understand more clearly the immense potential of future break-
throughs in computing power, data science, data abundance, ML, and
the development of superior algorithms.
Computing and Big Data: Some Historical Perspective
I take it on faith - and as empirical fact - that problems have
solutions and decisions have consequences. Not all solutions and con-
sequences are equal, however, and much of human history can be
explained by our quest for better outcomes. We have proved, as a
species, to be tireless in our efforts to enhance our problem-solving
and
decision-making
with
tools,
machines,
and
methodologies.
Contemporary computing technologies are simply the latest, though
arguably the most consequential, manifestation of this eternal drive.
How is it that these technologies have come to be so dominant in our
lives, and why are initiatives such as IDSS important for advancing the
academic pursuit of better outcomes?

Consider two simple data points as a measure of our societal
journey in computing from the 1950s to the 2020s. In 1953, just as IBM
was preparing to manufacture the world's ﬁrst mass-produced com-
puter (the "650"), approximately 100 computers were functioning on
the planet according to some estimates. I haven't been able to verify that
number, but even if it's remotely accurate, that's in the range of one
computer for every 20-30 million people. By late 2014, several sources
put the number of personal computers (PCs) in use around the globe at
roughly two billion - one computer for every 3.6 people. And that
number doesn't include smartphones and tablets, which are even
more plentiful than desktops and laptops and have lots more memory,
faster processors, and greater storage capacities than the original
IBM 650.
Of course, this simple accounting of existing hardware only
hints at how the evolution of computing has transformed our capabil-
ities and dependencies, redeﬁned our economic and social ecosystems,
and altered the course of human history. It does illustrate, however, just
how dominant computer technologies have become in almost every-
one's life. When I reﬂect on my own journey in computing, I ﬁnd that
certain developments during the last several decades, beginning in the
1960s, are particularly relevant to the creation of IDSS.
The Roots of Mainframe Computers
Analog machines - which measure and process data based on
streams of physical input such as light, temperature, or voltage - date
back millennia. As long ago as 100 BC, inventors built devices to
calculate astronomical positions, solve complex mathematical equa-
tions, predict tides, and even aim naval artillery. At the beginning of
the modern era, mechanically, hydraulically, and electrically driven
computers were developed for ﬁre control and electrical power systems.
Because these "network analyzers" were able to handle complex numer-
ical calculations, they were also used by nuclear physicists and struc-
tural engineers before and after World War II. One of the ﬁrst digital-
analog vacuum tube computers in this lineage that provided real-time
outputs was the Whirlwind I, developed in the Servomechanisms Lab at
MIT in the 1940s and 1950s. MIT's Laboratory of Decision Systems
(LIDS) - with which I'm proudly afﬁliated - is a descendant of Project
Whirlwind and the lab that gave birth to it.
20
/
A Conﬂuence of Fields: Some Historical Perspective

Analog computers are still in use for specialized applications.
Their overall importance, however, started to wane in the second half of
the twentieth century. With the development of vacuum tubes, transis-
tors, integrated circuits, and then microprocessors, government- and
industry-backed scientists and engineers began to create a new class of
general-purpose computers that were faster and more versatile than
analog devices. Mass production of these multi-cabinet, multi-central
processing
unit
(CPU)
machines
-
generically
referred
to
as
mainframes - ramped up in the 1950s, and mainframes grew to domin-
ate academic and industrial computing in the 1960s and 1970s.
Why Mainframes Matter: First Revolution
The development of mainframes provided the foundation for
entire categories of modern computer technologies that we take for
granted today - memory storage media, user interfaces, programming
languages, discrete transistors, integrated circuits, output and interface
devices (i.e., printers, graphic displays, mouses, and trackpads), just to
name a few. The unprecedented capabilities of those technologies
prompted physicist Howard H. Aiken to quip, "We'll have to think up
bigger problems if we want to keep them [computers] busy." Landmark
machines such as the Universal Automatic Computer (UNIVAC),
Whirlwind, Automatic Computing machine of the Mathematica Center
(ARMAC), and ATLAS were developed for tasks ranging from compiling
US census data and processing payroll at General Electric to controlling
transportation networks (e.g., air and rail trafﬁc) and monitoring
America's airborne early warning defense system.
Mainframes also enabled large-scale simulations of complex phe-
nomena such as weather fronts, ﬁnancial systems, transportation networks,
and aerospace ﬂight. These and related hardware capabilities were of
particular interest to me in the 1980s as I began my research into control
theory and capabilities and limitations of control systems under uncertainty.
My PhD thesis adopted a wholly computational perspective in the design of
control systems, diverging signiﬁcantly from the analytical "closed-form"
solutions that have traditionally been the focal point of the ﬁeld. In the
decades that followed the invention of mainframe computers, I and my
colleagues in electrical engineering, applied mathematics, computer science,
and statistics would never be at a loss for bigger and more complex
problems to challenge faster and more sophisticated computing capabilities.
21
/
Why Mainframes Matter: First Revolution

Vignette
The era of mainframes undoubtedly launched a paradigm shift in how
systems and processes were handled, a time when we crossed the threshold
into a world where advances in computing began to move much more rapidly
than our ability to adapt to them. This revolution produced, and continues to
produce, many unintended consequences that have not been positive for
society at large. Many scholars contend, for example, that mainframe com-
puting facilitated racial division in America by helping to statistically codify
the now-debunked notion of single-race individuals in the decennial compil-
ing of the US Census. The power of databases, in particular, facilitated
racialized data collection that was used to develop racialized - and discrimin-
atory - policies in housing, healthcare access and research, and other areas.
The Turing Test
In the mid-twentieth century, the increasing power (in speed
and storage) of mainframe computers triggered profound philosophical
and scientiﬁc inquiries. One of the most widely recognized voices in the
debate was the pioneering mathematician and computer scientist Alan
Turing. In 1950, he posed the fundamental question, "Can machines
think?" That question, which epitomized what became known as the
Turing Test, sparked extensive discussions about the nature of intelli-
gence and the potential of machines.
To answer his famous question, Turing plumbed the depths of
predicate calculus and predicate logic, conducting rigorous analyses of
formulas and functions. Through his exploration, he arrived at a sig-
niﬁcant conclusion - neither a Turing Machine nor any other logical
method could answer every mathematical question. That insight laid the
groundwork for understanding the inherent limitations of formal
systems and their applicability to real-world problem-solving. Those
limitations, however, did not hinder the development of the technology
we refer to as AI that is poised to transform contemporary society.
The Founding Event of AI
In
September
1955,
mathematicians
and
engineers
John
McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon
22
/
A Conﬂuence of Fields: Some Historical Perspective

proposed what would become the founding event for the ﬁeld of AI,
the 1956 Dartmouth Summer Research Project on Artiﬁcial Intelligence.
The conference convened a small, but enthusiastic, group of scientists
and researchers with high hopes for the development of intelligent
machines. Unfortunately, the outcomes of the project fell well short
of the intent, and the group concluded that achieving true machine
intelligence was a far more complex endeavor than initially projected.
Despite that overarching disappointment, the participants succeeded
in laying the essential groundwork for the development of AI that
followed.
In 1963, the Defense Advanced Research Projects Agency
(DARPA) launched AI research at MIT. The initiative marked a signiﬁ-
cant step toward government involvement in AI research that helped
accelerate advances in the ﬁeld. In the early 1970s, MIT Professor
Marvin Minsky ignited a surge of excitement and anticipation within
the scientiﬁc community (and beyond) when he predicted that AI -
equivalent to the cognitive abilities of an average human - could be
achieved within three to eight years - a somewhat optimistic prediction.
The ﬁrst manifestation of a technology matching Minsky's prediction
came in the early 1980s with the creation of "expert systems" by
Stanford Professor Edward Feigenbaum. Those systems proliferated
rapidly and deeply inﬂuenced the AI landscape for the remainder of
the decade.
Can Neural Networks Learn Everything?
In parallel with the pioneering work of Turing, Minsky,
Feigenbaum, and others, machine learning (ML) researchers were explor-
ing the potential of artiﬁcial neural networks (NNs), which also played a
key role in the development of AI. That work originated with Canadian
psychologist Donald O. Hebb's formulation of the neuronal model in his
1949 book The Organization of Behavior. Hebb's model proposed that
collections of interconnected neurons could give rise to complex behav-
ior. The concept of interconnected neural units laid the groundwork for
understanding how learning and memory might be represented in the
human brain. It also set the stage for brain-inspired computational
models that can be trained to execute a wide range of tasks.
The Perceptron, introduced by psychologist Frank Rosenblatt
in 1957, marked a turning point in the development of NNs. Building
23
/
Can Neural Networks Learn Everything?

on Hebb's 1949 neuronal model, the Perceptron used a simple math-
ematical model with an associated algorithm to learn a simple binary
classiﬁer. A full layer of Perceptrons then incorporated an algorithm for
learning a system's parameters and demonstrated the potential pattern
recognition and decision-making capabilities of machines. Further
research in this vein produced more efﬁcient and powerful multilayer
networks, which, in turn, led to the rise of DNNs. The "deep" in DNN
refers to the presence of multiple layers hidden between input and
output layers. Those deeper architectures facilitated the representation
of complex and hierarchical relationships in data and enabled DNNs to
excel in various tasks such as image and speech recognition, natural
language processing, and more.
In 1950, IBM computer scientist Arthur L. Samuel introduced a
computer program capable of playing checkers. Prior to his break-
through, the vast number of possible states in a game of checkers
couldn't be navigated exhaustively by machine. To surmount that difﬁ-
culty, Samuel employed a scoring mechanism that estimated the prob-
ability of winning at each board conﬁguration and guided the program
to make strategic moves. He updated these probabilities by having the
program play itself. The approach was an early manifestation of
reinforcement learning (RL), and it marked a seminal step in the use
of algorithms and heuristics to tackle complex decision-making tasks
that mimic human decision-making - the genesis of contemporary AI.
We note, however, that Samuel did not use NNs for his work.
Progress in ML using NNs and DNNs remained ﬁrmly
grounded in the realm of statistical learning theory. Considerations
related to predictive accuracy, data resilience, and the time complexity
associated with the quantity of data necessary for achieving precision
were closely intertwined with the capacity to ﬁne-tune multiple param-
eters of NNs. The domains of high-dimensional statistics and optimiza-
tion were pivotal in shaping the core of ML progression.
The parallel development of predicate logic, ﬁrst pursued by
Turing and other researchers, as well as the exploration of NNs for ML
(by Rosenblatt and others), showcases the diverse avenues of inquiry
during the early days of AI. These distinct paths represented contrasting
approaches to understanding and building AI. And yet, the progress of
ML research was signiﬁcantly hindered by the physical limitations of
computational power and storage capacity at that time. For a decade or
more, researchers grappled with the formidable challenges posed by the
24
/
A Conﬂuence of Fields: Some Historical Perspective

sheer
computational
complexity
of
training
and
running
ML
algorithms. How could those dual tracks - rooted in different theoret-
ical approaches - converge in a multidisciplinary ﬁeld that combined
logic, computation, and neuroscience to create the modern landscape of
AI and ML? The ﬁelds were crying out for a computational hardware
and software revolution.
Personal Computers to the Rescue: Second Revolution
I still recall getting my ﬁrst Apple II PC in 1981. Setting it up
was an ordeal worth enduring. Suddenly, I had all the computational
power I needed at my disposal. On the PC, I was able to perform all the
scientiﬁc programming needed for my courses and research, as well as
write and edit my undergraduate thesis. Although PCs democratized
computation, it was still largely limited to people interested in
scientiﬁc computing.
Less than a decade later, however, our PCs were connected to
the internet. With the development of Mosaic, the World Wide Web,
and subsequent web browsers, our personal workstations became tools
for both computation and communication. This was the tipping point
for the universal adoption of PCs, and we began applying computing
capabilities to every aspect of our lives. The advent of networked
computers incentivized a new era of distributed computing that has
enabled transformative developments in those capabilities.
An "Aloha!" Moment for Computing
I ﬁnd it ﬁtting that the ﬁrst great breakthrough in network
computing - courtesy of the University of Hawaii in 1971 - bore the
name "ALOHAnet." The word "aloha" is commonly understood by
non-islanders to mean both "hello" and "goodbye." The metaphor is
poignant because one of the most signiﬁcant technical achievements of
ALOHAnet was a method for gatekeeping the communications between
satellite-campus
"client"
machines
and
the
main-campus
"hub"
machine. In essence, the hub computer at Oahu controlled and veriﬁed
the beginning and end of transmissions between itself and each of the
node computers on the other islands - a sort of hello, goodbye, and
hello again mechanism. That system was key to enabling the hub and all
client nodes to interact on the same frequency, a key starting point for
25
/
An "Aloha!" Moment for Computing

what we know today as distributed computing, whereby multiple
machines at different nodes share resources and computational power
to execute one computational task.
You can take the metaphor just a bit further, I think, when you
consider what the ALOHAnet breakthrough meant to the evolution of
computing. Just as native Hawaiians hold the deeper meaning of aloha
to include mutual respect and understanding, so does distributed
computing offer the possibility of exchanges of information, ideas,
collaborations, and solutions that promote mutual respect and shared
progress. Although the second decade of the twenty-ﬁrst century has
revealed some of the dark side of digital information exchange, I would
argue that the promise of its demonstrated and potential beneﬁts can
outweigh its perils - provided we commit our minds and resources to
its upsides.
Why did distributed systems and computing become all the
rage? Expense, for one thing. A cluster of networked, low-end com-
puters can often complete high-performance computing tasks less
expensively than a single high-end computer. Such systems also tend
to be easier to manage and expand, and they offer greater reliability
because they don't have a single point of failure. Most important,
arguably, was the capability of distributed systems to transmit data
collected or produced at one location for processing or analysis at
another location using existing and new communications networks - a
core characteristic of twenty-ﬁrst-century computing.
Smart Phone and Cell Tower Coevolution: Third Revolution
If you didn't live through the early days of mobile computing in
the early 1980s, you may ﬁnd it nearly impossible to fathom that
display screens on the ﬁrst laptops (e.g., Osborne 1, Epson HX-20,
and Kyocera Kyotronic) were smaller than the average smartphone of
2020 - and only 10-20% of the surface area of those early machines'
faceplates. Such devices were mobile in the sense that a person of
average strength could carry one from location A to location B, power
up, and run a handful of basic software applications (spreadsheets,
word processors, calculators, and mail mergers) without being plugged
into a power source for some period of time. In most other respects, they
bore almost no resemblance to the supercharged, handheld wonders
of today.
26
/
A Conﬂuence of Fields: Some Historical Perspective

As cumbersome as those machines would appear to us today,
they unquestionably sparked the imagination of the public at large -
and ultimately launched a worldwide passion for computing on the go.
By the late 1990s, mobile computing was synonymous with portability,
interactivity, user speciﬁcity, and most signiﬁcant to the evolution of
computing
in
the
twenty-ﬁrst
century,
connectivity.
Hardware
advances - such as metal-oxide-silicon transistors, mobile transceivers,
base stations, routers, telecommunications circuits, and radio trans-
ceivers - provided the physical infrastructure for 2G, 3G, and 4G
wireless networks.
Wireless communication facilitated a third revolution in com-
puting, elevating it to an entirely new level in our lives. Early cell phones
were used primarily for making phone calls and possibly sending short
text messages. Within a few years, however, new communication proto-
cols enabled us to send and receive images and videos along with voice
and texts from our handheld devices. By 2010, the incredible computa-
tion and communication capabilities of mobile technologies allowed us
to take such actions as exchanging money on Venmo and buying and
selling stocks in real time. As of 2022, nearly 84% of the world's
population used smartphones, mostly for activities other than voice
conversations.
ML Emerges from a Slump
ML, which slumped for decades, bounced back in a big way in
the 1990s, thanks to the availability of vastly more data and increased
computational power. That marriage of big data with advanced com-
putation set the stage for a series of ML breakthroughs. Many research
communities began conceiving ways to incorporate autonomous learn-
ing into their modeling approaches. Control researchers, for instance,
led a large initiative in "robust learning" for use in the design of safety-
critical systems. The optimization community shifted its attention to
creating new techniques for non-convex, very high-dimensional opti-
mization problems that emerged from training DNNs. The statistical
community continued its efforts in assessing and calibrating the models
that were emerging from such optimization techniques.
In the realm of complex decision-making, NNs re-emerged in
the form of neuro-dynamic programming. The 1996 book Neuro-
Dynamic Programming by fellow MIT professors and colleagues
27
/
ML Emerges from a Slump

Dimitri Bertsekas and John Tsitsiklis played a pivotal role in the ﬁeld's
development, as did Reinforcement Learning: An Introduction by
Richard S. Sutton and Andrew G. Barto. Research in this area focuses
on deriving optimal decision strategies through an iterative process of
enhancing existing decision strategies, generally referred to as RL. The
work builds on the well-developed theory of optimal control and
dynamic programming that traces its roots back to the 1950s and
1960s and constitutes a merger between optimal control and NNs.
Progress
in
decision
theory
made
world
headlines
in
1997 when IBM's Deep Blue defeated world chess champion Gary
Kasparov in a six-game match. It was the ﬁrst victory by a computer
over a reigning champ under tournament conditions, and it fulﬁlled
the long-standing prediction that computers could outperform human
experts in complex decision-making tasks. Deep Blue's key innov-
ation was to develop a score for each conﬁguration of the board that
measured the likelihood of winning the game. Using that score, Deep
Blue would calculate six to eight moves deep for both players before
picking its next move. To increase the speed of Deep Blue's decision-
making process, its developers trained Deep Blue on thousands of
historical matches - an algorithmic technique that led to further
breakthroughs in RL methods as well as twenty-ﬁrst-century develop-
ments in decision-making.
By the third decade of the new millennium, developers have
seamlessly integrated ML into algorithmic intelligence and solidiﬁed its
position as a subset of the broader ﬁeld of AI - vaguely deﬁned as any
system that may mimic or imitate human intelligence. Although one
could make a philosophical argument that the boundary between ML
and AI is blurred, or even irrelevant, many still distinguish between the
traditional approach of predicate logic (i.e., expert systems) and the
modern data-driven decision-making methods employed in ML.
My view is that AI has evolved far beyond the bounds of those
approaches and now encompasses a wide array of algorithmic
decision-making capabilities present in a multitude of technologies
and applications that may or may not imitate human intelligence (e.g.,
an AI system that responds to voice commands in an elevator is hardly
an approximation of human intelligence).
AI's expansive capabilities now encompass a broad spectrum of
applications that include natural language processing, computer vision,
robotics, as well as creative tasks such as generating visual art and
28
/
A Conﬂuence of Fields: Some Historical Perspective

musical compositions. Those advances, coupled with the availability of
almost incomprehensible amounts of data, have paved the way for AI
systems to make informed and increasingly complex choices without
explicit human intervention. AI's ever-growing ability to analyze vast
data sets, identify patterns, and initiate actions is leading to a future in
which automated decision-making is an integral part of our social,
economic, and technological landscapes. Formally, AI systems are
deﬁned as systems that mimic human intelligence both in learning about
their environment and in their capabilities to make decisions. In general
conversation, AI systems broadly refer to all algorithmic methods for
learning and decisions. I will contend that many systems we refer to as
AI systems today are not intelligent at all.
The Rise of Embedded Systems: Fourth Revolution
As someone working to understand how we can best navigate
these turbulent waters, I ﬁnd societies' comfort with and dependence on
distributed and mobile systems simultaneously thrilling and terrifying.
To appreciate how we've arrived at this uncertain frontier, it helps to
look brieﬂy at the conﬂuence of three additional phenomena that pro-
liferated during the ﬁrst two decades of the new millennium - embedded
(mobile) devices, faster computers, and the capacity to implement deci-
sion systems in distributed environments.
Embedded systems comprise elements of computational hard-
ware (processors and memory), one or more peripheral input/output
components (joystick, camera, LED display, touch screen, etc.), and
specialized software programs all housed in larger devices that may
include additional electronic, electrical, and mechanical parts. The list
of device types that contain embedded systems is practically inﬁnite,
ranging from digital watches, dishwashers, and MP3 players to hybrid
cars, medical imaging equipment, and ﬂight control systems exemplify-
ing the convergence of sensing, communication, and control. A typical
Tesla electric vehicle (EV), for example, rolls off the assembly line with
numerous embedded systems (spatial sensors, autopilot controls, multi-
media display, etc.) as well as a nested hierarchy of embedded systems
(e.g., sensors embedded in the autopilot, autopilot embedded in
the vehicle).
It may be only a slight exaggeration to say that you would ﬁnd
it harder to avoid interacting with embedded systems in your daily
29
/
The Rise of Embedded Systems: Fourth Revolution

routine than it would be to catch a ride into Earth's orbit on a private
spacecraft. The triumph of embedded systems is also of great economic
consequence. In 2009, software engineer and embedded system expert
Michael Barr estimated that 98% of all new CPUs produced were being
embedded. Writing on statista.com in February 2022, electrical and
computer engineer Thomas Alsop reported the value of the global
embedded market to be approximately $34.63 billion, an amount fore-
cast to nearly double by 2027.
So Much Data, So Many Decisions
In the English language, we call a group of hummingbirds a
charm. Eagles are a convocation. And geese in ﬂight are a skein
("gaggle" is reserved for those same geese when on the ground). But
what should we call a formation of unmanned aerial vehicles (UAVs)?
Given that the tech community has yet to brand the UAV grouping
phenomenon for marketing purposes, I'll stick to the boring descriptive
phrase computer scientists and control theorists commonly use - a
networked decision system.
Although such systems are the fruition of hundreds of advances
across many decades, they could not exist without the convergence of
sensing, communication, and decision systems made possible by faster
processors, smaller computers, and embedded device hardware and
software solutions. UAV formations are a great illustration of these
developments. As my coauthor Michael Rinehart and I explain in our
2011 book chapter "Networked Decisions Systems" from The Impact
of Control Technology:
Each UAV has a local controller to control its ﬂight, but it must
also follow commanded trajectories while avoiding collisions
and the like. This may require information from other nearby
UAVs, ground bases, or other information sources. In addition,
a leader UAV may need to provide trajectory or waypoint
commands to the formation. These decisions can be communi-
cated through the formation itself (as a multihop routing net-
work) or through other nodes. Other examples of networked
decision
systems
include
distributed
emergency
response
systems, interconnected transportation, energy systems, and
even social networks.
30
/
A Conﬂuence of Fields: Some Historical Perspective

We typically imagine UAVs in the order of 10s of vehicles. But the
autonomous car revolution will create dynamically changing networks
of hundreds of vehicles. More so, distributed and networked decision
systems
aren't
limited
solely
to
drones.
A
2019
report
for
HP®TechTakes by Tom Gerencer noted that our daily data ﬂow was
being fed that year by approximately 20 billion devices and more than
50 billion sensors, quantities that undoubtedly have grown since the
publication of this book. Just as surely, our ability to stay abreast and
make sense of all that data is in catchup mode. What that means for the
future of computing, and its impact on societies around the world,
brings us to one of the key themes of this book - how the convergence
of computation, communication, and decision systems provides the
foundation for what is often called, for lack of a better, widely adopted
label, AI.
Vignette
Is AI more surprising than useful? The three-game 2017 Go match pitting
reigning #1 human player Ke Jie against the computer program AlphaGo
Master resulted in a 3-0 victory for AlphaGo - and sparked greater
worldwide fascination with AI. Developed by DeepMind, a Google sub-
sidiary, AlphaGo's competitive success in Go, chess, and other games was
the result of a combination of clever searches, a pre-learned neural network
for move outcomes based on board conﬁgurations, and a vast number of
self-playing trajectories. Those trajectories represented a computational
innovation that incorporated strategies not typically found in human
players' repertoire and gave the AI system a competitive advantage over
human champions.
The Rise of Generative AI
In 2022, ChatGPT emerged as the ﬁrst widely available large
language model (LLM) and launched a transformative wave across the
AI landscape. Armed with the ability to engage in human-like conver-
sations and extensively trained on vast data sets, ChatGPT's debut
ushered in a plethora of potential AI applications. In addition to
sparking pioneering research and entrepreneurial ventures, ChatGPT's
proliferation has raised concerns about the existential risks posed by AI.
31
/
The Rise of Generative AI

Historian Yuval Harari and numerous others have joined hundreds of
top AI academicians and researchers in identifying ChatGPT as a sub-
stantial existential threat to humanity. Human society, the argument
goes, fundamentally relies on language-based communication. The cap-
acity of ChatGPT and similar platforms to craft coherent and relevant
narratives that inﬂuence the historical context and reshape our compre-
hension of human existence is highly disruptive and potentially destruc-
tive to productive interactions.
One particularly problematic LLM characteristic is hallucination -
a phenomenon in which an LLM fabricates an entire narrative.
Instances of hallucination arise because LLMs lack an intrinsic under-
standing of conﬁdence levels and margins of error. And although LLMs
can declare instances when they are uncertain, they struggle to genu-
inely abstain from generating content beyond their capabilities.
LLMs are just one manifestation of generative AI - a specialized
ﬁeld that focuses on producing output that resembles real content (be it
human-created or machine-generated). Another is diffusion modeling,
which uses a single image such as a dog to generate numerous add-
itional images of dogs. Diffusion models - which use system theory and
dynamics to control the stability of diffusion - offer a potent means of
generating substantial volumes of labeled data from relatively small
data sets. Related techniques that employ low-dimensional structures
are also capable of generating much data from a relatively sparse
starting point. Inaccuracy problems notwithstanding, one of the sub-
stantial beneﬁts emerging from these technologies is the transfer learn-
ing that occurs when generative AI borrows knowledge from one ﬁeld
and applies it to another.
What Real Problems Are We Solving?
Given the widespread recreational applications of AI-powered
LLMs, it's reasonable to ask whether AI has genuinely resolved any
tangible real-world challenges. One meaningful use case can be found in
autonomous control systems such as airplane autopilots, which consist-
ently manage successful landings and enhance commercial airline safety
and dependability. Although such systems haven't been labeled as AI
historically, they certainly ﬁt within our current understanding of the
category. That decades-long track record of success notwithstanding,
the application of the technology remains relatively circumscribed.
32
/
A Conﬂuence of Fields: Some Historical Perspective

We have not reached the point where we trust AI to totally control the
aircraft autonomously.
Applying the commercial airline scenario to a broader context
exempliﬁes embedded systems that enable shared decision-making
between humans and machines. The intricate interplay of the two
entities is relevant to numerous emerging use cases. The future of
passenger road vehicles, for example, is likely to include a transition
from total human control to mixed autonomy (shared human-AI con-
trol) to fully autonomous AI control. When - or if - we completely cede
control to machines, how do we ensure that those machines adhere to
predetermined moral and ethical boundaries? We also must reiterate
our initial question: what real problems does this solve, and are the risks
worth the gains?
The Crucial Challenge of Protein Folding
Each cell in a human body contains between 20,000 and
100,000 distinct proteins, and each protein has its particular function
within a given cell. Every protein sequence exhibits a folding behavior
that enables it to perform its speciﬁc function. When that folding goes
awry, biological processes essential to human health can break down,
which is why researchers have been working for decades to build a
better understanding of the way in which proteins fold - or fail to
fold - properly.
Despite the challenge of observing that molecular process
without disrupting it, scientists have been able to describe approxi-
mately 150,000 distinct protein folding patterns since the 1990s.
Unfortunately, millions more such patterns remain undiscovered, poten-
tially hampering the development of more effective therapies for devas-
tating conditions such as cancer, cystic ﬁbrosis, and Alzheimer's disease.
Enter AlphaFold - a groundbreaking AI system developed to predict the
folded 3D structure of proteins based on amino acid sequences. The
underlying theory for this approach was pioneered by the 1972 recipi-
ents of the Nobel Prize in Chemistry, Christian Anﬁnsen, Stanford
Moore, and William Howard Stein, who postulated that a protein's
amino acid sequence should fully determine its structure and conse-
quently affect its functionality.
Between 2021 and 2023, AlphaFold's owner, DeepMind,
partnered with the European Bioinformatics Institute to populate a
33
/
The Crucial Challenge of Protein Folding

public database (the AlphaFold Protein Structure Database) with pre-
dictions for more than 200 million protein structures. The accuracy
rates of those predictions - which may help to unlock revolutionary
treatment options for complex syndromes and new vaccines - often
match those of experimentally derived measurements. A great strength
of systems such as AlphaFold is the AI-ML self-training system that
uses an iterative loop to create an initial model, predict new folds,
evaluate them against hidden experimental folds, and then integrate
the best outcomes back into the data pool. As the reliability of various
predictions is determined, conﬁdence levels are recorded and used to
enrich the existing database. Logic-based systems have been used to
model the interactions between protein structures, but those tend to be
approximate and static. AI-ML systems, in contrast, learn from suc-
cesses and failures and improve themselves over time.
The Quest for Speed
Computer scientists, hardware and software engineers, and tech
entrepreneurs have always had their feet on the accelerator - the faster
we can compute, the better. And although our fantasies about what we
might do with more powerful machines mostly outpaced our technical
advances, the industry achieved phenomenal leaps forward in the
1990s. We witnessed a greater than tenfold increase in processor clock
speeds during that decade, and feverous competition among chip
makers such as AMD and Intel resulted in new microprocessor tech-
nologies that were smaller, cheaper, faster, and more energy efﬁcient.
In the early 2000s, when the processing speed revolution hit a
wall related to the practical limitations of heat dissipation, the demand
for fast computing roared ahead unabated. Billions of users worldwide
became accustomed to multitasking on numerous devices and platforms
via high-speed internet connections. To satisfy this demand, engineers
devised a path around the heat dissipation barrier by creating power-
efﬁcient processors with multiple cores, each able to solve some portion
of a larger problem simultaneously rather than sequentially.
That approach, referred to as parallel computing, historically
had been limited to activities such as scientiﬁc computing and simula-
tions of natural and engineered systems. Researchers and industry
experts have been using parallel computing to analyze data from com-
plex systems such as weather, navigation, trafﬁc, ﬁnance, industrial and
34
/
A Conﬂuence of Fields: Some Historical Perspective

agricultural production, climate, and healthcare for decades. With
advances in manufacturing and programming, however, parallel com-
puting became as much the norm for the average desktop, laptop, and
smartphone as for NASA's space shuttle computer system. Today's
graphics processing units (GPUs) - the next generation of processing
units after CPUs - accelerate complex algorithms and AI solutions by
efﬁciently performing parallel computations.
Omnipresent and Omnivorous Computing
At the turn of the new millennium, few of us anticipated how
the platforms that would take the world by storm - Google (1996),
Facebook (2004), YouTube (2005), and Twitter (2006) - would
unleash a tidal wave of data and personal interactions in which we
have the potential to rise or drown. Video conferencing, online multi-
player games, social media platforms, WiFi networks, the Internet, and
just about everything else people around the world commonly associate
with computing in 2023 are part of a distributed system. Less under-
stood as distributed computing by the general public, but equally vital
to contemporary life, are cellular networks, aircraft controls, banking
systems, weather-forecasting models, windmills, and travel reservation
platforms.
These examples just scratch the surface of our networked
planet, such that it is difﬁcult to imagine how contemporary global
society could function without this level of interconnectivity and inter-
change. AI will enter each and every one of these disciplines and
potentially transform them. The tools and methodologies affecting
those transformations most likely will be owned and developed by
domain experts - not data or computer scientists. The development of
domain experts who possess the necessary data-science capabilities was
the prime motivation for the creation of IDSS.
"Prediction Is Difﬁcult, Especially If It Is about the
Future." (Niels Bohr)
In the current era of data science and AI research and develop-
ment, I'm mindful of the witty warning from Niels Bohr, Nobel laureate
in physics, about the difﬁculty of making predictions based on past data
35
/
"Prediction Is Difﬁcult, Especially If It Is about the Future." (Niels Bohr)

and experience. On our current trajectory, however, I feel safe in
projecting that digital platforms such as Facebook and Netﬂix will
continue
to
gather
ever
greater
quantities
of
personal
data.
Increasingly powerful algorithmic systems will leverage that data to
provide personalized experiences and manage the behaviors of such
platforms. AI systems will make more and more critical decisions
related to data sharing and personalized recommendations. Even in
the realm of ﬁnance, AI-powered systems will increasingly inﬂuence
major investment decisions.
One possible approach to that very probable future involves
pushing the capabilities of AI to the limits of our current computational
power as we develop ever more sophisticated algorithms. This scenario
assumes that Moore's Law - implying an exponential growth in com-
puting resources - also will kick in and provide us with more powerful
computing resources. A fundamental ﬂaw in that approach is that it
assumes that advancing hardware and software technologies guarantees
progress on a multitude of challenges facing humanity.
Complications
with
that
approach,
however,
are
already
emerging. The global climate crisis, for example, demands that we ques-
tion the massive energy consumption associated with projected algorith-
mic and hardware advances. Speed and energy limitations may temper
the adverse effects of the generative AI revolution, but we also must
vigorously evaluate the knowledge being assimilated by our machines
and examine the reliability of the decisions they make. We need a multi-
faceted, transdisciplinary approach that continually interrogates the
relentless growth of AI and its potential ramiﬁcations for society.
The Arab Spring: Case in Point
Arguably, the most important societal element dictatorships con-
trol is information. Authoritarian leaders broadcast ofﬁcial positions on
state-controlled media outlets and inﬁltrate the masses with spies to dis-
courage the free expression of dissent. Information and communication
technologies (ICT), particularly encrypted ones, can be used to challenge
that suppression and provide a means for people to share stories, coordin-
ate activities, and elicit help from external players. Dictatorial regimes
respond by blocking such communications or smearing them with disin-
formation to such an extent that people lose conﬁdence in the sources.
Those methods are effective - but not foolproof.
36
/
A Conﬂuence of Fields: Some Historical Perspective

In 2010, a YouTube video of street vendor Mohamed Bouazizi
self-immolating in the city trafﬁc of Sidi Bouzid, Tunisia, focused the
world's attention on the state of poor people in that country.
Information related to that event spread like wildﬁre throughout
Tunisia and then to neighboring countries - Libya, Egypt, Syria,
Yemen, and Bahrain - and set in motion the wave of uprisings known
collectively as the Arab Spring. While it is unclear whether people's lives
improved in any of these countries as a result of the protest, analysts
and observers generally agree that ICT contributed to the social conta-
gion that spread the unrest.
Certain and Uncertain Effects of Social Media
Many revolutions have occurred without the help of ICT, and
the causal effect of ICT on various manifestations of the Arab Spring
must be rigorously examined. The 2011 revolution in Egypt, however,
was unquestionably the result of efforts coordinated on Facebook.
A page established by Egyptian-born computer science student Wael
Ghonim is credited with drawing hundreds of thousands of protesters to
Cairo's Tahrir Square and launching the popular movement that
brought down the country's government. Ghonim's impetus was the
killing of protester Khalid Said, about whom Ghonim wrote, "Today
they killed Khaled. If I don't act for his sake, tomorrow they will kill
me." Ghonim's Facebook page, "Kullena Khaled Said" (We Are All
Khaled Said), attracted more than a quarter of a million members within
a few months.
One fascinating aspect of the Egyptian story is that Ghonim,
with no formal training in activism, managed to create a highly effective
forum for democratic discourse on Facebook. Rather than using the
platform to propagate his own opinions, he created a forum where
people could exchange ideas, consult with one another on how to
advertise the cause, and ultimately coordinate actions that would dis-
rupt the existing regime - including the physical rallies in Tahrir Square
that forced the resignation of President Hosni Mubarak.
A Data-and-Society Reckoning
I must stress that while Facebook in Egypt had a causal effect on
the uprising in Tahrir Square, we cannot assert the counterfactual
37
/
A Data-and-Society Reckoning

causality that if Facebook did not exist, the uprising would not have
happened. We need more research into other causal forces to make such
an assertion. As for the broader connection between social media usage
and democratic activism, Non-resident senior fellow Sahar Khamis
provided an interesting analysis for Arab Center Washington DC.
In her 2020 article "Media Use and Its Anomalies a Decade after the
Arab Spring," Khamis observed that increases in social media use did
not result in increased activism in repressed countries. Rather, it appears
to have shifted such activism to the diaspora of individuals who ﬂed to
countries where they could speak freely. Khamis's observations indicate
that many years after the revolution, oppressive regimes may have
found other ways of exerting control over their citizens.
From the cascade of events associated with the Arab Spring to
election interference in Ukraine in 2014 and the US in 2016, we now
know that massive amounts of data and social networking may be
weaponized to foment misunderstanding and chaos as readily as they
can be directed toward social and technical progress. Distributed
computing architecture, combined with the proliferation of wireless
and mobile network solutions in the 1990s, paved the way for a data
deluge and social media revolution that rocked our world in the two
decades that followed - and triggered a digital reckoning that only just
came into focus in the 2020s.
When AI Undercuts Democratic Principles and Practices
A prime example of the looming digital reckoning can be found
in the Cambridge Analytica-Facebook scandal, a complex situation
associated with the 2016 US presidential election in which Facebook
was implicated in the sale of millions of data sets belonging to its
individual subscribers. The incident highlighted ethical concerns sur-
rounding the misuse of user data as well as the broader implications for
democracy and the need to regulate the use of such data in political
campaigns. It also illustrated the delicate balance between the role social
media platforms play in facilitating communication and the responsi-
bility those platforms must bear to protect the privacy and integrity of
individuals and elections.
In general terms, the scandal involved the extraction of intricate
psychological proﬁles by Cambridge Analytica from Facebook user
data. That information was used to gain insights into the personalities
38
/
A Conﬂuence of Fields: Some Historical Perspective

and preferences of individual Facebook users. Those psychological
proﬁles were subsequently leveraged to target individuals with pinpoint
political advertising. If a person was judged to be anxious, for example,
that person might be shown advertising on their Facebook page related
to a terrorist act, with the goal being to sway that person's political
views or biases toward a speciﬁc candidate or policy.
More Questions than Answers
The Cambridge Analytica-Facebook scandal raised numerous
urgent ethical questions about ML and AI that we're only just beginning
to grapple with. Can those technologies, for instance, genuinely predict
the emotional responses of individuals to particular information?
Although it is correct to say that the interdisciplinary ﬁeld of AI com-
bines machines, behavior analysis, neuroscience, and psychology, many
researchers dispute whether we have reached the point where such
predictions are entirely accurate. Intertwined with that technical ques-
tion is the critical ethical concern that Facebook violated the privacy of
users when it communicated and shared sensitive personal data without
obtaining explicit consent.
Assessing the causal inﬂuence and culpability of Cambridge
Analytica's tactics related to the outcome of the election remains
extremely challenging. Voting behavior often is private and difﬁcult to
monitor, and some experts contend that people typically stick to their
political convictions regardless of being psychologically targeted by
advertising. For its part, Cambridge Analytica asserted that they did
not run such ads themselves but simply provided the data to the Trump
campaign. That line of argument raises the specter of voter data
manipulation by campaign operatives. Should we, as a democratic
society, put regulations in place that restrict the extent to which such
data can be utilized to inﬂuence voters? Building on questions of the
manipulation of individual voters, we must interrogate network effects
and consensus. Manipulating information and individuals within social
networks can lead to different outcomes that are difﬁcult to predict and
control. Should we allow AI tools to be used without regulation, given
the extent to which those tools can be abused to skew public under-
standing and consensus?
39
/
More Questions than Answers



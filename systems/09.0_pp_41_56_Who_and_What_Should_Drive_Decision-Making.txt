3
WHO - AND WHAT - SHOULD DRIVE
DECISION-MAKING?
Harnessing Data for the Good of Society
In
late
2023,
the
New
York
Times
ﬁled
a
copyright-
infringement lawsuit in US federal court alleging that ChatGPT creators
OpenAI and Microsoft used NYT's material without permission to train
its chatbots. Earlier that same year, the European Union enacted the
world's ﬁrst international regulatory framework for AI. And in 2018,
researchers at Harvard and MIT launched a collaboration with the
Privacy and Data Protection Directorate of the Canadian government
to examine the human-rights ramiﬁcations of AI.
The consequences of those and similar efforts to reign in mostly
unfettered use of data for proﬁt and power will likely take decades to
unfold. The deliberative nature of that process makes it all the more
important that as many private individuals, academicians, public ser-
vants, advocacy groups, and corporate decision-makers as possible join
the debate. In this chapter, we hope to help lay the foundation for some
of those conversations by introducing a few key topics and questions.
How do we strike the right balance between mathematical logic and
social justice? Can we program out the persistent biases that have
plagued civilization for millennia? How much control - and account-
ability - should humans retain over generative systems? Should we
apply the same set of ethics to machines as we currently do to humans?
Must we simply accept that money makes the world go 'round and
hope that the free market will deliver what's best for society in the
long run?

Early Examples of Promises and Pitfalls
AI-informed chemical experiments produce revolutionary new
materials over a period of months rather than years. ML algorithms
discover previously unknown craters on the surface of Mars. AI-guided
lab robots unlock promising clean-energy solutions 1,000 times faster
than
conventional
laboratory
approaches.
Continuously
learning
algorithms study pedestrian behaviors to make self-driving cars safer
at intersections.
The ﬂipside of such revolutionary advances is the dystopian
horrors. Chatbots spew racist rhetoric and generate inﬂammatory
emails. Cameras surveil Beijing pedestrians to spot "criminal" gaits.
Autonomous vehicles strike and kill pedestrians. Algorithms use proxies
to race to assess loan applications. These real-life nightmares loom large
in our collective imagination and inspire the general public's distrust of
the data-driven computation-based and computation-aided decision-
making commonly referred to as AI and ML.
Algorithmic Logic versus Justice
From groundbreaking drugs and healthcare strategies to clean
energy networks and sustainable urban development, the potential of AI
and ML to help us solve our most vexing and life-threatening problems
is irrefutable. The challenge lies in remaining clear-eyed and critical
about what our technological wonders do and do not do well. Just
because we can program such tools to be logically consistent does not
make them rational or socially beneﬁcial.
We also must question the limits of our reason in leading us to
fair and equitable decision systems. When our controlling biases -
whether conscious or unconscious - reinforce the status quo, existing
regimens of oppression and inequity will persist. Data are susceptible to
willful misuse and misinterpretation for narrow, anti-social objectives,
and individuals are all too easily inspired to engage in computationally
ampliﬁed behaviors that sow confusion, distrust, and division. Few of
us working in computation-related ﬁelds in the late twentieth and early
twenty-ﬁrst centuries anticipated how rapidly AI and ML technologies
could be weaponized and deployed for such purposes.
If recent human history has taught us anything, it's that we are
not predominantly rational beings. The irrational aspects of our natures
42
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

often generate compelling arguments for unjust and inhumane actions
and institutional practices. Whatever we create with AI, ML, and the
new technologies that follow in those wakes, we must never allow
ourselves to be transformed from thinkers and doers into followers
and facilitators.
Humans versus Distributed Computing: An AI Showdown?
Like it or not, decisions that directly inﬂuence individual
lives, shape societies, and affect the health of our planet will continue
to be guided by data that is collected, analyzed, reported, and, in
some cases, acted on by distributed agents (e.g., physical electronic
sensors, data sharing bots, problem-solving, and implementation
algorithms) with AI and ML capabilities. What we gain and what
we lose as a consequence will depend on the nature and depth of our
engagement with the distributed decision systems we've created. The
technologies of data gathering, computation, and data-driven deci-
sions are racing ahead, and we must accelerate our own mastery of
these new capabilities to reap the societal beneﬁts they promise.
Alarmists caution that we are about to, or already have, cross the
Rubicon into an us-versus-them relationship with the distributed
agents we've created by voluntarily surrendering too much power
to them. Much about the future of AI and ML is up for grabs, but
one thing is clear - machines certainly will be smarter than humans if
we persist in acting dumber than machines. As mathematician
Norbert Wiener expressed it, "The world of the future will be an
even more demanding struggle against the limitations of our intelli-
gence, not a comfortable hammock in which we can lie down to be
waited upon by our robot slaves."
A simple example of a distributed-agent system is local trafﬁc-
camera enforcement. Violating posted speed restrictions triggers a
photograph or two of your vehicle and license plate and a potential
monetary penalty. The majority of us accept our mistake, pay the ﬁne,
and think no more of it. In doing so, we fail to appreciate the full set of
automated capabilities that measure and record our behavior, analyze it
in relation to existing laws, and report it to other components within the
system - a system that has the authority to issue us a ticket and create a
permanent record of our moving violations without any direct
human intervention.
43
/
Humans versus Distributed Computing: An AI Showdown?

The Persistent Challenges of Bias
Trafﬁc-camera enforcement may seem relatively innocuous
until you apply the principles and components of such systems to more
complex law enforcement activities. We know that predictive policing
tools, for example, are imbued with racial biases when algorithms are
trained on pre-existing arrest data from police departments. For
decades, police have made more arrests in neighborhoods with higher
percentages of Black and Brown residents. The use of such data teaches
AI tools to steer more law enforcement to those areas, resulting in still
more arrests - thus perpetuating a vicious data-driven cycle of
racist policing.
Facial recognition tools have the alarming potential to perpetu-
ate their own biases. Professor Craig Watkins, an MLK fellow at MIT
and IDSS, recounts a distressing incident involving a young Black man
who, in the presence of his family and friends, was arrested under
suspicion of shoplifting expensive watches worth more than $4,000.
Subsequent thorough investigations revealed that the police had appre-
hended the wrong individual. In light of that and similar instances,
builders and users of these technologies must carefully examine what
led to such grave mistakes.
Numerous researchers have documented that facial recognition
algorithms exhibit signiﬁcant errors when applied to individuals with
darker skin tones. Their work demonstrates that the problems stem
entirely from the practice of training these algorithms on biased data
(predominantly White). In the speciﬁc case highlighted by Watkins, the
facial recognition algorithm mistakenly identiﬁed the wrong person as a
result of its ﬂawed understanding of diverse facial features and appear-
ances. The consequences of such errors can be severe, leading to inno-
cent individuals being wrongly accused and negatively impacted by
unintentionally biased technologies.
Alternative Training Presents Its Own Problems
Unfortunately, efforts to "train out" those biases have shown
that technical ﬁxes are easier to imagine than to implement. As reported
in MIT Technology Review in January 2021, Carnegie Mellon research-
ers Nil-Jana Akpinar and Alexandra Chouldechova demonstrated that
44
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

training predictive policing algorithms in ways purported to lessen bias
had very little actual beneﬁt.
When the team focused their "well-trained" tool on Bogotá,
Colombia, they discovered that the AI algorithm underreported actual
hotspots by roughly 80% in areas with few victim reports of crime and
overreported by 20% in areas with a larger number of reports. Despite
such fundamental ﬂaws in predictive-policing AI, private surveillance
companies are racing ahead with complex networks of drones, security
cameras, smartphone trackers, and license plate readers (i.e., distributed
agents) that produce compelling AI-driven data fusions that have law-
enforcement agencies chomping at the bit. But are we at risk of becom-
ing enamored of and dependent on these elaborate systems before we
know if they are just and equitable?
The Bogotá study shows how self-fulﬁlling phenomena - in this
case, more arrest data from a particular geography suggests increased
surveillance, which leads to more arrests - can unintentionally weapo-
nize data against the communities it could be helping. Negative bias also
can infect the use of data when human programmers lack the domain
expertise to account for all the appropriate independent variables within
a domain-speciﬁc decision-making system.
These overlooked variables, or confounding factors, result in
over-simpliﬁed models and inherently biased decisions. Preventable, but
sadly prevalent, examples of such bias occur in loan approval
algorithms. When such a system is supplied with data showing that
Black and Brown borrowers historically have higher rates of mortgage
defaults than the general population, individual customers from those
groups are more likely to be denied loans and refused opportunities
to appeal the determinations. Consequently, this becomes a barrier
to building wealth among such communities and reinforces a vicious
cycle.
Although those rejections may appear justiﬁed to a software
engineer focused on a single metric, economists and sociologists tell us
that historical inequities in incomes, job stability, career opportun-
ities, access to education, and family circumstances - that is, con-
founding factors - should be considered. In short, you must be more
than a skilled programmer or computer scientist to create AI and ML
tools that use data equitably and promote the greater good of society.
Understanding the importance of sampling bias, data imbalance,
45
/
Alternative Training Presents Its Own Problems

confounders, and causality in inference and learning is key, and I'll
talk a bit more about that in the next chapter.
Ethical Decision-Making
Ethics provide societies with systems of acceptable behavior,
but those guidelines are not immutable. A society may change what it
collectively considers acceptable through changes in law or the evolu-
tion of social norms. Humans have lived with that phenomenon for
generations, but researchers are only just beginning to grapple with the
implications of malleable ethics for AI technologies.
In the 2015 MIT Technology Review article "Why Self-Driving
Cars Must Be Programmed to Kill," cognitive psychologist Jean-
François Bonnefon discusses research he and colleagues conducted on
the ethical complexities surrounding autonomous vehicles. The work
examined a very probable scenario that poses a complex moral
dilemma - should a self-driving car prioritize the safety of its passengers
or the lives of pedestrians outside the vehicle?
The question is complicated by an intricate interplay of ethics
and economics, and the potential ramiﬁcations for consumers and
manufacturers are immense. One approach is to consistently prioritize
the lives of pedestrians, which presents a commercial downside of
potentially putting drivers (aka buyers) of autonomous vehicles at
greater risk. An alternative to that scenario is to equip cars with the
ability to assess casualty numbers and make decisions that minimize the
overall death toll per incident - a utilitarian model that seems distant
from our human decision-making processes.
The conundrum grows when you consider two contradictory
projections for the technology - while autonomous systems have the
potential to signiﬁcantly reduce the overall number of accidents, they
also may lead to a higher aggregate death toll for drivers or pedes-
trians. Who will decide where the correct balance lies between indi-
vidual safety and collective wellbeing? Bonnefon's research on this
question examined the role of public opinion in determining the right
course of action. He and his colleagues discovered an overall prefer-
ence for the utilitarian approach of minimizing casualties. They also
discovered a paradox. People say they prefer the concept of self-
driving cars that sacriﬁce the occupants to protect people outside
the vehicle - as long as they don't have to drive such a vehicle
46
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

themselves. That ﬁnding underscores the complexity of applying utili-
tarian principles to AI technologies.
One Standard for Machines, Another for Humans?
Curiously, we don't train human drivers to calculate potential
casualties when navigating complex situations. But why isn't casualty
assessment part of driver education? And what accounts for our greater
comfort with the imperfect outcomes of human decisions, even if those
decisions result in higher casualty rates? People generally seem to be
more willing to forgive human operators for car accidents than their
autonomous counterparts.
Our near-zero tolerance for errors by AI systems appears to
vary, however, depending on the application. While we expect autono-
mous vehicles to perform ﬂawlessly, we accept a wider range of errors in
decisions involving loan approvals or text generated by LLMs. The
ethical framework suggested by this dual bifurcated mindset warrants
examination. Does ethical education play a role in fostering responsible
behavior? Could it have deterred the creation and use of the atomic
bomb, a devastating weapon that claimed the lives of 150,000 people in
a single detonation? Are we on track to introduce similarly destructive
inventions into the world because we fail to interrogate the intricate
relationship among ethics, decision-making, and the technologies we
create and deploy.
Sadly, the pace of technological advancement is outpacing our
capacity to fully grasp the societal and humanistic consequences of
deployment. We must discipline ourselves to consistently consider the
social and ethical ramiﬁcations of potent technologies such as AI and
ML, and we must learn from our past. A prior generation argued fervently
that certain television content could be harmful to young children.
Although the outcomes were mixed, those debates underscored the sig-
niﬁcance of regulating televised content based on age appropriateness.
In our day, the analogous debate seems to have ﬁzzled. We've
largely surrendered to the widespread availability of the internet,
allowing very young children to access highly potent material they are
ill-equipped to comprehend. I contend, however, that we must not use
ease of access as an excuse to disregard the responsibilities we bear
toward society. Perhaps a robust debate could generate fundamental
guiding principles for the future of AI applications.
47
/
One Standard for Machines, Another for Humans?

The Challenge Is Larger for Large Language Models (LLMs)
The concept of robustness in ML highlights the surprising
vulnerability of these algorithms to the smallest intentional alterations.
The introduction of minor, purposeful changes to objects can yield
entirely new outcomes - a phenomenon covered nicely in the 2013 paper
"Intriguing properties of neural networks" by Christian Szegedy and
colleagues. A slight perturbation to a stop sign, for example, can cause a
computer vision algorithm to perceive it as a speed-limit sign. A minor
adjustment to speciﬁc pixels in an image can transform a panda into the
semblance of a gibbon, even when the adjustments are imperceptible to
the human eye. Can we understand the inner workings of our machines
well enough to track the details they identify and subsequently amplify?
That perturbation phenomenon extends beyond individual
occurrences and raises broad concerns. Unstructured ML systems are
susceptible to learning entirely incorrect associations that potentially
lead them to highlight strange and unrelated attributes within images.
The phenomenon raises crucial questions about how to ensure the
robustness,
learnability,
and
conﬁdence
of
these
algorithms.
To address the challenge, we must revisit the fundamental principles
of ML, use accurate and relevant data, select appropriate models,
employ suitable algorithms, and provide robust statistical guarantees.
Unfortunately, the lack of robustness in ML extends to LLMs. These
models lack a conscious understanding of when they provide coherent
information versus when they assemble ideas at random. As with other
computer vision algorithms, LLMs must harness the power of statistical
accuracy to ensure the reliability of the information they generate.
Back to Causality
Causality, in my opinion, remains the Holy Grail in the pursuit
of algorithmic fairness. We cannot conduct randomized trials of police
brutality, so we must join forces with sociologists and law-enforcement
professionals to precisely understand all contributing factors. In the case
of excessive force by a White ofﬁcer against a Black civilian, we must
account for individual prejudice, law-enforcement hiring and training
protocols, previous encounters with police, and high-crime-neighbor-
hood designations, among other factors. I am encouraged by the enthu-
siasm of many of my colleagues at IDSS for such lines of research.
48
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

To grasp the essence of causal analysis in connection with police
brutality, we can examine the controversial stop-and-frisk policy imple-
mented in New York City that was credited by some in law enforcement
with reducing crime rates. It has been widely argued, however, that the
policy led to discrimination - speciﬁcally, a disproportionate number of
stop-and-frisk incidents conducted on Black and Hispanic individuals.
To validate this claim, we need to develop a model to understand how
such stop-and-frisk events may arise. We can, for example, construct an
inﬂuence diagram in which a frisk is caused by race (an observable
element), by an ofﬁcer's suspicion that an individual is carrying a
weapon a (nonobservable element), or by some combination of the
two. The outcome of the suspicion is observed when a frisk is con-
ducted - revealing whether or not the individual is in possession of a
weapon. The challenge lies in the fact that a police ofﬁcer can only
suspect if someone might be carrying a weapon but cannot observe the
outcome until they perform the frisk. What the ofﬁcer can observe
before the frisk, of course, is the race of the individual being stopped.
When we gather data on stop-and-frisk incidents, we can con-
struct an inﬂuence diagram that estimates the effects of race and suspi-
cion on the likelihood of being frisked. That also allows us to explore
counterfactual questions such as whether an individual would have
been stopped if they were White. Researchers, including Matt Kusner
and his colleagues in their 2017 article "Counterfactual Fairness," have
presented compelling evidence of bias in stop-and-frisk arrests. They
suggest that race played a signiﬁcant role in the decision-making pro-
cess, leading to a disproportionate number of frisks on Black and
Hispanic individuals - even when suspicion of carrying a weapon was
controlled for. We must recognize, however, that causal analysis
requires deep expertise and great care. An inﬂuence diagram may be
inconclusive or incorrect if it omits one or more key variables (e.g.,
location) that may have inﬂuenced outcomes. This careful analysis was
exercised in the paper by Kusner referenced earlier.
As a standard of practice, researchers routinely examine a variety
of potential confounders to conclude causality. In "A large-scale analysis
of racial disparities in police stops across the United States," for example,
Emma Pierson and 10 colleagues conducted a comprehensive examin-
ation of racial disparities in police stops. By analyzing a data set compris-
ing nearly 100 million trafﬁc stops conducted nationwide, the team
revealed a consequential confounder - that Black drivers were less likely
49
/
Back to Causality

to be stopped after sunset. That observation is signiﬁcant because it
corresponds to the so-called veil of darkness (i.e., a time when a driver's
race is not easily identiﬁed), which indicates the potential inﬂuence of
racial bias in stop decisions. By analyzing search rates among stopped
drivers and the likelihood of discovering contraband, the study also
suggests that the threshold for searching Black and Hispanic drivers is
lower than that for White drivers.
Who Will Drive Decisions: The Legal Landscape
Companies, countries, states, municipalities, universities, and
other large entities increasingly rely on data-driven decision-making.
As they develop AI systems to achieve their objectives, regulating those
systems - and the use of external data feeding such systems - is an
immense challenge for policymakers. Given the pervasive placement of
sensors, can anyone realistically expect their personal data to be
immune from harvesting and manipulation?
The fundamental legal frameworks we construct must address
issues of data privacy and ownership. Those frameworks also must
resolve the inherent conﬂicts of interest among individuals and organ-
izations. When we use Google search, for example, Google contends
that it is offering us a free service. Is it truly free? And does the absence
of a monetary charge entitle Google to access and use our data? Google
and other commercial entities have asserted that you should pay a fee to
protect your privacy. The question is further complicated by the fact
that your privacy may also be invaded when the data of your family
members and friends are harvested. I'll have more to say about that later
in this chapter.
The European Union (EU) has been much more systematic in
creating and enforcing regulations on privacy and use of individuals'
data. The EU's General Data Privacy Regulations (GDPR), for example,
are designed to prevent harmful misuse of personal data by imposing
substantial penalties on entities that mishandle private individual infor-
mation. The GDPR undoubtedly plays a crucial role in safeguarding
privacy, but economists Roslyn Mae Layton and Silvia Elaluf-
Calderwood argue in their 2020 paper "A social economic analysis of
the impact of GDPR on security and privacy practices" that such
regulations pose challenges for small tech companies striving to develop
meaningful products.
50
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

Layton and Elaluf-Calderwood contend that smaller EU enter-
prises are hindered when competing with US-based global giants such as
Google, which face fewer restrictions than EU companies and can more
easily absorb the ﬁnancial impact of GDPR penalties within their sub-
stantial proﬁt margins. That potentially unintended consequence has
inspired some critics to call GDPR the "Google Data Privacy
Regulation." The critiques highlight a key difﬁculty with region-speciﬁc
regulations - they inadvertently stiﬂe innovation in more-heavily regu-
lated zones. Such is the intricate and interconnected nature of privacy
laws in the global marketplace.
As we transition toward more automated solutions and AI
becomes integral to decision-making processes, we must monitor for
potential biases in systems designed to process data and make decisions.
Ultimately, the responsibility to prevent bias should rest with the indi-
viduals who design these systems. Unfortunately, the discovery of sys-
temic biases often occurs after multiple algorithmic iterations, a
problem that becomes more complex when AI systems are the result
of federated learning. The very nature of these technologies poses
complicated legal challenges at the intersection of data use and AI
regulation - for developers and enforcement bodies alike.
Vignette
Consider a healthcare AI system that provides initial diagnoses to patients,
a tool that may depend upon multiple decentralized data sets and models
developed in a distributed manner. The question of how we assess the
compliance of such a system is critical - as is how we assign responsibility
for any bias we uncover in the system. The process of debiasing in data sets
and learning methods requires a thorough understanding of the system
architecture that identiﬁes the complete value chain. The lackluster
response to that challenge in healthcare systems thus far stands in contrast
to safety compliance in the aviation industry, where systems are built with
extensive domain knowledge, models, and experience.
Differential Privacy
Even when algorithms process individual data, the critical ques-
tion of whether the outcomes adequately protect data privacy persists.
51
/
Differential Privacy

In their inﬂuential 2006 paper "Calibrating Noise to Sensitivity in
Private Data Analysis," researchers Cynthia Dwork, Frank McSherry,
Kobbi Nissim, and Adam Smith formalized the concept of "differential
privacy" within algorithms. By carefully adding calibrated noise to data
so that it does not materially alter the outcome of the algorithmic
process, the algorithm remains insensitive to any single data point and
is deemed differentially private. This property ensures that the algo-
rithm cannot be reverse-engineered or used to infer private information
from its output.
Designing such algorithms comes with speciﬁc constraints, and
not
all
algorithms
can
feasibly
be
made
differentially
private.
Algorithms that calculate the median of a data set, for instance, are
highly sensitive to noise and require precise data. Algorithms that
compute averages, on the other hand, can be made differentially private
simply by introducing noise to the sum of the data points. As of 2024,
such differentially private algorithms are being considered as potential
alternatives to regulatory measures and have gained signiﬁcant adop-
tion rates among researchers and industry professionals. We still have
much to learn about the potential of this approach, and we must
consider it in the larger context of data set externalities and a nuanced
deﬁnition of one's data (as you will see later in this chapter).
Humans Must Remain Accountable
As we shift the focus of decision-making in many sectors of
society, it is imperative that we not cede decision-making entirely to
machines. For every application, we must assess how much authority
for ﬁnal decisions individuals should retain. This is especially true for
intricately interconnected systems in which humans may struggle to
comprehend the multifaceted consequences of their choices. Reﬂecting
on incidents related to the Chernobyl nuclear meltdown, for example,
engineers and policymakers agreed that human intervention potentially
contributed to this disaster (e.g., human operators prematurely
launched a catastrophic safety test that would have been correctly
delayed by an automatic control system). Granting that this mistake
may have been the last fatal error in a string of safety violations, we can
understand that the complexity of the systems at work could have
clouded human perception of the devastating consequences of a single
real-time action.
52
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

Our constructed systems should serve as aids to our tasks rather
than as surrogates for human decision-making. This is especially true
when the job involves more than routine decisions based on a ﬁxed set
of variables. ML methods are excelling in areas such as drug design, for
example, but we must maintain our understanding of the underlying
biology or chemistry to make the most of the tools - especially as our
understanding of these domains evolves over time. That approach will
expedite the process of discovery across various ﬁelds and foster greater
innovation in the application of data science and AI to complex
problems.
We must also bear in mind that any technology can be applied
in negative ways - ML and generative AI are no exceptions. What
makes these systems distinctly dangerous, however, is their ability to
invent artiﬁcial situations and contexts that seem real and that appear to
be the outcome of human decision-making. This may have a profound
impact on how devious such systems can be, thus adding to the chal-
lenge of monitoring and regulating such systems even as we continue to
develop and deploy them.
Market Forces and Data Monetization: Case in Point
Since the ﬁrst digital advertisements appeared in 1994, the
worldwide online ad market has grown to approximately $400 billion.
Platforms not only enable online social media and e-commerce, they
also produce loops of continuous feedback in which the information
gathered about users is reﬂected back to them in the form of product
recommendations and other marketing efforts. Such recommendations
directly inﬂuence people's behavior by nudging their decision-making
on just about everything - from what moisturizer to try or automobile
to buy to which candidates they should support in the next election.
On the simpler end of the nudging spectrum, a platform such as
Netﬂix focuses solely on its own products. It uses past data on what
users watch and how they rate movies and series to build predictive
models of people's interests. The company uses collaborative ﬁltering -
a predictive model that analyzes groups of customers based on derived
similarities - to translate shared behaviors into more accurate recom-
mendations. The downside of this approach is that it tends to produce
relatively
narrow
genre
recommendations
for
many
groupings.
To counter that problem, Netﬂix has adopted more sophisticated
53
/
Market Forces and Data Monetization: Case in Point

algorithms that use exploration/exploitation strategies to optimize the
outcomes of its recommendations. Such algorithms toggle between
making recommendations that align with the customer's historical
viewership proﬁle and suggesting new movies to explore the customer's
interests.
On the more complicated end of the spectrum, platforms such
as Amazon and Google sell advertising opportunities to a very large set
of companies that are eager to market a wide range of products. As a
consequence, when you sign in to Amazon, for example, a portion of
the page you see is populated with ads from companies intent on
targeting you. While each of those companies may have its own histor-
ical data about you, they also have access to a market in which they can
buy additional data about you and other individual users - a highly
inefﬁcient, though still lucrative, market model.
Using consumer's purchased data and existing predictive
models, each company placing ads on Amazon decides how much to
bid for a particular piece of screen space. The company also uses the
data to determine which product to market speciﬁcally to you. If you
click on the ad, the company's algorithm adds this new experimental
data point to its knowledge of your interests and buying habits. Your
individual click adds to the overall click rate, which is the most preva-
lent metric of success in digital advertising markets.
How Data Markets Can Amplify Biases
As is typical of markets, the big players in data markets have
outsized power. A large company that can bid higher on available ad
space holds a competitive advantage over smaller companies, which
often are priced out unless they have a niche product of keen interest
to a particular group of people. Such markets also can be skewed in how
they reinforce misperceptions and amplify biases.
In their research paper "Algorithmic Bias: A study of data-
based discrimination in the serving of ads in Social Media," Anja
Lambrecht and Catherine E. Tucker questioned why ads for STEM
programs tend to be more heavily targeted toward men than women.
The disparity is counterintuitive -women demonstrate strong interest in
STEM, they are at least as likely to click on such ads as men, and
colleges and universities are making concerted efforts to attract more
women to their STEM programs through marketing.
54
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

It turns out that this counterproductive outcome is driven by the
nature of the digital advertising marketplace. In general, retailers
targeting ads to women typically bid higher for the available ad space
on websites than the institutions and organizations seeking to market
STEM programs. Unable to win pricey ad space targeting female con-
sumers, STEM providers are reaching more men than women by default
rather than intent. This produces a double-negative effect of attracting
more men than women to STEM and creating new biased data that
appears to show less interest among women. The harms are both to
women and to the STEM education ecosystem.
What Are You Getting for Your Data?
The example of STEM advertising raises an interesting question
about the fuel on which the digital advertising market runs. The value of
the product advertisers pay for is not derived from the virtual real estate
on the platform (Google, Safari, Firefox, etc.) where the ad appears but
the data points advertisers collect about you when you click on their
digital ad - namely, that you are interested in their product. Essentially,
you are the product, although the platform is collecting all the revenue.
Should you be compensated for creating value for the platform? In what
sense are you the owner of this data about yourself?
Within the context of this data marketplace, you could attempt
the equivalent of a work stoppage. If you and other platform users
collectively refused to click on any ads, the value of the ad space would
evaporate and that digital market would disappear. Theoretically, the
stoppage could give you leverage to demand that the platform compen-
sate you for your clicks. The most obvious ﬂaw in this scenario is the
highly unlikely and extremely difﬁcult global coordination it would
require among you and millions of other platform users.
Stoppage or not, platforms enable this exchange because they
provide a service (e-commerce for Amazon and the search for Google).
Such companies have argued that if users wanted ownership of their
data, then they should pay for the service (or equivalently pay to retain
their ownership). This value exchange is extremely challenging from
an economic perspective as the value of data is driven from the way it
is utilized - it does not have intrinsic value. Data ownership and
privacy are likely to remain a challenge within these platforms and
markets.
55
/
What Are You Getting for Your Data?

Externalities in Data Markets: What Deﬁnes Your Data
When information about individual users is being bought and
sold in the marketplace, as discussed above, we must examine the
question of externality -a side-effect or consequence in commercial
activity that affects third parties without being reﬂected in the cost of
the goods or services being sold. When, for example, data about you is
sold to multiple companies, the value of that data is reduced for each
company that purchases it. The mere fact that a competitor has the same
information about you causes negative utility for other companies
holding that data.
From the perspective of you as the data commodity, informa-
tion about you may be gathered as a side-effect of online activity
undertaken by some other user whose behavior is correlated to yours.
Web-based genealogical and genetics platforms such as 23andMe and
Ancestry expose potential downsides for individual users. These busi-
nesses are another form of data market in which you pay a fee to learn
about your lineage and ancestors and possibly learn about genetic
diseases in your family tree. The resulting database of genetic infor-
mation is extremely valuable and could be used against you. It doesn't
take a great imaginative leap to foresee that your genetic predisposition
for certain diseases could result in an outsized insurance premium
one day.
The loss of privacy inherent in such scenarios, while concerning,
has upsides that are welcomed by many individuals, organizations, and
government entities. The 2018 arrest of a suspect in the notorious
"Golden State Killer" case from the 1970s and 1980s owes much to
the open-source genealogy website GEDmatch. Investigators were able
to comb through numerous genetic proﬁles without a court order until
they zeroed in on the genetic proﬁle of someone who appeared to be
related to the killer. By surveilling their prime suspect, investigators
were able to collect a contemporary DNA sample that provided strong
evidence that the suspect was the killer. A loss for privacy advocates and
a win for law enforcement - and a reminder that notions of privacy
cannot be assessed in isolation from externalities.
56
/
Who - and What - Should Drive Decision-Making? Harnessing Data for the Good of Society

